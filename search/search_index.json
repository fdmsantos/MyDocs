{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation \u00b6 Topic Link Markdown https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Mkdocs https://www.mkdocs.org/ Material https://squidfunk.github.io/mkdocs-material/ Mermaid https://mermaidjs.github.io/ phpdoc http://phpdoc2cheatsheet.com/#mult_types Commands \u00b6 py -m mkdocs new [dir-name] - Create a new project. py -m mkdocs serve - Start the live-reloading docs server. py -m mkdocs build - Build the documentation site. py -m mkdocs help - Print this help message.","title":"Index"},{"location":"#documentation","text":"Topic Link Markdown https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Mkdocs https://www.mkdocs.org/ Material https://squidfunk.github.io/mkdocs-material/ Mermaid https://mermaidjs.github.io/ phpdoc http://phpdoc2cheatsheet.com/#mult_types","title":"Documentation"},{"location":"#commands","text":"py -m mkdocs new [dir-name] - Create a new project. py -m mkdocs serve - Start the live-reloading docs server. py -m mkdocs build - Build the documentation site. py -m mkdocs help - Print this help message.","title":"Commands"},{"location":"certifications/aws/security-specialty/","text":"AWS Certified Security Specialty \u00b6 Security 101 \u00b6 Security Basics - Models \u00b6 CIA Confidentiality - IAM, MFA Integrity - Certificate Manager, IAM, Bucket Policies Availability - Auto-Scaling, Multi-AZ AAA Authentication - IAM Authorization - Policies Accounting - Cloudtrail Non-repudiation ( Can't deny you did something) Shared Responsibility Model \u00b6 Infrastructure (EC2, EBS, VPC) Container (RDS, EMR, Elastic Beanstalk) Abstracted (S3, Glacier, DynamoDB, SQS, SES) Security IN AWS \u00b6 Visibility AWS Config Auditability AWS CloudTrail Controllability AWS KMS - Multi-Tenant AWS CloudHSM - Dedicated - FIPS 140-2 Compliance Agility AWS Cloudformation AWS Elastic Beanstalk Automation AWS OpsWorks AWS CodeDeploy IAM, S3 & Security Policies \u00b6 Resetting Root Users \u00b6 Create a new root user password and strong password policy Delete previous 2 factor authentication and re-create Check if the root user has an Access Key Id and Secret Access Key. If so delete these immediately Check other user accounts. Verify they are legitimate and if not, delete these IAM Policies \u00b6 IAM is global. Applies to all areas of AWS, not just S3 Three different types os IAM Policies AWS Managed Policies Customer Managed Policies Inline Policies S3 Bucket Policies \u00b6 S3 Bucket policies are attached only to S3 Buckets. S3 Bucket policies specify what actions are allowed or denied on the bucket. They can be broken down to a user level, So Alice can PUT but not DELETE and John can REAd but not PUT. Bucket level only, S3 only Explicit Deny always Overrides an Allow S3 ACL's \u00b6 Are Legacy access control. AWS recommend IAm Policies and S3 Bucket Policies Use them if you need apply policies on the objects themselves. Bucket policies can be applied at bucket level, and S3 ACLS can be applied to individual files (Objects) Policy Conflicts - Visual Diagram \u00b6 Forcing Encryption using S3 \u00b6 aws:SecureTransport # Deny This \"Condition\" : { \"Bool\" : { \"aws:SecureTransport\" : false } } Cross Region Replication \u00b6 Requirements Do not need to use a bucket policy with aws:SecureTransport to replicate objects using SSL. It is done by default Versioning must be enabled (In two buckets) It's possible to use CRR from one AWS account to another. The IAM role must have permissions ro replicate objects in the destination bucket. If the object owner is different than source bucket owner, the object owner must grant the bucket owner the READ and READ_ACP permissions via the object ACL In the replication configuration, can optionally direct Amazon S3 to change the ownership of object replica to the AWs account that owns the destination bucket What is Replicated Any objects created after add a replication configuration Object metadata , ACL Updates, Tags Replicates only objects in the source bucket for which the bucket owner has permissions to read objects and read access control lists (ACL) What is NOT Replicated Anything created before CRR is turned on Objects created with Server-side encryption using customer-provided (SSE-C) encryption keys Objects created with server-side encryption using AWS KMS - managed encryption (SSE-KMS) unless you explicitly enable this option Delete markers are replicated, deleted versions of files are NOT S3 with CloudFront \u00b6 Forcing S3 to Use cloudfront It's necessary check \"Restrict Bucket Access\" option It's necessary create a (or reuse) Origin Access Identity (something like a cloudfront user to access to bucket) It's necessary update Bucket Policy to give permissions to Orign Access identity access to Bucket CloudFront SSL Certificates If you are happy for users to access your content using *.cloudfront.net domain name, then select the default CloudFront Certificate If you want to use a domain name that you already own, you will need to use a custome SSL certificate Custom SSL certificates must be stored in either ACM in the us-east-1 (North Virgina) or you can also store them in IAM using the IAM Cli. S3 Pre-Signed URLs \u00b6 You can access objects using pre-signed URL's Typically these are done via the SDK but can also be done using the CLI They exist for a certain length of time in seconds. Default is 1 Hour. You can change this using \"--expires-in\" followed by the number of seconds Web Identity Federation \u00b6 Federation allows users to authenticate with a web identity provider (Google, Facebook, Amazon) The User authenticates first with the Web ID Provider and receives an authentication token, which is exchanged for temporary AWS credentials allowing them to assume an IAM role Cognito is an Identity Broker which handles interaction between your applications and the Web ID provider (You don't need to write you own code to do this) Provides sign-up, sign-in, and guest user access Syncs user data for a seamless experience across your devices Cognito is the AWS recommended approach for Web ID federation particularly for mobile apps Cognito User Pools \u00b6 Cognito uses User Pools to manage user sign-up and sign-in directly or via web Identity Providers Cognito acts as an Identity broker, handling all interaction with Web Identity Providers Glacier Vault \u00b6 Glacier is low-cost storage for archiving and long-term backup Files are stored as Archives, Archives are stored in Vaults A Vault Lock Policy allows you to configure and enforce compliance controls for Glacier Vaults Configure WORM (write once read many) archives Create data retentions policys, 5 year retention Enforce regulartory and compliance controls You have 24 hours to validate the lock policy, once validated, Lock policies are immutable AWS Organizations \u00b6 Allows you to organize your accounts into groups / OUs for access control and centralized billing Attach policy based controls - Service Control Policies Centrally manage permissions for OUs - groups of accounts or individual accounts SCPs allow you to create a Permissions Boundary and restrict the permissions of child accounts including of the root user SCPs allow can deny, not allow Logging And Monitoring \u00b6 Cloud Trail \u00b6 Validating CloudTrail Log File Integrity Was the log file modified, or deleted? CloudTail log file integrity validation: SHA-256 hashing SHA-256 with RSA for digital signing Log files are delivered with a 'digest' file Digest file can be used to validate the integrity of the log file What is Logged Metadata around API calls The identity of the API caller The time of the API call The source IP address of the API caller The request parameters The response elements returned by the service CloudTrail Event Logs Sent to an S3 bucket You manage the retention in S3 Delivered every 5 (active) minutes with up 15 minute delay Can be aggregated across regions Can be aggregated across accounts Protect you CloudTrail logs, they contain everything that you are doing in your AWS account and may contain PII Allow your security people admin access to Cloudtrail, auditors read only access to CloudTrail using IAM Restrict Access to S3 using bucket policies, and use MFA delete on your objects Use lifecycle rules to move data to Glacier or to delete it Check the interity of your log files using digest files Cloudwatch \u00b6 Key Components CloudWatch CloudWatch logs Cloudwatch Events Real Time Metrics Alarms Notifications Custom metrics CloudWatch Logs Pushed from AWS services (including CloudTrail) Pushed from your applications/systems Metrics from log entry matches Stored indefinitely (not user S3) Cloudwatch Events Near real-time stream of system events Events AWS Resources state change AWS CloudTrail (API Calls) Custom Events (code) Scheduled Rules - match incoming events and route them to one or more targets Targets - Lambda functions, SNS topics, SQS queues, Kinesis streams","title":"Security-Specialty"},{"location":"certifications/aws/security-specialty/#aws-certified-security-specialty","text":"","title":"AWS Certified Security Specialty"},{"location":"certifications/aws/security-specialty/#security-101","text":"","title":"Security 101"},{"location":"certifications/aws/security-specialty/#security-basics-models","text":"CIA Confidentiality - IAM, MFA Integrity - Certificate Manager, IAM, Bucket Policies Availability - Auto-Scaling, Multi-AZ AAA Authentication - IAM Authorization - Policies Accounting - Cloudtrail Non-repudiation ( Can't deny you did something)","title":"Security Basics - Models"},{"location":"certifications/aws/security-specialty/#shared-responsibility-model","text":"Infrastructure (EC2, EBS, VPC) Container (RDS, EMR, Elastic Beanstalk) Abstracted (S3, Glacier, DynamoDB, SQS, SES)","title":"Shared Responsibility Model"},{"location":"certifications/aws/security-specialty/#security-in-aws","text":"Visibility AWS Config Auditability AWS CloudTrail Controllability AWS KMS - Multi-Tenant AWS CloudHSM - Dedicated - FIPS 140-2 Compliance Agility AWS Cloudformation AWS Elastic Beanstalk Automation AWS OpsWorks AWS CodeDeploy","title":"Security IN AWS"},{"location":"certifications/aws/security-specialty/#iam-s3-security-policies","text":"","title":"IAM, S3 &amp; Security Policies"},{"location":"certifications/aws/security-specialty/#resetting-root-users","text":"Create a new root user password and strong password policy Delete previous 2 factor authentication and re-create Check if the root user has an Access Key Id and Secret Access Key. If so delete these immediately Check other user accounts. Verify they are legitimate and if not, delete these","title":"Resetting Root Users"},{"location":"certifications/aws/security-specialty/#iam-policies","text":"IAM is global. Applies to all areas of AWS, not just S3 Three different types os IAM Policies AWS Managed Policies Customer Managed Policies Inline Policies","title":"IAM Policies"},{"location":"certifications/aws/security-specialty/#s3-bucket-policies","text":"S3 Bucket policies are attached only to S3 Buckets. S3 Bucket policies specify what actions are allowed or denied on the bucket. They can be broken down to a user level, So Alice can PUT but not DELETE and John can REAd but not PUT. Bucket level only, S3 only Explicit Deny always Overrides an Allow","title":"S3 Bucket Policies"},{"location":"certifications/aws/security-specialty/#s3-acls","text":"Are Legacy access control. AWS recommend IAm Policies and S3 Bucket Policies Use them if you need apply policies on the objects themselves. Bucket policies can be applied at bucket level, and S3 ACLS can be applied to individual files (Objects)","title":"S3 ACL's"},{"location":"certifications/aws/security-specialty/#policy-conflicts-visual-diagram","text":"","title":"Policy Conflicts - Visual Diagram"},{"location":"certifications/aws/security-specialty/#forcing-encryption-using-s3","text":"aws:SecureTransport # Deny This \"Condition\" : { \"Bool\" : { \"aws:SecureTransport\" : false } }","title":"Forcing Encryption using S3"},{"location":"certifications/aws/security-specialty/#cross-region-replication","text":"Requirements Do not need to use a bucket policy with aws:SecureTransport to replicate objects using SSL. It is done by default Versioning must be enabled (In two buckets) It's possible to use CRR from one AWS account to another. The IAM role must have permissions ro replicate objects in the destination bucket. If the object owner is different than source bucket owner, the object owner must grant the bucket owner the READ and READ_ACP permissions via the object ACL In the replication configuration, can optionally direct Amazon S3 to change the ownership of object replica to the AWs account that owns the destination bucket What is Replicated Any objects created after add a replication configuration Object metadata , ACL Updates, Tags Replicates only objects in the source bucket for which the bucket owner has permissions to read objects and read access control lists (ACL) What is NOT Replicated Anything created before CRR is turned on Objects created with Server-side encryption using customer-provided (SSE-C) encryption keys Objects created with server-side encryption using AWS KMS - managed encryption (SSE-KMS) unless you explicitly enable this option Delete markers are replicated, deleted versions of files are NOT","title":"Cross Region Replication"},{"location":"certifications/aws/security-specialty/#s3-with-cloudfront","text":"Forcing S3 to Use cloudfront It's necessary check \"Restrict Bucket Access\" option It's necessary create a (or reuse) Origin Access Identity (something like a cloudfront user to access to bucket) It's necessary update Bucket Policy to give permissions to Orign Access identity access to Bucket CloudFront SSL Certificates If you are happy for users to access your content using *.cloudfront.net domain name, then select the default CloudFront Certificate If you want to use a domain name that you already own, you will need to use a custome SSL certificate Custom SSL certificates must be stored in either ACM in the us-east-1 (North Virgina) or you can also store them in IAM using the IAM Cli.","title":"S3 with CloudFront"},{"location":"certifications/aws/security-specialty/#s3-pre-signed-urls","text":"You can access objects using pre-signed URL's Typically these are done via the SDK but can also be done using the CLI They exist for a certain length of time in seconds. Default is 1 Hour. You can change this using \"--expires-in\" followed by the number of seconds","title":"S3 Pre-Signed URLs"},{"location":"certifications/aws/security-specialty/#web-identity-federation","text":"Federation allows users to authenticate with a web identity provider (Google, Facebook, Amazon) The User authenticates first with the Web ID Provider and receives an authentication token, which is exchanged for temporary AWS credentials allowing them to assume an IAM role Cognito is an Identity Broker which handles interaction between your applications and the Web ID provider (You don't need to write you own code to do this) Provides sign-up, sign-in, and guest user access Syncs user data for a seamless experience across your devices Cognito is the AWS recommended approach for Web ID federation particularly for mobile apps","title":"Web Identity Federation"},{"location":"certifications/aws/security-specialty/#cognito-user-pools","text":"Cognito uses User Pools to manage user sign-up and sign-in directly or via web Identity Providers Cognito acts as an Identity broker, handling all interaction with Web Identity Providers","title":"Cognito User Pools"},{"location":"certifications/aws/security-specialty/#glacier-vault","text":"Glacier is low-cost storage for archiving and long-term backup Files are stored as Archives, Archives are stored in Vaults A Vault Lock Policy allows you to configure and enforce compliance controls for Glacier Vaults Configure WORM (write once read many) archives Create data retentions policys, 5 year retention Enforce regulartory and compliance controls You have 24 hours to validate the lock policy, once validated, Lock policies are immutable","title":"Glacier Vault"},{"location":"certifications/aws/security-specialty/#aws-organizations","text":"Allows you to organize your accounts into groups / OUs for access control and centralized billing Attach policy based controls - Service Control Policies Centrally manage permissions for OUs - groups of accounts or individual accounts SCPs allow you to create a Permissions Boundary and restrict the permissions of child accounts including of the root user SCPs allow can deny, not allow","title":"AWS Organizations"},{"location":"certifications/aws/security-specialty/#logging-and-monitoring","text":"","title":"Logging And Monitoring"},{"location":"certifications/aws/security-specialty/#cloud-trail","text":"Validating CloudTrail Log File Integrity Was the log file modified, or deleted? CloudTail log file integrity validation: SHA-256 hashing SHA-256 with RSA for digital signing Log files are delivered with a 'digest' file Digest file can be used to validate the integrity of the log file What is Logged Metadata around API calls The identity of the API caller The time of the API call The source IP address of the API caller The request parameters The response elements returned by the service CloudTrail Event Logs Sent to an S3 bucket You manage the retention in S3 Delivered every 5 (active) minutes with up 15 minute delay Can be aggregated across regions Can be aggregated across accounts Protect you CloudTrail logs, they contain everything that you are doing in your AWS account and may contain PII Allow your security people admin access to Cloudtrail, auditors read only access to CloudTrail using IAM Restrict Access to S3 using bucket policies, and use MFA delete on your objects Use lifecycle rules to move data to Glacier or to delete it Check the interity of your log files using digest files","title":"Cloud Trail"},{"location":"certifications/aws/security-specialty/#cloudwatch","text":"Key Components CloudWatch CloudWatch logs Cloudwatch Events Real Time Metrics Alarms Notifications Custom metrics CloudWatch Logs Pushed from AWS services (including CloudTrail) Pushed from your applications/systems Metrics from log entry matches Stored indefinitely (not user S3) Cloudwatch Events Near real-time stream of system events Events AWS Resources state change AWS CloudTrail (API Calls) Custom Events (code) Scheduled Rules - match incoming events and route them to one or more targets Targets - Lambda functions, SNS topics, SQS queues, Kinesis streams","title":"Cloudwatch"},{"location":"certifications/azure/fundamentals/","text":"Azure Fundamentals \u00b6 Cloud Concepts \u00b6 Language of Cloud Computing \u00b6 High availability means VMs can spin up fast to help process requests Fault Tolerance describes how azure will ensure you have zero downtime for services provided be Azure Disaster Recovery uses time-to-recovery and recovery point metrics to recover from tornados, floods and more Scalability refers to scaling out or scaling up Elasticity is the ability to quickly increase or decrease computer processing and resources Agility means do the ability to rapidly develop, test and launch software applications Languague of Cloud Economics \u00b6 Capital expenditure (CapEx) is buying hardware outright, paid upfront as a one time purchase Operational Expenditure (OpEx) is ongoing costs needed to run your business Consumption-based pricing let's you pay only for what you use Cloud Service Models \u00b6 IaaS provides servers, storage and networking as a service PaaS is a superset of IaaS and also includes middleware, such as database management tools SaaS is when a service is built on top of PaaS, like Office 365 Serverless means that you don't have any servers. Let's a single function be hosted, deployed, run and managed on its own Cloud Architect Models \u00b6 Private Cloud is azure on your own hardware in a location of your choice. All the benefits of public cloud, but you can lock it down. A Lot of staff required Public Cloud Is Azure, AWS, GCP. No upfront costs, but monthly usage. Little control over services and infrastructure Hybrid Cloud model is the best of private and public, but could be complex Compute \u00b6 Virtual Machines \u00b6 Pricing - Calculated Hourly A Virtual Machine is your machine exclusively You don't buy, own or control any hardware. Azure does this VMs are an IaaS offering, where you are responsible for the entire machine Azure virtual machines take advantage of azure tools Pricing goes up as resources go up, and you pay by the hour Scale Sets \u00b6 Scale sets are identical VMs. They can be activated or deactivated as needed A baseline VM for the scale set ensures application stability. A baseline VM is what yoy copy to make up the scale set VMS As resources usage increases, more VMs are activated to take the load You only pay for the VM, storage and networking resources you use. Nothing additional for scale sets App Services \u00b6 App services are a PaaS offering on Azure Web Apps are used to ost web sites and web applications Web Apps for containers can host your existing container images API Apps can host your data backend services Kubernetes \u00b6 Kubernetes = AKS EMR = ACR Functions Benefits \u00b6 Only Runs When needed Saves Money Resilience Networking \u00b6 VNet \u00b6 An address space is a range of IP addresses you can use for your resources A subnet is smaller network, which is part of your VNET. Use these for security and logical division of resources A VNet is in a single region and single subscription VNets in the cloud can scale, have high availability and isolation VPN Gateway \u00b6 A VPN Gateway is a specific VNet Gateway, It consists of two or more dedicated VMS VNET Gateway + \"vpn\" becomes a VPN Connection Send encrypted data between Azure and on premises network Azure Gateway Subnet, secure tunnel and on-premises gateway makes up a VPN Gateway Scenario Application Gateway \u00b6 It works on the HTTP request of the traffic, instead of the IP address and port Traffic from a specific web address can go to a specific machine Is a fit for most other Azure Services Supports auto-scaling, end-to-end encryption, zone redundancy ans multi-site hosting Storage \u00b6 BLOB \u00b6 Types Block Store text and binary data up to 4.7TB. Made up of individually managed blocks of data Append Block blobs that are optimized for append operations. Works well for logging where data is constantly appended. Page Store files up to 8TB. Any part of the file could be accessed at any time, for example a virtual hard drive Pricing Hot Frequently accessed files. Lower access times and higher access costs Cool Lower storage costs and higher access times. Data remains here for at least 30 days Archive Lowest costs and highest access times Disk \u00b6 Types HDD Spinning hard drive. Low cost and suitable for backups Standard SSD Standard for production. Higher reliability, scalability and lower latency over HDD Premium SSD Super fast and high performance. Very low latency. Use for critical workloads Ultra Disk For the most demanding, data-intensive workloads. Disks up to 64 TB Databases \u00b6 Cosmos DB Globally distributed database. It is super fast and easy to manage Scale to infinite performance and size Can be Costly Azure SQL Fully managed and using stable Microsoft SQL Technology Azure Database for MySQL Azure Database for PostGreSQL Horizontal scaling Database Migration Services Authentication and Authorization \u00b6 Azure Active Directory \u00b6 Active Directory (AD) is not the same as Azure Active Directory Different skillset from AD to Azure AD Every Azure account will have and azure AD service Organization A tenant represents the organization Dedicated AAD A tenant is a dedicated of AAD that an organization receives when signing up for Azure Separate Each tenant is distinct and completely separate from other AAD tenants One User - One Tenant Each user in Azure only belong to a single tenant Users can be guests of other tenants Azure AD can help manage users in a hybrid cloud setup Subscription \u00b6 Billing Entity All Resources within a subscription are billed together Cost Separation You can have multiple subscriptions within a tenant to separate costs Payment If a subscription ins't paid, all the resources and services associated with the subscription stop Azure Solutions \u00b6 IoT \u00b6 IoT Hub PaaS Solution that provides more control over the IoT data collection and processing Managed and Secure Ease of Deployment Platform-as-a-service Scaling and Authentication IoT Central SaaS solution that provides pre-made IoT connections and dashboards to get set up quickly Software-as-a-service Simplify and speed up the implementation of your IoT solution No Coding Needed You don't have to know how to write code to deploy your IoT project Receive feeds from devices and focus on metrics and business value Pre-made Connectors Use any of the hundreds of connectors that are ready to use in IoT Central Big Data \u00b6 Data Lake Analytics Large Amounts of Data A data lake is a very large body of data Parallel Processing Two or more processes or computers processing the same data at the same time Ready to Go Servers, processes and any other needed services are ready to go from the start Jump straight into the data analytics HD Insights Similar to Azure Data Lake Analytics Open Source, which is free and community supported Includes Apache Hadoop, spark and kafka Azure Databricks Based on Apache Spark, a distributed cluster-computing framework Run and process a dataset on many computers simultaneously Databricks provides all the computing power Integrates with other Azure Storage Services Artificial Intelligence \u00b6 Machine Learning/AI Models The definition of what your machine learning application is learning A model is a set of rules of how to use the data provided The model finds patterns based on the rules Knowledge Mining Use Azure search to finding existing insights in your data File relationships, geography connections and more Built-in Apps Azure has a number of built-in apps that you can use for machine learning and AI straight away. These include cognitive services and bot services Azure Cognitive Services Vision You can use the vision service to recognize, identify and caption your videos and images. Automatically Decision Apps can make decisions based on content. Detect potential offensive language, detect IoT anomalies and leverage data analytics Speech Automatic speech-to-text transcription. Speaker identification and verification Azure Machine Learning Studio Supports all Azure Machine Learning tools Pre-made modules for your project Use for real world scenarios such as twitter sentiment analysis, photo grouping and movie recommendations Machine Learning Service End-to-End Service The service to use AI and machine learning almost anywhere on Azure Tooling The Machine Learning service is a collection of tools to help you build AI applications Automation Axure automatically recognizes trends in your applications and creates models for you Serverless \u00b6 Azure Functions Logic Apps Are a quick and simple way to integrate systems and applications inside and outside of azure Connect Systems Connect systems bot inside and outside of the Azure platform Integrate not only apps, but also data flows, services and entire systems Automation A lot of ways to schedule, automate and orchestrate tasks and processes Quick Start No coding required to get started straight away. You just need access to your apps to integrate Event Grid DevOps \u00b6 Azure Boards Keep track of work tasks, timelines, issues, planning and much more Azure pipelines Produce and test your software automatically and continuously Azure Repos Azure Test Plans Design tests of applications to implement automatically Azure Artifacts Share applications and code libraries with other teams inside and outside your organization Azure DevTest Labs Environment Management Allow developers and engineers to create environment for test and development Cost Management You won't incur unexpected costs and will even minimize wast of resources on your account Templates Tailor your development and test environments and reuse them with template deployments Security \u00b6 Azure Security Center Threat Alerts Ready for Hybrid architectures Each VM has an agent installed that sends data Azure analyzes the data and alerts if necessary Policy and compliance metrics A \"secure score\" to entice great security hygiene Integrate with other cloud providers Alerts for resources that aren't secure Define policies Protect Resources Response Azure Advisor for security Assistance Azure Key Vault Azure Information Protection Advanced Threat Protection Monitor users Baseline Behavior Suggest Changes Privacy, Compliance and Trust \u00b6 Azure Policy \u00b6 Azure Policy ensures that policies applied to resources are compliant A policy is a set of rules to ensure compliant resources RBAC Security Principal An object representing an entity such as user or group, which can access the resource Role Definition A collection of permissions such as read, write and delete Scope The resources the access applies to. Specify which role can access a resource or resource group Locks Assigning Assign a lock to a subscription, resource group or resource Types A lock can be of two types. Delete, where you can't delete the locked object. Read-only, where you can't make any changes to the object Azure Blueprints \u00b6 Templates for creating standard Azure environments Use Blueprints Resource templaes Role Based Access Control (RBAC) Policies Samples for common regulations Azure Monitor \u00b6 Constant Feed Most Azure services feed telemetry data into Azure Monitor Even on-premises services can send telemetry data to Azure Monitor Fully Managed Azure Monitor is centralized and fully managed. You can analyze all the data from one place Query Language Full access to an interactive query language to learn about the telemetry data Machine Learning Predict and recognize problems faster with built-in machine learning Azure Service Health \u00b6 Dashboard Custom Alerts Real-Time tracking Free Service Compliance \u00b6 Industry Regulations General Data Protection Regulation ISO Standard NIST Azure compliance manager Azure knows about compliance and resources, and can give you recommendations through the compliance manager Recommendations Tasks Compliance Score Secure Storage Reports Exam Tips GDPR, ISO and NIST are regulations and standards to ensure compliance with applicable legislation Azure compliance manager provides recommendations, tasks to assign team members, a compliance score, secure document storage and reports Azure government cloud provids dedicated datacenters to US Government bodies. Compliant with US federal, state and local requirements Azure china region contains all data and datacenters within china. Complies with all applicable chinese regulations Privacy \u00b6 Azure Information Protection Classify, label and protect data based on data sensitivity Azure Policy Define and enforce rules to ensure privacy and external regulations Guides Use Guides on azure to respond and comply with GDPR privacy requests Compliance Manager Make sure you are following privacy guidelines Trust \u00b6 TrustCenter Learn about Microsoft's effort on security, privacy, GDPR, data location, compliance about trust in each product and service Service Trust Portal Review all the independent reports and audits performed on Microsoft products and services Azures complies with more standards than any cloud provider Pricing \u00b6 Subscriptions \u00b6 Multiple Subscriptions Any Azure account can have multiple subscriptions Useful for organizing who pays for what Billing Admin One or more users can be a \"billing admin\", which manages anything to do with billing and invoicing on azure Ensures separation of responsibility Billing Cycle A billing Cycle on Azure is either 30 or 60 days Management Groups Group subscriptions Take action across subscriptions in bulk Very useful in large organizations with many subscriptions Organize Manage access, policies and compliance in bulk E.g. have a management group per country or department Billing logic You maintain the billing associated with the right budgets Nest management groups to indicated hierarchy and relationship Cost Management \u00b6 Pricing Calculator \u00b6 Limit Default Limit Some Azure accounts with monthly credits to use will have default spending limits When the credits are used, the limit kicks in No increase When the credits are gone, either remove the limit entirely or leave it in effect No spending limit Pay-as-you-go accounts have no spending limit functionality Quotas Property limit A quota is a limit on a certain property of an azure service For example, a maximum of 100 namespaces for event hub Ensure service level The quotas are necessary to ensure azure can maintain high service level Quota Change If you need to increase the quota for a particular service, you can ask Microsoft to increase them Tags Identity Roles Protect sensitive data by defining which roles can access a resource Filter Filter resources per project, customer or for reporting purposes Related Resources To make bulk processing and updating easier, define which resources are related Unambiguous Create a list for tags used that includes: description, tag name, and potential values Support \u00b6 Plans \u00b6 Basic 24/7 Access Online Self-Help Forums Azure Advisor Service Health Developer Bus. Hours Email Standard 24/7 Email/phone Professional Direct Operations Support - Onboarding Reviews Webinarrs Premier Tech reviews, Reporting, Tech Account Man Training On-demand Support Channels \u00b6 Azure Documentation Forums Social Media","title":"Fundamentals"},{"location":"certifications/azure/fundamentals/#azure-fundamentals","text":"","title":"Azure Fundamentals"},{"location":"certifications/azure/fundamentals/#cloud-concepts","text":"","title":"Cloud Concepts"},{"location":"certifications/azure/fundamentals/#language-of-cloud-computing","text":"High availability means VMs can spin up fast to help process requests Fault Tolerance describes how azure will ensure you have zero downtime for services provided be Azure Disaster Recovery uses time-to-recovery and recovery point metrics to recover from tornados, floods and more Scalability refers to scaling out or scaling up Elasticity is the ability to quickly increase or decrease computer processing and resources Agility means do the ability to rapidly develop, test and launch software applications","title":"Language of Cloud Computing"},{"location":"certifications/azure/fundamentals/#languague-of-cloud-economics","text":"Capital expenditure (CapEx) is buying hardware outright, paid upfront as a one time purchase Operational Expenditure (OpEx) is ongoing costs needed to run your business Consumption-based pricing let's you pay only for what you use","title":"Languague of Cloud Economics"},{"location":"certifications/azure/fundamentals/#cloud-service-models","text":"IaaS provides servers, storage and networking as a service PaaS is a superset of IaaS and also includes middleware, such as database management tools SaaS is when a service is built on top of PaaS, like Office 365 Serverless means that you don't have any servers. Let's a single function be hosted, deployed, run and managed on its own","title":"Cloud Service Models"},{"location":"certifications/azure/fundamentals/#cloud-architect-models","text":"Private Cloud is azure on your own hardware in a location of your choice. All the benefits of public cloud, but you can lock it down. A Lot of staff required Public Cloud Is Azure, AWS, GCP. No upfront costs, but monthly usage. Little control over services and infrastructure Hybrid Cloud model is the best of private and public, but could be complex","title":"Cloud Architect Models"},{"location":"certifications/azure/fundamentals/#compute","text":"","title":"Compute"},{"location":"certifications/azure/fundamentals/#virtual-machines","text":"Pricing - Calculated Hourly A Virtual Machine is your machine exclusively You don't buy, own or control any hardware. Azure does this VMs are an IaaS offering, where you are responsible for the entire machine Azure virtual machines take advantage of azure tools Pricing goes up as resources go up, and you pay by the hour","title":"Virtual Machines"},{"location":"certifications/azure/fundamentals/#scale-sets","text":"Scale sets are identical VMs. They can be activated or deactivated as needed A baseline VM for the scale set ensures application stability. A baseline VM is what yoy copy to make up the scale set VMS As resources usage increases, more VMs are activated to take the load You only pay for the VM, storage and networking resources you use. Nothing additional for scale sets","title":"Scale Sets"},{"location":"certifications/azure/fundamentals/#app-services","text":"App services are a PaaS offering on Azure Web Apps are used to ost web sites and web applications Web Apps for containers can host your existing container images API Apps can host your data backend services","title":"App Services"},{"location":"certifications/azure/fundamentals/#kubernetes","text":"Kubernetes = AKS EMR = ACR","title":"Kubernetes"},{"location":"certifications/azure/fundamentals/#functions-benefits","text":"Only Runs When needed Saves Money Resilience","title":"Functions Benefits"},{"location":"certifications/azure/fundamentals/#networking","text":"","title":"Networking"},{"location":"certifications/azure/fundamentals/#vnet","text":"An address space is a range of IP addresses you can use for your resources A subnet is smaller network, which is part of your VNET. Use these for security and logical division of resources A VNet is in a single region and single subscription VNets in the cloud can scale, have high availability and isolation","title":"VNet"},{"location":"certifications/azure/fundamentals/#vpn-gateway","text":"A VPN Gateway is a specific VNet Gateway, It consists of two or more dedicated VMS VNET Gateway + \"vpn\" becomes a VPN Connection Send encrypted data between Azure and on premises network Azure Gateway Subnet, secure tunnel and on-premises gateway makes up a VPN Gateway Scenario","title":"VPN Gateway"},{"location":"certifications/azure/fundamentals/#application-gateway","text":"It works on the HTTP request of the traffic, instead of the IP address and port Traffic from a specific web address can go to a specific machine Is a fit for most other Azure Services Supports auto-scaling, end-to-end encryption, zone redundancy ans multi-site hosting","title":"Application Gateway"},{"location":"certifications/azure/fundamentals/#storage","text":"","title":"Storage"},{"location":"certifications/azure/fundamentals/#blob","text":"Types Block Store text and binary data up to 4.7TB. Made up of individually managed blocks of data Append Block blobs that are optimized for append operations. Works well for logging where data is constantly appended. Page Store files up to 8TB. Any part of the file could be accessed at any time, for example a virtual hard drive Pricing Hot Frequently accessed files. Lower access times and higher access costs Cool Lower storage costs and higher access times. Data remains here for at least 30 days Archive Lowest costs and highest access times","title":"BLOB"},{"location":"certifications/azure/fundamentals/#disk","text":"Types HDD Spinning hard drive. Low cost and suitable for backups Standard SSD Standard for production. Higher reliability, scalability and lower latency over HDD Premium SSD Super fast and high performance. Very low latency. Use for critical workloads Ultra Disk For the most demanding, data-intensive workloads. Disks up to 64 TB","title":"Disk"},{"location":"certifications/azure/fundamentals/#databases","text":"Cosmos DB Globally distributed database. It is super fast and easy to manage Scale to infinite performance and size Can be Costly Azure SQL Fully managed and using stable Microsoft SQL Technology Azure Database for MySQL Azure Database for PostGreSQL Horizontal scaling Database Migration Services","title":"Databases"},{"location":"certifications/azure/fundamentals/#authentication-and-authorization","text":"","title":"Authentication and Authorization"},{"location":"certifications/azure/fundamentals/#azure-active-directory","text":"Active Directory (AD) is not the same as Azure Active Directory Different skillset from AD to Azure AD Every Azure account will have and azure AD service Organization A tenant represents the organization Dedicated AAD A tenant is a dedicated of AAD that an organization receives when signing up for Azure Separate Each tenant is distinct and completely separate from other AAD tenants One User - One Tenant Each user in Azure only belong to a single tenant Users can be guests of other tenants Azure AD can help manage users in a hybrid cloud setup","title":"Azure Active Directory"},{"location":"certifications/azure/fundamentals/#subscription","text":"Billing Entity All Resources within a subscription are billed together Cost Separation You can have multiple subscriptions within a tenant to separate costs Payment If a subscription ins't paid, all the resources and services associated with the subscription stop","title":"Subscription"},{"location":"certifications/azure/fundamentals/#azure-solutions","text":"","title":"Azure Solutions"},{"location":"certifications/azure/fundamentals/#iot","text":"IoT Hub PaaS Solution that provides more control over the IoT data collection and processing Managed and Secure Ease of Deployment Platform-as-a-service Scaling and Authentication IoT Central SaaS solution that provides pre-made IoT connections and dashboards to get set up quickly Software-as-a-service Simplify and speed up the implementation of your IoT solution No Coding Needed You don't have to know how to write code to deploy your IoT project Receive feeds from devices and focus on metrics and business value Pre-made Connectors Use any of the hundreds of connectors that are ready to use in IoT Central","title":"IoT"},{"location":"certifications/azure/fundamentals/#big-data","text":"Data Lake Analytics Large Amounts of Data A data lake is a very large body of data Parallel Processing Two or more processes or computers processing the same data at the same time Ready to Go Servers, processes and any other needed services are ready to go from the start Jump straight into the data analytics HD Insights Similar to Azure Data Lake Analytics Open Source, which is free and community supported Includes Apache Hadoop, spark and kafka Azure Databricks Based on Apache Spark, a distributed cluster-computing framework Run and process a dataset on many computers simultaneously Databricks provides all the computing power Integrates with other Azure Storage Services","title":"Big Data"},{"location":"certifications/azure/fundamentals/#artificial-intelligence","text":"Machine Learning/AI Models The definition of what your machine learning application is learning A model is a set of rules of how to use the data provided The model finds patterns based on the rules Knowledge Mining Use Azure search to finding existing insights in your data File relationships, geography connections and more Built-in Apps Azure has a number of built-in apps that you can use for machine learning and AI straight away. These include cognitive services and bot services Azure Cognitive Services Vision You can use the vision service to recognize, identify and caption your videos and images. Automatically Decision Apps can make decisions based on content. Detect potential offensive language, detect IoT anomalies and leverage data analytics Speech Automatic speech-to-text transcription. Speaker identification and verification Azure Machine Learning Studio Supports all Azure Machine Learning tools Pre-made modules for your project Use for real world scenarios such as twitter sentiment analysis, photo grouping and movie recommendations Machine Learning Service End-to-End Service The service to use AI and machine learning almost anywhere on Azure Tooling The Machine Learning service is a collection of tools to help you build AI applications Automation Axure automatically recognizes trends in your applications and creates models for you","title":"Artificial Intelligence"},{"location":"certifications/azure/fundamentals/#serverless","text":"Azure Functions Logic Apps Are a quick and simple way to integrate systems and applications inside and outside of azure Connect Systems Connect systems bot inside and outside of the Azure platform Integrate not only apps, but also data flows, services and entire systems Automation A lot of ways to schedule, automate and orchestrate tasks and processes Quick Start No coding required to get started straight away. You just need access to your apps to integrate Event Grid","title":"Serverless"},{"location":"certifications/azure/fundamentals/#devops","text":"Azure Boards Keep track of work tasks, timelines, issues, planning and much more Azure pipelines Produce and test your software automatically and continuously Azure Repos Azure Test Plans Design tests of applications to implement automatically Azure Artifacts Share applications and code libraries with other teams inside and outside your organization Azure DevTest Labs Environment Management Allow developers and engineers to create environment for test and development Cost Management You won't incur unexpected costs and will even minimize wast of resources on your account Templates Tailor your development and test environments and reuse them with template deployments","title":"DevOps"},{"location":"certifications/azure/fundamentals/#security","text":"Azure Security Center Threat Alerts Ready for Hybrid architectures Each VM has an agent installed that sends data Azure analyzes the data and alerts if necessary Policy and compliance metrics A \"secure score\" to entice great security hygiene Integrate with other cloud providers Alerts for resources that aren't secure Define policies Protect Resources Response Azure Advisor for security Assistance Azure Key Vault Azure Information Protection Advanced Threat Protection Monitor users Baseline Behavior Suggest Changes","title":"Security"},{"location":"certifications/azure/fundamentals/#privacy-compliance-and-trust","text":"","title":"Privacy, Compliance and Trust"},{"location":"certifications/azure/fundamentals/#azure-policy","text":"Azure Policy ensures that policies applied to resources are compliant A policy is a set of rules to ensure compliant resources RBAC Security Principal An object representing an entity such as user or group, which can access the resource Role Definition A collection of permissions such as read, write and delete Scope The resources the access applies to. Specify which role can access a resource or resource group Locks Assigning Assign a lock to a subscription, resource group or resource Types A lock can be of two types. Delete, where you can't delete the locked object. Read-only, where you can't make any changes to the object","title":"Azure Policy"},{"location":"certifications/azure/fundamentals/#azure-blueprints","text":"Templates for creating standard Azure environments Use Blueprints Resource templaes Role Based Access Control (RBAC) Policies Samples for common regulations","title":"Azure Blueprints"},{"location":"certifications/azure/fundamentals/#azure-monitor","text":"Constant Feed Most Azure services feed telemetry data into Azure Monitor Even on-premises services can send telemetry data to Azure Monitor Fully Managed Azure Monitor is centralized and fully managed. You can analyze all the data from one place Query Language Full access to an interactive query language to learn about the telemetry data Machine Learning Predict and recognize problems faster with built-in machine learning","title":"Azure Monitor"},{"location":"certifications/azure/fundamentals/#azure-service-health","text":"Dashboard Custom Alerts Real-Time tracking Free Service","title":"Azure Service Health"},{"location":"certifications/azure/fundamentals/#compliance","text":"Industry Regulations General Data Protection Regulation ISO Standard NIST Azure compliance manager Azure knows about compliance and resources, and can give you recommendations through the compliance manager Recommendations Tasks Compliance Score Secure Storage Reports Exam Tips GDPR, ISO and NIST are regulations and standards to ensure compliance with applicable legislation Azure compliance manager provides recommendations, tasks to assign team members, a compliance score, secure document storage and reports Azure government cloud provids dedicated datacenters to US Government bodies. Compliant with US federal, state and local requirements Azure china region contains all data and datacenters within china. Complies with all applicable chinese regulations","title":"Compliance"},{"location":"certifications/azure/fundamentals/#privacy","text":"Azure Information Protection Classify, label and protect data based on data sensitivity Azure Policy Define and enforce rules to ensure privacy and external regulations Guides Use Guides on azure to respond and comply with GDPR privacy requests Compliance Manager Make sure you are following privacy guidelines","title":"Privacy"},{"location":"certifications/azure/fundamentals/#trust","text":"TrustCenter Learn about Microsoft's effort on security, privacy, GDPR, data location, compliance about trust in each product and service Service Trust Portal Review all the independent reports and audits performed on Microsoft products and services Azures complies with more standards than any cloud provider","title":"Trust"},{"location":"certifications/azure/fundamentals/#pricing","text":"","title":"Pricing"},{"location":"certifications/azure/fundamentals/#subscriptions","text":"Multiple Subscriptions Any Azure account can have multiple subscriptions Useful for organizing who pays for what Billing Admin One or more users can be a \"billing admin\", which manages anything to do with billing and invoicing on azure Ensures separation of responsibility Billing Cycle A billing Cycle on Azure is either 30 or 60 days Management Groups Group subscriptions Take action across subscriptions in bulk Very useful in large organizations with many subscriptions Organize Manage access, policies and compliance in bulk E.g. have a management group per country or department Billing logic You maintain the billing associated with the right budgets Nest management groups to indicated hierarchy and relationship","title":"Subscriptions"},{"location":"certifications/azure/fundamentals/#cost-management","text":"","title":"Cost Management"},{"location":"certifications/azure/fundamentals/#pricing-calculator","text":"Limit Default Limit Some Azure accounts with monthly credits to use will have default spending limits When the credits are used, the limit kicks in No increase When the credits are gone, either remove the limit entirely or leave it in effect No spending limit Pay-as-you-go accounts have no spending limit functionality Quotas Property limit A quota is a limit on a certain property of an azure service For example, a maximum of 100 namespaces for event hub Ensure service level The quotas are necessary to ensure azure can maintain high service level Quota Change If you need to increase the quota for a particular service, you can ask Microsoft to increase them Tags Identity Roles Protect sensitive data by defining which roles can access a resource Filter Filter resources per project, customer or for reporting purposes Related Resources To make bulk processing and updating easier, define which resources are related Unambiguous Create a list for tags used that includes: description, tag name, and potential values","title":"Pricing Calculator"},{"location":"certifications/azure/fundamentals/#support","text":"","title":"Support"},{"location":"certifications/azure/fundamentals/#plans","text":"Basic 24/7 Access Online Self-Help Forums Azure Advisor Service Health Developer Bus. Hours Email Standard 24/7 Email/phone Professional Direct Operations Support - Onboarding Reviews Webinarrs Premier Tech reviews, Reporting, Tech Account Man Training On-demand","title":"Plans"},{"location":"certifications/azure/fundamentals/#support-channels","text":"Azure Documentation Forums Social Media","title":"Support Channels"},{"location":"certifications/google/associate-cloud-engineer/","text":"Associate Cloud Engineer \u00b6 Billing \u00b6 Export must be set up per billing account Resources should be placed into appropriate projects Resources should be tagged with labels Billing export is not real-time Delay is Hours Cloud Storage \u00b6 Names are globally unique Location Type: Region - Lowest latency within a single region Multi-Region - Highest availability across largest area Dual-Region - High availability and low latency across 2 regions Storage Class Standard - Best for short-ter storage and frequently accessed data Nearline - Best for backups and data accessed less than once a month Coldline - Best for disaster recovery and data accessed les once a quarter Archive - Best for long-term digital preservation of data accessed less than once a year Access Fine-Grained - Access per object (ACLS) in addition to bucket level permissions Uniform - Permissions only using Bucket-Level permissions (IAM) Encryption Google managed Key Customer managed key Retention Policy Minimum duration that this buckets objects must be protected from deletion or modification after they're uploaded.","title":"Associate-Cloud-Engineer"},{"location":"certifications/google/associate-cloud-engineer/#associate-cloud-engineer","text":"","title":"Associate Cloud Engineer"},{"location":"certifications/google/associate-cloud-engineer/#billing","text":"Export must be set up per billing account Resources should be placed into appropriate projects Resources should be tagged with labels Billing export is not real-time Delay is Hours","title":"Billing"},{"location":"certifications/google/associate-cloud-engineer/#cloud-storage","text":"Names are globally unique Location Type: Region - Lowest latency within a single region Multi-Region - Highest availability across largest area Dual-Region - High availability and low latency across 2 regions Storage Class Standard - Best for short-ter storage and frequently accessed data Nearline - Best for backups and data accessed less than once a month Coldline - Best for disaster recovery and data accessed les once a quarter Archive - Best for long-term digital preservation of data accessed less than once a year Access Fine-Grained - Access per object (ACLS) in addition to bucket level permissions Uniform - Permissions only using Bucket-Level permissions (IAM) Encryption Google managed Key Customer managed key Retention Policy Minimum duration that this buckets objects must be protected from deletion or modification after they're uploaded.","title":"Cloud Storage"},{"location":"certifications/kubernetes/ckad/","text":"Exam tips \u00b6 source < ( kubectl completion bash ) echo \"source <(kubectl completion bash)\" >> ~/.bashrc export ns = default alias k = 'kubectl -n $ns' # This helps when namespace in question doesn't have a friendly name # n\u00e3o funciona alias kdr = 'kubectl -n $ns -o yaml --dry-run' . # run commands in dry run mode and generate yaml. export KUBE_EDITOR = \"nano\" $ k run nginx --image = nginx --restart = Never --dry-run -o yaml > mypod.yaml $ k run nginx --image = nginx ( deployment ) $ k run nginx --image = nginx --restart = Never ( pod ) $ k run busybox --image = busybox --restart = OnFailure ( job ) $ k run busybox --image = busybox --schedule = \"* * * * *\" --restart = OnFailure ( cronJob ) $ k explain pod.spec.containers $ k explain deployment.spec.template.metadata Useful Links \u00b6 Exercises CKAD-Home Kubernetes Docs","title":"CKAD"},{"location":"certifications/kubernetes/ckad/#exam-tips","text":"source < ( kubectl completion bash ) echo \"source <(kubectl completion bash)\" >> ~/.bashrc export ns = default alias k = 'kubectl -n $ns' # This helps when namespace in question doesn't have a friendly name # n\u00e3o funciona alias kdr = 'kubectl -n $ns -o yaml --dry-run' . # run commands in dry run mode and generate yaml. export KUBE_EDITOR = \"nano\" $ k run nginx --image = nginx --restart = Never --dry-run -o yaml > mypod.yaml $ k run nginx --image = nginx ( deployment ) $ k run nginx --image = nginx --restart = Never ( pod ) $ k run busybox --image = busybox --restart = OnFailure ( job ) $ k run busybox --image = busybox --schedule = \"* * * * *\" --restart = OnFailure ( cronJob ) $ k explain pod.spec.containers $ k explain deployment.spec.template.metadata","title":"Exam tips"},{"location":"certifications/kubernetes/ckad/#useful-links","text":"Exercises CKAD-Home Kubernetes Docs","title":"Useful Links"},{"location":"databases/elasticsearch/beat/","text":"ES Beats \u00b6 Create \u00b6 Creating New Beat Examples Know Errors \u00b6 bash: mage: command not found Use vendoring \u00b6 We recommend to use vendoring for your beat. This means the dependencies are put into your beat folder. The beats team currently uses govendor for vendoring. govendor init govendor update +e This will create a directory vendor inside your repository. To make sure all dependencies for the Makefile commands are loaded from the vendor directory, find the following line in your Makefile: ES_BEATS = ${ GOPATH } /src/github.com/elastic/beats Replace it with: ES_BEATS = ./vendor/github.com/elastic/beats To Fetch: govendor fetch github.com/vmware/govmomi/^ +out","title":"Beat"},{"location":"databases/elasticsearch/beat/#es-beats","text":"","title":"ES Beats"},{"location":"databases/elasticsearch/beat/#create","text":"Creating New Beat Examples","title":"Create"},{"location":"databases/elasticsearch/beat/#know-errors","text":"bash: mage: command not found","title":"Know Errors"},{"location":"databases/elasticsearch/beat/#use-vendoring","text":"We recommend to use vendoring for your beat. This means the dependencies are put into your beat folder. The beats team currently uses govendor for vendoring. govendor init govendor update +e This will create a directory vendor inside your repository. To make sure all dependencies for the Makefile commands are loaded from the vendor directory, find the following line in your Makefile: ES_BEATS = ${ GOPATH } /src/github.com/elastic/beats Replace it with: ES_BEATS = ./vendor/github.com/elastic/beats To Fetch: govendor fetch github.com/vmware/govmomi/^ +out","title":"Use vendoring"},{"location":"databases/elasticsearch/cat/","text":"Cat \u00b6 Indices \u00b6 List GET /_cat/indices?v With Selected columns GET /_cat/indices?h=creation.date.string Help (Get Indices Columns) GET /_cat/indices?help","title":"Cat"},{"location":"databases/elasticsearch/cat/#cat","text":"","title":"Cat"},{"location":"databases/elasticsearch/cat/#indices","text":"List GET /_cat/indices?v With Selected columns GET /_cat/indices?h=creation.date.string Help (Get Indices Columns) GET /_cat/indices?help","title":"Indices"},{"location":"databases/elasticsearch/cluster/","text":"Cluster \u00b6 Health \u00b6 GET /_cluster/health Allocation Errors Explain \u00b6 GET /_cluster/allocation/explain Know Errors \u00b6 Max virtual memory (vm.max_map_count) \u00b6 max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] sudo sysctl -w vm.max_map_count = 262144","title":"Cluster"},{"location":"databases/elasticsearch/cluster/#cluster","text":"","title":"Cluster"},{"location":"databases/elasticsearch/cluster/#health","text":"GET /_cluster/health","title":"Health"},{"location":"databases/elasticsearch/cluster/#allocation-errors-explain","text":"GET /_cluster/allocation/explain","title":"Allocation Errors Explain"},{"location":"databases/elasticsearch/cluster/#know-errors","text":"","title":"Know Errors"},{"location":"databases/elasticsearch/cluster/#max-virtual-memory-vmmax_map_count","text":"max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] sudo sysctl -w vm.max_map_count = 262144","title":"Max virtual memory (vm.max_map_count)"},{"location":"databases/elasticsearch/indices/","text":"Indices \u00b6 Open \u00b6 POST / { index } /_open Close \u00b6 POST /_all/_close Know Errors \u00b6 Forbidden Index x-read-only-allow-delete-api \u00b6 Forbidden Issue PUT .kibana/_settings { \"index\" : { \"blocks\" : { \"read_only_allow_delete\" : \"false\" } } }","title":"Indices"},{"location":"databases/elasticsearch/indices/#indices","text":"","title":"Indices"},{"location":"databases/elasticsearch/indices/#open","text":"POST / { index } /_open","title":"Open"},{"location":"databases/elasticsearch/indices/#close","text":"POST /_all/_close","title":"Close"},{"location":"databases/elasticsearch/indices/#know-errors","text":"","title":"Know Errors"},{"location":"databases/elasticsearch/indices/#forbidden-index-x-read-only-allow-delete-api","text":"Forbidden Issue PUT .kibana/_settings { \"index\" : { \"blocks\" : { \"read_only_allow_delete\" : \"false\" } } }","title":"Forbidden Index x-read-only-allow-delete-api"},{"location":"databases/elasticsearch/kibana/","text":"kibana \u00b6 Time Series \u00b6 Filters performancemanager.virtualmachines.metric.info.metric: \"cpu.usagemhz.average\" AND NOT performancemanager.virtualmachines.metric.sample.instance: \"*\" AND performancemanager.hosts.metric.sample.instance: \"*\" Timelion \u00b6 Expression .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metric.sample.value,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000 ) .label ( \"Disk Provisioned [TB]\" ) .color ( black ) .lines ( fill = 1 ,width = 2 ) .title ( \"Capacity Assessment\" ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000000 ) .label ( \"Total Capacity [TB]\" ) .color ( yellow ) .lines ( fill = 2 ,width = 2 ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metric.sample.value,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.used.latest\" ) .divide ( 1000000000 ) .label ( \"Disk Used [TB]\" ) .color ( green ) .lines ( fill = 3 ,width = 1 ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000000 ) .multiply ( 1 .1 ) .label ( \"Provisioning Threshold [TB]\" ) .color ( red ) .lines ( fill = 0 ,width = 3 ) ,","title":"Kibana"},{"location":"databases/elasticsearch/kibana/#kibana","text":"","title":"kibana"},{"location":"databases/elasticsearch/kibana/#time-series","text":"Filters performancemanager.virtualmachines.metric.info.metric: \"cpu.usagemhz.average\" AND NOT performancemanager.virtualmachines.metric.sample.instance: \"*\" AND performancemanager.hosts.metric.sample.instance: \"*\"","title":"Time Series"},{"location":"databases/elasticsearch/kibana/#timelion","text":"Expression .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metric.sample.value,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000 ) .label ( \"Disk Provisioned [TB]\" ) .color ( black ) .lines ( fill = 1 ,width = 2 ) .title ( \"Capacity Assessment\" ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000000 ) .label ( \"Total Capacity [TB]\" ) .color ( yellow ) .lines ( fill = 2 ,width = 2 ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metric.sample.value,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.used.latest\" ) .divide ( 1000000000 ) .label ( \"Disk Used [TB]\" ) .color ( green ) .lines ( fill = 3 ,width = 1 ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000000 ) .multiply ( 1 .1 ) .label ( \"Provisioning Threshold [TB]\" ) .color ( red ) .lines ( fill = 0 ,width = 3 ) ,","title":"Timelion"},{"location":"databases/elasticsearch/nodes/","text":"Nodes \u00b6 Stats \u00b6 GET /_nodes/stats","title":"Nodes"},{"location":"databases/elasticsearch/nodes/#nodes","text":"","title":"Nodes"},{"location":"databases/elasticsearch/nodes/#stats","text":"GET /_nodes/stats","title":"Stats"},{"location":"databases/elasticsearch/search/","text":"Search \u00b6 All \u00b6 GET /vspherebeat/_search { \"query\" : { \"match_all\" : {} } } Field Match string \u00b6 GET /vspherebeat/_search { \"query\" : { \"match\" : { \"performancemanager.hosts.metaData.name\" : \"nsvwsdv001.mngt.local\" } } } Field doesn't match string \u00b6 GET /vspherebeat/_search { \"query\" : { \"bool\" : { \"must_not\" : [ { \"match\" : { \"performancemanager.virtualmachines.metaData.name\" : \"WSTPMNGT007\" } } ] } } } Bool Query (Match and Not Match) \u00b6 GET /vspherebeat/_search { \"query\" : { \"bool\" : { \"must\" : [ { \"match\" : { \"performancemanager.virtualmachines.metaData.name\" : \"APM-Server\" } }, { \"match\" : { \"performancemanager.virtualmachines.metric.info.metric\" : \"cpu.usage.average\" } } ], \"must_not\" : [ { \"match\" : { \"performancemanager.virtualmachines.metric.sample.instance\" : \"*\" } } ] } } } Exists Specified Field \u00b6 GET /vspherebeat/_search { \"query\" : { \"exists\" : { \"field\" : \"performancemanager.hosts\" } } } Unique Values from a field \u00b6 GET vspherebeat/_search { \"size\" : \"0\" , \"aggs\" : { \"uniq_hotsr\" : { \"terms\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } } Total of unique values from a field \u00b6 GET /vspherebeat/_search { \"size\" : 0 , \"aggs\" : { \"distinct_hots\" : { \"cardinality\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Search"},{"location":"databases/elasticsearch/search/#search","text":"","title":"Search"},{"location":"databases/elasticsearch/search/#all","text":"GET /vspherebeat/_search { \"query\" : { \"match_all\" : {} } }","title":"All"},{"location":"databases/elasticsearch/search/#field-match-string","text":"GET /vspherebeat/_search { \"query\" : { \"match\" : { \"performancemanager.hosts.metaData.name\" : \"nsvwsdv001.mngt.local\" } } }","title":"Field Match string"},{"location":"databases/elasticsearch/search/#field-doesnt-match-string","text":"GET /vspherebeat/_search { \"query\" : { \"bool\" : { \"must_not\" : [ { \"match\" : { \"performancemanager.virtualmachines.metaData.name\" : \"WSTPMNGT007\" } } ] } } }","title":"Field doesn't match string"},{"location":"databases/elasticsearch/search/#bool-query-match-and-not-match","text":"GET /vspherebeat/_search { \"query\" : { \"bool\" : { \"must\" : [ { \"match\" : { \"performancemanager.virtualmachines.metaData.name\" : \"APM-Server\" } }, { \"match\" : { \"performancemanager.virtualmachines.metric.info.metric\" : \"cpu.usage.average\" } } ], \"must_not\" : [ { \"match\" : { \"performancemanager.virtualmachines.metric.sample.instance\" : \"*\" } } ] } } }","title":"Bool Query (Match and Not Match)"},{"location":"databases/elasticsearch/search/#exists-specified-field","text":"GET /vspherebeat/_search { \"query\" : { \"exists\" : { \"field\" : \"performancemanager.hosts\" } } }","title":"Exists Specified Field"},{"location":"databases/elasticsearch/search/#unique-values-from-a-field","text":"GET vspherebeat/_search { \"size\" : \"0\" , \"aggs\" : { \"uniq_hotsr\" : { \"terms\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Unique Values from a field"},{"location":"databases/elasticsearch/search/#total-of-unique-values-from-a-field","text":"GET /vspherebeat/_search { \"size\" : 0 , \"aggs\" : { \"distinct_hots\" : { \"cardinality\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Total of unique values from a field"},{"location":"databases/elasticsearch/snapshots/","text":"Snapshots \u00b6 Repositories \u00b6 Create \u00b6 PUT /_snapshot/my_backup { \"type\" : \"fs\" , \"settings\" : { \"location\" : \"/usr/share/elasticsearch/snapshots/backup\" } } List \u00b6 GET /_cat/repositories?v GET /_snapshot/_all Delete \u00b6 DELETE /_snapshot/ { repository } Snapshots \u00b6 Elasticsearch config Change parameter in elastic search config file for all nodes The path.repo needs to be a shared folder beetween the cluster path.repo : [ \"/usr/share/elasticsearch/snapshots\" ] Create \u00b6 PUT /_snapshot/ { repository } / { snapshot } ?wait_for_completion= true List all Snapshots from a repo \u00b6 GET /_cat/snapshots/ { repository } ?v&s=id Delete \u00b6 DELETE /_snapshot/ { repository } / { snapshot } Restore \u00b6 Restore - Official Doc Restore From one Snapshot \u00b6 POST /_snapshot/ { repository } / { snapshot } /_restore","title":"Snapshots"},{"location":"databases/elasticsearch/snapshots/#snapshots","text":"","title":"Snapshots"},{"location":"databases/elasticsearch/snapshots/#repositories","text":"","title":"Repositories"},{"location":"databases/elasticsearch/snapshots/#create","text":"PUT /_snapshot/my_backup { \"type\" : \"fs\" , \"settings\" : { \"location\" : \"/usr/share/elasticsearch/snapshots/backup\" } }","title":"Create"},{"location":"databases/elasticsearch/snapshots/#list","text":"GET /_cat/repositories?v GET /_snapshot/_all","title":"List"},{"location":"databases/elasticsearch/snapshots/#delete","text":"DELETE /_snapshot/ { repository }","title":"Delete"},{"location":"databases/elasticsearch/snapshots/#snapshots_1","text":"Elasticsearch config Change parameter in elastic search config file for all nodes The path.repo needs to be a shared folder beetween the cluster path.repo : [ \"/usr/share/elasticsearch/snapshots\" ]","title":"Snapshots"},{"location":"databases/elasticsearch/snapshots/#create_1","text":"PUT /_snapshot/ { repository } / { snapshot } ?wait_for_completion= true","title":"Create"},{"location":"databases/elasticsearch/snapshots/#list-all-snapshots-from-a-repo","text":"GET /_cat/snapshots/ { repository } ?v&s=id","title":"List all Snapshots from a repo"},{"location":"databases/elasticsearch/snapshots/#delete_1","text":"DELETE /_snapshot/ { repository } / { snapshot }","title":"Delete"},{"location":"databases/elasticsearch/snapshots/#restore","text":"Restore - Official Doc","title":"Restore"},{"location":"databases/elasticsearch/snapshots/#restore-from-one-snapshot","text":"POST /_snapshot/ { repository } / { snapshot } /_restore","title":"Restore From one Snapshot"},{"location":"databases/mysql/galera-cluster/","text":"Docker \u00b6 Know Errors \u00b6 WSREP: failed to open gcomm backend connection \u00b6 [ERROR] WSREP: failed to open gcomm backend connection: 131: invalid UUID: 00000000 (FATAL) at gcomm/src/pc.cpp:PC():271 Related Links itheadaches Github Issues","title":"Galera"},{"location":"databases/mysql/galera-cluster/#docker","text":"","title":"Docker"},{"location":"databases/mysql/galera-cluster/#know-errors","text":"","title":"Know Errors"},{"location":"databases/mysql/galera-cluster/#wsrep-failed-to-open-gcomm-backend-connection","text":"[ERROR] WSREP: failed to open gcomm backend connection: 131: invalid UUID: 00000000 (FATAL) at gcomm/src/pc.cpp:PC():271 Related Links itheadaches Github Issues","title":"WSREP: failed to open gcomm backend connection"},{"location":"developer/ciscoAci/cobra/","text":"Cisco ACI Cobra \u00b6 Create Session \u00b6 from credentials import * import cobra.mit.session import cobra.mit.access auth = cobra . mit . session . LoginSession ( URL , LOGIN , PASSWORD ) session = cobra . mit . access . MoDirectory ( auth ) session . login () DN Query \u00b6 import cobra.mit.request tenant_query = cobra . mit . request . DnQuery ( \"uni/tn-Heroes\" ) heroes_tenant = session . query ( tenant_query ) heroes = heroes_tenant [ 0 ] dir ( heroes ) Class Query \u00b6 propFilter (query-target-filter) filters the results based on class attributes. queryTarget (query-target) specifies what part of the MIT to query: self children subtree classFilter (target-subtree-class) can be used when queryTarget is set to either \"children\" or \"subtree\" to filter the returned children to only objects of a specific class or classes. For example, to filter for Application Profiles use fvAp. *subtree (rsp-subtree) specifies how much of the subtree to retrieve: no children full subtreeClassFilter (rsp-subtree-class) filters what subtree classes to include in the response. subtreeInclude (rsp-subtree-include) specifies what type of information to include in the subtree audit-logs event-logs faults faults,no-scoped health Make a Class Query for All App Profiles \u00b6 ```python app_query = cobra.mit.request.ClassQuery('fvAp') apps = session.query(app_query) ### Scope the Query to Applications Named \"Save_The_Planet\" ```python # set the property filter to only return the app named \"Save_The_Planet\" app_query.propFilter = 'eq(fvAp.name, \"Save_The_Planet\")' save_the_planet_app = session.query(app_query) Child Objects \u00b6 Have the Query Return Child Objects \u00b6 # set the scope to subtree full app_query . subtree = \"full\" # demonstrate a typo; cobra provides the acceptable options app_query . queryTarget = \"subtre\" Traceback ( most recent call last ): ... ( value , str ( allowedValues ))) ValueError : \"subtre\" is invalid , valid values are \"set(['self', 'subtree', 'children'])\" # scope the query to subtree app_query . queryTarget = \"subtree\" # look at the applied query scopes using .options app_query . options 'rsp-subtree=full&query-target-filter=eq(fvAp.name, \"Save_The_Planet\")&query-target=subtree' save_the_planet_app_subtree = session . query ( app_query ) View the Child Objects for the Query \u00b6 save_the_planet_app_subtree [ < cobra . modelimpl . fv . ap . Ap object at 0x7f3c3c735150 > ] save_the_planet_app_subtree [ 0 ] . numChildren 3 for epg in save_the_planet_app_subtree [ 0 ] . children : print epg . dn uni / tn - Heroes / ap - Save_The_Planet / epg - web uni / tn - Heroes / ap - Save_The_Planet / epg - app uni / tn - Heroes / ap - Save_The_Planet / epg - db Submitting Configurations \u00b6 # submit staged configuration to APIC config_request = cobra . mit . request . ConfigRequest () config_request . addMo ( epg ) session . commit ( config_request ) # You will see these two output lines # SSL Warning # <Response [200]> Lookups \u00b6 LookupbyDn \u00b6 # set top level universe directory uniMo = moDir . lookupByDn ( 'uni' ) lookupByClass \u00b6 # use the \"lookupByClass\" method to find all endpoints (fvCEp) endpoints = md . lookupByClass ( 'fvCEp' ) print ([ str ( ep . dn ) for ep in endpoints ])","title":"Cisco ACI"},{"location":"developer/ciscoAci/cobra/#cisco-aci-cobra","text":"","title":"Cisco ACI Cobra"},{"location":"developer/ciscoAci/cobra/#create-session","text":"from credentials import * import cobra.mit.session import cobra.mit.access auth = cobra . mit . session . LoginSession ( URL , LOGIN , PASSWORD ) session = cobra . mit . access . MoDirectory ( auth ) session . login ()","title":"Create Session"},{"location":"developer/ciscoAci/cobra/#dn-query","text":"import cobra.mit.request tenant_query = cobra . mit . request . DnQuery ( \"uni/tn-Heroes\" ) heroes_tenant = session . query ( tenant_query ) heroes = heroes_tenant [ 0 ] dir ( heroes )","title":"DN Query"},{"location":"developer/ciscoAci/cobra/#class-query","text":"propFilter (query-target-filter) filters the results based on class attributes. queryTarget (query-target) specifies what part of the MIT to query: self children subtree classFilter (target-subtree-class) can be used when queryTarget is set to either \"children\" or \"subtree\" to filter the returned children to only objects of a specific class or classes. For example, to filter for Application Profiles use fvAp. *subtree (rsp-subtree) specifies how much of the subtree to retrieve: no children full subtreeClassFilter (rsp-subtree-class) filters what subtree classes to include in the response. subtreeInclude (rsp-subtree-include) specifies what type of information to include in the subtree audit-logs event-logs faults faults,no-scoped health","title":"Class Query"},{"location":"developer/ciscoAci/cobra/#make-a-class-query-for-all-app-profiles","text":"```python app_query = cobra.mit.request.ClassQuery('fvAp') apps = session.query(app_query) ### Scope the Query to Applications Named \"Save_The_Planet\" ```python # set the property filter to only return the app named \"Save_The_Planet\" app_query.propFilter = 'eq(fvAp.name, \"Save_The_Planet\")' save_the_planet_app = session.query(app_query)","title":"Make a Class Query for All App Profiles"},{"location":"developer/ciscoAci/cobra/#child-objects","text":"","title":"Child Objects"},{"location":"developer/ciscoAci/cobra/#have-the-query-return-child-objects","text":"# set the scope to subtree full app_query . subtree = \"full\" # demonstrate a typo; cobra provides the acceptable options app_query . queryTarget = \"subtre\" Traceback ( most recent call last ): ... ( value , str ( allowedValues ))) ValueError : \"subtre\" is invalid , valid values are \"set(['self', 'subtree', 'children'])\" # scope the query to subtree app_query . queryTarget = \"subtree\" # look at the applied query scopes using .options app_query . options 'rsp-subtree=full&query-target-filter=eq(fvAp.name, \"Save_The_Planet\")&query-target=subtree' save_the_planet_app_subtree = session . query ( app_query )","title":"Have the Query Return Child Objects"},{"location":"developer/ciscoAci/cobra/#view-the-child-objects-for-the-query","text":"save_the_planet_app_subtree [ < cobra . modelimpl . fv . ap . Ap object at 0x7f3c3c735150 > ] save_the_planet_app_subtree [ 0 ] . numChildren 3 for epg in save_the_planet_app_subtree [ 0 ] . children : print epg . dn uni / tn - Heroes / ap - Save_The_Planet / epg - web uni / tn - Heroes / ap - Save_The_Planet / epg - app uni / tn - Heroes / ap - Save_The_Planet / epg - db","title":"View the Child Objects for the Query"},{"location":"developer/ciscoAci/cobra/#submitting-configurations","text":"# submit staged configuration to APIC config_request = cobra . mit . request . ConfigRequest () config_request . addMo ( epg ) session . commit ( config_request ) # You will see these two output lines # SSL Warning # <Response [200]>","title":"Submitting Configurations"},{"location":"developer/ciscoAci/cobra/#lookups","text":"","title":"Lookups"},{"location":"developer/ciscoAci/cobra/#lookupbydn","text":"# set top level universe directory uniMo = moDir . lookupByDn ( 'uni' )","title":"LookupbyDn"},{"location":"developer/ciscoAci/cobra/#lookupbyclass","text":"# use the \"lookupByClass\" method to find all endpoints (fvCEp) endpoints = md . lookupByClass ( 'fvCEp' ) print ([ str ( ep . dn ) for ep in endpoints ])","title":"lookupByClass"},{"location":"developer/python/packages/","text":"Python Packages \u00b6 No Internet Access and No Root \u00b6 No PIP \u00b6 export PYTHONPATH = ~/lib/python # Download and upload package tar.gz tar -xvz package-name.tar.gz cd package-name python3.6 setup.py install --home ~ PIP \u00b6 This process install all dependencies Stackoverflow # Machine with Internet Access # Use Docker to have environment similiar to target machine mkdir keystone-deps pip download python-keystoneclient -d \"/home/aviuser/keystone-deps\" tar cvfz keystone-deps.tgz keystone-deps # Machine to install Package tar xvfz keystone-deps.tgz cd keystone-deps pip install --target = ~/lib/python keystone-deps.whl -f ./ --no-index PIP but not installed \u00b6 Stackoverflow","title":"Packages"},{"location":"developer/python/packages/#python-packages","text":"","title":"Python Packages"},{"location":"developer/python/packages/#no-internet-access-and-no-root","text":"","title":"No Internet Access and No Root"},{"location":"developer/python/packages/#no-pip","text":"export PYTHONPATH = ~/lib/python # Download and upload package tar.gz tar -xvz package-name.tar.gz cd package-name python3.6 setup.py install --home ~","title":"No PIP"},{"location":"developer/python/packages/#pip","text":"This process install all dependencies Stackoverflow # Machine with Internet Access # Use Docker to have environment similiar to target machine mkdir keystone-deps pip download python-keystoneclient -d \"/home/aviuser/keystone-deps\" tar cvfz keystone-deps.tgz keystone-deps # Machine to install Package tar xvfz keystone-deps.tgz cd keystone-deps pip install --target = ~/lib/python keystone-deps.whl -f ./ --no-index","title":"PIP"},{"location":"developer/python/packages/#pip-but-not-installed","text":"Stackoverflow","title":"PIP but not installed"},{"location":"developer/robotframework/ddd/","text":"Data Driver \u00b6 Install \u00b6 pip install robotframework-datadriver Resources \u00b6 *** Settings *** Documentation Suite description Library SeleniumLibrary *** Variables *** ${ browser } chrome ${ url } https://admin-demo.nopcommerce.com *** Keywords *** Open my Browser Open Browser ${ url } ${ browser } maximize browser window Close Browsers close all browsers Open Login Page go to ${ url } Input username [Arguments] ${ username } input text id:Email ${ username } Input pwd [Arguments] ${ password } input text id:Password ${ password } click login button click element xpath://input[@class='button-1 login-button'] Error message should be visible page should contain Login was unsuccessful Robot File with Data \u00b6 *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/login_resources.robot Suite Setup Open my Browser Suite Teardown Close Browsers Test Template Invalid Login *** Test Cases *** username password Right user empty pwd admin@yourstore.com ${ EMPTY } Right user wrong pass admin@yourstore.com xyz Wrong user right pass adm@yourstore.com admin Wrong user empty pass adm@yourstore.com ${ EMPTY } Wrong user wrong pass adm@yourstore.com xyz *** Keywords *** Invalid Login [Arguments] ${ username } ${ password } Input username ${ username } Input pwd ${ password } click login button Error message should be visible From Excel and CSV \u00b6 *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/login_resources.robot #Library DataDriver ../TestData/LoginData.xlsx sheet_name=Sheet1 Library DataDriver ../TestData/LoginData.csv Suite Setup Open my Browser Suite Teardown Close Browsers Test Template Invalid Login *** Test Cases *** LoginTestWithExcel using ${ username } and ${ password } *** Keywords *** Invalid Login [Arguments] ${ username } ${ password } Input username ${ username } Input pwd ${ password } click login button Error message should be visible","title":"Data-Driver"},{"location":"developer/robotframework/ddd/#data-driver","text":"","title":"Data Driver"},{"location":"developer/robotframework/ddd/#install","text":"pip install robotframework-datadriver","title":"Install"},{"location":"developer/robotframework/ddd/#resources","text":"*** Settings *** Documentation Suite description Library SeleniumLibrary *** Variables *** ${ browser } chrome ${ url } https://admin-demo.nopcommerce.com *** Keywords *** Open my Browser Open Browser ${ url } ${ browser } maximize browser window Close Browsers close all browsers Open Login Page go to ${ url } Input username [Arguments] ${ username } input text id:Email ${ username } Input pwd [Arguments] ${ password } input text id:Password ${ password } click login button click element xpath://input[@class='button-1 login-button'] Error message should be visible page should contain Login was unsuccessful","title":"Resources"},{"location":"developer/robotframework/ddd/#robot-file-with-data","text":"*** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/login_resources.robot Suite Setup Open my Browser Suite Teardown Close Browsers Test Template Invalid Login *** Test Cases *** username password Right user empty pwd admin@yourstore.com ${ EMPTY } Right user wrong pass admin@yourstore.com xyz Wrong user right pass adm@yourstore.com admin Wrong user empty pass adm@yourstore.com ${ EMPTY } Wrong user wrong pass adm@yourstore.com xyz *** Keywords *** Invalid Login [Arguments] ${ username } ${ password } Input username ${ username } Input pwd ${ password } click login button Error message should be visible","title":"Robot File with Data"},{"location":"developer/robotframework/ddd/#from-excel-and-csv","text":"*** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/login_resources.robot #Library DataDriver ../TestData/LoginData.xlsx sheet_name=Sheet1 Library DataDriver ../TestData/LoginData.csv Suite Setup Open my Browser Suite Teardown Close Browsers Test Template Invalid Login *** Test Cases *** LoginTestWithExcel using ${ username } and ${ password } *** Keywords *** Invalid Login [Arguments] ${ username } ${ password } Input username ${ username } Input pwd ${ password } click login button Error message should be visible","title":"From Excel and CSV"},{"location":"developer/robotframework/gettingstarted/","text":"Getting Started \u00b6 Install \u00b6 pip install robotframework Structure \u00b6 SCALAR ${VARIABLE NAME} value LIST @{VARIABLE NAME} value1 value2 DICTIONARY &{VARIABLE NAME} k1=value1 k2=value2 ENVIRONMENT %{USERNAME} BUILT-IN Robot Commands \u00b6 # Run One Test robot FirstTestSuite.robot # Run All Tests in folder robot TestCases/ # Run Tests based in regular expressions robot TestsCases \\R eg*.robot # With Tags robot --include = sanity Setup_TearDown.robot robot -i sanity -i regression Setup_TearDown.robot robot -e regression Setup_TearDown.robot robot -i sanity -e regression Setup_TearDown.robot Pabot Commands \u00b6 pip install -U robotframework-pabot pabot --processes 2 TestCases/*.robot pabot --processes 2 --outputdir Results TestCases/*.robot","title":"Getting Started"},{"location":"developer/robotframework/gettingstarted/#getting-started","text":"","title":"Getting Started"},{"location":"developer/robotframework/gettingstarted/#install","text":"pip install robotframework","title":"Install"},{"location":"developer/robotframework/gettingstarted/#structure","text":"SCALAR ${VARIABLE NAME} value LIST @{VARIABLE NAME} value1 value2 DICTIONARY &{VARIABLE NAME} k1=value1 k2=value2 ENVIRONMENT %{USERNAME} BUILT-IN","title":"Structure"},{"location":"developer/robotframework/gettingstarted/#robot-commands","text":"# Run One Test robot FirstTestSuite.robot # Run All Tests in folder robot TestCases/ # Run Tests based in regular expressions robot TestsCases \\R eg*.robot # With Tags robot --include = sanity Setup_TearDown.robot robot -i sanity -i regression Setup_TearDown.robot robot -e regression Setup_TearDown.robot robot -i sanity -e regression Setup_TearDown.robot","title":"Robot Commands"},{"location":"developer/robotframework/gettingstarted/#pabot-commands","text":"pip install -U robotframework-pabot pabot --processes 2 TestCases/*.robot pabot --processes 2 --outputdir Results TestCases/*.robot","title":"Pabot Commands"},{"location":"developer/robotframework/pom/","text":"Page Object Model \u00b6 Locators \u00b6 File: PageObjects/Locators.py # Login Page Elements txt_loginUserName = \"name:userName\" txt_loginPassword = \"name:password\" btn_signIn = \"xpath://input[@name='login']\" # Registration Page Elements link_Reg = \"link:REGISTER\" txt_firstName = \"name:firstName\" txt_lastName = \"name:lastName\" txt_phone = \"name:phone\" txt_email = \"name:userName\" txt_add1 = \"name:address1\" txt_add2 = \"name:address2\" txt_city = \"name:city\" txt_state = \"name:state\" txt_postCode = \"name:postalCode\" drp_country = \"name:country\" txt_userName = \"name:email\" txt_Password = \"name:password\" txt_conformedPassword = \"name:confirmPassword\" btn_submit = \"xpath://input[@name='register']\" Resources \u00b6 File: Resources/LoginKeywords.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Variables ../PageObjects/Locators.py *** Keywords *** Open my Browser [Arguments] ${ SiteUrl } ${ Browser } Open Browser ${ SiteUrl } ${ Browser } Maximize browser window Enter UserName [Arguments] ${ username } Input Text ${ txt_loginUserName } ${ username } Enter Password [Arguments] ${ password } Input Text ${ txt_loginPassword } ${ password } Click SignIn Click Button ${ btn_signIn } Verify Succesfull Login title should be Find a Flight: Mercury Tours: Close my browsers close all browsers File: Resources/Registration.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Variables ../PageObjects/Locators.py *** Keywords *** Open my Browser [Arguments] ${ SiteUrl } ${ Browser } Open Browser ${ SiteUrl } ${ Browser } Maximize browser window Click Register Link click link ${ link_Reg } Enter FirstName [Arguments] ${ firstName } Input Text ${ txt_firstName } ${ firstName } Enter LastName [Arguments] ${ lastName } Input Text ${ txt_lastName } ${ lastName } Enter Phone [Arguments] ${ phone } Input Text ${ txt_phone } ${ phone } Enter Email [Arguments] ${ email } Input Text ${ txt_email } ${ email } Enter Address1 [Arguments] ${ add1 } Input Text ${ txt_add1 } ${ add1 } Enter Address2 [Arguments] ${ add2 } Input Text ${ txt_add2 } ${ add2 } Enter City [Arguments] ${ city } Input Text ${ txt_city } ${ city } Enter State [Arguments] ${ state } Input Text ${ txt_state } ${ state } Enter Postal Code [Arguments] ${ postalcode } Input Text ${ txt_postCode } ${ postalcode } Select Country [Arguments] ${ country } Select from list by label ${ drp_country } ${ country } Enter User Name [Arguments] ${ username } Input Text ${ txt_userName } ${ username } Enter Password [Arguments] ${ password } Input Text ${ txt_Password } ${ password } Enter Confirmed Password [Arguments] ${ password } Input Text ${ txt_conformedPassword } ${ password } Click Submit Click button ${ btn_submit } Verify Succesfull Registration page should contain Thank you for registeringdsadsa Close my browsers close all browsers Tests \u00b6 File: TestCases/LoginTest.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/LoginKeywords.robot *** Variables *** ${ Browser } chrome ${ SiteUrl } http://newtours.demoaut.com/ ${ user } tutorial ${ pwd } tutorial *** Test Cases *** LoginTest Open my Browser ${ SiteUrl } ${ Browser } Enter UserName ${ user } Enter Password ${ pwd } Click SignIn Sleep 3 seconds Verify Succesfull Login Close my browsers File: TestCases/RegistrationTest.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../ Resources / RegistrationKeywords . robot *** Variables *** ${ Browser } headlesschrome ${ SiteUrl } http: // newtours . demoaut . com / *** Test Cases *** RegistrationTest Open my Browser ${ SiteUrl } ${ Browser } Click Register Link Enter FirstName David Enter LastName John Enter Phone 1234567890 Enter Email davidjohn @gmail . com Enter Address1 Toronto Enter Address2 Canada Enter City Toronto Enter State Brampton Enter Postal Code L3S 1E7 Select Country CANADA Enter User Name johnxyz Enter Password johnxyz Enter Confirmed Password johnxyz Click Submit Verify Succesfull Registration Close my browsers","title":"Page-Object Model"},{"location":"developer/robotframework/pom/#page-object-model","text":"","title":"Page Object Model"},{"location":"developer/robotframework/pom/#locators","text":"File: PageObjects/Locators.py # Login Page Elements txt_loginUserName = \"name:userName\" txt_loginPassword = \"name:password\" btn_signIn = \"xpath://input[@name='login']\" # Registration Page Elements link_Reg = \"link:REGISTER\" txt_firstName = \"name:firstName\" txt_lastName = \"name:lastName\" txt_phone = \"name:phone\" txt_email = \"name:userName\" txt_add1 = \"name:address1\" txt_add2 = \"name:address2\" txt_city = \"name:city\" txt_state = \"name:state\" txt_postCode = \"name:postalCode\" drp_country = \"name:country\" txt_userName = \"name:email\" txt_Password = \"name:password\" txt_conformedPassword = \"name:confirmPassword\" btn_submit = \"xpath://input[@name='register']\"","title":"Locators"},{"location":"developer/robotframework/pom/#resources","text":"File: Resources/LoginKeywords.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Variables ../PageObjects/Locators.py *** Keywords *** Open my Browser [Arguments] ${ SiteUrl } ${ Browser } Open Browser ${ SiteUrl } ${ Browser } Maximize browser window Enter UserName [Arguments] ${ username } Input Text ${ txt_loginUserName } ${ username } Enter Password [Arguments] ${ password } Input Text ${ txt_loginPassword } ${ password } Click SignIn Click Button ${ btn_signIn } Verify Succesfull Login title should be Find a Flight: Mercury Tours: Close my browsers close all browsers File: Resources/Registration.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Variables ../PageObjects/Locators.py *** Keywords *** Open my Browser [Arguments] ${ SiteUrl } ${ Browser } Open Browser ${ SiteUrl } ${ Browser } Maximize browser window Click Register Link click link ${ link_Reg } Enter FirstName [Arguments] ${ firstName } Input Text ${ txt_firstName } ${ firstName } Enter LastName [Arguments] ${ lastName } Input Text ${ txt_lastName } ${ lastName } Enter Phone [Arguments] ${ phone } Input Text ${ txt_phone } ${ phone } Enter Email [Arguments] ${ email } Input Text ${ txt_email } ${ email } Enter Address1 [Arguments] ${ add1 } Input Text ${ txt_add1 } ${ add1 } Enter Address2 [Arguments] ${ add2 } Input Text ${ txt_add2 } ${ add2 } Enter City [Arguments] ${ city } Input Text ${ txt_city } ${ city } Enter State [Arguments] ${ state } Input Text ${ txt_state } ${ state } Enter Postal Code [Arguments] ${ postalcode } Input Text ${ txt_postCode } ${ postalcode } Select Country [Arguments] ${ country } Select from list by label ${ drp_country } ${ country } Enter User Name [Arguments] ${ username } Input Text ${ txt_userName } ${ username } Enter Password [Arguments] ${ password } Input Text ${ txt_Password } ${ password } Enter Confirmed Password [Arguments] ${ password } Input Text ${ txt_conformedPassword } ${ password } Click Submit Click button ${ btn_submit } Verify Succesfull Registration page should contain Thank you for registeringdsadsa Close my browsers close all browsers","title":"Resources"},{"location":"developer/robotframework/pom/#tests","text":"File: TestCases/LoginTest.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/LoginKeywords.robot *** Variables *** ${ Browser } chrome ${ SiteUrl } http://newtours.demoaut.com/ ${ user } tutorial ${ pwd } tutorial *** Test Cases *** LoginTest Open my Browser ${ SiteUrl } ${ Browser } Enter UserName ${ user } Enter Password ${ pwd } Click SignIn Sleep 3 seconds Verify Succesfull Login Close my browsers File: TestCases/RegistrationTest.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../ Resources / RegistrationKeywords . robot *** Variables *** ${ Browser } headlesschrome ${ SiteUrl } http: // newtours . demoaut . com / *** Test Cases *** RegistrationTest Open my Browser ${ SiteUrl } ${ Browser } Click Register Link Enter FirstName David Enter LastName John Enter Phone 1234567890 Enter Email davidjohn @gmail . com Enter Address1 Toronto Enter Address2 Canada Enter City Toronto Enter State Brampton Enter Postal Code L3S 1E7 Select Country CANADA Enter User Name johnxyz Enter Password johnxyz Enter Confirmed Password johnxyz Click Submit Verify Succesfull Registration Close my browsers","title":"Tests"},{"location":"developer/robotframework/robotfile/","text":"Robot File \u00b6 *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/RegistrationKeywords.robot Suite Setup Log This is open browser Test Suite Teardown Log This is close browser Test Test Setup Log This is Login Test Test Teardown Log This is Logout Test *** Variables *** ${ Browser } Chrome ${ URL } https://opensource-demo.orangehrmlive.com/ @{Credentials} Admin admin123 & {LoginData} username=Admin password=admin123 *** Test Cases *** TC_001 [Tags] sanity ${ PageTitle } = launchBrowser ${ url } ${ browser } log to console ${ PageTitle } log ${ PageTitle } Input Text name:userName mercury Input Text name:password mercury TC_002 [Tags] sanity Log This is postpaid recharge test TC_003 [Tags] regression Log This is search test TC_004 [Tags] regression Log This is Advanced search test *** Keywords *** Do login [Arguments] ${ test } Input Text id:txtUsername @{Credentials}[0] Input Text id:txtPassword & {LoginData}[password] Click Element name:Submit","title":"RobotFile"},{"location":"developer/robotframework/robotfile/#robot-file","text":"*** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/RegistrationKeywords.robot Suite Setup Log This is open browser Test Suite Teardown Log This is close browser Test Test Setup Log This is Login Test Test Teardown Log This is Logout Test *** Variables *** ${ Browser } Chrome ${ URL } https://opensource-demo.orangehrmlive.com/ @{Credentials} Admin admin123 & {LoginData} username=Admin password=admin123 *** Test Cases *** TC_001 [Tags] sanity ${ PageTitle } = launchBrowser ${ url } ${ browser } log to console ${ PageTitle } log ${ PageTitle } Input Text name:userName mercury Input Text name:password mercury TC_002 [Tags] sanity Log This is postpaid recharge test TC_003 [Tags] regression Log This is search test TC_004 [Tags] regression Log This is Advanced search test *** Keywords *** Do login [Arguments] ${ test } Input Text id:txtUsername @{Credentials}[0] Input Text id:txtPassword & {LoginData}[password] Click Element name:Submit","title":"Robot File"},{"location":"developer/robotframework/selenium/","text":"Selenium \u00b6 Install \u00b6 pip install -U selenium pip install robotframework-seleniumlibrary # Download Browser Drivers Input Text and Click \u00b6 *** Test Cases *** Login Test Open Browser $ { url } $ { browser } loginToApplication Close Browser *** Keywords *** loginToApplication Click Link xpath : //a[@class='ico-login'] Input Text id : Email pavanoltraining @ gmail . com Input Text id : Password Test @123 Click Element xpath : //input[@class='button-1 login-button'] Radio Buttons and Check Boxes \u00b6 Testing Radio Buttons and Check Boxes Open Browser ${ url } ${ browser } Maximize Browser Window Select Radio Button sex Female Select Radio Button exp 5 Select Checkbox BlackTea Select Checkbox RedTea Unselect Checkbox BlackTea Close Browser Dropdown and Lists \u00b6 Handling DropDownds and Lists Open Browser ${ url } ${ browser } Maximize Browser Window Select From List By Label continents Asia Sleep 5 Select From List By Index continents 5 Select From List By Label selenium_commands Switch Commands Select From List By Label selenium_commands WebElement Commands Sleep 5 Unselect From List By Label selenium_commands Switch Commands HTML Table \u00b6 TableValidations open browser https://www.testautomationpractice.blogspot.com/ chrome maximize browser window ${ rows } = get element count xpath://table[@name='BookTable']/tbody/tr ${ cols } = get element count xpath://table[@name='BookTable']/tbody/tr[1]/th log to console ${ rows } log to console ${ cols } ${ data } = get text xpath://table[@name='BookTable']/tbody/tr[5]/td[1] log to console ${ data } table column should contain xpath://table[@name='BookTable'] 2 Author table row should contain xpath://table[@name='BookTable'] 4 Learn JS table cell should contain xpath://table[@name='BookTable'] 5 2 Mukesh table header should contain xpath://table[@name='BookTable'] BookName close browser Speed \u00b6 RegTest ${ speed } = Get Selenium Speed Log To Console ${ speed } Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Speed 3 seconds Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn ${ speed } = Get Selenium Speed Log To Console ${ speed } Close Browser Timeouts \u00b6 Timeouts Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Timeout 10 seconds Wait Until Page Contains Registeration Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn Close Browser Implict Wait \u00b6 Implict Wait Test Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Implicit wait 10 seconds Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn Close Browser Multiple Browsers \u00b6 MultipleBrowsers Open Browser https://www.google.com/ chrome Maximize Browser Window Sleep 3 Open Browser https://www.bing.com/ chrome Maximize Browser Window Switch Browser 1 ${ title1 } = Get Title log to console ${ title1 } Switch Browser 2 ${ title2 } = Get Title log to console ${ title2 } Sleep 3 Close All Browsers Tabbed Windows \u00b6 TabbedWindowsTest Open Browser http://demo.automationtesting.in/Windows.html chrome Click Element xpath://*[@id=\"Tabbed\"]/a/button Select Window title=Sakinalium | Home Click Element xpath://*[@id=\"container\"]/header/div/div/div[2]/ul/Li[4]/a Sleep 3 Close All Browsers Navigation Keywords \u00b6 Navigation Test Open Browser https://www.google.com chrome ${ loc } = Get Location log to console ${ loc } sleep 3 go to https://www.bing.com/ ${ loc } = Get Location log to console ${ loc } sleep 3 go back ${ loc } = Get Location log to console ${ loc } sleep 3 close browser Handling Alerts \u00b6 Handling Alerts Open Browser https://testautomationpractice.blogspot.com/ chrome Click Element xpath://*[@id=\"HTML9\"]/div[1]/button # Opens Alert Sleep 1 Alert Should be Present Press a button! Handle Alert accept Handling Frames \u00b6 Testing Frames Open Browser https://seleniumhq.github.io/selenium/docs/api/java/index chrome Select Frame packageListFrame Click Link org.openqa.selenium Unselect Frame Select Frame packageFrame Click Link WebDriver Unselect Frame Select Frame classFrame Click Link Help Close Browser Scroll And Executing Javascript \u00b6 ScrollingTest Open Browser https://www.countries-ofthe-world.com/flags-of-the-world.html chrome execute javascript window.scrollTo(0,2500) scroll element into view xpath://table[1]//tbody[1]//tr[105]//td[1]//img[1] execute javascript window.scrollTo(0,document.body.scrollHeight) # End of the Page sleep 5 execute javascript window.scrollTo(0,-document.body.scrollHeight) # Starting Point","title":"Selenium"},{"location":"developer/robotframework/selenium/#selenium","text":"","title":"Selenium"},{"location":"developer/robotframework/selenium/#install","text":"pip install -U selenium pip install robotframework-seleniumlibrary # Download Browser Drivers","title":"Install"},{"location":"developer/robotframework/selenium/#input-text-and-click","text":"*** Test Cases *** Login Test Open Browser $ { url } $ { browser } loginToApplication Close Browser *** Keywords *** loginToApplication Click Link xpath : //a[@class='ico-login'] Input Text id : Email pavanoltraining @ gmail . com Input Text id : Password Test @123 Click Element xpath : //input[@class='button-1 login-button']","title":"Input Text and Click"},{"location":"developer/robotframework/selenium/#radio-buttons-and-check-boxes","text":"Testing Radio Buttons and Check Boxes Open Browser ${ url } ${ browser } Maximize Browser Window Select Radio Button sex Female Select Radio Button exp 5 Select Checkbox BlackTea Select Checkbox RedTea Unselect Checkbox BlackTea Close Browser","title":"Radio Buttons and Check Boxes"},{"location":"developer/robotframework/selenium/#dropdown-and-lists","text":"Handling DropDownds and Lists Open Browser ${ url } ${ browser } Maximize Browser Window Select From List By Label continents Asia Sleep 5 Select From List By Index continents 5 Select From List By Label selenium_commands Switch Commands Select From List By Label selenium_commands WebElement Commands Sleep 5 Unselect From List By Label selenium_commands Switch Commands","title":"Dropdown and Lists"},{"location":"developer/robotframework/selenium/#html-table","text":"TableValidations open browser https://www.testautomationpractice.blogspot.com/ chrome maximize browser window ${ rows } = get element count xpath://table[@name='BookTable']/tbody/tr ${ cols } = get element count xpath://table[@name='BookTable']/tbody/tr[1]/th log to console ${ rows } log to console ${ cols } ${ data } = get text xpath://table[@name='BookTable']/tbody/tr[5]/td[1] log to console ${ data } table column should contain xpath://table[@name='BookTable'] 2 Author table row should contain xpath://table[@name='BookTable'] 4 Learn JS table cell should contain xpath://table[@name='BookTable'] 5 2 Mukesh table header should contain xpath://table[@name='BookTable'] BookName close browser","title":"HTML Table"},{"location":"developer/robotframework/selenium/#speed","text":"RegTest ${ speed } = Get Selenium Speed Log To Console ${ speed } Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Speed 3 seconds Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn ${ speed } = Get Selenium Speed Log To Console ${ speed } Close Browser","title":"Speed"},{"location":"developer/robotframework/selenium/#timeouts","text":"Timeouts Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Timeout 10 seconds Wait Until Page Contains Registeration Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn Close Browser","title":"Timeouts"},{"location":"developer/robotframework/selenium/#implict-wait","text":"Implict Wait Test Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Implicit wait 10 seconds Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn Close Browser","title":"Implict Wait"},{"location":"developer/robotframework/selenium/#multiple-browsers","text":"MultipleBrowsers Open Browser https://www.google.com/ chrome Maximize Browser Window Sleep 3 Open Browser https://www.bing.com/ chrome Maximize Browser Window Switch Browser 1 ${ title1 } = Get Title log to console ${ title1 } Switch Browser 2 ${ title2 } = Get Title log to console ${ title2 } Sleep 3 Close All Browsers","title":"Multiple Browsers"},{"location":"developer/robotframework/selenium/#tabbed-windows","text":"TabbedWindowsTest Open Browser http://demo.automationtesting.in/Windows.html chrome Click Element xpath://*[@id=\"Tabbed\"]/a/button Select Window title=Sakinalium | Home Click Element xpath://*[@id=\"container\"]/header/div/div/div[2]/ul/Li[4]/a Sleep 3 Close All Browsers","title":"Tabbed Windows"},{"location":"developer/robotframework/selenium/#navigation-keywords","text":"Navigation Test Open Browser https://www.google.com chrome ${ loc } = Get Location log to console ${ loc } sleep 3 go to https://www.bing.com/ ${ loc } = Get Location log to console ${ loc } sleep 3 go back ${ loc } = Get Location log to console ${ loc } sleep 3 close browser","title":"Navigation Keywords"},{"location":"developer/robotframework/selenium/#handling-alerts","text":"Handling Alerts Open Browser https://testautomationpractice.blogspot.com/ chrome Click Element xpath://*[@id=\"HTML9\"]/div[1]/button # Opens Alert Sleep 1 Alert Should be Present Press a button! Handle Alert accept","title":"Handling Alerts"},{"location":"developer/robotframework/selenium/#handling-frames","text":"Testing Frames Open Browser https://seleniumhq.github.io/selenium/docs/api/java/index chrome Select Frame packageListFrame Click Link org.openqa.selenium Unselect Frame Select Frame packageFrame Click Link WebDriver Unselect Frame Select Frame classFrame Click Link Help Close Browser","title":"Handling Frames"},{"location":"developer/robotframework/selenium/#scroll-and-executing-javascript","text":"ScrollingTest Open Browser https://www.countries-ofthe-world.com/flags-of-the-world.html chrome execute javascript window.scrollTo(0,2500) scroll element into view xpath://table[1]//tbody[1]//tr[105]//td[1]//img[1] execute javascript window.scrollTo(0,document.body.scrollHeight) # End of the Page sleep 5 execute javascript window.scrollTo(0,-document.body.scrollHeight) # Starting Point","title":"Scroll And Executing Javascript"},{"location":"devops/git/","text":"Remove file from History \u00b6 git filter-branch --force --index-filter \"git rm --cached --ignore-unmatch private.key\" --prune-empty --tag-name-filter cat -- --all git push origin --force --all ( master branch needs to be unprotected ) git push origin --force --tags","title":"Git"},{"location":"devops/git/#remove-file-from-history","text":"git filter-branch --force --index-filter \"git rm --cached --ignore-unmatch private.key\" --prune-empty --tag-name-filter cat -- --all git push origin --force --all ( master branch needs to be unprotected ) git push origin --force --tags","title":"Remove file from History"},{"location":"devops/jenkins/","text":"Jenkins \u00b6 Recommended Plugins \u00b6 Ant Plugin Credentials Binding Plugin LDAP Plugin Pipeline: Stage View Plugin Pipeline OWASP Markup Formatter Plugin Email Extension Plugin Mailer Plugin SSH Slaves Plugin Github Organization Folder Plugin Build timeout Plugin Git Plugin Matrix Authorization Strategy Plugin Subversion Plug-in Workspace Cleanup Plugin CloudBees Folders Plugin Gradle Plugin PAM Authentication Plugin Timestamper Declarative Pipelines \u00b6 pipeline { agent any stages { agent { label \"master\" } options { # Runs before agent provisioning timeout ( time: 1 , unit: 'HOURS' ) } steps { sh \"./might-hang.sh\" } } post { always { echo 'This will always run' } success { echo 'This will run only if successful' } failure { echo 'This will run only if failed' } unstable { echo 'This will run only if the run was marked as unstable' } changed { echo 'This will run only if the state of the pipeline has changed' echo 'For example, if the pipeline was previously failing but is now successful' } } } Post Conditions \u00b6 cleanup Always run regardless of build or stage result, but runs after all other post conditions fixed Runs if the current build's status is SUCCESS and the previous build's status was either FAILURE or UNSTABLE regression Runs if the current build's status ir woese than the previous build's status Tag \u00b6 When building a tag you might for example want to run more tests and publish the binaries etc Will run the stage if the tab being built is starting with \"release-\" when { tag \"release-*\" } To match on any tag there is an alias that doesn't take an argument when { buildingTag () } Change Request \u00b6 Runs the stage if a change Request, a.k.a Pull Request on Github, is currently building when { changeRequest () } You might for example only care about running static analysis on change requests when { not { changeRequest () } } You can also check the various attributes of the pull request for more fine control id, target, branch, fork, url, title, author, authorDisplayName, authorEmail when { changeRequest authorEmail: 'rsandel@cloudbees.com' } Comparator \u00b6 Both changeRequest and tag has gotten on optional parameter; comparator where you can set how to compare the values EQUALS - simple string comparison GLOB - ANT style glob REGEXP - Regular Expression PreserveStashes \u00b6 options { preserveStashes ( buildCount: 5 ) } Github \u00b6 Setting Up GitHub Webhooks \u00b6 Create an access token in GitHub that has permission to read and create webhooks Add a GitHub server in Jenkins for Github.com Create a jenkins credential with the token and configure the GitHub server configuration to use it Check \"Manage Hooks\" for the GitHub server configuration In the project configuration, under \"Build Triggers\", select \"Github hook trigger for GitScm polling\" Tips \u00b6 Try Avoid script Try avoid variables across stages When { equals expected: ..., actual: ... } Covers a lot of common usages of when { expression {...} } } No more if (currentBuild.result == \"SUCCESS\") - just use equals expected: \"SUCCEESS\", actual: currentBuild.result knowledge Center \u00b6 Tomcat \u00b6 The username you provided is not allowed to use the text-based Tomcat Manager (error 403) when deploying on remote Tomcat8 using Jenkins Cause : By default Tomcat does not allow access to the manager from external machines Solve: Link Common Errors \u00b6 Send Email With Gmail \u00b6 StackOverfow Look at this response","title":"Jenkins"},{"location":"devops/jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"devops/jenkins/#recommended-plugins","text":"Ant Plugin Credentials Binding Plugin LDAP Plugin Pipeline: Stage View Plugin Pipeline OWASP Markup Formatter Plugin Email Extension Plugin Mailer Plugin SSH Slaves Plugin Github Organization Folder Plugin Build timeout Plugin Git Plugin Matrix Authorization Strategy Plugin Subversion Plug-in Workspace Cleanup Plugin CloudBees Folders Plugin Gradle Plugin PAM Authentication Plugin Timestamper","title":"Recommended Plugins"},{"location":"devops/jenkins/#declarative-pipelines","text":"pipeline { agent any stages { agent { label \"master\" } options { # Runs before agent provisioning timeout ( time: 1 , unit: 'HOURS' ) } steps { sh \"./might-hang.sh\" } } post { always { echo 'This will always run' } success { echo 'This will run only if successful' } failure { echo 'This will run only if failed' } unstable { echo 'This will run only if the run was marked as unstable' } changed { echo 'This will run only if the state of the pipeline has changed' echo 'For example, if the pipeline was previously failing but is now successful' } } }","title":"Declarative Pipelines"},{"location":"devops/jenkins/#post-conditions","text":"cleanup Always run regardless of build or stage result, but runs after all other post conditions fixed Runs if the current build's status is SUCCESS and the previous build's status was either FAILURE or UNSTABLE regression Runs if the current build's status ir woese than the previous build's status","title":"Post Conditions"},{"location":"devops/jenkins/#tag","text":"When building a tag you might for example want to run more tests and publish the binaries etc Will run the stage if the tab being built is starting with \"release-\" when { tag \"release-*\" } To match on any tag there is an alias that doesn't take an argument when { buildingTag () }","title":"Tag"},{"location":"devops/jenkins/#change-request","text":"Runs the stage if a change Request, a.k.a Pull Request on Github, is currently building when { changeRequest () } You might for example only care about running static analysis on change requests when { not { changeRequest () } } You can also check the various attributes of the pull request for more fine control id, target, branch, fork, url, title, author, authorDisplayName, authorEmail when { changeRequest authorEmail: 'rsandel@cloudbees.com' }","title":"Change Request"},{"location":"devops/jenkins/#comparator","text":"Both changeRequest and tag has gotten on optional parameter; comparator where you can set how to compare the values EQUALS - simple string comparison GLOB - ANT style glob REGEXP - Regular Expression","title":"Comparator"},{"location":"devops/jenkins/#preservestashes","text":"options { preserveStashes ( buildCount: 5 ) }","title":"PreserveStashes"},{"location":"devops/jenkins/#github","text":"","title":"Github"},{"location":"devops/jenkins/#setting-up-github-webhooks","text":"Create an access token in GitHub that has permission to read and create webhooks Add a GitHub server in Jenkins for Github.com Create a jenkins credential with the token and configure the GitHub server configuration to use it Check \"Manage Hooks\" for the GitHub server configuration In the project configuration, under \"Build Triggers\", select \"Github hook trigger for GitScm polling\"","title":"Setting Up GitHub Webhooks"},{"location":"devops/jenkins/#tips","text":"Try Avoid script Try avoid variables across stages When { equals expected: ..., actual: ... } Covers a lot of common usages of when { expression {...} } } No more if (currentBuild.result == \"SUCCESS\") - just use equals expected: \"SUCCEESS\", actual: currentBuild.result","title":"Tips"},{"location":"devops/jenkins/#knowledge-center","text":"","title":"knowledge Center"},{"location":"devops/jenkins/#tomcat","text":"The username you provided is not allowed to use the text-based Tomcat Manager (error 403) when deploying on remote Tomcat8 using Jenkins Cause : By default Tomcat does not allow access to the manager from external machines Solve: Link","title":"Tomcat"},{"location":"devops/jenkins/#common-errors","text":"","title":"Common Errors"},{"location":"devops/jenkins/#send-email-with-gmail","text":"StackOverfow Look at this response","title":"Send Email With Gmail"},{"location":"devops/packer/","text":"Install \u00b6 wget https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip unzip packer_1.4.2_linux_amd64.zip rm packer_1.4.2_linux_amd64.zip sudo mv packer /usr/bin/ packer -v Commands \u00b6 packer validate packer.json packer build -var 'tag=0.0.1' -var 'another=212' packer.json # Not Tested it packer fix packer validate Templates \u00b6 packer.json AMI \u00b6 { \"variables\" : { \"subnet_id\" : \"\" , \"instance_size\" : \"t2.micro\" , \"ami_name\" : \"bastion\" , \"ssh_username\" : \"ec2-user\" }, \"builders\" : [ { \"type\" : \"amazon-ebs\" , \"instance_type\" : \"{{user `instance_size`}}\" , \"ssh_username\" : \"{{user `ssh_username`}}\" , \"ssh_timeout\" : \"20m\" , \"ssh_pty\" : \"true\" , \"ami_name\" : \"{{user `ami_name`}}\" , \"subnet_id\" : \"{{user `subnet_id`}}\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"amzn2-ami-hvm-2.0.*-x86_64-gp2*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"amazon\" ], \"most_recent\" : true }, \"tags\" : { \"Name\" : \"{{user `ami_name`}}\" , \"BuiltBy\" : \"Packer\" } } ], \"description\" : \"AWS Bastion AMI\" , \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"sudo yum update -y\" , \"sudo hostnamectl set-hostname bastion\" ] } ] } Docker \u00b6 { \"variables\" : { \"repository\" : \"la/express\" , \"tag\" : \"0.1.0\" }, \"builders\" : [ { \"type\" : \"docker\" , \"author\" : \"Fabio Santos\" , \"image\" : \"node\" , \"commit\" : \"true\" , \"changes\" : [ \"EXPOSE 3000\" ] } ], \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"apt-get update -y && apt-get install curl -y\" , \"mkdir -p /var/code\" , \"cd /root\" , \"curl -L https://github.com/linuxacademy/content-nodejs-hello-world/archive/v1.0.tar.gz -o code.tar.gz\" , \"tar zxvf code.tar.gz -C /var/code --strip-components=1\" , \"cd /var/code\" , \"npm install\" ] } ], \"post-processors\" : [ { \"type\" : \"docker-tag\" , \"repository\" : \"{{user `repository`}}\" , \"tag\" : \"{{user `tag`}}\" } ] } Vsphere \u00b6 { \"variables\" : { \"vcenter\" : \"\" , \"username\" : \"\" , \"password\" : \"\" , \"insecure\" : \"true\" , \"datacenter\" : \"\" , \"cluster\" : \"\" , \"host\" : \"\" , \"vm_name\" : \"\" , \"datastore\" : \"\" , \"cpus\" : \"\" , \"ram\" : \"\" , \"network\" : \"\" , \"guest_os\" : \"\" , \"disk_size\" : \"\" , \"iso\" : \"\" , \"ip\" : \"\" , \"netmask\" : \"\" , \"gateway\" : \"\" , \"localIp\" : \"\" }, \"sensitive-variables\" : [ \"password\" ], \"builders\" : [ { \"type\" : \"vsphere-iso\" , \"vcenter_server\" : \"{{user `vcenter`}}\" , \"username\" : \"{{user `username`}}\" , \"password\" : \"{{user `password`}}\" , \"insecure_connection\" : \"{{user `insecure`}}\" , \"datacenter\" : \"{{user `datacenter`}}\" , \"cluster\" : \"{{user `cluster`}}\" , \"host\" : \"{{user `host`}}\" , \"datastore\" : \"{{user `datastore`}}\" , \"vm_name\" : \"{{user `vm_name`}}\" , \"CPUs\" : \"{{user `cpus`}}\" , \"ram\" : \"{{user `ram`}}\" , \"network\" : \"{{user `network`}}\" , \"network_card\" : \"vmxnet3\" , \"guest_os_type\" : \"{{user `guest_os`}}\" , \"disk_size\" : \"{{user `disk_size`}}\" , \"iso_paths\" : [ \"{{user `iso`}}\" ], \"cdrom_type\" : \"sata\" , \"ssh_username\" : \"jenkins\" , \"ssh_password\" : \"jenkins\" , \"http_directory\" : \"http\" , \"boot_wait\" : \"5s\" , \"boot_command\" : [ \"<up><wait><tab> ip={{user `ip`}} netmask={{user `netmask`}} gateway={{user `gateway`}} text ks=http://{{user `localIp`}}:{{ .HTTPPort}}/ks.cfg <enter>\" ] } ], \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"dnf -y update\" , \"dnf install -y java-1.8.0-openjdk-devel\" , \"sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo\" , \"sudo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key\" , \"sudo yum install jenkins -y\" ] } ] }","title":"Packer"},{"location":"devops/packer/#install","text":"wget https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip unzip packer_1.4.2_linux_amd64.zip rm packer_1.4.2_linux_amd64.zip sudo mv packer /usr/bin/ packer -v","title":"Install"},{"location":"devops/packer/#commands","text":"packer validate packer.json packer build -var 'tag=0.0.1' -var 'another=212' packer.json # Not Tested it packer fix packer validate","title":"Commands"},{"location":"devops/packer/#templates","text":"packer.json","title":"Templates"},{"location":"devops/packer/#ami","text":"{ \"variables\" : { \"subnet_id\" : \"\" , \"instance_size\" : \"t2.micro\" , \"ami_name\" : \"bastion\" , \"ssh_username\" : \"ec2-user\" }, \"builders\" : [ { \"type\" : \"amazon-ebs\" , \"instance_type\" : \"{{user `instance_size`}}\" , \"ssh_username\" : \"{{user `ssh_username`}}\" , \"ssh_timeout\" : \"20m\" , \"ssh_pty\" : \"true\" , \"ami_name\" : \"{{user `ami_name`}}\" , \"subnet_id\" : \"{{user `subnet_id`}}\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"amzn2-ami-hvm-2.0.*-x86_64-gp2*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"amazon\" ], \"most_recent\" : true }, \"tags\" : { \"Name\" : \"{{user `ami_name`}}\" , \"BuiltBy\" : \"Packer\" } } ], \"description\" : \"AWS Bastion AMI\" , \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"sudo yum update -y\" , \"sudo hostnamectl set-hostname bastion\" ] } ] }","title":"AMI"},{"location":"devops/packer/#docker","text":"{ \"variables\" : { \"repository\" : \"la/express\" , \"tag\" : \"0.1.0\" }, \"builders\" : [ { \"type\" : \"docker\" , \"author\" : \"Fabio Santos\" , \"image\" : \"node\" , \"commit\" : \"true\" , \"changes\" : [ \"EXPOSE 3000\" ] } ], \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"apt-get update -y && apt-get install curl -y\" , \"mkdir -p /var/code\" , \"cd /root\" , \"curl -L https://github.com/linuxacademy/content-nodejs-hello-world/archive/v1.0.tar.gz -o code.tar.gz\" , \"tar zxvf code.tar.gz -C /var/code --strip-components=1\" , \"cd /var/code\" , \"npm install\" ] } ], \"post-processors\" : [ { \"type\" : \"docker-tag\" , \"repository\" : \"{{user `repository`}}\" , \"tag\" : \"{{user `tag`}}\" } ] }","title":"Docker"},{"location":"devops/packer/#vsphere","text":"{ \"variables\" : { \"vcenter\" : \"\" , \"username\" : \"\" , \"password\" : \"\" , \"insecure\" : \"true\" , \"datacenter\" : \"\" , \"cluster\" : \"\" , \"host\" : \"\" , \"vm_name\" : \"\" , \"datastore\" : \"\" , \"cpus\" : \"\" , \"ram\" : \"\" , \"network\" : \"\" , \"guest_os\" : \"\" , \"disk_size\" : \"\" , \"iso\" : \"\" , \"ip\" : \"\" , \"netmask\" : \"\" , \"gateway\" : \"\" , \"localIp\" : \"\" }, \"sensitive-variables\" : [ \"password\" ], \"builders\" : [ { \"type\" : \"vsphere-iso\" , \"vcenter_server\" : \"{{user `vcenter`}}\" , \"username\" : \"{{user `username`}}\" , \"password\" : \"{{user `password`}}\" , \"insecure_connection\" : \"{{user `insecure`}}\" , \"datacenter\" : \"{{user `datacenter`}}\" , \"cluster\" : \"{{user `cluster`}}\" , \"host\" : \"{{user `host`}}\" , \"datastore\" : \"{{user `datastore`}}\" , \"vm_name\" : \"{{user `vm_name`}}\" , \"CPUs\" : \"{{user `cpus`}}\" , \"ram\" : \"{{user `ram`}}\" , \"network\" : \"{{user `network`}}\" , \"network_card\" : \"vmxnet3\" , \"guest_os_type\" : \"{{user `guest_os`}}\" , \"disk_size\" : \"{{user `disk_size`}}\" , \"iso_paths\" : [ \"{{user `iso`}}\" ], \"cdrom_type\" : \"sata\" , \"ssh_username\" : \"jenkins\" , \"ssh_password\" : \"jenkins\" , \"http_directory\" : \"http\" , \"boot_wait\" : \"5s\" , \"boot_command\" : [ \"<up><wait><tab> ip={{user `ip`}} netmask={{user `netmask`}} gateway={{user `gateway`}} text ks=http://{{user `localIp`}}:{{ .HTTPPort}}/ks.cfg <enter>\" ] } ], \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"dnf -y update\" , \"dnf install -y java-1.8.0-openjdk-devel\" , \"sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo\" , \"sudo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key\" , \"sudo yum install jenkins -y\" ] } ] }","title":"Vsphere"},{"location":"devops/terraform/","text":"Terraform \u00b6 Commands \u00b6 Workspaces \u00b6 terraform workspace list Shell Script Problem Link # Use like this. With quotes echo \" $( terraform workspace list ) \"","title":"Terraform"},{"location":"devops/terraform/#terraform","text":"","title":"Terraform"},{"location":"devops/terraform/#commands","text":"","title":"Commands"},{"location":"devops/terraform/#workspaces","text":"terraform workspace list Shell Script Problem Link # Use like this. With quotes echo \" $( terraform workspace list ) \"","title":"Workspaces"},{"location":"platforms/aws/compute/ebs/","text":"Resize Root Volumes \u00b6 Manual method Create a snapshot of the current root volume Create a new volume from the snapshot with new storage specifications Select larger \"size\", thus increasing IOPS Must select the same availability zone as the source Stop the Instance Detach the original root volume Attach the new volume to the instance at same mount point (xvda) \"Automated\" method We can replace the launch configuration of an Auto Scaling Group In a true 3-tier application that is decoupled, we should then be able to terminate instances one by one to recreate them using new configuration Note: If the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity Monitoring \u00b6 VolumeReadOps Total number of IOPS in a specific period of time. To 1000 in 1 minute = 1000/60 IOPS - Change IOPS based off of usage. VolumeWriteOps Total number of IOPS in a specific period of time . TO 1000 in 1 minute = 1000/60 iops - Change IOPS based off of usage VolumeQueueLength The number of read/write operation requests waiting to process. We want close at 0. If higher than 0, increase the number of available IOPS VolumeReadBytes VolumeWriteBytes VolumeTotalReadTime VolumeTotalWriteTime VolumeIdleTime VolumeThroughputPercentage Provisioned IOPS SSD only *Determine the percentage of I/O operations delivered of the total IOPS provisioned for an EBS volume. During a write, it there are no other pending I/O requests in a minute, the metric value will be 100 percent. VolumeConsumedReadWriteOps Provisioned IOPS only BurstBalance (gp2, st1, and sc1 volumes only) Reach IOPS limit of your volume will result in: Will start to get your IO requests queuing Depending on your applications sensitivity to IOPS and latency, you may see your applications becaming slow","title":"EBS"},{"location":"platforms/aws/compute/ebs/#resize-root-volumes","text":"Manual method Create a snapshot of the current root volume Create a new volume from the snapshot with new storage specifications Select larger \"size\", thus increasing IOPS Must select the same availability zone as the source Stop the Instance Detach the original root volume Attach the new volume to the instance at same mount point (xvda) \"Automated\" method We can replace the launch configuration of an Auto Scaling Group In a true 3-tier application that is decoupled, we should then be able to terminate instances one by one to recreate them using new configuration Note: If the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity","title":"Resize Root Volumes"},{"location":"platforms/aws/compute/ebs/#monitoring","text":"VolumeReadOps Total number of IOPS in a specific period of time. To 1000 in 1 minute = 1000/60 IOPS - Change IOPS based off of usage. VolumeWriteOps Total number of IOPS in a specific period of time . TO 1000 in 1 minute = 1000/60 iops - Change IOPS based off of usage VolumeQueueLength The number of read/write operation requests waiting to process. We want close at 0. If higher than 0, increase the number of available IOPS VolumeReadBytes VolumeWriteBytes VolumeTotalReadTime VolumeTotalWriteTime VolumeIdleTime VolumeThroughputPercentage Provisioned IOPS SSD only *Determine the percentage of I/O operations delivered of the total IOPS provisioned for an EBS volume. During a write, it there are no other pending I/O requests in a minute, the metric value will be 100 percent. VolumeConsumedReadWriteOps Provisioned IOPS only BurstBalance (gp2, st1, and sc1 volumes only) Reach IOPS limit of your volume will result in: Will start to get your IO requests queuing Depending on your applications sensitivity to IOPS and latency, you may see your applications becaming slow","title":"Monitoring"},{"location":"platforms/aws/compute/ec2/","text":"Restore Snapshots \u00b6 Read all blocks to eliminate penalty in production sudo dd if = /dev/xvdf of = /dev/null bs = 1M # Or sudo fio \u2013filename = /dev/xvdf \u2013rw = read \u2013bs = 128k \u2013iodepth = 32 \u2013ioengine = libaio \u2013direct = 1 \u2013name = volume-initialize Configure SSH Config \u00b6 Proxy SSH Recover Instance \u00b6 Recover Instance without console access How To Repair An AWS EC2 Instance Without Console Duplicate XFS UUIDs","title":"EC2"},{"location":"platforms/aws/compute/ec2/#restore-snapshots","text":"Read all blocks to eliminate penalty in production sudo dd if = /dev/xvdf of = /dev/null bs = 1M # Or sudo fio \u2013filename = /dev/xvdf \u2013rw = read \u2013bs = 128k \u2013iodepth = 32 \u2013ioengine = libaio \u2013direct = 1 \u2013name = volume-initialize","title":"Restore Snapshots"},{"location":"platforms/aws/compute/ec2/#configure-ssh-config","text":"Proxy SSH","title":"Configure SSH Config"},{"location":"platforms/aws/compute/ec2/#recover-instance","text":"Recover Instance without console access How To Repair An AWS EC2 Instance Without Console Duplicate XFS UUIDs","title":"Recover Instance"},{"location":"platforms/aws/compute/eks/","text":"Eks \u00b6 KubeConfig \u00b6 Create KubeConfig Users/Roles \u00b6 New User/Role Expose Services \u00b6 Expose","title":"EKS"},{"location":"platforms/aws/compute/eks/#eks","text":"","title":"Eks"},{"location":"platforms/aws/compute/eks/#kubeconfig","text":"Create KubeConfig","title":"KubeConfig"},{"location":"platforms/aws/compute/eks/#usersroles","text":"New User/Role","title":"Users/Roles"},{"location":"platforms/aws/compute/eks/#expose-services","text":"Expose","title":"Expose Services"},{"location":"platforms/aws/compute/elb/","text":"Monitoring \u00b6 ActiveConnectionCount HealthyHostCount, UnHealthyHostCount, HostCount BackendConnectionErrors The count of the number of connections that were NOT successfully established between the Load Balancer and the registered instance Average stat is most useful Will report errors for all AZ's Could indicate an issue with the web server HTTPCode_Backend_2XX,3XX,4XX,5XX Generated by registered instances HTTPCode_ELB_4XX,5XX Generated by Load Balancer ActiveFlowCount (Network Load Balancer) Metrics for performance Latency Measures the time elapsed (in seconds) after the request leaves the Load Balancer until the response is received Average stat is most useful Will report latency for all AZs Page Load Time RequestCount Number of requests completed/connections made during specified interval (1 or 5 mins) SurgeQueueLenght (Classic only) A count of the total number of requests that are pending submissions to a registered instance Max queue size is 1024 Additional requests will be rejected Scaling up instances to ensure that never increases beyond the maximum queue capacity SpilloverCount (Classic only) Number of requests rejected because the surge queue is full Direct consequence of a surge queue length increase Should monitor SpilloverCount and SurgeQueueLenght High numbers in these metrics can indicate a performance issue, need to scale infrastructure, etc","title":"ELB"},{"location":"platforms/aws/compute/elb/#monitoring","text":"ActiveConnectionCount HealthyHostCount, UnHealthyHostCount, HostCount BackendConnectionErrors The count of the number of connections that were NOT successfully established between the Load Balancer and the registered instance Average stat is most useful Will report errors for all AZ's Could indicate an issue with the web server HTTPCode_Backend_2XX,3XX,4XX,5XX Generated by registered instances HTTPCode_ELB_4XX,5XX Generated by Load Balancer ActiveFlowCount (Network Load Balancer) Metrics for performance Latency Measures the time elapsed (in seconds) after the request leaves the Load Balancer until the response is received Average stat is most useful Will report latency for all AZs Page Load Time RequestCount Number of requests completed/connections made during specified interval (1 or 5 mins) SurgeQueueLenght (Classic only) A count of the total number of requests that are pending submissions to a registered instance Max queue size is 1024 Additional requests will be rejected Scaling up instances to ensure that never increases beyond the maximum queue capacity SpilloverCount (Classic only) Number of requests rejected because the surge queue is full Direct consequence of a surge queue length increase Should monitor SpilloverCount and SurgeQueueLenght High numbers in these metrics can indicate a performance issue, need to scale infrastructure, etc","title":"Monitoring"},{"location":"platforms/aws/databases/dynamodb/","text":"Monitoring \u00b6 If an application\u2019s read or write requests exceed the provisioned throughput for a table, Amazon DynamoDB might throttle that request. When this happens, the request fails with an HTTP 400 code (Bad Request), accompanied by a ProvisionedThroughputExceededException High Availability \u00b6 Scalability Unlimited amount of storage Elasticity Increase additional IOPS for additional spikes in traffic. Drecrease that IOPS after spike We can increase or decrease read and write throughput capacity on demand As read requests increase, we can increase read throughput capacity As read requests slow down, we can decrease capacity","title":"DynamoDB"},{"location":"platforms/aws/databases/dynamodb/#monitoring","text":"If an application\u2019s read or write requests exceed the provisioned throughput for a table, Amazon DynamoDB might throttle that request. When this happens, the request fails with an HTTP 400 code (Bad Request), accompanied by a ProvisionedThroughputExceededException","title":"Monitoring"},{"location":"platforms/aws/databases/dynamodb/#high-availability","text":"Scalability Unlimited amount of storage Elasticity Increase additional IOPS for additional spikes in traffic. Drecrease that IOPS after spike We can increase or decrease read and write throughput capacity on demand As read requests increase, we can increase read throughput capacity As read requests slow down, we can decrease capacity","title":"High Availability"},{"location":"platforms/aws/databases/elasticache/","text":"Monitoring \u00b6 CPU Utilization \u00b6 MemCached Multi-threaded Can handle loads of up to 90%. if it exceeds 90% Increase the node family or add more nodes to cluster or read replicas Redis Not multi-threaded. To determine the point in which to scale, take 90 and divide by the number of cores Example: if you have 4 cpus, the threshold would be ( 90 / 4 = 22.5%) if this threshold is exceeded and the main workload is from read requests, scale the cache cluster out by adding read replicas. If the main workload is from write requests, AWS recommends scaling up by using a larger cache instance type Evictions \u00b6 An Eviction occurs when a new item is added and an old item must be removed due to lack of free space in the system Older items are removed to free up memory for new items Frequent evictions will decrease performance - Increase the node size (memory) No recommended setting. Choose the threshold based off of application requirements Memcached Scale UP ( increase the memory of existing nodes) OR Scale OUT ( add more nodes) Redis Scale Out ( add more read replicas) Swap Usage \u00b6 Swap usage is simply the amount of the Swap file that is used. Swap file (or paging file) is the amount of disk storage space reserved on disk if your computer runs out of ram. Typically the size of swap file = the size of the RAM. So if you have 4GB of RAM, you will have 4GB SWAP file Memcached Affects performance if increased Should be close to 0 (should not exceed 50mb) If this exceeds 50 Mb you should increase the memcached_connections_overhead parameter Defines the amount of memory to be reserved for connections and other misc/overhead. Redis AWS suggest not setting a cloudwatch alarm as there is no suggested \"web service\" fix No SwapUsage metric, instead use reserved-memory CurrConnections \u00b6 MemCached & Redis No recommended setting, Chooses threshold based off of application requirements Increased CurrConnections could indicate a larger issue with your application OR the ELB may not be releasing connections (tied down) or is a large and sustained spike in the number of concurrent connections Remember to set an alarm on the number of concurrent connections for elasticcache","title":"Elasticache"},{"location":"platforms/aws/databases/elasticache/#monitoring","text":"","title":"Monitoring"},{"location":"platforms/aws/databases/elasticache/#cpu-utilization","text":"MemCached Multi-threaded Can handle loads of up to 90%. if it exceeds 90% Increase the node family or add more nodes to cluster or read replicas Redis Not multi-threaded. To determine the point in which to scale, take 90 and divide by the number of cores Example: if you have 4 cpus, the threshold would be ( 90 / 4 = 22.5%) if this threshold is exceeded and the main workload is from read requests, scale the cache cluster out by adding read replicas. If the main workload is from write requests, AWS recommends scaling up by using a larger cache instance type","title":"CPU Utilization"},{"location":"platforms/aws/databases/elasticache/#evictions","text":"An Eviction occurs when a new item is added and an old item must be removed due to lack of free space in the system Older items are removed to free up memory for new items Frequent evictions will decrease performance - Increase the node size (memory) No recommended setting. Choose the threshold based off of application requirements Memcached Scale UP ( increase the memory of existing nodes) OR Scale OUT ( add more nodes) Redis Scale Out ( add more read replicas)","title":"Evictions"},{"location":"platforms/aws/databases/elasticache/#swap-usage","text":"Swap usage is simply the amount of the Swap file that is used. Swap file (or paging file) is the amount of disk storage space reserved on disk if your computer runs out of ram. Typically the size of swap file = the size of the RAM. So if you have 4GB of RAM, you will have 4GB SWAP file Memcached Affects performance if increased Should be close to 0 (should not exceed 50mb) If this exceeds 50 Mb you should increase the memcached_connections_overhead parameter Defines the amount of memory to be reserved for connections and other misc/overhead. Redis AWS suggest not setting a cloudwatch alarm as there is no suggested \"web service\" fix No SwapUsage metric, instead use reserved-memory","title":"Swap Usage"},{"location":"platforms/aws/databases/elasticache/#currconnections","text":"MemCached & Redis No recommended setting, Chooses threshold based off of application requirements Increased CurrConnections could indicate a larger issue with your application OR the ELB may not be releasing connections (tied down) or is a large and sustained spike in the number of concurrent connections Remember to set an alarm on the number of concurrent connections for elasticcache","title":"CurrConnections"},{"location":"platforms/aws/databases/rds/","text":"Monitoring \u00b6 BinLogDiskUsage Amount of disk space occupied by binary logs on the primary database. Applies to MySQL Read Replicas and is measured in bytes DatabaseConnections Number of database connections in use DiskQueueDepth The number of outstanding read and write requests waiting to access the disk FreeStorageSpace Amount of available storage space FreeableMemory Amount of available RAM NetworkReceiveThroughput Inbound network traffic on the DB instance that includes both customer database traffic and Amazon RDS traffic used for monitoring and replication NetworkTransmitThroughput Outbound network traffic on the DB instance that includes both customer database traffic and Amazon RDS traffic used for monitoring and replication ReadIOPS/WriteIOPS Average number of disk I/O operations per second Use this to determine storage type changes ReadLatency /WriteLatency Average amount of time taken per disk I/O operation More IOPS needed ReadThroughput/WriteThroughput Average number of bytes read from disk per second ReplicaLag Amount of time a Read Replica DB instance lags behind the source DB instance. Applies to MySQL, MariaDB, and PostgreSQL Read Replicas SwapUsage Amount of swap space used on the DB instance If increase - low or no available ram High CPU or RAM consumption High values for CPU or RAM consumption might be appropriate, provided that they are within an expected range. Disk space consumption Investigate disk space consumption if there is consistently less than 15 percent of available free space Network traffic Investigate network traffic if throughput is consistently lower than expected Database connections If an increase of usage causes performance issues, consider limiting the number of available database connections. The best number of user connections for a DB instance will depend on the instance class and the complexity of the operations performed. IOPS metrics The expected values for IOPS metrics depend on disk specification and server configuration. Establish a baseline to know what utilization is typical. For best IOPS performance, make sure that the typical working set will fit into memory to minimize read and write operations. Aurora 100% CPU utilization Is it writes causing the issue? if so scale up (increase instance size) Is it reads causing the issue? If so scale out (increase the number of read replicas)","title":"RDS"},{"location":"platforms/aws/databases/rds/#monitoring","text":"BinLogDiskUsage Amount of disk space occupied by binary logs on the primary database. Applies to MySQL Read Replicas and is measured in bytes DatabaseConnections Number of database connections in use DiskQueueDepth The number of outstanding read and write requests waiting to access the disk FreeStorageSpace Amount of available storage space FreeableMemory Amount of available RAM NetworkReceiveThroughput Inbound network traffic on the DB instance that includes both customer database traffic and Amazon RDS traffic used for monitoring and replication NetworkTransmitThroughput Outbound network traffic on the DB instance that includes both customer database traffic and Amazon RDS traffic used for monitoring and replication ReadIOPS/WriteIOPS Average number of disk I/O operations per second Use this to determine storage type changes ReadLatency /WriteLatency Average amount of time taken per disk I/O operation More IOPS needed ReadThroughput/WriteThroughput Average number of bytes read from disk per second ReplicaLag Amount of time a Read Replica DB instance lags behind the source DB instance. Applies to MySQL, MariaDB, and PostgreSQL Read Replicas SwapUsage Amount of swap space used on the DB instance If increase - low or no available ram High CPU or RAM consumption High values for CPU or RAM consumption might be appropriate, provided that they are within an expected range. Disk space consumption Investigate disk space consumption if there is consistently less than 15 percent of available free space Network traffic Investigate network traffic if throughput is consistently lower than expected Database connections If an increase of usage causes performance issues, consider limiting the number of available database connections. The best number of user connections for a DB instance will depend on the instance class and the complexity of the operations performed. IOPS metrics The expected values for IOPS metrics depend on disk specification and server configuration. Establish a baseline to know what utilization is typical. For best IOPS performance, make sure that the typical working set will fit into memory to minimize read and write operations. Aurora 100% CPU utilization Is it writes causing the issue? if so scale up (increase instance size) Is it reads causing the issue? If so scale out (increase the number of read replicas)","title":"Monitoring"},{"location":"platforms/aws/databases/redshift/","text":"Monitoring \u00b6 Database Audit logging Events and Notifications Performance CPU utilization Latency Throughput Redshift also provides query and load performance data to help monitor database activity in a cluster","title":"Redshift"},{"location":"platforms/aws/databases/redshift/#monitoring","text":"Database Audit logging Events and Notifications Performance CPU utilization Latency Throughput Redshift also provides query and load performance data to help monitor database activity in a cluster","title":"Monitoring"},{"location":"platforms/aws/devops/cloudformation/","text":"CDK \u00b6 Workshop app.py \u00b6 app = core . App () SecurityGroupsStack ( app , \"security-groups\" , env = { 'account' : os . environ [ 'CDK_DEFAULT_ACCOUNT' ], 'region' : os . environ [ 'CDK_DEFAULT_REGION' ] }) app . synth () Stack.py \u00b6 # Reference a outside VPC vpc = ec2 . Vpc . from_lookup ( self , id = \"VPC\" , vpc_id = config [ 'VPC_ID' ]) CloudFormation \u00b6 Deploy \u00b6 aws cloudformation package --template-file template.yaml --s3-bucket ${ bucket } --output-template-file output-template.yaml aws cloudformation deploy --template-file output-template.yaml --capabilities CAPABILITY_IAM --stack-name ${ stack_name } Delete Stack \u00b6 aws cloudformation delete-stack --stack-name ${ stack_name } Init \u00b6 Samples Logs are stored in: /var/log/cfn-init* /var/log/cloud-init* AWSTemplateFormatVersion : \"2010-09-09\" Description : Cloud Formation Template to create Jenkins Cluster Parameters : VPC : Type : AWS::EC2::VPC::Id MasterSubnet : Type : AWS::EC2::Subnet::Id MasterInstanceType : Type : String Default : t2.micro KeyPair : Type : AWS::EC2::KeyPair::KeyName Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Network Configuration\" Parameters : - VPC - Label : default : \"Master Jenkins Configuration\" Parameters : - MasterSubnet - MasterInstanceType - Label : default : \"Common Configuration\" Parameters : - KeyPair ParameterLabels : VPC : default : \"Which VPC should this be deployed to?\" MasterSubnet : default : \"Which Subnet should this be deployed to?\" MasterInstanceType : default : \"Which Instance type should this use?\" KeyPair : default : \"Which KeyPair should this use?\" Mappings : AMI : eu-west-1 : \"HVM64\" : \"ami-04d5cc9b88f9d1d39\" Resources : Master : Type : AWS::EC2::Instance Metadata : AWS::CloudFormation::Init : configSets : full_install : - prepare - install prepare : files : /etc/yum.repos.d/jenkins.repo : source : http://pkg.jenkins-ci.org/redhat/jenkins.repo commands : jenkins-import-key : command : rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key test : rpm -q gpg-pubkey-d50582e6-4a3feef6 | grep \"not installed\" install : packages : yum : java-1.8.0-openjdk-devel : [] jenkins : [] services : sysvinit : jenkins : enabled : \"true\" ensureRunning : \"true\" Properties : ImageId : !FindInMap [ AMI , !Ref \"AWS::Region\" , HVM64 ] InstanceType : !Ref MasterInstanceType KeyName : !Ref KeyPair SubnetId : !Ref MasterSubnet UserData : 'Fn::Base64' : !Sub | #!/bin/bash -x yum update -y /opt/aws/bin/cfn-init -v -c full_install --stack ${AWS::StackName} --resource Master --region ${AWS::Region} /opt/aws/bin/cfn-signal -e 0 --stack ${AWS::StackName} --resource Master --region ${AWS::Region} Tags : - Key : Name Value : Jenkins-Master","title":"Cloudformation"},{"location":"platforms/aws/devops/cloudformation/#cdk","text":"Workshop","title":"CDK"},{"location":"platforms/aws/devops/cloudformation/#apppy","text":"app = core . App () SecurityGroupsStack ( app , \"security-groups\" , env = { 'account' : os . environ [ 'CDK_DEFAULT_ACCOUNT' ], 'region' : os . environ [ 'CDK_DEFAULT_REGION' ] }) app . synth ()","title":"app.py"},{"location":"platforms/aws/devops/cloudformation/#stackpy","text":"# Reference a outside VPC vpc = ec2 . Vpc . from_lookup ( self , id = \"VPC\" , vpc_id = config [ 'VPC_ID' ])","title":"Stack.py"},{"location":"platforms/aws/devops/cloudformation/#cloudformation","text":"","title":"CloudFormation"},{"location":"platforms/aws/devops/cloudformation/#deploy","text":"aws cloudformation package --template-file template.yaml --s3-bucket ${ bucket } --output-template-file output-template.yaml aws cloudformation deploy --template-file output-template.yaml --capabilities CAPABILITY_IAM --stack-name ${ stack_name }","title":"Deploy"},{"location":"platforms/aws/devops/cloudformation/#delete-stack","text":"aws cloudformation delete-stack --stack-name ${ stack_name }","title":"Delete Stack"},{"location":"platforms/aws/devops/cloudformation/#init","text":"Samples Logs are stored in: /var/log/cfn-init* /var/log/cloud-init* AWSTemplateFormatVersion : \"2010-09-09\" Description : Cloud Formation Template to create Jenkins Cluster Parameters : VPC : Type : AWS::EC2::VPC::Id MasterSubnet : Type : AWS::EC2::Subnet::Id MasterInstanceType : Type : String Default : t2.micro KeyPair : Type : AWS::EC2::KeyPair::KeyName Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Network Configuration\" Parameters : - VPC - Label : default : \"Master Jenkins Configuration\" Parameters : - MasterSubnet - MasterInstanceType - Label : default : \"Common Configuration\" Parameters : - KeyPair ParameterLabels : VPC : default : \"Which VPC should this be deployed to?\" MasterSubnet : default : \"Which Subnet should this be deployed to?\" MasterInstanceType : default : \"Which Instance type should this use?\" KeyPair : default : \"Which KeyPair should this use?\" Mappings : AMI : eu-west-1 : \"HVM64\" : \"ami-04d5cc9b88f9d1d39\" Resources : Master : Type : AWS::EC2::Instance Metadata : AWS::CloudFormation::Init : configSets : full_install : - prepare - install prepare : files : /etc/yum.repos.d/jenkins.repo : source : http://pkg.jenkins-ci.org/redhat/jenkins.repo commands : jenkins-import-key : command : rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key test : rpm -q gpg-pubkey-d50582e6-4a3feef6 | grep \"not installed\" install : packages : yum : java-1.8.0-openjdk-devel : [] jenkins : [] services : sysvinit : jenkins : enabled : \"true\" ensureRunning : \"true\" Properties : ImageId : !FindInMap [ AMI , !Ref \"AWS::Region\" , HVM64 ] InstanceType : !Ref MasterInstanceType KeyName : !Ref KeyPair SubnetId : !Ref MasterSubnet UserData : 'Fn::Base64' : !Sub | #!/bin/bash -x yum update -y /opt/aws/bin/cfn-init -v -c full_install --stack ${AWS::StackName} --resource Master --region ${AWS::Region} /opt/aws/bin/cfn-signal -e 0 --stack ${AWS::StackName} --resource Master --region ${AWS::Region} Tags : - Key : Name Value : Jenkins-Master","title":"Init"},{"location":"platforms/aws/devops/cloudwatch/","text":"Filter Events \u00b6 { ( ( $.eventSource = \"iam.amazonaws.com\" ) && ( $.requestParameters.userName = \"sceptre\" ) ) } { ( $.eventSource = \"iam.amazonaws.com\" ) } { ( ( $.eventSource = \"iam.amazonaws.com\" ) && (( $.eventName = \"Add*\" ) || ( $.eventName = \"Attach*\" ) || ( $.eventName = \"Change*\" ) || ( $.eventName = \"Create*\" ) || ( $.eventName = \"Deactivate*\" ) || ( $.eventName = \"Delete*\" ) || ( $.eventName = \"Detach*\" ) || ( $.eventName = \"Enable*\" ) || ( $.eventName = \"Put*\" ) || ( $.eventName = \"Remove*\" ) || ( $.eventName = \"Set*\" ) || ( $.eventName = \"Update*\" ) || ( $.eventName = \"Upload*\" )) ) } { ( ( $.eventSource = \"iam.amazonaws.com\" ) && (( $.eventName = \"Put*Policy\" ) || ( $.eventName = \"Attach*\" ) || ( $.eventName = \"Detach*\" ) || ( $.eventName = \"Create*\" ) || ( $.eventName = \"Update*\" ) || ( $.eventName = \"Upload*\" ) || ( $.eventName = \"Delete*\" ) || ( $.eventName = \"Remove*\" ) || ( $.eventName = \"Set*\" )) ) }","title":"CloudWatch"},{"location":"platforms/aws/devops/cloudwatch/#filter-events","text":"{ ( ( $.eventSource = \"iam.amazonaws.com\" ) && ( $.requestParameters.userName = \"sceptre\" ) ) } { ( $.eventSource = \"iam.amazonaws.com\" ) } { ( ( $.eventSource = \"iam.amazonaws.com\" ) && (( $.eventName = \"Add*\" ) || ( $.eventName = \"Attach*\" ) || ( $.eventName = \"Change*\" ) || ( $.eventName = \"Create*\" ) || ( $.eventName = \"Deactivate*\" ) || ( $.eventName = \"Delete*\" ) || ( $.eventName = \"Detach*\" ) || ( $.eventName = \"Enable*\" ) || ( $.eventName = \"Put*\" ) || ( $.eventName = \"Remove*\" ) || ( $.eventName = \"Set*\" ) || ( $.eventName = \"Update*\" ) || ( $.eventName = \"Upload*\" )) ) } { ( ( $.eventSource = \"iam.amazonaws.com\" ) && (( $.eventName = \"Put*Policy\" ) || ( $.eventName = \"Attach*\" ) || ( $.eventName = \"Detach*\" ) || ( $.eventName = \"Create*\" ) || ( $.eventName = \"Update*\" ) || ( $.eventName = \"Upload*\" ) || ( $.eventName = \"Delete*\" ) || ( $.eventName = \"Remove*\" ) || ( $.eventName = \"Set*\" )) ) }","title":"Filter Events"},{"location":"platforms/aws/migrate/vmware/","text":"VMWare \u00b6 VM Import \u00b6 AWS Link Create Ova File \u00b6 Install ovf tool ovftool Build the image with the new version bundle file downloaded from VMWare site docker run --rm -it -v $( pwd ) :/tmp ovftool --noSSLVerify vi://<user>@<ip>/LAB/vm/aws /tmp/aws.ova AWS PreRequisites \u00b6 Create vmimport role Documentation The documentation has a error in trust policy \"sts:Externalid\": \"vmimport\" should be \"sts:ExternalId\": \"vmimport\" Error explain Upload to S3 Use Multipart Upload MultiPart Upload Import Image \u00b6 aws ec2 import-image --description \"My server VM\" --disk-containers \"file://containers.json\" --region eu-west-1 # Monitor aws ec2 describe-import-image-tasks --import-task-ids ${ TASK_ID } --region eu-west-1 Containers.json file [ { \"Description\" : \"My Server OVA\" , \"Format\" : \"ova\" , \"UserBucket\" : { \"S3Bucket\" : \"fsantos-vmware-ova-templates\" , \"S3Key\" : \"aws.ova\" } }]","title":"VmWare"},{"location":"platforms/aws/migrate/vmware/#vmware","text":"","title":"VMWare"},{"location":"platforms/aws/migrate/vmware/#vm-import","text":"AWS Link","title":"VM Import"},{"location":"platforms/aws/migrate/vmware/#create-ova-file","text":"Install ovf tool ovftool Build the image with the new version bundle file downloaded from VMWare site docker run --rm -it -v $( pwd ) :/tmp ovftool --noSSLVerify vi://<user>@<ip>/LAB/vm/aws /tmp/aws.ova","title":"Create Ova File"},{"location":"platforms/aws/migrate/vmware/#aws-prerequisites","text":"Create vmimport role Documentation The documentation has a error in trust policy \"sts:Externalid\": \"vmimport\" should be \"sts:ExternalId\": \"vmimport\" Error explain Upload to S3 Use Multipart Upload MultiPart Upload","title":"AWS PreRequisites"},{"location":"platforms/aws/migrate/vmware/#import-image","text":"aws ec2 import-image --description \"My server VM\" --disk-containers \"file://containers.json\" --region eu-west-1 # Monitor aws ec2 describe-import-image-tasks --import-task-ids ${ TASK_ID } --region eu-west-1 Containers.json file [ { \"Description\" : \"My Server OVA\" , \"Format\" : \"ova\" , \"UserBucket\" : { \"S3Bucket\" : \"fsantos-vmware-ova-templates\" , \"S3Key\" : \"aws.ova\" } }]","title":"Import Image"},{"location":"platforms/aws/security/acm/","text":"Self-Certificate \u00b6 Create Self-Certificate","title":"ACM"},{"location":"platforms/aws/security/acm/#self-certificate","text":"Create Self-Certificate","title":"Self-Certificate"},{"location":"platforms/aws/serverless/amplify/","text":"AWS Amplify \u00b6 Getting Started \u00b6 # Install Cli npm i -g amplify-cli # Init Project amplify init # Status amplify status # Delete amplify delete # Add Feature amplify add auth amplify add analytics amplify add api amplify configure api amplify update api # Environment amplify env list amplify env add amplify env remove <env> amplify env checkout <env> # Add Library to project npm install aws-amplify # Add framework-specific library npm install aws-amplify-react React Libs \u00b6 yarn add aws-amplify aws-amplify-react # Or npm install aws-amplify aws-amplify-react","title":"Amplify"},{"location":"platforms/aws/serverless/amplify/#aws-amplify","text":"","title":"AWS Amplify"},{"location":"platforms/aws/serverless/amplify/#getting-started","text":"# Install Cli npm i -g amplify-cli # Init Project amplify init # Status amplify status # Delete amplify delete # Add Feature amplify add auth amplify add analytics amplify add api amplify configure api amplify update api # Environment amplify env list amplify env add amplify env remove <env> amplify env checkout <env> # Add Library to project npm install aws-amplify # Add framework-specific library npm install aws-amplify-react","title":"Getting Started"},{"location":"platforms/aws/serverless/amplify/#react-libs","text":"yarn add aws-amplify aws-amplify-react # Or npm install aws-amplify aws-amplify-react","title":"React Libs"},{"location":"platforms/aws/serverless/apigateway/","text":"Api Gateway \u00b6 Integration \u00b6 Step Functions \u00b6 Request \u00b6 { \"input\" : \"$util.escapeJavaScript($input.json('$'))\" , \"stateMachineArn\" : \"${RegisterDeviceStateMachine}\" } Response \u00b6 { \"stateMachineExecution\" : \"$input.json('$.executionArn').split(':')[7].replace('\" ' , \"\" )\" } Models \u00b6 { \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"title\" : \"Request Validator\" , \"type\" : \"object\" , \"properties\" : { \"DeviceID\" : { \"type\" : \"string\" }, \"Name\" : { \"type\" : \"string\" } }, \"required\" : [ \"DeviceID\" , \"Name\" ], \"additionalProperties\" : false }","title":"ApiGateway"},{"location":"platforms/aws/serverless/apigateway/#api-gateway","text":"","title":"Api Gateway"},{"location":"platforms/aws/serverless/apigateway/#integration","text":"","title":"Integration"},{"location":"platforms/aws/serverless/apigateway/#step-functions","text":"","title":"Step Functions"},{"location":"platforms/aws/serverless/apigateway/#request","text":"{ \"input\" : \"$util.escapeJavaScript($input.json('$'))\" , \"stateMachineArn\" : \"${RegisterDeviceStateMachine}\" }","title":"Request"},{"location":"platforms/aws/serverless/apigateway/#response","text":"{ \"stateMachineExecution\" : \"$input.json('$.executionArn').split(':')[7].replace('\" ' , \"\" )\" }","title":"Response"},{"location":"platforms/aws/serverless/apigateway/#models","text":"{ \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"title\" : \"Request Validator\" , \"type\" : \"object\" , \"properties\" : { \"DeviceID\" : { \"type\" : \"string\" }, \"Name\" : { \"type\" : \"string\" } }, \"required\" : [ \"DeviceID\" , \"Name\" ], \"additionalProperties\" : false }","title":"Models"},{"location":"platforms/aws/serverless/cognito/","text":"Change User Status FORCE_CHANGE_PASSWORD \u00b6 Link - StackOverflow Use AWS CLI to change the users password. Get a session token for the desired user: aws cognito-idp admin-initiate-auth --user-pool-id %USER POOL ID% --client-id %APP CLIENT ID% --auth-flow ADMIN_NO_SRP_AUTH --auth-parameters USERNAME = %USERS USERNAME%,PASSWORD = %USERS CURRENT PASSWORD% This will respond with the challenge \"NEW_PASSWORD_REQUIRED\", other challenge parameters and the users session key. Then, run the second command to issue the challenge response: aws cognito-idp admin-respond-to-auth-challenge --user-pool-id %USER POOL ID% --client-id %CLIENT ID% --challenge-name NEW_PASSWORD_REQUIRED --challenge-responses NEW_PASSWORD = %DESIRED PASSWORD%,USERNAME = %USERS USERNAME% --session %SESSION KEY FROM PREVIOUS COMMAND with \"\" %","title":"Cognito"},{"location":"platforms/aws/serverless/cognito/#change-user-status-force_change_password","text":"Link - StackOverflow Use AWS CLI to change the users password. Get a session token for the desired user: aws cognito-idp admin-initiate-auth --user-pool-id %USER POOL ID% --client-id %APP CLIENT ID% --auth-flow ADMIN_NO_SRP_AUTH --auth-parameters USERNAME = %USERS USERNAME%,PASSWORD = %USERS CURRENT PASSWORD% This will respond with the challenge \"NEW_PASSWORD_REQUIRED\", other challenge parameters and the users session key. Then, run the second command to issue the challenge response: aws cognito-idp admin-respond-to-auth-challenge --user-pool-id %USER POOL ID% --client-id %CLIENT ID% --challenge-name NEW_PASSWORD_REQUIRED --challenge-responses NEW_PASSWORD = %DESIRED PASSWORD%,USERNAME = %USERS USERNAME% --session %SESSION KEY FROM PREVIOUS COMMAND with \"\" %","title":"Change User Status FORCE_CHANGE_PASSWORD"},{"location":"platforms/aws/serverless/lambda/","text":"Build \u00b6 Runtime Python 3.6 How to create an AWS Lambda python 3.6 deployment package using Docker sudo docker run -it dacut/amazon-linux-python-3.6 mkdir <DOCKER_PROJEC_DIR> cd <DOCKER_PROJECT_DIR> pip3 install <PACKAGE_NAME> -t ./ zip -r <PROJECT_NAME>.zip * exit sudo docker ps -a | grep \"dacut\" | awk '{print $1}' sudo docker cp <CONTAINER_ID>:<DOCKER_PROJECT_DIR>/<PROJECT_NAME>.zip <LOCAL_PROJECT_PATH> unzip <LOCAL_PROJECT_PATH>/<PROJECT_NAME>.zip rm <LOCAL_PROJECT_PATH>/<PROJECT_NAME>.zip # Add Lambda File to zip zip -ur <PROJECT_NAME>.zip <PATH_TO_LAMBDA_FUNCTION_FILE>","title":"Lambda"},{"location":"platforms/aws/serverless/lambda/#build","text":"Runtime Python 3.6 How to create an AWS Lambda python 3.6 deployment package using Docker sudo docker run -it dacut/amazon-linux-python-3.6 mkdir <DOCKER_PROJEC_DIR> cd <DOCKER_PROJECT_DIR> pip3 install <PACKAGE_NAME> -t ./ zip -r <PROJECT_NAME>.zip * exit sudo docker ps -a | grep \"dacut\" | awk '{print $1}' sudo docker cp <CONTAINER_ID>:<DOCKER_PROJECT_DIR>/<PROJECT_NAME>.zip <LOCAL_PROJECT_PATH> unzip <LOCAL_PROJECT_PATH>/<PROJECT_NAME>.zip rm <LOCAL_PROJECT_PATH>/<PROJECT_NAME>.zip # Add Lambda File to zip zip -ur <PROJECT_NAME>.zip <PATH_TO_LAMBDA_FUNCTION_FILE>","title":"Build"},{"location":"platforms/aws/serverless/stepfunctions/","text":"Task \u00b6 Invoke Lambda \u00b6 \"GetLinks\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::lambda:invoke\" , \"Parameters\" : { \"FunctionName\" : \"arn:aws:lambda:eu-central-1:951313074793:function:test-api-branch_io-links:$LATEST\" , \"Payload\" : { \"Input.$\" : \"$\" } }, \"OutputPath\" : \"$.Payload\" , \"Next\" : \"SendLinkToSQSQueue\" } DynamoDB \u00b6 Put Item \u00b6 \"StoreDeviceInDynamoDB\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::dynamodb:putItem\" , \"Parameters\" : { \"TableName\" : \"Devices\" , \"Item\" : { \"DeviceID\" : { \"N.$\" : \"$.DeviceID\" }, \"Name\" : { \"S.$\" : \"$.Name\" } } }, \"Retry\" : [ { \"ErrorEquals\" : [ \"States.Timeout\" ], \"IntervalSeconds\" : 3 , \"MaxAttempts\" : 2 , \"BackoffRate\" : 1.5 } ] } Map \u00b6 SQS: Callback Pattern \"SendLinkToSQSQueue\" : { \"Type\" : \"Map\" , \"MaxConcurrency\" : 20 , \"ItemsPath\" : \"$\" , \"Iterator\" : { \"StartAt\" : \"Send Link to SQS\" , \"States\" : { \"Send Link to SQS\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::sqs:sendMessage.waitForTaskToken\" , \"Parameters\" : { \"QueueUrl\" : \"https://sqs.eu-central-1.amazonaws.com/951313074793/test--branch_io-queue\" , \"MessageBody\" : { \"Url.$\" : \"$.MessageBody\" , \"TaskToken.$\" : \"$$.Task.Token\" }, \"MessageAttributes.$\" : \"$.MessageAttributes\" }, \"End\" : true } } }, \"Next\" : \"World\" }","title":"StepFunctions"},{"location":"platforms/aws/serverless/stepfunctions/#task","text":"","title":"Task"},{"location":"platforms/aws/serverless/stepfunctions/#invoke-lambda","text":"\"GetLinks\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::lambda:invoke\" , \"Parameters\" : { \"FunctionName\" : \"arn:aws:lambda:eu-central-1:951313074793:function:test-api-branch_io-links:$LATEST\" , \"Payload\" : { \"Input.$\" : \"$\" } }, \"OutputPath\" : \"$.Payload\" , \"Next\" : \"SendLinkToSQSQueue\" }","title":"Invoke Lambda"},{"location":"platforms/aws/serverless/stepfunctions/#dynamodb","text":"","title":"DynamoDB"},{"location":"platforms/aws/serverless/stepfunctions/#put-item","text":"\"StoreDeviceInDynamoDB\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::dynamodb:putItem\" , \"Parameters\" : { \"TableName\" : \"Devices\" , \"Item\" : { \"DeviceID\" : { \"N.$\" : \"$.DeviceID\" }, \"Name\" : { \"S.$\" : \"$.Name\" } } }, \"Retry\" : [ { \"ErrorEquals\" : [ \"States.Timeout\" ], \"IntervalSeconds\" : 3 , \"MaxAttempts\" : 2 , \"BackoffRate\" : 1.5 } ] }","title":"Put Item"},{"location":"platforms/aws/serverless/stepfunctions/#map","text":"SQS: Callback Pattern \"SendLinkToSQSQueue\" : { \"Type\" : \"Map\" , \"MaxConcurrency\" : 20 , \"ItemsPath\" : \"$\" , \"Iterator\" : { \"StartAt\" : \"Send Link to SQS\" , \"States\" : { \"Send Link to SQS\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::sqs:sendMessage.waitForTaskToken\" , \"Parameters\" : { \"QueueUrl\" : \"https://sqs.eu-central-1.amazonaws.com/951313074793/test--branch_io-queue\" , \"MessageBody\" : { \"Url.$\" : \"$.MessageBody\" , \"TaskToken.$\" : \"$$.Task.Token\" }, \"MessageAttributes.$\" : \"$.MessageAttributes\" }, \"End\" : true } } }, \"Next\" : \"World\" }","title":"Map"},{"location":"platforms/aws/storage/s3/","text":"MultiPart Upload \u00b6 Documentation # Split the File export BUCKET = fsantos-vmware-ova-templates export FILE = aws.ova split -b <bytes> <file> # Upload aws s3api create-multipart-upload --bucket ${ BUCKET } --key ${ FILE } export UPLOAD_ID = # Repeat to all parts - Changing the body and increase part number aws s3api upload-part --bucket ${ BUCKET } --key ${ FILE } --part-number 1 --body <file-part1> --upload-id ${ UPLOAD_ID } # List all uploaded parts and get ETags aws s3api list-parts --bucket ${ BUCKET } --key ${ FILE } --upload-id ${ UPLOAD_ID } # Create fileparts.json with all Etags # Complete Multipart upload aws s3api complete-multipart-upload --multipart-upload file://fileparts.json --bucket ${ BUCKET } --key ${ FILE } --upload-id ${ UPLOAD_ID } filepats.json example { \"Parts\" : [{ \"ETag\" : \"\\\"example8be9a0268ebfb8b115d4c1fd3\\\"\" , \"PartNumber\" : 1 }, .... { \"ETag\" : \"\\\"example246e31ab807da6f62802c1ae8\\\"\" , \"PartNumber\" : 4 }] }","title":"S3"},{"location":"platforms/aws/storage/s3/#multipart-upload","text":"Documentation # Split the File export BUCKET = fsantos-vmware-ova-templates export FILE = aws.ova split -b <bytes> <file> # Upload aws s3api create-multipart-upload --bucket ${ BUCKET } --key ${ FILE } export UPLOAD_ID = # Repeat to all parts - Changing the body and increase part number aws s3api upload-part --bucket ${ BUCKET } --key ${ FILE } --part-number 1 --body <file-part1> --upload-id ${ UPLOAD_ID } # List all uploaded parts and get ETags aws s3api list-parts --bucket ${ BUCKET } --key ${ FILE } --upload-id ${ UPLOAD_ID } # Create fileparts.json with all Etags # Complete Multipart upload aws s3api complete-multipart-upload --multipart-upload file://fileparts.json --bucket ${ BUCKET } --key ${ FILE } --upload-id ${ UPLOAD_ID } filepats.json example { \"Parts\" : [{ \"ETag\" : \"\\\"example8be9a0268ebfb8b115d4c1fd3\\\"\" , \"PartNumber\" : 1 }, .... { \"ETag\" : \"\\\"example246e31ab807da6f62802c1ae8\\\"\" , \"PartNumber\" : 4 }] }","title":"MultiPart Upload"},{"location":"platforms/kubernetes/admin/admin/","text":"Pre-Requisites \u00b6 Redhat \u00b6 Disable SELinux sudo setenforce 0 sudo sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux Enable the br_netfilter module for cluster communication sudo modprobe br_netfilter sudo echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables Disable swap to prevent memory allocation issues swapoff -a sudo vi /etc/fstab -> Comment out the swap line Docker \u00b6 Redhat \u00b6 Install the Docker prerequisites sudo yum install -y yum-utils device-mapper-persistent-data lvm2 Add the Docker repo and install Docker sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install -y docker-ce Configure the Docker Cgroup Driver to systemd, enable and start Docker sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker --now Ubuntu \u00b6 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce = 18 .06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce Amazon AMI Linux 2 \u00b6 sudo amazon-linux-extras install -y docker sudo cat > /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo systemctl daemon-reload sudo systemctl enable docker sudo systemctl start docker sudo usermod -a -G docker ec2-user # Exit e login na sessao kubernetes \u00b6 Redhat \u00b6 cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF sudo yum install -y kubelet kubeadm kubectl sudo systemctl enable kubelet kubeadm init --pod-network-cidr = 10 .244.0.0/16 # Exit sudo user mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config Ubuntu \u00b6 curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .12.7-00 kubeadm = 1 .12.7-00 kubectl = 1 .12.7-00 sudo apt-mark hold kubelet kubeadm kubectl echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config Upgrade \u00b6 Ubuntu \u00b6 kubectl version --short # Release the hold on versions of kubeadm and kubelet sudo apt-mark unhold kubeadm kubelet sudo apt install -y kubeadm = 1 .14.1-00 sudo apt-mark hold kubeadm kubeadm version sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.14.1 sudo apt-mark unhold kubectl sudo apt install -y kubectl = 1 .14.1-00 sudo apt-mark hold kubectl sudo apt-mark unhold kubelet sudo apt install -y kubelet = 1 .14.1-00 sudo apt-mark hold kubelet Maintenance And Troubleshooting \u00b6 Evict Pods \u00b6 # Evict the pods on a node kubectl drain [ node_name ] --ignore-daemonsets # Watch as the node changes status: kubectl get nodes -w # Rollback - Schedule pods to the node after maintenance is complete kubectl uncordon [ node_name ] Delete Node \u00b6 # Remove a node from the cluster: kubectl delete node [ node_name ] sudo kubeadm token generate # List the tokens: sudo kubeadm token list # Print the kubeadm join command to join a node to the cluster sudo kubeadm token create [ token_name ] --ttl 2h --print-join-command Taint \u00b6 kubectl taint node <node_name> node-type = prod:NoSchedule Pod with Toleration \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : prod spec : replicas : 1 selector : matchLabels : app : prod template : metadata : labels : app : prod spec : containers : - args : - sleep - \"3600\" image : busybox name : main tolerations : - key : node-type operator : Equal value : prod effect : NoSchedule Commands \u00b6 kubectl get componentstatus kubectl api-resources -o wide Autocomplete \u00b6 Redhat \u00b6 # Enable Epel Repo sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel # Instal bash completion yum install bash-completion bash-completion-extras vi /.bashrc # Add Following lines # alias k=kubectl # source <(kubectl completion bash | sed s/kubectl/k/g) Kubectl \u00b6 Install \u00b6 wget https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/ kubectl version --client Set Remote \u00b6 kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://localhost:6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way Insecure \u00b6 File: $HOME/.kube/config - cluster : insecure-skip-tls-verify : true Api Resources \u00b6 kubectl api-resources -o name Kube Config \u00b6 Generate Kube Configs \u00b6 See How Generate TLS Certificates # Create an environment variable to store the address of the Kubernetes API, and set it to the private IP of your load balancer cloud server: KUBERNETES_ADDRESS = <load balancer private ip> # Generate a kubelet kubeconfig for each worker node: for instance in <worker 1 hostname> <worker 2 hostname> ; do kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_ADDRESS } :6443 \\ --kubeconfig = ${ instance } .kubeconfig kubectl config set-credentials system:node: ${ instance } \\ --client-certificate = ${ instance } .pem \\ --client-key = ${ instance } -key.pem \\ --embed-certs = true \\ --kubeconfig = ${ instance } .kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:node: ${ instance } \\ --kubeconfig = ${ instance } .kubeconfig kubectl config use-context default --kubeconfig = ${ instance } .kubeconfig done # Generate a kube-proxy kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_ADDRESS } :6443 \\ --kubeconfig = kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate = kube-proxy.pem \\ --client-key = kube-proxy-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-proxy.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-proxy \\ --kubeconfig = kube-proxy.kubeconfig kubectl config use-context default --kubeconfig = kube-proxy.kubeconfig } # Generate a kube-controller-manager kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig } # Generate a kube-scheduler kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate = kube-scheduler.pem \\ --client-key = kube-scheduler-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-scheduler \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig = kube-scheduler.kubeconfig } # Generate an admin kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem \\ --embed-certs = true \\ --kubeconfig = admin.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = admin \\ --kubeconfig = admin.kubeconfig kubectl config use-context default --kubeconfig = admin.kubeconfig } Move kubeconfig files to the worker nodes: \u00b6 scp <worker 1 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 1 public IP>:~/ scp <worker 2 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 2 public IP>:~/ Move kubeconfig files to the master nodes: \u00b6 scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 1 public IP>:~/ scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 2 public IP>:~/ Data Encryption Config \u00b6 # Generate the Kubernetes Data encrpytion config file containing the encrpytion key: export ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) cat > encryption-config.yaml << EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF # Copy the file to both master servers: scp encryption-config.yaml user@<master 1 public ip>:~/ scp encryption-config.yaml user@<master 2 public ip>:~/ LoadBalancer \u00b6 Setting UP # Here are the commands you can use to set up the nginx load balancer. Run these on the server that you have designated as your load balancer server: sudo apt-get install -y nginx sudo systemctl enable nginx sudo mkdir -p /etc/nginx/tcpconf.d sudo vi /etc/nginx/nginx.conf # Add the following to the end of nginx.conf: include /etc/nginx/tcpconf.d/* ; # Set up some environment variables for the lead balancer config file: export CONTROLLER0_IP = <controller 0 private ip> export CONTROLLER1_IP = <controller 1 private ip> # Create the load balancer nginx config file: cat << EOF | sudo tee /etc/nginx/tcpconf.d/kubernetes.conf stream { upstream kubernetes { server $CONTROLLER0_IP:6443; server $CONTROLLER1_IP:6443; } server { listen 6443; listen 443; proxy_pass kubernetes; } } EOF # Reload the nginx configuration: sudo nginx -s reload # You can verify that the load balancer is working like so: curl -k https://localhost:6443/version","title":"Administrator"},{"location":"platforms/kubernetes/admin/admin/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"platforms/kubernetes/admin/admin/#redhat","text":"Disable SELinux sudo setenforce 0 sudo sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux Enable the br_netfilter module for cluster communication sudo modprobe br_netfilter sudo echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables Disable swap to prevent memory allocation issues swapoff -a sudo vi /etc/fstab -> Comment out the swap line","title":"Redhat"},{"location":"platforms/kubernetes/admin/admin/#docker","text":"","title":"Docker"},{"location":"platforms/kubernetes/admin/admin/#redhat_1","text":"Install the Docker prerequisites sudo yum install -y yum-utils device-mapper-persistent-data lvm2 Add the Docker repo and install Docker sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install -y docker-ce Configure the Docker Cgroup Driver to systemd, enable and start Docker sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker --now","title":"Redhat"},{"location":"platforms/kubernetes/admin/admin/#ubuntu","text":"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce = 18 .06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce","title":"Ubuntu"},{"location":"platforms/kubernetes/admin/admin/#amazon-ami-linux-2","text":"sudo amazon-linux-extras install -y docker sudo cat > /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo systemctl daemon-reload sudo systemctl enable docker sudo systemctl start docker sudo usermod -a -G docker ec2-user # Exit e login na sessao","title":"Amazon AMI Linux 2"},{"location":"platforms/kubernetes/admin/admin/#kubernetes","text":"","title":"kubernetes"},{"location":"platforms/kubernetes/admin/admin/#redhat_2","text":"cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF sudo yum install -y kubelet kubeadm kubectl sudo systemctl enable kubelet kubeadm init --pod-network-cidr = 10 .244.0.0/16 # Exit sudo user mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config","title":"Redhat"},{"location":"platforms/kubernetes/admin/admin/#ubuntu_1","text":"curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .12.7-00 kubeadm = 1 .12.7-00 kubectl = 1 .12.7-00 sudo apt-mark hold kubelet kubeadm kubectl echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config","title":"Ubuntu"},{"location":"platforms/kubernetes/admin/admin/#upgrade","text":"","title":"Upgrade"},{"location":"platforms/kubernetes/admin/admin/#ubuntu_2","text":"kubectl version --short # Release the hold on versions of kubeadm and kubelet sudo apt-mark unhold kubeadm kubelet sudo apt install -y kubeadm = 1 .14.1-00 sudo apt-mark hold kubeadm kubeadm version sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.14.1 sudo apt-mark unhold kubectl sudo apt install -y kubectl = 1 .14.1-00 sudo apt-mark hold kubectl sudo apt-mark unhold kubelet sudo apt install -y kubelet = 1 .14.1-00 sudo apt-mark hold kubelet","title":"Ubuntu"},{"location":"platforms/kubernetes/admin/admin/#maintenance-and-troubleshooting","text":"","title":"Maintenance And Troubleshooting"},{"location":"platforms/kubernetes/admin/admin/#evict-pods","text":"# Evict the pods on a node kubectl drain [ node_name ] --ignore-daemonsets # Watch as the node changes status: kubectl get nodes -w # Rollback - Schedule pods to the node after maintenance is complete kubectl uncordon [ node_name ]","title":"Evict Pods"},{"location":"platforms/kubernetes/admin/admin/#delete-node","text":"# Remove a node from the cluster: kubectl delete node [ node_name ] sudo kubeadm token generate # List the tokens: sudo kubeadm token list # Print the kubeadm join command to join a node to the cluster sudo kubeadm token create [ token_name ] --ttl 2h --print-join-command","title":"Delete Node"},{"location":"platforms/kubernetes/admin/admin/#taint","text":"kubectl taint node <node_name> node-type = prod:NoSchedule","title":"Taint"},{"location":"platforms/kubernetes/admin/admin/#pod-with-toleration","text":"apiVersion : apps/v1 kind : Deployment metadata : name : prod spec : replicas : 1 selector : matchLabels : app : prod template : metadata : labels : app : prod spec : containers : - args : - sleep - \"3600\" image : busybox name : main tolerations : - key : node-type operator : Equal value : prod effect : NoSchedule","title":"Pod with Toleration"},{"location":"platforms/kubernetes/admin/admin/#commands","text":"kubectl get componentstatus kubectl api-resources -o wide","title":"Commands"},{"location":"platforms/kubernetes/admin/admin/#autocomplete","text":"","title":"Autocomplete"},{"location":"platforms/kubernetes/admin/admin/#redhat_3","text":"# Enable Epel Repo sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel # Instal bash completion yum install bash-completion bash-completion-extras vi /.bashrc # Add Following lines # alias k=kubectl # source <(kubectl completion bash | sed s/kubectl/k/g)","title":"Redhat"},{"location":"platforms/kubernetes/admin/admin/#kubectl","text":"","title":"Kubectl"},{"location":"platforms/kubernetes/admin/admin/#install","text":"wget https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/ kubectl version --client","title":"Install"},{"location":"platforms/kubernetes/admin/admin/#set-remote","text":"kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://localhost:6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way","title":"Set Remote"},{"location":"platforms/kubernetes/admin/admin/#insecure","text":"File: $HOME/.kube/config - cluster : insecure-skip-tls-verify : true","title":"Insecure"},{"location":"platforms/kubernetes/admin/admin/#api-resources","text":"kubectl api-resources -o name","title":"Api Resources"},{"location":"platforms/kubernetes/admin/admin/#kube-config","text":"","title":"Kube Config"},{"location":"platforms/kubernetes/admin/admin/#generate-kube-configs","text":"See How Generate TLS Certificates # Create an environment variable to store the address of the Kubernetes API, and set it to the private IP of your load balancer cloud server: KUBERNETES_ADDRESS = <load balancer private ip> # Generate a kubelet kubeconfig for each worker node: for instance in <worker 1 hostname> <worker 2 hostname> ; do kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_ADDRESS } :6443 \\ --kubeconfig = ${ instance } .kubeconfig kubectl config set-credentials system:node: ${ instance } \\ --client-certificate = ${ instance } .pem \\ --client-key = ${ instance } -key.pem \\ --embed-certs = true \\ --kubeconfig = ${ instance } .kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:node: ${ instance } \\ --kubeconfig = ${ instance } .kubeconfig kubectl config use-context default --kubeconfig = ${ instance } .kubeconfig done # Generate a kube-proxy kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_ADDRESS } :6443 \\ --kubeconfig = kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate = kube-proxy.pem \\ --client-key = kube-proxy-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-proxy.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-proxy \\ --kubeconfig = kube-proxy.kubeconfig kubectl config use-context default --kubeconfig = kube-proxy.kubeconfig } # Generate a kube-controller-manager kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig } # Generate a kube-scheduler kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate = kube-scheduler.pem \\ --client-key = kube-scheduler-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-scheduler \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig = kube-scheduler.kubeconfig } # Generate an admin kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem \\ --embed-certs = true \\ --kubeconfig = admin.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = admin \\ --kubeconfig = admin.kubeconfig kubectl config use-context default --kubeconfig = admin.kubeconfig }","title":"Generate Kube Configs"},{"location":"platforms/kubernetes/admin/admin/#move-kubeconfig-files-to-the-worker-nodes","text":"scp <worker 1 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 1 public IP>:~/ scp <worker 2 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 2 public IP>:~/","title":"Move kubeconfig files to the worker nodes:"},{"location":"platforms/kubernetes/admin/admin/#move-kubeconfig-files-to-the-master-nodes","text":"scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 1 public IP>:~/ scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 2 public IP>:~/","title":"Move kubeconfig files to the master nodes:"},{"location":"platforms/kubernetes/admin/admin/#data-encryption-config","text":"# Generate the Kubernetes Data encrpytion config file containing the encrpytion key: export ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) cat > encryption-config.yaml << EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF # Copy the file to both master servers: scp encryption-config.yaml user@<master 1 public ip>:~/ scp encryption-config.yaml user@<master 2 public ip>:~/","title":"Data Encryption Config"},{"location":"platforms/kubernetes/admin/admin/#loadbalancer","text":"Setting UP # Here are the commands you can use to set up the nginx load balancer. Run these on the server that you have designated as your load balancer server: sudo apt-get install -y nginx sudo systemctl enable nginx sudo mkdir -p /etc/nginx/tcpconf.d sudo vi /etc/nginx/nginx.conf # Add the following to the end of nginx.conf: include /etc/nginx/tcpconf.d/* ; # Set up some environment variables for the lead balancer config file: export CONTROLLER0_IP = <controller 0 private ip> export CONTROLLER1_IP = <controller 1 private ip> # Create the load balancer nginx config file: cat << EOF | sudo tee /etc/nginx/tcpconf.d/kubernetes.conf stream { upstream kubernetes { server $CONTROLLER0_IP:6443; server $CONTROLLER1_IP:6443; } server { listen 6443; listen 443; proxy_pass kubernetes; } } EOF # Reload the nginx configuration: sudo nginx -s reload # You can verify that the load balancer is working like so: curl -k https://localhost:6443/version","title":"LoadBalancer"},{"location":"platforms/kubernetes/admin/etcd/","text":"Create Cluster \u00b6 # Here are the commands used in the demo (note that these have to be run on both controller servers, with a few differences between them): wget -q --show-progress --https-only --timestamping \\ \"https://github.com/coreos/etcd/releases/download/v3.3.5/etcd-v3.3.5-linux-amd64.tar.gz\" tar -xvf etcd-v3.3.5-linux-amd64.tar.gz sudo mv etcd-v3.3.5-linux-amd64/etcd* /usr/local/bin/ sudo mkdir -p /etc/etcd /var/lib/etcd sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ # Set up the following environment variables. Be sure you replace all of the <placeholder values> with their corresponding real values: export ETCD_NAME = <cloud server hostname> export INTERNAL_IP = $( curl http://169.254.169.254/latest/meta-data/local-ipv4 ) export INITIAL_CLUSTER = <controller 1 hostname> = https://<controller 1 private ip>:2380,<controller 2 hostname> = https://<controller 2 private ip>:2380 # Create the systemd unit file for etcd using this command. Note that this command uses the environment variables that were set earlier: cat << EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster ${INITIAL_CLUSTER} \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the etcd service: sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd # You can verify that the etcd service started up successfully like so: sudo systemctl status etcd # Use this command to verify that etcd is working correctly. The output should list your two etcd nodes: sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem ETCD Command Line \u00b6 Install \u00b6 wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz tar xvf etcd-v3.3.12-linux-amd64.tar.gz sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin Snapshot \u00b6 sudo ETCDCTL_API = 3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View that the snapshot was successful ETCDCTL_API = 3 etcdctl --write-out = table snapshot status snapshot.db Docker \u00b6 Find Command Parameters \u00b6 ps -ef | grep etcd Get All Keys \u00b6 docker exec -it 3606376c1aba /bin/sh -c \"export ETCDCTL_API=3 && etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\" Backup and Restore \u00b6 # Get the etcd binaries: wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz # Unzip the compressed binaries: tar xvf etcd-v3.3.12-linux-amd64.tar.gz # Move the files into /usr/local/bin: sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin # Take a snapshot of the etcd datastore using etcdctl: sudo ETCDCTL_API = 3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View the help page for etcdctl: ETCDCTL_API = 3 etcdctl --help # Browse to the folder that contains the certificate files: cd /etc/kubernetes/pki/etcd/ # View that the snapshot was successful: ETCDCTL_API = 3 etcdctl --write-out = table snapshot status snapshot.db # Zip up the contents of the etcd directory: sudo tar -zcvf etcd.tar.gz /etc/kubernetes/pki/etcd # Copy the etcd directory to another server: scp etcd.tar.gz cloud_user@18.219.235.42:~/","title":"Etcd"},{"location":"platforms/kubernetes/admin/etcd/#create-cluster","text":"# Here are the commands used in the demo (note that these have to be run on both controller servers, with a few differences between them): wget -q --show-progress --https-only --timestamping \\ \"https://github.com/coreos/etcd/releases/download/v3.3.5/etcd-v3.3.5-linux-amd64.tar.gz\" tar -xvf etcd-v3.3.5-linux-amd64.tar.gz sudo mv etcd-v3.3.5-linux-amd64/etcd* /usr/local/bin/ sudo mkdir -p /etc/etcd /var/lib/etcd sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ # Set up the following environment variables. Be sure you replace all of the <placeholder values> with their corresponding real values: export ETCD_NAME = <cloud server hostname> export INTERNAL_IP = $( curl http://169.254.169.254/latest/meta-data/local-ipv4 ) export INITIAL_CLUSTER = <controller 1 hostname> = https://<controller 1 private ip>:2380,<controller 2 hostname> = https://<controller 2 private ip>:2380 # Create the systemd unit file for etcd using this command. Note that this command uses the environment variables that were set earlier: cat << EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster ${INITIAL_CLUSTER} \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the etcd service: sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd # You can verify that the etcd service started up successfully like so: sudo systemctl status etcd # Use this command to verify that etcd is working correctly. The output should list your two etcd nodes: sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem","title":"Create Cluster"},{"location":"platforms/kubernetes/admin/etcd/#etcd-command-line","text":"","title":"ETCD Command Line"},{"location":"platforms/kubernetes/admin/etcd/#install","text":"wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz tar xvf etcd-v3.3.12-linux-amd64.tar.gz sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin","title":"Install"},{"location":"platforms/kubernetes/admin/etcd/#snapshot","text":"sudo ETCDCTL_API = 3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View that the snapshot was successful ETCDCTL_API = 3 etcdctl --write-out = table snapshot status snapshot.db","title":"Snapshot"},{"location":"platforms/kubernetes/admin/etcd/#docker","text":"","title":"Docker"},{"location":"platforms/kubernetes/admin/etcd/#find-command-parameters","text":"ps -ef | grep etcd","title":"Find Command Parameters"},{"location":"platforms/kubernetes/admin/etcd/#get-all-keys","text":"docker exec -it 3606376c1aba /bin/sh -c \"export ETCDCTL_API=3 && etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\"","title":"Get All Keys"},{"location":"platforms/kubernetes/admin/etcd/#backup-and-restore","text":"# Get the etcd binaries: wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz # Unzip the compressed binaries: tar xvf etcd-v3.3.12-linux-amd64.tar.gz # Move the files into /usr/local/bin: sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin # Take a snapshot of the etcd datastore using etcdctl: sudo ETCDCTL_API = 3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View the help page for etcdctl: ETCDCTL_API = 3 etcdctl --help # Browse to the folder that contains the certificate files: cd /etc/kubernetes/pki/etcd/ # View that the snapshot was successful: ETCDCTL_API = 3 etcdctl --write-out = table snapshot status snapshot.db # Zip up the contents of the etcd directory: sudo tar -zcvf etcd.tar.gz /etc/kubernetes/pki/etcd # Copy the etcd directory to another server: scp etcd.tar.gz cloud_user@18.219.235.42:~/","title":"Backup and Restore"},{"location":"platforms/kubernetes/admin/hardway/","text":"Master \u00b6 Install # You can install the control plane binaries on each master node like this: sudo mkdir -p /etc/kubernetes/config wget -q --timestamping \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-apiserver\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-controller-manager\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-scheduler\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl\" chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/ API-Server \u00b6 Setting Up # You can configure the Kubernetes API server like so: sudo mkdir -p /var/lib/kubernetes/ sudo cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/ # Set some environment variables that will be used to create the systemd unit file. Make sure you replace the placeholders with their actual values: export INTERNAL_IP = $( curl http://169.254.169.254/latest/meta-data/local-ipv4 ) export CONTROLLER0_IP = <private ip of controller 0 > export CONTROLLER1_IP = <private ip of controller 1 > # Generate the kube-apiserver unit file for systemd: cat << EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://$CONTROLLER0_IP:2379,https://$CONTROLLER1_IP:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 \\\\ --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-apiserver sudo systemctl start kube-apiserver sudo systemctl status kube-apiserver Controller Manager \u00b6 Setting Up # You can configure the Kubernetes Controller Manager like so: sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/ # Generate the kube-controller-manager systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-controller-manager sudo systemctl start kube-controller-manager sudo systemctl status kube-controller-manager Scheduler \u00b6 Setting Up # Copy kube-scheduler.kubeconfig into the proper location: sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/ # Generate the kube-scheduler yaml config file. cat << EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: componentconfig/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: true EOF # Create the kube-scheduler systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-scheduler sudo systemctl start kube-scheduler sudo systemctl status kube-scheduler Default Scheduler \u00b6 Does the node have adequate hardware resources? Is the node running out of resources? Does the pod request a specific node? Does the node have a matching label? If the pod requests a port, is it available? If the pod requests a volume, can it be mounted? Does the pod tolerate the taints of the node? Does the pod specify node or pod affinity? Custom Scheduler \u00b6 ClusterRole \u00b6 apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : csinodes-admin rules : - apiGroups : [ \"storage.k8s.io\" ] resources : [ \"csinodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] ClusterRoleBinding \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-csinodes-global subjects : - kind : ServiceAccount name : my-scheduler namespace : kube-system roleRef : kind : ClusterRole name : csinodes-admin apiGroup : rbac.authorization.k8s.io Role \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : system:serviceaccount:kube-system:my-scheduler namespace : kube-system rules : - apiGroups : - storage.k8s.io resources : - csinodes verbs : - get - list - watch RoleBinding \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : read-csinodes namespace : kube-system subjects : - kind : User name : kubernetes-admin apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : system:serviceaccount:kube-system:my-scheduler apiGroup : rbac.authorization.k8s.io Custom-scheduler \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : my-scheduler namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : my-scheduler-as-kube-scheduler subjects : - kind : ServiceAccount name : my-scheduler namespace : kube-system roleRef : kind : ClusterRole name : system:kube-scheduler apiGroup : rbac.authorization.k8s.io --- apiVersion : apps/v1 kind : Deployment metadata : labels : component : scheduler tier : control-plane name : my-scheduler namespace : kube-system spec : selector : matchLabels : component : scheduler tier : control-plane replicas : 1 template : metadata : labels : component : scheduler tier : control-plane version : second spec : serviceAccountName : my-scheduler containers : - command : - /usr/local/bin/kube-scheduler - --address=0.0.0.0 - --leader-elect=false - --scheduler-name=my-scheduler image : chadmcrowell/custom-scheduler livenessProbe : httpGet : path : /healthz port : 10251 initialDelaySeconds : 15 name : kube-second-scheduler readinessProbe : httpGet : path : /healthz port : 10251 resources : requests : cpu : '0.1' securityContext : privileged : false volumeMounts : [] hostNetwork : false hostPID : false volumes : [] Deploy \u00b6 kubectl create -f clusterrole.yaml kubectl create -f clusterrolebinding.yaml kubectl create -f role.yaml kubectl create -f rolebinding.yaml kubectl edit clusterrole system:kube-scheduler # And add the following - apiGroups : - \"\" resourceNames : - kube-scheduler - my-scheduler resources : - endpoints verbs : - delete - get - patch - update - apiGroups : - storage.k8s.io resources : - storageclasses verbs : - watch - list - get kubectl create -f my-scheduler.yaml kubectl get pods -n kube-system Get config \u00b6 kubectl get endpoints kube-scheduler -n kube-system -o yaml Troubleshooting \u00b6 kubectl describe pods [ scheduler_pod_name ] -n kube-system kubectl logs [ kube_scheduler_pod_name ] -n kube-system cat /var/log/kube-scheduler.log RBAC \u00b6 Setting UP # Create a role with the necessary permissions: cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF # Bind the role to the kubernetes user cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF Nodes \u00b6 Install \u00b6 # You can install the worker binaries like so. Run these commands on both worker nodes: sudo apt-get -y install socat conntrack ipset wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.0/containerd-1.1.0.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.0.linux-amd64.tar.gz -C / ContainerD \u00b6 # You can configure the containerd service like so. Run these commands on both worker nodes: sudo mkdir -p /etc/containerd/ # Create the containerd config.toml: cat << EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\" [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runsc\" runtime_root = \"/run/containerd/runsc\" EOF # Create the containerd unit file: cat << EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/bin/containerd Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable containerd sudo systemctl start containerd sudo systemctl status containerd Kubelet \u00b6 # Set a HOSTNAME environment variable that will be used to generate your config files. Make sure you set the HOSTNAME appropriately for each worker node: export HOSTNAME = $( hostname ) sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ # Create the kubelet config file: cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" authorization: mode: Webhook clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${HOSTNAME}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${HOSTNAME}-key.pem\" EOF # Create the kubelet unit file: cat << EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 \\\\ --hostname-override=${HOSTNAME} \\\\ --allow-privileged=true Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kubelet sudo systemctl start kubelet sudo systemctl status kubelet Kube-Proxy \u00b6 # You can configure the kube-proxy service like so. Run these commands on both worker nodes: sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig # Create the kube-proxy config file: cat << EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"10.200.0.0/16\" EOF # Create the kube-proxy unit file: cat << EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kube-proxy sudo systemctl start kube-proxy sudo systemctl status kube-proxy Commands \u00b6 Components Status \u00b6 # Use kubectl to check componentstatuses: kubectl get componentstatuses --kubeconfig admin.kubeconfig # You should get output that looks like this: NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy { \"health\" : \"true\" } etcd-1 Healthy { \"health\" : \"true\" } Enable HTTP Health Checks \u00b6 Using Nginx Load Balancer \u00b6 On Master Nodes # You can set up a basic nginx proxy for the healthz endpoint by first installing nginx\" sudo apt-get install -y nginx # Create an nginx configuration for the health check proxy: cat > kubernetes.default.svc.cluster.local << EOF server { listen 80; server_name kubernetes.default.svc.cluster.local; location /healthz { proxy_pass https://127.0.0.1:6443/healthz; proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem; } } EOF # Set up the proxy configuration so that it is loaded by nginx: sudo mv kubernetes.default.svc.cluster.local /etc/nginx/sites-available/kubernetes.default.svc.cluster.local sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/ sudo systemctl restart nginx sudo systemctl enable nginx # You can verify that everything is working like so: curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz","title":"Hardway"},{"location":"platforms/kubernetes/admin/hardway/#master","text":"Install # You can install the control plane binaries on each master node like this: sudo mkdir -p /etc/kubernetes/config wget -q --timestamping \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-apiserver\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-controller-manager\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-scheduler\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl\" chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/","title":"Master"},{"location":"platforms/kubernetes/admin/hardway/#api-server","text":"Setting Up # You can configure the Kubernetes API server like so: sudo mkdir -p /var/lib/kubernetes/ sudo cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/ # Set some environment variables that will be used to create the systemd unit file. Make sure you replace the placeholders with their actual values: export INTERNAL_IP = $( curl http://169.254.169.254/latest/meta-data/local-ipv4 ) export CONTROLLER0_IP = <private ip of controller 0 > export CONTROLLER1_IP = <private ip of controller 1 > # Generate the kube-apiserver unit file for systemd: cat << EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://$CONTROLLER0_IP:2379,https://$CONTROLLER1_IP:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 \\\\ --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-apiserver sudo systemctl start kube-apiserver sudo systemctl status kube-apiserver","title":"API-Server"},{"location":"platforms/kubernetes/admin/hardway/#controller-manager","text":"Setting Up # You can configure the Kubernetes Controller Manager like so: sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/ # Generate the kube-controller-manager systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-controller-manager sudo systemctl start kube-controller-manager sudo systemctl status kube-controller-manager","title":"Controller Manager"},{"location":"platforms/kubernetes/admin/hardway/#scheduler","text":"Setting Up # Copy kube-scheduler.kubeconfig into the proper location: sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/ # Generate the kube-scheduler yaml config file. cat << EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: componentconfig/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: true EOF # Create the kube-scheduler systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-scheduler sudo systemctl start kube-scheduler sudo systemctl status kube-scheduler","title":"Scheduler"},{"location":"platforms/kubernetes/admin/hardway/#default-scheduler","text":"Does the node have adequate hardware resources? Is the node running out of resources? Does the pod request a specific node? Does the node have a matching label? If the pod requests a port, is it available? If the pod requests a volume, can it be mounted? Does the pod tolerate the taints of the node? Does the pod specify node or pod affinity?","title":"Default Scheduler"},{"location":"platforms/kubernetes/admin/hardway/#custom-scheduler","text":"","title":"Custom Scheduler"},{"location":"platforms/kubernetes/admin/hardway/#clusterrole","text":"apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : csinodes-admin rules : - apiGroups : [ \"storage.k8s.io\" ] resources : [ \"csinodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ]","title":"ClusterRole"},{"location":"platforms/kubernetes/admin/hardway/#clusterrolebinding","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-csinodes-global subjects : - kind : ServiceAccount name : my-scheduler namespace : kube-system roleRef : kind : ClusterRole name : csinodes-admin apiGroup : rbac.authorization.k8s.io","title":"ClusterRoleBinding"},{"location":"platforms/kubernetes/admin/hardway/#role","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : system:serviceaccount:kube-system:my-scheduler namespace : kube-system rules : - apiGroups : - storage.k8s.io resources : - csinodes verbs : - get - list - watch","title":"Role"},{"location":"platforms/kubernetes/admin/hardway/#rolebinding","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : read-csinodes namespace : kube-system subjects : - kind : User name : kubernetes-admin apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : system:serviceaccount:kube-system:my-scheduler apiGroup : rbac.authorization.k8s.io","title":"RoleBinding"},{"location":"platforms/kubernetes/admin/hardway/#custom-scheduler_1","text":"apiVersion : v1 kind : ServiceAccount metadata : name : my-scheduler namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : my-scheduler-as-kube-scheduler subjects : - kind : ServiceAccount name : my-scheduler namespace : kube-system roleRef : kind : ClusterRole name : system:kube-scheduler apiGroup : rbac.authorization.k8s.io --- apiVersion : apps/v1 kind : Deployment metadata : labels : component : scheduler tier : control-plane name : my-scheduler namespace : kube-system spec : selector : matchLabels : component : scheduler tier : control-plane replicas : 1 template : metadata : labels : component : scheduler tier : control-plane version : second spec : serviceAccountName : my-scheduler containers : - command : - /usr/local/bin/kube-scheduler - --address=0.0.0.0 - --leader-elect=false - --scheduler-name=my-scheduler image : chadmcrowell/custom-scheduler livenessProbe : httpGet : path : /healthz port : 10251 initialDelaySeconds : 15 name : kube-second-scheduler readinessProbe : httpGet : path : /healthz port : 10251 resources : requests : cpu : '0.1' securityContext : privileged : false volumeMounts : [] hostNetwork : false hostPID : false volumes : []","title":"Custom-scheduler"},{"location":"platforms/kubernetes/admin/hardway/#deploy","text":"kubectl create -f clusterrole.yaml kubectl create -f clusterrolebinding.yaml kubectl create -f role.yaml kubectl create -f rolebinding.yaml kubectl edit clusterrole system:kube-scheduler # And add the following - apiGroups : - \"\" resourceNames : - kube-scheduler - my-scheduler resources : - endpoints verbs : - delete - get - patch - update - apiGroups : - storage.k8s.io resources : - storageclasses verbs : - watch - list - get kubectl create -f my-scheduler.yaml kubectl get pods -n kube-system","title":"Deploy"},{"location":"platforms/kubernetes/admin/hardway/#get-config","text":"kubectl get endpoints kube-scheduler -n kube-system -o yaml","title":"Get config"},{"location":"platforms/kubernetes/admin/hardway/#troubleshooting","text":"kubectl describe pods [ scheduler_pod_name ] -n kube-system kubectl logs [ kube_scheduler_pod_name ] -n kube-system cat /var/log/kube-scheduler.log","title":"Troubleshooting"},{"location":"platforms/kubernetes/admin/hardway/#rbac","text":"Setting UP # Create a role with the necessary permissions: cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF # Bind the role to the kubernetes user cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF","title":"RBAC"},{"location":"platforms/kubernetes/admin/hardway/#nodes","text":"","title":"Nodes"},{"location":"platforms/kubernetes/admin/hardway/#install","text":"# You can install the worker binaries like so. Run these commands on both worker nodes: sudo apt-get -y install socat conntrack ipset wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.0/containerd-1.1.0.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.0.linux-amd64.tar.gz -C /","title":"Install"},{"location":"platforms/kubernetes/admin/hardway/#containerd","text":"# You can configure the containerd service like so. Run these commands on both worker nodes: sudo mkdir -p /etc/containerd/ # Create the containerd config.toml: cat << EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\" [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runsc\" runtime_root = \"/run/containerd/runsc\" EOF # Create the containerd unit file: cat << EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/bin/containerd Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable containerd sudo systemctl start containerd sudo systemctl status containerd","title":"ContainerD"},{"location":"platforms/kubernetes/admin/hardway/#kubelet","text":"# Set a HOSTNAME environment variable that will be used to generate your config files. Make sure you set the HOSTNAME appropriately for each worker node: export HOSTNAME = $( hostname ) sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ # Create the kubelet config file: cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" authorization: mode: Webhook clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${HOSTNAME}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${HOSTNAME}-key.pem\" EOF # Create the kubelet unit file: cat << EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 \\\\ --hostname-override=${HOSTNAME} \\\\ --allow-privileged=true Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kubelet sudo systemctl start kubelet sudo systemctl status kubelet","title":"Kubelet"},{"location":"platforms/kubernetes/admin/hardway/#kube-proxy","text":"# You can configure the kube-proxy service like so. Run these commands on both worker nodes: sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig # Create the kube-proxy config file: cat << EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"10.200.0.0/16\" EOF # Create the kube-proxy unit file: cat << EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kube-proxy sudo systemctl start kube-proxy sudo systemctl status kube-proxy","title":"Kube-Proxy"},{"location":"platforms/kubernetes/admin/hardway/#commands","text":"","title":"Commands"},{"location":"platforms/kubernetes/admin/hardway/#components-status","text":"# Use kubectl to check componentstatuses: kubectl get componentstatuses --kubeconfig admin.kubeconfig # You should get output that looks like this: NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy { \"health\" : \"true\" } etcd-1 Healthy { \"health\" : \"true\" }","title":"Components Status"},{"location":"platforms/kubernetes/admin/hardway/#enable-http-health-checks","text":"","title":"Enable HTTP Health Checks"},{"location":"platforms/kubernetes/admin/hardway/#using-nginx-load-balancer","text":"On Master Nodes # You can set up a basic nginx proxy for the healthz endpoint by first installing nginx\" sudo apt-get install -y nginx # Create an nginx configuration for the health check proxy: cat > kubernetes.default.svc.cluster.local << EOF server { listen 80; server_name kubernetes.default.svc.cluster.local; location /healthz { proxy_pass https://127.0.0.1:6443/healthz; proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem; } } EOF # Set up the proxy configuration so that it is loaded by nginx: sudo mv kubernetes.default.svc.cluster.local /etc/nginx/sites-available/kubernetes.default.svc.cluster.local sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/ sudo systemctl restart nginx sudo systemctl enable nginx # You can verify that everything is working like so: curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz","title":"Using Nginx Load Balancer"},{"location":"platforms/kubernetes/admin/kops/","text":"Kops \u00b6 Install \u00b6 # Install Kops wget https://github.com/kubernetes/kops/releases/download/1.13.0/kops-linux-amd64 chmod +x kops-linux-amd64 sudo mv kops-linux-amd64 /usr/local/bin/kops export REGION = eu-west-1 # Create S3 aws s3 mb s3://fsantos-k8s-state-store --region ${ REGION } aws s3api put-bucket-versioning --bucket fsantos-k8s-state-store --versioning-configuration Status = Enabled aws s3api put-bucket-encryption --bucket fsantos-k8s-state-store --server-side-encryption-configuration '{\"Rules\":[{\"ApplyServerSideEncryptionByDefault\":{\"SSEAlgorithm\":\"AES256\"}}]}' export KOPS_STATE_STORE = s3://fsantos-k8s-state-store export NAME = tiagomsantos.com kops create secret --name ${ NAME } sshpublickey admin -i ~/.ssh/id_rsa.pub kops create cluster \\ --state ${ KOPS_STATE_STORE } \\ --zones \"eu-west-1a,eu-west-1b,eu-west-1c\" \\ --api-loadbalancer-type public \\ --master-count 3 \\ --master-size = t2.micro \\ --master-volume-size 8 \\ --node-count 3 \\ --node-size = t2.micro \\ --node-volume-size 8 \\ --name ${ NAME } \\ --cloud aws \\ --networking calico \\ --topology private \\ --ssh-public-key ~/.ssh/id_rsa.pub \\ --api-ssl-certificate arn:aws:acm:eu-west-1:395563851492:certificate/9190b1fe-8a63-4432-9f6f-01a631c4f1b2 \\ --bastion \\ --yes Suspend Autoscaling Process \u00b6 for groupName in $( aws autoscaling describe-auto-scaling-groups --query 'AutoScalingGroups[*].AutoScalingGroupName' --output text ) ; do if [[ $groupName = ~ ${ NAME } ]] then echo \"Processing autoscaling $groupName ...\" aws autoscaling suspend-processes --auto-scaling-group-name $groupName --scaling-processes HealthCheck ReplaceUnhealthy AlarmNotification AZRebalance Launch Terminate ScheduledActions AddToLoadBalancer RemoveFromLoadBalancerLowPriority fi done","title":"Kops"},{"location":"platforms/kubernetes/admin/kops/#kops","text":"","title":"Kops"},{"location":"platforms/kubernetes/admin/kops/#install","text":"# Install Kops wget https://github.com/kubernetes/kops/releases/download/1.13.0/kops-linux-amd64 chmod +x kops-linux-amd64 sudo mv kops-linux-amd64 /usr/local/bin/kops export REGION = eu-west-1 # Create S3 aws s3 mb s3://fsantos-k8s-state-store --region ${ REGION } aws s3api put-bucket-versioning --bucket fsantos-k8s-state-store --versioning-configuration Status = Enabled aws s3api put-bucket-encryption --bucket fsantos-k8s-state-store --server-side-encryption-configuration '{\"Rules\":[{\"ApplyServerSideEncryptionByDefault\":{\"SSEAlgorithm\":\"AES256\"}}]}' export KOPS_STATE_STORE = s3://fsantos-k8s-state-store export NAME = tiagomsantos.com kops create secret --name ${ NAME } sshpublickey admin -i ~/.ssh/id_rsa.pub kops create cluster \\ --state ${ KOPS_STATE_STORE } \\ --zones \"eu-west-1a,eu-west-1b,eu-west-1c\" \\ --api-loadbalancer-type public \\ --master-count 3 \\ --master-size = t2.micro \\ --master-volume-size 8 \\ --node-count 3 \\ --node-size = t2.micro \\ --node-volume-size 8 \\ --name ${ NAME } \\ --cloud aws \\ --networking calico \\ --topology private \\ --ssh-public-key ~/.ssh/id_rsa.pub \\ --api-ssl-certificate arn:aws:acm:eu-west-1:395563851492:certificate/9190b1fe-8a63-4432-9f6f-01a631c4f1b2 \\ --bastion \\ --yes","title":"Install"},{"location":"platforms/kubernetes/admin/kops/#suspend-autoscaling-process","text":"for groupName in $( aws autoscaling describe-auto-scaling-groups --query 'AutoScalingGroups[*].AutoScalingGroupName' --output text ) ; do if [[ $groupName = ~ ${ NAME } ]] then echo \"Processing autoscaling $groupName ...\" aws autoscaling suspend-processes --auto-scaling-group-name $groupName --scaling-processes HealthCheck ReplaceUnhealthy AlarmNotification AZRebalance Launch Terminate ScheduledActions AddToLoadBalancer RemoveFromLoadBalancerLowPriority fi done","title":"Suspend Autoscaling Process"},{"location":"platforms/kubernetes/admin/kubecontrollermanager/","text":"Manifest File \u00b6 Edit and re-deploy is automatically /etc/kubernetes/manifests/kube-controller-manager.yaml Metrics-Server \u00b6 Change Scale Down Time Change --horizontal-pod-autoscaler-downscale-stabilization Default 5 minutes Link","title":"KubeControllerManager"},{"location":"platforms/kubernetes/admin/kubecontrollermanager/#manifest-file","text":"Edit and re-deploy is automatically /etc/kubernetes/manifests/kube-controller-manager.yaml","title":"Manifest File"},{"location":"platforms/kubernetes/admin/kubecontrollermanager/#metrics-server","text":"Change Scale Down Time Change --horizontal-pod-autoscaler-downscale-stabilization Default 5 minutes Link","title":"Metrics-Server"},{"location":"platforms/kubernetes/admin/monitoring/","text":"Monitoring \u00b6 Cluster Components \u00b6 Metrics Server \u00b6 git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/ # Get a response from the metrics server API: kubectl get --raw /apis/metrics.k8s.io/ kubectl top node kubectl top pods kubectl top pods --all-namespaces kubectl top pods -n kube-system kubectl top pod -l run = pod-with-defaults kubectl top pod pod-with-defaults # Get the CPU and memory of the containers inside the pod kubectl top pods group-context --containers Applications \u00b6 Liveness Probe \u00b6 apiVersion : v1 kind : Pod metadata : name : liveness spec : containers : - image : linuxacademycontent/kubeserve name : kubeserve livenessProbe : httpGet : path : / port : 80 apiVersion : v1 kind : Pod metadata : name : my-liveness-pod spec : containers : - name : myapp-container image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] livenessProbe : exec : command : - echo - testing initialDelaySeconds : 5 periodSeconds : 5 Readiness Probe \u00b6 apiVersion : v1 kind : Service metadata : name : nginx spec : type : LoadBalancer ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : v1 kind : Pod metadata : name : nginx labels : app : nginx spec : containers : - name : nginx image : nginx readinessProbe : httpGet : path : / port : 80 initialDelaySeconds : 5 periodSeconds : 5 --- apiVersion : v1 kind : Pod metadata : name : nginxpd labels : app : nginx spec : containers : - name : nginx image : nginx:191 readinessProbe : httpGet : path : / port : 80 initialDelaySeconds : 5 periodSeconds : 5 Logs \u00b6 Cluster \u00b6 Dirs \u00b6 The directory where the continainer logs reside ls /var/log/containers The directory where kubelet stores its logs ls /var/log SideCar Container \u00b6 The YAML for a sidecar container that will tail the logs for each type apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : busybox args : - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts : - name : varlog mountPath : /var/log - name : count-log-1 image : busybox args : [ /bin/sh , -c , 'tail -n+1 -f /var/log/1.log' ] volumeMounts : - name : varlog mountPath : /var/log - name : count-log-2 image : busybox args : [ /bin/sh , -c , 'tail -n+1 -f /var/log/2.log' ] volumeMounts : - name : varlog mountPath : /var/log volumes : - name : varlog emptyDir : {} kubectl logs counter count-log-1 kubectl logs counter count-log-2 Application \u00b6 # Get the logs from a pod: kubectl logs nginx # Get the logs from a specific container on a pod: kubectl logs counter -c count-log-1 # Get the logs from all containers on the pod: kubectl logs counter --all-containers = true # Get the logs from containers with a certain label: kubectl logs -lapp = nginx # Get the logs from a previously terminated container within a pod: kubectl logs -p -c nginx nginx # Stream the logs from a container in a pod: kubectl logs -f -c count-log-1 counter # Tail the logs to only view a certain number of lines: kubectl logs --tail = 20 nginx # View the logs from a previous time duration: kubectl logs --since = 1h nginx # View the logs from a container within a pod within a deployment: kubectl logs deployment/nginx -c nginx # Redirect the output of the logs to a file: kubectl logs counter -c count-log-1 > count.log Troubleshooting \u00b6 Applications \u00b6 Use Termination Reason \u00b6 apiVersion : v1 kind : Pod metadata : name : pod2 spec : containers : - image : busybox name : main command : - sh - -c - 'echo \"I '' ve had enough\" > /var/termination-reason ; exit 1' terminationMessagePath : /var/termination-reason Healthz \u00b6 Not all pods have healthz configured apiVersion : v1 kind : Pod metadata : name : liveness spec : containers : - image : linuxacademycontent/candy-service:2 name : kubeserve livenessProbe : httpGet : path : /healthz port : 8081 Steps \u00b6 kubectl describe po pod2 kubectl logs pod-with-defaults kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml Cluster \u00b6 # Check the events in the kube-system namespace for errors kubectl get events -n kube-system kubectl logs [ kube_scheduler_pod_name ] -n kube-system # Check the status of the Docker service: sudo systemctl status docker sudo systemctl enable docker && systemctl start docker # Check the status of the kubelet service: sudo systemctl status kubelet sudo systemctl enable kubelet && systemctl start kubelet # Turn off swap on your machine sudo su - swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # Check if you have a firewall running: sudo systemctl status firewalld sudo systemctl disable firewalld && systemctl stop firewalld Worker Node \u00b6 kubectl get nodes kubectl describe nodes chadcrowell2c.mylabserver.com # Create New Worker Server # Generate a new token after spinning up a new server: sudo kubeadm token generate # Create the kubeadm join command for your new worker node: # sudo kubeadm token create [token_name] --ttl 2h --print-join-command # View the journalctl logs: sudo journalctl -u kubelet # View the syslogs: sudo more syslog | tail -120 | grep kubelet Networking \u00b6 DNS \u00b6 # Run an interactive busybox pod: kubectl run -it --rm --restart = Never busybox --image = busybox:1.28 sh # From the pod, check if DNS is resolving hostnames: nslookup hostnames # From the pod, cat out the /etc/resolv.conf file: cat /etc/resolv.conf # From the pod, look up the DNS name of the Kubernetes service: nslookup kubernetes.default nslookup kube-dns.kube-system.svc.cluster.loca # Look up a service in your Kubernetes cluster nslookup [ pod-ip-address ] .default.pod.cluster.local # Logs Core Dns kubectl logs [ coredns-pod-name ] Kube-Proxy \u00b6 # View the endpoints for your service: kubectl get ep # Communicate with the pod directly (without the service): wget -qO- 10 .244.1.6:9376 # Check if kube-proxy is running on the nodes: ps auxw | grep kube-proxy # Check if kube-proxy is writing iptables: kubectl get services -o wide iptables-save | grep hostnames sudo iptables-save | grep KUBE | grep <service-name> # View the list of kube-system pods: kubectl get pods -n kube-system # Connect to your kube-proxy pod in the kube-system namespace: kubectl exec -it kube-proxy-cqptg -n kube-system -- sh Change CNI Plugin \u00b6 # Delete the flannel CNI plugin: kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml # Apply the Weave Net CNI plugin: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \" Smoke Testing \u00b6 Data Encryption \u00b6 # Create a test secret: kubectl create secret generic kubernetes-the-hard-way --from-literal = \"mykey=mydata\" # Log in to one of your master servers, and get the raw data for the test secret from etcd: sudo ETCDCTL_API = 3 etcdctl get \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem \\ /registry/secrets/default/kubernetes-the-hard-way | hexdump -C # Your output should look something like this: 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 | /registry/secret | 00000010 73 2f 64 65 66 61 75 6c 74 2f 6b 75 62 65 72 6e | s/default/kubern | 00000020 65 74 65 73 2d 74 68 65 2d 68 61 72 64 2d 77 61 | etes-the-hard-wa | 00000030 79 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 | y.k8s:enc:aescbc | 00000040 3a 76 31 3a 6b 65 79 31 3a fc 21 ee dc e5 84 8a | :v1:key1:.!..... | 00000050 53 8e fd a9 72 a8 75 25 65 30 55 0e 72 43 1f 20 | S...r.u%e0U.rC. | 00000060 9f 07 15 4f 69 8a 79 a4 70 62 e9 ab f9 14 93 2e | ...Oi.y.pb...... | 00000070 e5 59 3f ab a7 b2 d8 d6 05 84 84 aa c3 6f 8d 5c | .Y?..........o. \\| 00000080 09 7a 2f 82 81 b5 d5 ec ba c7 23 34 46 d9 43 02 | .z/.......#4F.C. | 00000090 88 93 57 26 66 da 4e 8e 5c 24 44 6e 3e ec 9c 8e | ..W & f.N. \\$ Dn>... | 000000a0 83 ff 40 9a fb 94 07 3c 08 52 0e 77 50 81 c9 d0 | ..@....<.R.wP... | 000000b0 b7 30 68 ba b1 b3 26 eb b1 9f 3f f1 d7 76 86 09 | .0h... & ...?..v.. | 000000c0 d8 14 02 12 09 30 b0 60 b2 ad dc bb cf f5 77 e0 | .....0. ` ......w. | 000000d0 4f 0b 1f 74 79 c1 e7 20 1d 32 b2 68 01 19 93 fc | O..ty.. .2.h.... | 000000e0 f5 c8 8b 0b 16 7b 4f c2 6a 0a | ..... { O.j. | 000000ea # Look for k8s:enc:aescbc:v1:key1 on the right of the output to verify that the data is stored in an encrypted format! Deployments \u00b6 # Create a a simple nginx deployment: kubectl run nginx --image = nginx # Verify that the deployment created a pod and that the pod is running: kubectl get pods -l run = nginx # Verify that the output looks something like this: NAME READY STATUS RESTARTS AGE nginx-65899c769f-9xnqm 1 /1 Running 0 30s Port Forwarding \u00b6 # First, get the pod name of the nginx pod and store it an an environment variable: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # Forward port 8081 to the nginx pod: kubectl port-forward $POD_NAME 8081 :80 # Open up a new terminal on the same machine running the kubectl port-forward command and verify that the port forward works. curl --head http://127.0.0.1:8081 # You should get an http 200 OK response from the nginx pod. Logs \u00b6 # First, let's set an environment variable to the name of the nginx pod: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # Get the logs from the nginx pod: kubectl logs $POD_NAME # This command should return the nginx pod's logs. It will look something like this (but there could be more lines): 127 .0.0.1 - - [ 10 /Sep/2018:19:29:01 +0000 ] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\" Exec \u00b6 # First, let's set an environment variable to the name of the nginx pod: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # To test kubectl exec, execute a simple nginx -v command inside the nginx pod: kubectl exec -ti $POD_NAME -- nginx -v # This command should return the nginx version output, which should look like this: nginx version: nginx/1.15.3 Services \u00b6 # First, create a service to expose the nginx deployment: kubectl expose deployment nginx --port 80 --type NodePort # Get the node port assigned to the newly-created service and assign it to an environment variable: kubectl get svc # The output should look something like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .32.0.1 <none> 443 /TCP 20d nginx NodePort 10 .32.0.81 <none> 80 :32642/TCP 2m # Look for the service called nginx in that output. Under PORT(S), look for the second port, listed after 80:. In the example above, it is 32642. That is the node port, so make note of that value since you will need it in a moment. # Next, log in to one of your worker servers and make a request to the service using the node port. Be sure to replace the placeholder with the actual node port: curl -I localhost:<node port> # You should get an http 200 OK response. Untrusted Workloads \u00b6 # First, create an untrusted pod: cat << EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: untrusted annotations: io.kubernetes.cri.untrusted-workload: \"true\" spec: containers: - name: webserver image: gcr.io/hightowerlabs/helloworld:2.0.0 EOF # Make sure that the untrusted pod is running: kubectl get pods untrusted -o wide # Take note of which worker node the untrusted pod is running on, then log into that worker node. # On the worker node, list all of the containers running under gVisor: sudo runsc --root /run/containerd/runsc/k8s.io list # Get the pod ID of the untrusted pod and store it in an environment variable: POD_ID = $( sudo crictl -r unix:///var/run/containerd/containerd.sock \\ pods --name untrusted -q ) # Get the container ID of the container running in the untrusted pod and store it in an environment variable: CONTAINER_ID = $( sudo crictl -r unix:///var/run/containerd/containerd.sock \\ ps -p ${ POD_ID } -q ) # Get information about the process running in the container: sudo runsc --root /run/containerd/runsc/k8s.io ps ${ CONTAINER_ID } # Since we were able to get the process info using runsc, we know that the untrusted container is running securely as expected.","title":"Monitoring"},{"location":"platforms/kubernetes/admin/monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"platforms/kubernetes/admin/monitoring/#cluster-components","text":"","title":"Cluster Components"},{"location":"platforms/kubernetes/admin/monitoring/#metrics-server","text":"git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/ # Get a response from the metrics server API: kubectl get --raw /apis/metrics.k8s.io/ kubectl top node kubectl top pods kubectl top pods --all-namespaces kubectl top pods -n kube-system kubectl top pod -l run = pod-with-defaults kubectl top pod pod-with-defaults # Get the CPU and memory of the containers inside the pod kubectl top pods group-context --containers","title":"Metrics Server"},{"location":"platforms/kubernetes/admin/monitoring/#applications","text":"","title":"Applications"},{"location":"platforms/kubernetes/admin/monitoring/#liveness-probe","text":"apiVersion : v1 kind : Pod metadata : name : liveness spec : containers : - image : linuxacademycontent/kubeserve name : kubeserve livenessProbe : httpGet : path : / port : 80 apiVersion : v1 kind : Pod metadata : name : my-liveness-pod spec : containers : - name : myapp-container image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] livenessProbe : exec : command : - echo - testing initialDelaySeconds : 5 periodSeconds : 5","title":"Liveness Probe"},{"location":"platforms/kubernetes/admin/monitoring/#readiness-probe","text":"apiVersion : v1 kind : Service metadata : name : nginx spec : type : LoadBalancer ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : v1 kind : Pod metadata : name : nginx labels : app : nginx spec : containers : - name : nginx image : nginx readinessProbe : httpGet : path : / port : 80 initialDelaySeconds : 5 periodSeconds : 5 --- apiVersion : v1 kind : Pod metadata : name : nginxpd labels : app : nginx spec : containers : - name : nginx image : nginx:191 readinessProbe : httpGet : path : / port : 80 initialDelaySeconds : 5 periodSeconds : 5","title":"Readiness Probe"},{"location":"platforms/kubernetes/admin/monitoring/#logs","text":"","title":"Logs"},{"location":"platforms/kubernetes/admin/monitoring/#cluster","text":"","title":"Cluster"},{"location":"platforms/kubernetes/admin/monitoring/#dirs","text":"The directory where the continainer logs reside ls /var/log/containers The directory where kubelet stores its logs ls /var/log","title":"Dirs"},{"location":"platforms/kubernetes/admin/monitoring/#sidecar-container","text":"The YAML for a sidecar container that will tail the logs for each type apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : busybox args : - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts : - name : varlog mountPath : /var/log - name : count-log-1 image : busybox args : [ /bin/sh , -c , 'tail -n+1 -f /var/log/1.log' ] volumeMounts : - name : varlog mountPath : /var/log - name : count-log-2 image : busybox args : [ /bin/sh , -c , 'tail -n+1 -f /var/log/2.log' ] volumeMounts : - name : varlog mountPath : /var/log volumes : - name : varlog emptyDir : {} kubectl logs counter count-log-1 kubectl logs counter count-log-2","title":"SideCar Container"},{"location":"platforms/kubernetes/admin/monitoring/#application","text":"# Get the logs from a pod: kubectl logs nginx # Get the logs from a specific container on a pod: kubectl logs counter -c count-log-1 # Get the logs from all containers on the pod: kubectl logs counter --all-containers = true # Get the logs from containers with a certain label: kubectl logs -lapp = nginx # Get the logs from a previously terminated container within a pod: kubectl logs -p -c nginx nginx # Stream the logs from a container in a pod: kubectl logs -f -c count-log-1 counter # Tail the logs to only view a certain number of lines: kubectl logs --tail = 20 nginx # View the logs from a previous time duration: kubectl logs --since = 1h nginx # View the logs from a container within a pod within a deployment: kubectl logs deployment/nginx -c nginx # Redirect the output of the logs to a file: kubectl logs counter -c count-log-1 > count.log","title":"Application"},{"location":"platforms/kubernetes/admin/monitoring/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"platforms/kubernetes/admin/monitoring/#applications_1","text":"","title":"Applications"},{"location":"platforms/kubernetes/admin/monitoring/#use-termination-reason","text":"apiVersion : v1 kind : Pod metadata : name : pod2 spec : containers : - image : busybox name : main command : - sh - -c - 'echo \"I '' ve had enough\" > /var/termination-reason ; exit 1' terminationMessagePath : /var/termination-reason","title":"Use Termination Reason"},{"location":"platforms/kubernetes/admin/monitoring/#healthz","text":"Not all pods have healthz configured apiVersion : v1 kind : Pod metadata : name : liveness spec : containers : - image : linuxacademycontent/candy-service:2 name : kubeserve livenessProbe : httpGet : path : /healthz port : 8081","title":"Healthz"},{"location":"platforms/kubernetes/admin/monitoring/#steps","text":"kubectl describe po pod2 kubectl logs pod-with-defaults kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml","title":"Steps"},{"location":"platforms/kubernetes/admin/monitoring/#cluster_1","text":"# Check the events in the kube-system namespace for errors kubectl get events -n kube-system kubectl logs [ kube_scheduler_pod_name ] -n kube-system # Check the status of the Docker service: sudo systemctl status docker sudo systemctl enable docker && systemctl start docker # Check the status of the kubelet service: sudo systemctl status kubelet sudo systemctl enable kubelet && systemctl start kubelet # Turn off swap on your machine sudo su - swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # Check if you have a firewall running: sudo systemctl status firewalld sudo systemctl disable firewalld && systemctl stop firewalld","title":"Cluster"},{"location":"platforms/kubernetes/admin/monitoring/#worker-node","text":"kubectl get nodes kubectl describe nodes chadcrowell2c.mylabserver.com # Create New Worker Server # Generate a new token after spinning up a new server: sudo kubeadm token generate # Create the kubeadm join command for your new worker node: # sudo kubeadm token create [token_name] --ttl 2h --print-join-command # View the journalctl logs: sudo journalctl -u kubelet # View the syslogs: sudo more syslog | tail -120 | grep kubelet","title":"Worker Node"},{"location":"platforms/kubernetes/admin/monitoring/#networking","text":"","title":"Networking"},{"location":"platforms/kubernetes/admin/monitoring/#dns","text":"# Run an interactive busybox pod: kubectl run -it --rm --restart = Never busybox --image = busybox:1.28 sh # From the pod, check if DNS is resolving hostnames: nslookup hostnames # From the pod, cat out the /etc/resolv.conf file: cat /etc/resolv.conf # From the pod, look up the DNS name of the Kubernetes service: nslookup kubernetes.default nslookup kube-dns.kube-system.svc.cluster.loca # Look up a service in your Kubernetes cluster nslookup [ pod-ip-address ] .default.pod.cluster.local # Logs Core Dns kubectl logs [ coredns-pod-name ]","title":"DNS"},{"location":"platforms/kubernetes/admin/monitoring/#kube-proxy","text":"# View the endpoints for your service: kubectl get ep # Communicate with the pod directly (without the service): wget -qO- 10 .244.1.6:9376 # Check if kube-proxy is running on the nodes: ps auxw | grep kube-proxy # Check if kube-proxy is writing iptables: kubectl get services -o wide iptables-save | grep hostnames sudo iptables-save | grep KUBE | grep <service-name> # View the list of kube-system pods: kubectl get pods -n kube-system # Connect to your kube-proxy pod in the kube-system namespace: kubectl exec -it kube-proxy-cqptg -n kube-system -- sh","title":"Kube-Proxy"},{"location":"platforms/kubernetes/admin/monitoring/#change-cni-plugin","text":"# Delete the flannel CNI plugin: kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml # Apply the Weave Net CNI plugin: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \"","title":"Change CNI Plugin"},{"location":"platforms/kubernetes/admin/monitoring/#smoke-testing","text":"","title":"Smoke Testing"},{"location":"platforms/kubernetes/admin/monitoring/#data-encryption","text":"# Create a test secret: kubectl create secret generic kubernetes-the-hard-way --from-literal = \"mykey=mydata\" # Log in to one of your master servers, and get the raw data for the test secret from etcd: sudo ETCDCTL_API = 3 etcdctl get \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem \\ /registry/secrets/default/kubernetes-the-hard-way | hexdump -C # Your output should look something like this: 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 | /registry/secret | 00000010 73 2f 64 65 66 61 75 6c 74 2f 6b 75 62 65 72 6e | s/default/kubern | 00000020 65 74 65 73 2d 74 68 65 2d 68 61 72 64 2d 77 61 | etes-the-hard-wa | 00000030 79 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 | y.k8s:enc:aescbc | 00000040 3a 76 31 3a 6b 65 79 31 3a fc 21 ee dc e5 84 8a | :v1:key1:.!..... | 00000050 53 8e fd a9 72 a8 75 25 65 30 55 0e 72 43 1f 20 | S...r.u%e0U.rC. | 00000060 9f 07 15 4f 69 8a 79 a4 70 62 e9 ab f9 14 93 2e | ...Oi.y.pb...... | 00000070 e5 59 3f ab a7 b2 d8 d6 05 84 84 aa c3 6f 8d 5c | .Y?..........o. \\| 00000080 09 7a 2f 82 81 b5 d5 ec ba c7 23 34 46 d9 43 02 | .z/.......#4F.C. | 00000090 88 93 57 26 66 da 4e 8e 5c 24 44 6e 3e ec 9c 8e | ..W & f.N. \\$ Dn>... | 000000a0 83 ff 40 9a fb 94 07 3c 08 52 0e 77 50 81 c9 d0 | ..@....<.R.wP... | 000000b0 b7 30 68 ba b1 b3 26 eb b1 9f 3f f1 d7 76 86 09 | .0h... & ...?..v.. | 000000c0 d8 14 02 12 09 30 b0 60 b2 ad dc bb cf f5 77 e0 | .....0. ` ......w. | 000000d0 4f 0b 1f 74 79 c1 e7 20 1d 32 b2 68 01 19 93 fc | O..ty.. .2.h.... | 000000e0 f5 c8 8b 0b 16 7b 4f c2 6a 0a | ..... { O.j. | 000000ea # Look for k8s:enc:aescbc:v1:key1 on the right of the output to verify that the data is stored in an encrypted format!","title":"Data Encryption"},{"location":"platforms/kubernetes/admin/monitoring/#deployments","text":"# Create a a simple nginx deployment: kubectl run nginx --image = nginx # Verify that the deployment created a pod and that the pod is running: kubectl get pods -l run = nginx # Verify that the output looks something like this: NAME READY STATUS RESTARTS AGE nginx-65899c769f-9xnqm 1 /1 Running 0 30s","title":"Deployments"},{"location":"platforms/kubernetes/admin/monitoring/#port-forwarding","text":"# First, get the pod name of the nginx pod and store it an an environment variable: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # Forward port 8081 to the nginx pod: kubectl port-forward $POD_NAME 8081 :80 # Open up a new terminal on the same machine running the kubectl port-forward command and verify that the port forward works. curl --head http://127.0.0.1:8081 # You should get an http 200 OK response from the nginx pod.","title":"Port Forwarding"},{"location":"platforms/kubernetes/admin/monitoring/#logs_1","text":"# First, let's set an environment variable to the name of the nginx pod: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # Get the logs from the nginx pod: kubectl logs $POD_NAME # This command should return the nginx pod's logs. It will look something like this (but there could be more lines): 127 .0.0.1 - - [ 10 /Sep/2018:19:29:01 +0000 ] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\"","title":"Logs"},{"location":"platforms/kubernetes/admin/monitoring/#exec","text":"# First, let's set an environment variable to the name of the nginx pod: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # To test kubectl exec, execute a simple nginx -v command inside the nginx pod: kubectl exec -ti $POD_NAME -- nginx -v # This command should return the nginx version output, which should look like this: nginx version: nginx/1.15.3","title":"Exec"},{"location":"platforms/kubernetes/admin/monitoring/#services","text":"# First, create a service to expose the nginx deployment: kubectl expose deployment nginx --port 80 --type NodePort # Get the node port assigned to the newly-created service and assign it to an environment variable: kubectl get svc # The output should look something like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .32.0.1 <none> 443 /TCP 20d nginx NodePort 10 .32.0.81 <none> 80 :32642/TCP 2m # Look for the service called nginx in that output. Under PORT(S), look for the second port, listed after 80:. In the example above, it is 32642. That is the node port, so make note of that value since you will need it in a moment. # Next, log in to one of your worker servers and make a request to the service using the node port. Be sure to replace the placeholder with the actual node port: curl -I localhost:<node port> # You should get an http 200 OK response.","title":"Services"},{"location":"platforms/kubernetes/admin/monitoring/#untrusted-workloads","text":"# First, create an untrusted pod: cat << EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: untrusted annotations: io.kubernetes.cri.untrusted-workload: \"true\" spec: containers: - name: webserver image: gcr.io/hightowerlabs/helloworld:2.0.0 EOF # Make sure that the untrusted pod is running: kubectl get pods untrusted -o wide # Take note of which worker node the untrusted pod is running on, then log into that worker node. # On the worker node, list all of the containers running under gVisor: sudo runsc --root /run/containerd/runsc/k8s.io list # Get the pod ID of the untrusted pod and store it in an environment variable: POD_ID = $( sudo crictl -r unix:///var/run/containerd/containerd.sock \\ pods --name untrusted -q ) # Get the container ID of the container running in the untrusted pod and store it in an environment variable: CONTAINER_ID = $( sudo crictl -r unix:///var/run/containerd/containerd.sock \\ ps -p ${ POD_ID } -q ) # Get information about the process running in the container: sudo runsc --root /run/containerd/runsc/k8s.io ps ${ CONTAINER_ID } # Since we were able to get the process info using runsc, we know that the untrusted container is running securely as expected.","title":"Untrusted Workloads"},{"location":"platforms/kubernetes/admin/networking/","text":"Flannel \u00b6 Redhat \u00b6 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Ubuntu \u00b6 On all nodes echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Master kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml Weave Net \u00b6 Setting up # First, log in to both worker nodes and enable IP forwarding: sudo sysctl net.ipv4.conf.all.forwarding = 1 echo \"net.ipv4.conf.all.forwarding=1\" | sudo tee -a /etc/sysctl.conf # The remaining commands can be done using kubectl. To connect with kubectl, you can either log in to one of the control nodes and run kubectl there or open an SSH tunnel for port 6443 to the load balancer server and use kubectl locally. # You can open the SSH tunnel by running this in a separate terminal. Leave the session open while you are working to keep the tunnel active: ssh -L 6443 :localhost:6443 user@<your Load balancer cloud server public IP> kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) &env.IPALLOC_RANGE=10.200.0.0/16\" # Now Weave Net is installed, but we need to test our network to make sure everything is working. # First, make sure the Weave Net pods are up and running: kubectl get pods -n kube-system #cThis should return two Weave Net pods, and look something like this: NAME READY STATUS RESTARTS AGE weave-net-m69xq 2 /2 Running 0 11s weave-net-vmb2n 2 /2 Running 0 11s # Next, we want to test that pods can connect to each other and that they can connect to services. We will set up two Nginx pods and a service for those two pods. Then, we will create a busybox pod and use it to test connectivity to both Nginx pods and the service. # First, create an Nginx deployment with 2 replicas: cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: run: nginx replicas: 2 template: metadata: labels: run: nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EOF # Next, create a service for that deployment so that we can test connectivity to services as well: # kubectl expose deployment/nginx # Now let's start up another pod. We will use this pod to test our networking. We will test whether we can connect to the other pods and services from this pod. kubectl run busybox --image = radial/busyboxplus:curl --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) # Now let's get the IP addresses of our two Nginx pods: kubectl get ep nginx # There should be two IP addresses listed under ENDPOINTS, for example: NAME ENDPOINTS AGE nginx 10 .200.0.2:80,10.200.128.1:80 50m # Now let's make sure the busybox pod can connect to the Nginx pods on both of those IP addresses. kubectl exec $POD_NAME -- curl <first nginx pod IP address> kubectl exec $POD_NAME -- curl <second nginx pod IP address> # Both commands should return some HTML with the title \"Welcome to Nginx!\" This means that we can successfully connect to other pods. # Now let's verify that we can connect to services. kubectl get svc # This should display the IP address for our Nginx service. For example, in this case, the IP is 10.32.0.54: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .32.0.1 <none> 443 /TCP 1h nginx ClusterIP 10 .32.0.54 <none> 80 /TCP 53m # Let's see if we can access the service from the busybox pod! kubectl exec $POD_NAME -- curl <nginx service IP address> Network Policies \u00b6 Plugin Canal \u00b6 wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml Deny All \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : deny-all spec : podSelector : {} policyTypes : - Ingress Pod Selector \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : db-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - podSelector : matchLabels : app : web ports : - port : 5432 Namespace Policy \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ns-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - namespaceSelector : matchLabels : tenant : web ports : - port : 5432 Block IP \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ipblock-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - ipBlock : cidr : 192.168.1.0/24 Egress Policy \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : egress-netpol spec : podSelector : matchLabels : app : web egress : - to : - podSelector : matchLabels : app : db ports : - port : 5432 DNS \u00b6 Kube-Dns \u00b6 kubectl create -f https://storage.googleapis.com/kubernetes-the-hard-way/kube-dns.yaml # Verify that the kube-dns pod starts up correctly: kubectl get pods -l k8s-app = kube-dns -n kube-system # You should get output showing the kube-dns pod. It should look something like this: NAME READY STATUS RESTARTS AGE kube-dns-598d7bf7d4-spbmj 3 /3 Running 0 36s # Make sure that 3/3 containers are ready, and that the pod has a status of Running. It may take a moment for the pod to be fully up and running, so if READY is not 3/3 at first, check again after a few moments. # Now let's test our kube-dns installation by doing a DNS lookup from within a pod. First, we need to start up a pod that we can use for testing: kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) # Next, run an nslookup from inside the busybox container: kubectl exec -ti $POD_NAME -- nslookup kubernetes # You should get output that looks something like this: Server: 10 .32.0.10 Address 1 : 10 .32.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .32.0.1 kubernetes.default.svc.cluster.local Custom DNS \u00b6 apiVersion : v1 kind : Pod metadata : namespace : default name : dns-example spec : containers : - name : test image : nginx dnsPolicy : \"None\" dnsConfig : nameservers : - 8.8.8.8 searches : - ns1.svc.cluster.local - my.dns.search.suffix options : - name : ndots value : \"2\" - name : edns0","title":"Networking"},{"location":"platforms/kubernetes/admin/networking/#flannel","text":"","title":"Flannel"},{"location":"platforms/kubernetes/admin/networking/#redhat","text":"kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml","title":"Redhat"},{"location":"platforms/kubernetes/admin/networking/#ubuntu","text":"On all nodes echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Master kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml","title":"Ubuntu"},{"location":"platforms/kubernetes/admin/networking/#weave-net","text":"Setting up # First, log in to both worker nodes and enable IP forwarding: sudo sysctl net.ipv4.conf.all.forwarding = 1 echo \"net.ipv4.conf.all.forwarding=1\" | sudo tee -a /etc/sysctl.conf # The remaining commands can be done using kubectl. To connect with kubectl, you can either log in to one of the control nodes and run kubectl there or open an SSH tunnel for port 6443 to the load balancer server and use kubectl locally. # You can open the SSH tunnel by running this in a separate terminal. Leave the session open while you are working to keep the tunnel active: ssh -L 6443 :localhost:6443 user@<your Load balancer cloud server public IP> kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) &env.IPALLOC_RANGE=10.200.0.0/16\" # Now Weave Net is installed, but we need to test our network to make sure everything is working. # First, make sure the Weave Net pods are up and running: kubectl get pods -n kube-system #cThis should return two Weave Net pods, and look something like this: NAME READY STATUS RESTARTS AGE weave-net-m69xq 2 /2 Running 0 11s weave-net-vmb2n 2 /2 Running 0 11s # Next, we want to test that pods can connect to each other and that they can connect to services. We will set up two Nginx pods and a service for those two pods. Then, we will create a busybox pod and use it to test connectivity to both Nginx pods and the service. # First, create an Nginx deployment with 2 replicas: cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: run: nginx replicas: 2 template: metadata: labels: run: nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EOF # Next, create a service for that deployment so that we can test connectivity to services as well: # kubectl expose deployment/nginx # Now let's start up another pod. We will use this pod to test our networking. We will test whether we can connect to the other pods and services from this pod. kubectl run busybox --image = radial/busyboxplus:curl --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) # Now let's get the IP addresses of our two Nginx pods: kubectl get ep nginx # There should be two IP addresses listed under ENDPOINTS, for example: NAME ENDPOINTS AGE nginx 10 .200.0.2:80,10.200.128.1:80 50m # Now let's make sure the busybox pod can connect to the Nginx pods on both of those IP addresses. kubectl exec $POD_NAME -- curl <first nginx pod IP address> kubectl exec $POD_NAME -- curl <second nginx pod IP address> # Both commands should return some HTML with the title \"Welcome to Nginx!\" This means that we can successfully connect to other pods. # Now let's verify that we can connect to services. kubectl get svc # This should display the IP address for our Nginx service. For example, in this case, the IP is 10.32.0.54: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .32.0.1 <none> 443 /TCP 1h nginx ClusterIP 10 .32.0.54 <none> 80 /TCP 53m # Let's see if we can access the service from the busybox pod! kubectl exec $POD_NAME -- curl <nginx service IP address>","title":"Weave Net"},{"location":"platforms/kubernetes/admin/networking/#network-policies","text":"","title":"Network Policies"},{"location":"platforms/kubernetes/admin/networking/#plugin-canal","text":"wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml","title":"Plugin Canal"},{"location":"platforms/kubernetes/admin/networking/#deny-all","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : deny-all spec : podSelector : {} policyTypes : - Ingress","title":"Deny All"},{"location":"platforms/kubernetes/admin/networking/#pod-selector","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : db-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - podSelector : matchLabels : app : web ports : - port : 5432","title":"Pod Selector"},{"location":"platforms/kubernetes/admin/networking/#namespace-policy","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ns-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - namespaceSelector : matchLabels : tenant : web ports : - port : 5432","title":"Namespace Policy"},{"location":"platforms/kubernetes/admin/networking/#block-ip","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ipblock-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - ipBlock : cidr : 192.168.1.0/24","title":"Block IP"},{"location":"platforms/kubernetes/admin/networking/#egress-policy","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : egress-netpol spec : podSelector : matchLabels : app : web egress : - to : - podSelector : matchLabels : app : db ports : - port : 5432","title":"Egress Policy"},{"location":"platforms/kubernetes/admin/networking/#dns","text":"","title":"DNS"},{"location":"platforms/kubernetes/admin/networking/#kube-dns","text":"kubectl create -f https://storage.googleapis.com/kubernetes-the-hard-way/kube-dns.yaml # Verify that the kube-dns pod starts up correctly: kubectl get pods -l k8s-app = kube-dns -n kube-system # You should get output showing the kube-dns pod. It should look something like this: NAME READY STATUS RESTARTS AGE kube-dns-598d7bf7d4-spbmj 3 /3 Running 0 36s # Make sure that 3/3 containers are ready, and that the pod has a status of Running. It may take a moment for the pod to be fully up and running, so if READY is not 3/3 at first, check again after a few moments. # Now let's test our kube-dns installation by doing a DNS lookup from within a pod. First, we need to start up a pod that we can use for testing: kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) # Next, run an nslookup from inside the busybox container: kubectl exec -ti $POD_NAME -- nslookup kubernetes # You should get output that looks something like this: Server: 10 .32.0.10 Address 1 : 10 .32.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .32.0.1 kubernetes.default.svc.cluster.local","title":"Kube-Dns"},{"location":"platforms/kubernetes/admin/networking/#custom-dns","text":"apiVersion : v1 kind : Pod metadata : namespace : default name : dns-example spec : containers : - name : test image : nginx dnsPolicy : \"None\" dnsConfig : nameservers : - 8.8.8.8 searches : - ns1.svc.cluster.local - my.dns.search.suffix options : - name : ndots value : \"2\" - name : edns0","title":"Custom DNS"},{"location":"platforms/kubernetes/admin/security/","text":"Service Accounts \u00b6 Get \u00b6 kubectl get serviceaccounts Create \u00b6 kubectl get serviceaccounts kubectl get serviceaccounts jenkins -o yaml Pod Example \u00b6 apiVersion : v1 kind : Pod metadata : name : busybox namespace : default spec : serviceAccountName : jenkins containers : - image : busybox:1.28.4 command : - sleep - \"3600\" imagePullPolicy : IfNotPresent name : busybox restartPolicy : Always View the token file from within a pod \u00b6 kubectl get pods -n my-ns kubectl exec -it <name-of-pod> -n my-ns sh cat /var/run/secrets/kubernetes.io/serviceaccount/token Users \u00b6 Create \u00b6 kubectl config view kubectl config set-credentials chad --username = chad --password = password # Create a role binding for anonymous users (not recommended in production): kubectl create clusterrolebinding cluster-system-anonymous --clusterrole = cluster-admin --user = system:anonymous # Need Copy /etc/kubernetes/pki/ca.crt to remote machine # Remote Machine (Install Kubectl) kubectl config set-cluster kubernetes --server = https://172.31.41.61:6443 --certificate-authority = ca.crt --embed-certs = true kubectl config set-credentials chad --username = chad --password = password kubectl config set-context kubernetes --cluster = kubernetes --user = chad --namespace = default kubectl config use-context kubernetes Generate client Cert \u00b6 Generate Private Key openssl genrsa -out mia.key 2048 Generate CSR openssl req -new -key mia.key -out mia.csr -subj \"/CN=mia/O=acg\" Generate Client Certificate openssl x509 -req \\ -in mia.csr \\ -CA cluster.crt \\ -CAkey cluster.key \\ -CAcreateserial \\ -out mia.crt \\ -days 365 Kubectl kubectl config set-credentials mia --client-certificate = mia.crt --client-key = mia.key kubectl config set-context mia --cluster = tiagomsantos.com --namespace = development --user = mia kubectl config use-context mia Roles \u00b6 Role \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : web name : service-reader rules : - apiGroups : [ \"\" ] verbs : [ \"get\" , \"list\" ] resources : [ \"services\" ] RoleBinding \u00b6 kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web Cluster Roles \u00b6 Cluster Role \u00b6 kubectl create clusterrole pv-reader --verb = get,list --resource = persistentvolumes Cluster Role Binding \u00b6 kubectl create clusterrolebinding pv-test --clusterrole = pv-reader --serviceaccount = web:default Test \u00b6 apiVersion : v1 kind : Pod metadata : name : curlpod namespace : web spec : containers : - image : tutum/curl command : [ \"sleep\" , \"9999999\" ] name : main - image : linuxacademycontent/kubectl-proxy name : proxy restartPolicy : Always kubectl apply -f curl-pod.yaml kubectl get pods -n web kubectl exec -it curlpod -n web -- sh curl localhost:8001/api/v1/persistentvolumes TLS Certficates \u00b6 Install cfssl \u00b6 # Download the binaries for the cfssl tool: wget -q --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 # Make the binary files executable: chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson Create Certificate Authority to Kubernetes \u00b6 cd ~/ mkdir kthw cd kthw/ { cat > ca-config.json << EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json << EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca } Generating Client Certificates \u00b6 will generate the following client certificates: admin, kubelet (one for each worker node), kube-controller-manager, kube-proxy, and kube-scheduler Admin Client Certificate { cat > admin-csr.json << EOF { \"CN\": \"admin\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ admin-csr.json | cfssljson -bare admin } Kubelet Client certificates export WORKER0_HOST = <Public hostname of your first worker node cloud server> export WORKER0_IP = <Private IP of your first worker node cloud server> export WORKER1_HOST = <Public hostname of your second worker node cloud server> export WORKER1_IP = <Private IP of your second worker node cloud server> { cat > ${ WORKER0_HOST } -csr.json << EOF { \"CN\": \"system:node:${WORKER0_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ WORKER0_IP } , ${ WORKER0_HOST } \\ -profile = kubernetes \\ ${ WORKER0_HOST } -csr.json | cfssljson -bare ${ WORKER0_HOST } cat > ${ WORKER1_HOST } -csr.json << EOF { \"CN\": \"system:node:${WORKER1_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ WORKER1_IP } , ${ WORKER1_HOST } \\ -profile = kubernetes \\ ${ WORKER1_HOST } -csr.json | cfssljson -bare ${ WORKER1_HOST } } Controller Manager Client certificate { cat > kube-controller-manager-csr.json << EOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager } Kube-proxy Client certificate { cat > kube-proxy-csr.json << EOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy } Kube Scheduler Client Certificate { cat > kube-scheduler-csr.json << EOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler } Generating the Kubernetes API Server Certificate \u00b6 Note: 10.32.0.1 - Common use this IP. Can be used by the pods in some scenarios cd ~/kthw export CERT_HOSTNAME = 10 .32.0.1,<controller node 1 Private IP>,<controller node 1 hostname>,<controller node 2 Private IP>,<controller node 2 hostname>,<API load balancer Private IP>,<API load balancer hostname>,127.0.0.1,localhost,kubernetes.default { cat > kubernetes-csr.json << EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ CERT_HOSTNAME } \\ -profile = kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes } Generating the Service Account Key Pair \u00b6 cd ~/kthw { cat > service-account-csr.json << EOF { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ service-account-csr.json | cfssljson -bare service-account } Distributing the Certificate Files \u00b6 Move certificate files to the worker nodes: \u00b6 scp ca.pem <worker 1 hostname>-key.pem <worker 1 hostname>.pem user@<worker 1 public IP>:~/ scp ca.pem <worker 2 hostname>-key.pem <worker 2 hostname>.pem user@<worker 2 public IP>:~/ Move certificate files to the Master nodes: \u00b6 scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 1 public IP>:~/ scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 2 public IP>:~/ Create TLS For Applications (Not Cluster, only same pods) \u00b6 # Find the CA certificate on a pod in your cluster: kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount cfssl version # Create a CSR file - Need instal cfssl tool cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"my-svc.my-namespace.svc.cluster.local\", \"my-pod.my-namespace.pod.cluster.local\", \"172.168.0.24\", \"10.0.34.2\" ], \"CN\": \"my-pod.my-namespace.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF # Create a CertificateSigningRequest API object: cat <<EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: pod-csr.web spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF # View the CSRs in the cluster: kubectl get csr # View additional details about the CSR: kubectl describe csr pod-csr.web # Approve the CSR: kubectl certificate approve pod-csr.web # View the certificate within your CSR: kubectl get csr pod-csr.web -o yaml # Extract and decode your certificate to use in a file: kubectl get csr pod-csr.web -o jsonpath = '{.status.certificate}' \\ | base64 --decode > server.crt Container Registry \u00b6 Create # Create a new docker-registry secret: kubectl create secret docker-registry acr --docker-server = https://podofminerva.azurecr.io --docker-username = podofminerva --docker-password = 'otj701c9OucKZOCx5qrRblofcNRf3W+e' --docker-email = user@example.com # Modify the default service account to use your new docker-registry secret: kubectl patch sa default -p '{\"imagePullSecrets\": [{\"name\": \"acr\"}]}' apiVersion : v1 kind : Pod metadata : name : acr-pod labels : app : busybox spec : containers : - name : busybox image : podofminerva.azurecr.io/busybox:latest command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] imagePullPolicy : Always Security Contexts \u00b6 The YAML for a container that runs as a user apiVersion : v1 kind : Pod metadata : name : alpine-user-context spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 405 The YAML for a pod that runs the container as non-root apiVersion : v1 kind : Pod metadata : name : alpine-nonroot spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsNonRoot : true The YAML for a privileged container pod apiVersion : v1 kind : Pod metadata : name : privileged-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : privileged : true The YAML for a container that will allow you to change the time apiVersion : v1 kind : Pod metadata : name : kernelchange-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : capabilities : add : - SYS_TIME The YAML for a container that removes capabilities apiVersion : v1 kind : Pod metadata : name : remove-capabilities spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : capabilities : drop : - CHOWN The YAML for a pod container that can\u2019t write to the local filesystem apiVersion : v1 kind : Pod metadata : name : readonly-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : readOnlyRootFilesystem : true volumeMounts : - name : my-volume mountPath : /volume readOnly : false volumes : - name : my-volume emptyDir : The YAML for a pod that has different group permissions for different pods apiVersion : v1 kind : Pod metadata : name : group-context spec : securityContext : fsGroup : 555 supplementalGroups : [ 666 , 777 ] containers : - name : first image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 1111 volumeMounts : - name : shared-volume mountPath : /volume readOnly : false - name : second image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 2222 volumeMounts : - name : shared-volume mountPath : /volume readOnly : false volumes : - name : shared-volume emptyDir : Persistent Key Value Store \u00b6 # Generate a key for your https server: openssl genrsa -out https.key 2048 # Generate a certificate for the https server: openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN = www.example.com # Create an empty file to create the secret: touch file # Create a secret from your key, cert, and file: kubectl create secret generic example-https --from-file = https.key --from-file = https.cert --from-file = file Create the configMap that will mount to your pod apiVersion : v1 kind : ConfigMap metadata : name : config data : my-nginx-config.conf : | server { listen 80; listen 443 ssl; server_name www.example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval : | 25 The YAML for a pod using the new secret apiVersion : v1 kind : Pod metadata : name : example-https spec : containers : - image : linuxacademycontent/fortune name : html-web env : - name : INTERVAL valueFrom : configMapKeyRef : name : config key : sleep-interval volumeMounts : - name : html mountPath : /var/htdocs - image : nginx:alpine name : web-server volumeMounts : - name : html mountPath : /usr/share/nginx/html readOnly : true - name : config mountPath : /etc/nginx/conf.d readOnly : true - name : certs mountPath : /etc/nginx/certs/ readOnly : true ports : - containerPort : 80 - containerPort : 443 volumes : - name : html emptyDir : {} - name : config configMap : name : config items : - key : my-nginx-config.conf path : https.conf - name : certs secret : secretName : example-https # Use port forwarding on the pod to server traffic from 443: kubectl port-forward example-https 8443 :443 & # Curl the web server to get a response: curl https://localhost:8443 -k","title":"Security"},{"location":"platforms/kubernetes/admin/security/#service-accounts","text":"","title":"Service Accounts"},{"location":"platforms/kubernetes/admin/security/#get","text":"kubectl get serviceaccounts","title":"Get"},{"location":"platforms/kubernetes/admin/security/#create","text":"kubectl get serviceaccounts kubectl get serviceaccounts jenkins -o yaml","title":"Create"},{"location":"platforms/kubernetes/admin/security/#pod-example","text":"apiVersion : v1 kind : Pod metadata : name : busybox namespace : default spec : serviceAccountName : jenkins containers : - image : busybox:1.28.4 command : - sleep - \"3600\" imagePullPolicy : IfNotPresent name : busybox restartPolicy : Always","title":"Pod Example"},{"location":"platforms/kubernetes/admin/security/#view-the-token-file-from-within-a-pod","text":"kubectl get pods -n my-ns kubectl exec -it <name-of-pod> -n my-ns sh cat /var/run/secrets/kubernetes.io/serviceaccount/token","title":"View the token file from within a pod"},{"location":"platforms/kubernetes/admin/security/#users","text":"","title":"Users"},{"location":"platforms/kubernetes/admin/security/#create_1","text":"kubectl config view kubectl config set-credentials chad --username = chad --password = password # Create a role binding for anonymous users (not recommended in production): kubectl create clusterrolebinding cluster-system-anonymous --clusterrole = cluster-admin --user = system:anonymous # Need Copy /etc/kubernetes/pki/ca.crt to remote machine # Remote Machine (Install Kubectl) kubectl config set-cluster kubernetes --server = https://172.31.41.61:6443 --certificate-authority = ca.crt --embed-certs = true kubectl config set-credentials chad --username = chad --password = password kubectl config set-context kubernetes --cluster = kubernetes --user = chad --namespace = default kubectl config use-context kubernetes","title":"Create"},{"location":"platforms/kubernetes/admin/security/#generate-client-cert","text":"Generate Private Key openssl genrsa -out mia.key 2048 Generate CSR openssl req -new -key mia.key -out mia.csr -subj \"/CN=mia/O=acg\" Generate Client Certificate openssl x509 -req \\ -in mia.csr \\ -CA cluster.crt \\ -CAkey cluster.key \\ -CAcreateserial \\ -out mia.crt \\ -days 365 Kubectl kubectl config set-credentials mia --client-certificate = mia.crt --client-key = mia.key kubectl config set-context mia --cluster = tiagomsantos.com --namespace = development --user = mia kubectl config use-context mia","title":"Generate client Cert"},{"location":"platforms/kubernetes/admin/security/#roles","text":"","title":"Roles"},{"location":"platforms/kubernetes/admin/security/#role","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : web name : service-reader rules : - apiGroups : [ \"\" ] verbs : [ \"get\" , \"list\" ] resources : [ \"services\" ]","title":"Role"},{"location":"platforms/kubernetes/admin/security/#rolebinding","text":"kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web","title":"RoleBinding"},{"location":"platforms/kubernetes/admin/security/#cluster-roles","text":"","title":"Cluster Roles"},{"location":"platforms/kubernetes/admin/security/#cluster-role","text":"kubectl create clusterrole pv-reader --verb = get,list --resource = persistentvolumes","title":"Cluster Role"},{"location":"platforms/kubernetes/admin/security/#cluster-role-binding","text":"kubectl create clusterrolebinding pv-test --clusterrole = pv-reader --serviceaccount = web:default","title":"Cluster Role Binding"},{"location":"platforms/kubernetes/admin/security/#test","text":"apiVersion : v1 kind : Pod metadata : name : curlpod namespace : web spec : containers : - image : tutum/curl command : [ \"sleep\" , \"9999999\" ] name : main - image : linuxacademycontent/kubectl-proxy name : proxy restartPolicy : Always kubectl apply -f curl-pod.yaml kubectl get pods -n web kubectl exec -it curlpod -n web -- sh curl localhost:8001/api/v1/persistentvolumes","title":"Test"},{"location":"platforms/kubernetes/admin/security/#tls-certficates","text":"","title":"TLS Certficates"},{"location":"platforms/kubernetes/admin/security/#install-cfssl","text":"# Download the binaries for the cfssl tool: wget -q --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 # Make the binary files executable: chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson","title":"Install cfssl"},{"location":"platforms/kubernetes/admin/security/#create-certificate-authority-to-kubernetes","text":"cd ~/ mkdir kthw cd kthw/ { cat > ca-config.json << EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json << EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca }","title":"Create Certificate Authority to Kubernetes"},{"location":"platforms/kubernetes/admin/security/#generating-client-certificates","text":"will generate the following client certificates: admin, kubelet (one for each worker node), kube-controller-manager, kube-proxy, and kube-scheduler Admin Client Certificate { cat > admin-csr.json << EOF { \"CN\": \"admin\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ admin-csr.json | cfssljson -bare admin } Kubelet Client certificates export WORKER0_HOST = <Public hostname of your first worker node cloud server> export WORKER0_IP = <Private IP of your first worker node cloud server> export WORKER1_HOST = <Public hostname of your second worker node cloud server> export WORKER1_IP = <Private IP of your second worker node cloud server> { cat > ${ WORKER0_HOST } -csr.json << EOF { \"CN\": \"system:node:${WORKER0_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ WORKER0_IP } , ${ WORKER0_HOST } \\ -profile = kubernetes \\ ${ WORKER0_HOST } -csr.json | cfssljson -bare ${ WORKER0_HOST } cat > ${ WORKER1_HOST } -csr.json << EOF { \"CN\": \"system:node:${WORKER1_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ WORKER1_IP } , ${ WORKER1_HOST } \\ -profile = kubernetes \\ ${ WORKER1_HOST } -csr.json | cfssljson -bare ${ WORKER1_HOST } } Controller Manager Client certificate { cat > kube-controller-manager-csr.json << EOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager } Kube-proxy Client certificate { cat > kube-proxy-csr.json << EOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy } Kube Scheduler Client Certificate { cat > kube-scheduler-csr.json << EOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler }","title":"Generating Client Certificates"},{"location":"platforms/kubernetes/admin/security/#generating-the-kubernetes-api-server-certificate","text":"Note: 10.32.0.1 - Common use this IP. Can be used by the pods in some scenarios cd ~/kthw export CERT_HOSTNAME = 10 .32.0.1,<controller node 1 Private IP>,<controller node 1 hostname>,<controller node 2 Private IP>,<controller node 2 hostname>,<API load balancer Private IP>,<API load balancer hostname>,127.0.0.1,localhost,kubernetes.default { cat > kubernetes-csr.json << EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ CERT_HOSTNAME } \\ -profile = kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes }","title":"Generating the Kubernetes API Server Certificate"},{"location":"platforms/kubernetes/admin/security/#generating-the-service-account-key-pair","text":"cd ~/kthw { cat > service-account-csr.json << EOF { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ service-account-csr.json | cfssljson -bare service-account }","title":"Generating the Service Account Key Pair"},{"location":"platforms/kubernetes/admin/security/#distributing-the-certificate-files","text":"","title":"Distributing the Certificate Files"},{"location":"platforms/kubernetes/admin/security/#move-certificate-files-to-the-worker-nodes","text":"scp ca.pem <worker 1 hostname>-key.pem <worker 1 hostname>.pem user@<worker 1 public IP>:~/ scp ca.pem <worker 2 hostname>-key.pem <worker 2 hostname>.pem user@<worker 2 public IP>:~/","title":"Move certificate files to the worker nodes:"},{"location":"platforms/kubernetes/admin/security/#move-certificate-files-to-the-master-nodes","text":"scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 1 public IP>:~/ scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 2 public IP>:~/","title":"Move certificate files to the Master nodes:"},{"location":"platforms/kubernetes/admin/security/#create-tls-for-applications-not-cluster-only-same-pods","text":"# Find the CA certificate on a pod in your cluster: kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount cfssl version # Create a CSR file - Need instal cfssl tool cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"my-svc.my-namespace.svc.cluster.local\", \"my-pod.my-namespace.pod.cluster.local\", \"172.168.0.24\", \"10.0.34.2\" ], \"CN\": \"my-pod.my-namespace.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF # Create a CertificateSigningRequest API object: cat <<EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: pod-csr.web spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF # View the CSRs in the cluster: kubectl get csr # View additional details about the CSR: kubectl describe csr pod-csr.web # Approve the CSR: kubectl certificate approve pod-csr.web # View the certificate within your CSR: kubectl get csr pod-csr.web -o yaml # Extract and decode your certificate to use in a file: kubectl get csr pod-csr.web -o jsonpath = '{.status.certificate}' \\ | base64 --decode > server.crt","title":"Create TLS For Applications (Not Cluster, only same pods)"},{"location":"platforms/kubernetes/admin/security/#container-registry","text":"Create # Create a new docker-registry secret: kubectl create secret docker-registry acr --docker-server = https://podofminerva.azurecr.io --docker-username = podofminerva --docker-password = 'otj701c9OucKZOCx5qrRblofcNRf3W+e' --docker-email = user@example.com # Modify the default service account to use your new docker-registry secret: kubectl patch sa default -p '{\"imagePullSecrets\": [{\"name\": \"acr\"}]}' apiVersion : v1 kind : Pod metadata : name : acr-pod labels : app : busybox spec : containers : - name : busybox image : podofminerva.azurecr.io/busybox:latest command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] imagePullPolicy : Always","title":"Container Registry"},{"location":"platforms/kubernetes/admin/security/#security-contexts","text":"The YAML for a container that runs as a user apiVersion : v1 kind : Pod metadata : name : alpine-user-context spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 405 The YAML for a pod that runs the container as non-root apiVersion : v1 kind : Pod metadata : name : alpine-nonroot spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsNonRoot : true The YAML for a privileged container pod apiVersion : v1 kind : Pod metadata : name : privileged-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : privileged : true The YAML for a container that will allow you to change the time apiVersion : v1 kind : Pod metadata : name : kernelchange-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : capabilities : add : - SYS_TIME The YAML for a container that removes capabilities apiVersion : v1 kind : Pod metadata : name : remove-capabilities spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : capabilities : drop : - CHOWN The YAML for a pod container that can\u2019t write to the local filesystem apiVersion : v1 kind : Pod metadata : name : readonly-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : readOnlyRootFilesystem : true volumeMounts : - name : my-volume mountPath : /volume readOnly : false volumes : - name : my-volume emptyDir : The YAML for a pod that has different group permissions for different pods apiVersion : v1 kind : Pod metadata : name : group-context spec : securityContext : fsGroup : 555 supplementalGroups : [ 666 , 777 ] containers : - name : first image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 1111 volumeMounts : - name : shared-volume mountPath : /volume readOnly : false - name : second image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 2222 volumeMounts : - name : shared-volume mountPath : /volume readOnly : false volumes : - name : shared-volume emptyDir :","title":"Security Contexts"},{"location":"platforms/kubernetes/admin/security/#persistent-key-value-store","text":"# Generate a key for your https server: openssl genrsa -out https.key 2048 # Generate a certificate for the https server: openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN = www.example.com # Create an empty file to create the secret: touch file # Create a secret from your key, cert, and file: kubectl create secret generic example-https --from-file = https.key --from-file = https.cert --from-file = file Create the configMap that will mount to your pod apiVersion : v1 kind : ConfigMap metadata : name : config data : my-nginx-config.conf : | server { listen 80; listen 443 ssl; server_name www.example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval : | 25 The YAML for a pod using the new secret apiVersion : v1 kind : Pod metadata : name : example-https spec : containers : - image : linuxacademycontent/fortune name : html-web env : - name : INTERVAL valueFrom : configMapKeyRef : name : config key : sleep-interval volumeMounts : - name : html mountPath : /var/htdocs - image : nginx:alpine name : web-server volumeMounts : - name : html mountPath : /usr/share/nginx/html readOnly : true - name : config mountPath : /etc/nginx/conf.d readOnly : true - name : certs mountPath : /etc/nginx/certs/ readOnly : true ports : - containerPort : 80 - containerPort : 443 volumes : - name : html emptyDir : {} - name : config configMap : name : config items : - key : my-nginx-config.conf path : https.conf - name : certs secret : secretName : example-https # Use port forwarding on the pod to server traffic from 443: kubectl port-forward example-https 8443 :443 & # Curl the web server to get a response: curl https://localhost:8443 -k","title":"Persistent Key Value Store"},{"location":"platforms/kubernetes/developer/daemonsets/","text":"apiVersion : apps/v1beta2 kind : DaemonSet metadata : name : ssd-monitor spec : selector : matchLabels : app : ssd-monitor template : metadata : labels : app : ssd-monitor spec : nodeSelector : disk : ssd containers : - name : main image : linuxacademycontent/ssd-monitor","title":"DaemonSets"},{"location":"platforms/kubernetes/developer/deployments/","text":"Example \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : example-deployment labels : app : nginx spec : replicas : 2 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : darealmc/nginx-k8s:v1 ports : - containerPort : 80 Node affinity \u00b6 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : pref spec : replicas : 5 template : metadata : labels : app : pref spec : affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 80 preference : matchExpressions : - key : availability-zone operator : In values : - zone1 - weight : 20 preference : matchExpressions : - key : share-type operator : In values : - dedicated containers : - args : - sleep - \"99999\" image : busybox name : main Update Image \u00b6 kubectl set image deployment.v1.apps/example-deployment nginx = darealmc/nginx-k8s:v2 MicroServices Example \u00b6 cd ~/ git clone https://github.com/linuxacademy/robot-shop.git kubectl create namespace robot-shop kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors/ kubectl get pods -n robot-shop -w # Access in http://$kube_server_public_ip:30080 Application LifeCycle Manager \u00b6 Update \u00b6 kubectl apply -f kubeserve-deployment.yaml kubectl replace -f kubeserve-deployment.yaml Rolling Update \u00b6 kubectl set image deployments/kubeserve app = linuxacademycontent/kubeserve:v2 --v 6 Rollback \u00b6 # Use --record flag to create the deployment - kubectl create -f kubeserve-deployment.yaml --record kubectl rollout undo deployments kubeserve kubectl rollout history deployment kubeserve kubectl rollout undo deployment kubeserve --to-revision = 2 Pause / Resume \u00b6 kubectl rollout undo deployment kubeserve --to-revision = 2 kubectl rollout resume deployment kubeserve Readiness Probe \u00b6 apiVersion: apps/v1 kind: Deployment metadata: name: kubeserve spec: replicas: 3 selector: matchLabels: app: kubeserve minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubeserve labels: app: kubeserve spec: containers: - image: linuxacademycontent/kubeserve:v3 name: app readinessProbe: periodSeconds: 1 httpGet: path: / port: 80","title":"Deployments"},{"location":"platforms/kubernetes/developer/deployments/#example","text":"apiVersion : apps/v1 kind : Deployment metadata : name : example-deployment labels : app : nginx spec : replicas : 2 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : darealmc/nginx-k8s:v1 ports : - containerPort : 80","title":"Example"},{"location":"platforms/kubernetes/developer/deployments/#node-affinity","text":"apiVersion : extensions/v1beta1 kind : Deployment metadata : name : pref spec : replicas : 5 template : metadata : labels : app : pref spec : affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 80 preference : matchExpressions : - key : availability-zone operator : In values : - zone1 - weight : 20 preference : matchExpressions : - key : share-type operator : In values : - dedicated containers : - args : - sleep - \"99999\" image : busybox name : main","title":"Node affinity"},{"location":"platforms/kubernetes/developer/deployments/#update-image","text":"kubectl set image deployment.v1.apps/example-deployment nginx = darealmc/nginx-k8s:v2","title":"Update Image"},{"location":"platforms/kubernetes/developer/deployments/#microservices-example","text":"cd ~/ git clone https://github.com/linuxacademy/robot-shop.git kubectl create namespace robot-shop kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors/ kubectl get pods -n robot-shop -w # Access in http://$kube_server_public_ip:30080","title":"MicroServices Example"},{"location":"platforms/kubernetes/developer/deployments/#application-lifecycle-manager","text":"","title":"Application LifeCycle Manager"},{"location":"platforms/kubernetes/developer/deployments/#update","text":"kubectl apply -f kubeserve-deployment.yaml kubectl replace -f kubeserve-deployment.yaml","title":"Update"},{"location":"platforms/kubernetes/developer/deployments/#rolling-update","text":"kubectl set image deployments/kubeserve app = linuxacademycontent/kubeserve:v2 --v 6","title":"Rolling Update"},{"location":"platforms/kubernetes/developer/deployments/#rollback","text":"# Use --record flag to create the deployment - kubectl create -f kubeserve-deployment.yaml --record kubectl rollout undo deployments kubeserve kubectl rollout history deployment kubeserve kubectl rollout undo deployment kubeserve --to-revision = 2","title":"Rollback"},{"location":"platforms/kubernetes/developer/deployments/#pause-resume","text":"kubectl rollout undo deployment kubeserve --to-revision = 2 kubectl rollout resume deployment kubeserve","title":"Pause / Resume"},{"location":"platforms/kubernetes/developer/deployments/#readiness-probe","text":"apiVersion: apps/v1 kind: Deployment metadata: name: kubeserve spec: replicas: 3 selector: matchLabels: app: kubeserve minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubeserve labels: app: kubeserve spec: containers: - image: linuxacademycontent/kubeserve:v3 name: app readinessProbe: periodSeconds: 1 httpGet: path: / port: 80","title":"Readiness Probe"},{"location":"platforms/kubernetes/developer/ingress/","text":"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: service-ingress spec: rules: - host: kubeserve.example.com http: paths: - backend: serviceName: kubeserve2 servicePort: 80 - host: app.example.com http: paths: - backend: serviceName: nginx servicePort: 80 - http: paths: - backend: serviceName: httpd servicePort: 80","title":"Ingress"},{"location":"platforms/kubernetes/developer/jobs/","text":"Job \u00b6 apiVersion : batch/v1 kind : Job metadata : name : pi spec : template : spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never backoffLimit : 4 CronJob \u00b6 apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure","title":"Jobs"},{"location":"platforms/kubernetes/developer/jobs/#job","text":"apiVersion : batch/v1 kind : Job metadata : name : pi spec : template : spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never backoffLimit : 4","title":"Job"},{"location":"platforms/kubernetes/developer/jobs/#cronjob","text":"apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure","title":"CronJob"},{"location":"platforms/kubernetes/developer/pods/","text":"Pods \u00b6 Get \u00b6 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl get pods --namespace = podexample -o wide kubectl get pods -o custom-columns = POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system Example \u00b6 apiVersion : v1 kind : Pod metadata : name : examplepod namespace : pod-example spec : schedulerName : default-scheduler # To change scheduler - my-scheduler volumes : - name : html emptyDir : {} containers : - name : webcontainer image : nginx volumeMounts : - name : html mountPath : /usr/share/nginx/html - name : filecontainer image : debian volumeMounts : - name : html mountPath : /html command : [ \"/bin/sh\" , \"-c\" ] args : - while true; do date >> /html/index.html; sleep 1; done Resources Requests and Limits \u00b6 apiVersion : v1 kind : Pod metadata : name : resource-pod2 spec : nodeSelector : kubernetes.io/hostname : \"chadcrowell3c.mylabserver.com\" containers : - image : busybox command : [ \"dd\" , \"if=/dev/zero\" , \"of=/dev/null\" ] name : pod2 resources : requests : cpu : 1000m memory : 20Mi apiVersion : v1 kind : Pod metadata : name : limited-pod spec : containers : - image : busybox command : [ \"dd\" , \"if=/dev/zero\" , \"of=/dev/null\" ] name : main resources : limits : cpu : 1 memory : 20Mi Get Containers Name inside a pod \u00b6 kubectl get pods examplepod -n podexample -o jsonpath = '{.spec.containers[*].name}*' Use port forwarding to access a pod directly \u00b6 kubectl port-forward $pod_name 8081 :80","title":"Pods"},{"location":"platforms/kubernetes/developer/pods/#pods","text":"","title":"Pods"},{"location":"platforms/kubernetes/developer/pods/#get","text":"kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl get pods --namespace = podexample -o wide kubectl get pods -o custom-columns = POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system","title":"Get"},{"location":"platforms/kubernetes/developer/pods/#example","text":"apiVersion : v1 kind : Pod metadata : name : examplepod namespace : pod-example spec : schedulerName : default-scheduler # To change scheduler - my-scheduler volumes : - name : html emptyDir : {} containers : - name : webcontainer image : nginx volumeMounts : - name : html mountPath : /usr/share/nginx/html - name : filecontainer image : debian volumeMounts : - name : html mountPath : /html command : [ \"/bin/sh\" , \"-c\" ] args : - while true; do date >> /html/index.html; sleep 1; done","title":"Example"},{"location":"platforms/kubernetes/developer/pods/#resources-requests-and-limits","text":"apiVersion : v1 kind : Pod metadata : name : resource-pod2 spec : nodeSelector : kubernetes.io/hostname : \"chadcrowell3c.mylabserver.com\" containers : - image : busybox command : [ \"dd\" , \"if=/dev/zero\" , \"of=/dev/null\" ] name : pod2 resources : requests : cpu : 1000m memory : 20Mi apiVersion : v1 kind : Pod metadata : name : limited-pod spec : containers : - image : busybox command : [ \"dd\" , \"if=/dev/zero\" , \"of=/dev/null\" ] name : main resources : limits : cpu : 1 memory : 20Mi","title":"Resources Requests and Limits"},{"location":"platforms/kubernetes/developer/pods/#get-containers-name-inside-a-pod","text":"kubectl get pods examplepod -n podexample -o jsonpath = '{.spec.containers[*].name}*'","title":"Get Containers Name inside a pod"},{"location":"platforms/kubernetes/developer/pods/#use-port-forwarding-to-access-a-pod-directly","text":"kubectl port-forward $pod_name 8081 :80","title":"Use port forwarding to access a pod directly"},{"location":"platforms/kubernetes/developer/replicasets/","text":"apiVersion : apps/v1 kind : ReplicaSet metadata : name : frontend labels : app : nginx tier : frontend spec : replicas : 2 selector : matchLabels : tier : frontend matchExpressions : - { key : tier , operator : In , values : [ frontend ]} template : metadata : labels : app : nginx tier : frontend spec : containers : - name : nginx image : darealmc/nginx-k8s:v1 ports : - containerPort : 80","title":"Replicasets"},{"location":"platforms/kubernetes/developer/secrets_configmaps/","text":"Config Maps \u00b6 Create \u00b6 kubectl create configmap appconfig --from-literal = key1 = value1 --from-literal = key2 = value2 Deploy Pods \u00b6 Env Vars apiVersion : v1 kind : Pod metadata : name : configmap-pod spec : containers : - name : app-container image : busybox:1.28 command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] env : - name : MY_VAR valueFrom : configMapKeyRef : name : appconfig key : key1 Volume apiVersion : v1 kind : Pod metadata : name : configmap-volume-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] volumeMounts : - name : configmapvolume mountPath : /etc/config volumes : - name : configmapvolume configMap : name : appconfig Get \u00b6 kubectl get configmaps --all-namespaces kubectl get configmaps -n kube-system/kube-flannel-cfg kubectl get configmaps/kube-flannel-cfg -n kube-system kubectl describe configmaps/kube-flannel-cfg -n kube-system kubectl get configmap appconfig -o yaml Secrets \u00b6 Create \u00b6 apiVersion : v1 kind : Secret metadata : name : appsecret stringData : cert : value key : value Deploy Pods \u00b6 Env Vars apiVersion : v1 kind : Pod metadata : name : secret-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] env : - name : MY_CERT valueFrom : secretKeyRef : name : appsecret key : cert Volume apiVersion : v1 kind : Pod metadata : name : secret-volume-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] volumeMounts : - name : secretvolume mountPath : /etc/certs volumes : - name : secretvolume secret : secretName : appsecret","title":"Secrets&ConfigMaps"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#config-maps","text":"","title":"Config Maps"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#create","text":"kubectl create configmap appconfig --from-literal = key1 = value1 --from-literal = key2 = value2","title":"Create"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#deploy-pods","text":"Env Vars apiVersion : v1 kind : Pod metadata : name : configmap-pod spec : containers : - name : app-container image : busybox:1.28 command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] env : - name : MY_VAR valueFrom : configMapKeyRef : name : appconfig key : key1 Volume apiVersion : v1 kind : Pod metadata : name : configmap-volume-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] volumeMounts : - name : configmapvolume mountPath : /etc/config volumes : - name : configmapvolume configMap : name : appconfig","title":"Deploy Pods"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#get","text":"kubectl get configmaps --all-namespaces kubectl get configmaps -n kube-system/kube-flannel-cfg kubectl get configmaps/kube-flannel-cfg -n kube-system kubectl describe configmaps/kube-flannel-cfg -n kube-system kubectl get configmap appconfig -o yaml","title":"Get"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#secrets","text":"","title":"Secrets"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#create_1","text":"apiVersion : v1 kind : Secret metadata : name : appsecret stringData : cert : value key : value","title":"Create"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#deploy-pods_1","text":"Env Vars apiVersion : v1 kind : Pod metadata : name : secret-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] env : - name : MY_CERT valueFrom : secretKeyRef : name : appsecret key : cert Volume apiVersion : v1 kind : Pod metadata : name : secret-volume-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] volumeMounts : - name : secretvolume mountPath : /etc/certs volumes : - name : secretvolume secret : secretName : appsecret","title":"Deploy Pods"},{"location":"platforms/kubernetes/developer/services/","text":"Cluster IP \u00b6 kind : Service apiVersion : v1 metadata : name : my-awesome-service spec : type : ClusterIP selector : app : nginx ports : - protocol : TCP port : 32768 targetPort : 80 NodePort \u00b6 kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 32768 # Service Port targetPort: 80 # Pod Port nodePort: 30080 # Node Port LoadBalancer \u00b6 apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx # Study Things # Set the annotation to route load balancer traffic local to the node: kubectl annotate service kubeserve2 externalTrafficPolicy = Local # https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip Headless \u00b6 Test Excert from Kubenertes in Action by Marco Luksa Each connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is? For a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it. apiVersion : v1 kind : Service metadata : name : kube-headless spec : clusterIP : None ports : - port : 80 targetPort : 8080 selector : app : kubserve2 Troubleshooting \u00b6 # Look at the iptables rules for your services: sudo iptables-save | grep KUBE | grep nginx","title":"Services"},{"location":"platforms/kubernetes/developer/services/#cluster-ip","text":"kind : Service apiVersion : v1 metadata : name : my-awesome-service spec : type : ClusterIP selector : app : nginx ports : - protocol : TCP port : 32768 targetPort : 80","title":"Cluster IP"},{"location":"platforms/kubernetes/developer/services/#nodeport","text":"kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 32768 # Service Port targetPort: 80 # Pod Port nodePort: 30080 # Node Port","title":"NodePort"},{"location":"platforms/kubernetes/developer/services/#loadbalancer","text":"apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx # Study Things # Set the annotation to route load balancer traffic local to the node: kubectl annotate service kubeserve2 externalTrafficPolicy = Local # https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip","title":"LoadBalancer"},{"location":"platforms/kubernetes/developer/services/#headless","text":"Test Excert from Kubenertes in Action by Marco Luksa Each connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is? For a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it. apiVersion : v1 kind : Service metadata : name : kube-headless spec : clusterIP : None ports : - port : 80 targetPort : 8080 selector : app : kubserve2","title":"Headless"},{"location":"platforms/kubernetes/developer/services/#troubleshooting","text":"# Look at the iptables rules for your services: sudo iptables-save | grep KUBE | grep nginx","title":"Troubleshooting"},{"location":"platforms/kubernetes/developer/statefulsets/","text":"apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : serviceName : \"nginx\" replicas : 2 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi","title":"StatefulSets"},{"location":"platforms/kubernetes/developer/volumes/","text":"PV \u00b6 HostPath \u00b6 apiVersion : v1 kind : PersistentVolume metadata : name : pv-hostpath spec : storageClassName : local-storage capacity : storage : 1Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" PV Claim \u00b6 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mongodb-pvc spec : resources : requests : storage : 1Gi accessModes : - ReadWriteOnce storageClassName : \"local-storage\" Deploy Pod \u00b6 apiVersion : v1 kind : Pod metadata : name : mongodb spec : containers : - image : mongo name : mongodb volumeMounts : - name : mongodb-data mountPath : /data/db ports : - containerPort : 27017 protocol : TCP volumes : - name : mongodb-data persistentVolumeClaim : claimName : mongodb-pvc Storage Class \u00b6 apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : fast provisioner : kubernetes.io/gce-pd parameters : type : pd-ssd Empty Dir \u00b6 apiVersion : v1 kind : Pod metadata : name : emptydir-pod spec : containers : - image : busybox name : busybox command : [ \"/bin/sh\" , \"-c\" , \"while true; do sleep 3600; done\" ] volumeMounts : - mountPath : /tmp/storage name : vol volumes : - name : vol emptyDir : {}","title":"Volumes"},{"location":"platforms/kubernetes/developer/volumes/#pv","text":"","title":"PV"},{"location":"platforms/kubernetes/developer/volumes/#hostpath","text":"apiVersion : v1 kind : PersistentVolume metadata : name : pv-hostpath spec : storageClassName : local-storage capacity : storage : 1Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\"","title":"HostPath"},{"location":"platforms/kubernetes/developer/volumes/#pv-claim","text":"apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mongodb-pvc spec : resources : requests : storage : 1Gi accessModes : - ReadWriteOnce storageClassName : \"local-storage\"","title":"PV Claim"},{"location":"platforms/kubernetes/developer/volumes/#deploy-pod","text":"apiVersion : v1 kind : Pod metadata : name : mongodb spec : containers : - image : mongo name : mongodb volumeMounts : - name : mongodb-data mountPath : /data/db ports : - containerPort : 27017 protocol : TCP volumes : - name : mongodb-data persistentVolumeClaim : claimName : mongodb-pvc","title":"Deploy Pod"},{"location":"platforms/kubernetes/developer/volumes/#storage-class","text":"apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : fast provisioner : kubernetes.io/gce-pd parameters : type : pd-ssd","title":"Storage Class"},{"location":"platforms/kubernetes/developer/volumes/#empty-dir","text":"apiVersion : v1 kind : Pod metadata : name : emptydir-pod spec : containers : - image : busybox name : busybox command : [ \"/bin/sh\" , \"-c\" , \"while true; do sleep 3600; done\" ] volumeMounts : - mountPath : /tmp/storage name : vol volumes : - name : vol emptyDir : {}","title":"Empty Dir"},{"location":"platforms/unix/archlinux/","text":"Packages \u00b6 Packages Update Package sudo pacman -U <link - see above> Update corrupted or invalid database pacman-key --delete 91FFE0700E80619CEB73235CA88E23E377514E00 pacman-key --populate archlinux Resolve ICU Package Problem Link Download and Install old \"icu\" package ; wget https://archive.archlinux.org/packages/i/icu/icu-62.1-1-x86_64.pkg.tar.xz sudo pacman -U icu-62.1-1-x86_64.pkg.tar.xz Copy all \"icu\" files to a backup directory ; sudo mkdir /usr/lib/backup sudo cp -r /usr/lib/libicu* /usr/lib/backup/ Install new version of \"icu\" again ; sudo pacman -U /var/cache/pacman/pkg/icu-62.1-1-x86_64.pkg.tar.xz Copy this three files back to /var/lib/ direcotry ; sudo cp /var/lib/libicui18n.so.61 /usr/lib/ sudo cp /var/lib/libicuuc.so.61 /usr/lib/ sudo cp /var/lib/libicudata.so.61 /usr/lib/ You can now cleanup backup files ; sudo rm -rf /var/lib/backup","title":"Archlinux"},{"location":"platforms/unix/archlinux/#packages","text":"Packages Update Package sudo pacman -U <link - see above> Update corrupted or invalid database pacman-key --delete 91FFE0700E80619CEB73235CA88E23E377514E00 pacman-key --populate archlinux Resolve ICU Package Problem Link Download and Install old \"icu\" package ; wget https://archive.archlinux.org/packages/i/icu/icu-62.1-1-x86_64.pkg.tar.xz sudo pacman -U icu-62.1-1-x86_64.pkg.tar.xz Copy all \"icu\" files to a backup directory ; sudo mkdir /usr/lib/backup sudo cp -r /usr/lib/libicu* /usr/lib/backup/ Install new version of \"icu\" again ; sudo pacman -U /var/cache/pacman/pkg/icu-62.1-1-x86_64.pkg.tar.xz Copy this three files back to /var/lib/ direcotry ; sudo cp /var/lib/libicui18n.so.61 /usr/lib/ sudo cp /var/lib/libicuuc.so.61 /usr/lib/ sudo cp /var/lib/libicudata.so.61 /usr/lib/ You can now cleanup backup files ; sudo rm -rf /var/lib/backup","title":"Packages"},{"location":"platforms/unix/commands/","text":"Commands \u00b6 Folders \u00b6 Create Multiple Folders mkdir -p { networking,compute,storage } Create Multiple Files in Multiple Folders touch { networking,compute,storage } / { main.tf,variables.tf,outputs.tf } Services \u00b6 Journalctl \u00b6 journalctl -u kubelet.service journalctl # All journalctl -b # Last Boot journalctl -b 1 # Before Last Boot journalctl --since \"2 days ago\" vi /etc/system.d/journald.conf # Config Files Inodes \u00b6 Get File Inode ls -i /etc/passwd Find File by Inode find / -inum 55116 /etc/passwd Get Inodes Available df -i Disk Space \u00b6 du -h du -s # To sum the total du -s / du -h --max-depth = 1 # Show the disk usage by folder, and not the sub dirs because of Depth = 1 Tar \u00b6 # List files without extract tar -tvf etcd-v3.3.13-linux-amd64.tar.gz # Extract only one file tar -xvf etcd-v3.3.13-linux-amd64.tar.gz etcd-v3.3.13-linux-amd64/etcdctl Filesystem \u00b6 fsck (filesystem check) \u00b6 To check errors from filesystem Don't do in mounted filesystem sudo fsck /dev/sdb1 sudo umount /dev/sdb1 dumpe2fs \u00b6 Get info about filesystem sudo dumpe2fs -h /dev/sdb1 tune2fs \u00b6 To get info and tune filesystem tune2fs -l /dev/xvda1 Set Volume Name tune2fs -L Photos /dev/xvda1 Set mount counts will be filesystem checked tune2fs -c 10 /dev/xvda1 fuser \u00b6 Show which process is using the directory # fuser /mount/Photos /mount/Photos: 3157c # ps aux | grep 3157 -- Will see the process, probably /bin/bash # ps auxf -> Show which command executed and the login session called Kernel Modules \u00b6 # To view who use the modules lsmod # To remove sudo rmmod video # To enable sudo modprobe video Libs \u00b6 # Get Shared libs location cat /etc/ld.so.conf # To load new lib ldconfig # Change Library Path temporary export LD_LIBRARY_PATH = /home/nick/lib # Print shared libs from app ldd /bin/ls Hardware \u00b6 List pci Devices lspci lspci -v lspci -vvv Syslog \u00b6 Config Files cd /etc/rsyslog.d/ Rotate Files Conf vi /etc/logrotate.conf","title":"Commands"},{"location":"platforms/unix/commands/#commands","text":"","title":"Commands"},{"location":"platforms/unix/commands/#folders","text":"Create Multiple Folders mkdir -p { networking,compute,storage } Create Multiple Files in Multiple Folders touch { networking,compute,storage } / { main.tf,variables.tf,outputs.tf }","title":"Folders"},{"location":"platforms/unix/commands/#services","text":"","title":"Services"},{"location":"platforms/unix/commands/#journalctl","text":"journalctl -u kubelet.service journalctl # All journalctl -b # Last Boot journalctl -b 1 # Before Last Boot journalctl --since \"2 days ago\" vi /etc/system.d/journald.conf # Config Files","title":"Journalctl"},{"location":"platforms/unix/commands/#inodes","text":"Get File Inode ls -i /etc/passwd Find File by Inode find / -inum 55116 /etc/passwd Get Inodes Available df -i","title":"Inodes"},{"location":"platforms/unix/commands/#disk-space","text":"du -h du -s # To sum the total du -s / du -h --max-depth = 1 # Show the disk usage by folder, and not the sub dirs because of Depth = 1","title":"Disk Space"},{"location":"platforms/unix/commands/#tar","text":"# List files without extract tar -tvf etcd-v3.3.13-linux-amd64.tar.gz # Extract only one file tar -xvf etcd-v3.3.13-linux-amd64.tar.gz etcd-v3.3.13-linux-amd64/etcdctl","title":"Tar"},{"location":"platforms/unix/commands/#filesystem","text":"","title":"Filesystem"},{"location":"platforms/unix/commands/#fsck-filesystem-check","text":"To check errors from filesystem Don't do in mounted filesystem sudo fsck /dev/sdb1 sudo umount /dev/sdb1","title":"fsck (filesystem check)"},{"location":"platforms/unix/commands/#dumpe2fs","text":"Get info about filesystem sudo dumpe2fs -h /dev/sdb1","title":"dumpe2fs"},{"location":"platforms/unix/commands/#tune2fs","text":"To get info and tune filesystem tune2fs -l /dev/xvda1 Set Volume Name tune2fs -L Photos /dev/xvda1 Set mount counts will be filesystem checked tune2fs -c 10 /dev/xvda1","title":"tune2fs"},{"location":"platforms/unix/commands/#fuser","text":"Show which process is using the directory # fuser /mount/Photos /mount/Photos: 3157c # ps aux | grep 3157 -- Will see the process, probably /bin/bash # ps auxf -> Show which command executed and the login session called","title":"fuser"},{"location":"platforms/unix/commands/#kernel-modules","text":"# To view who use the modules lsmod # To remove sudo rmmod video # To enable sudo modprobe video","title":"Kernel Modules"},{"location":"platforms/unix/commands/#libs","text":"# Get Shared libs location cat /etc/ld.so.conf # To load new lib ldconfig # Change Library Path temporary export LD_LIBRARY_PATH = /home/nick/lib # Print shared libs from app ldd /bin/ls","title":"Libs"},{"location":"platforms/unix/commands/#hardware","text":"List pci Devices lspci lspci -v lspci -vvv","title":"Hardware"},{"location":"platforms/unix/commands/#syslog","text":"Config Files cd /etc/rsyslog.d/ Rotate Files Conf vi /etc/logrotate.conf","title":"Syslog"},{"location":"platforms/unix/debian/","text":"Debian \u00b6 Know Errors \u00b6 Unable to lock the administration directory \u00b6 Desc: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? Solution sudo rm /var/lib/apt/lists/lock Dpkg \u00b6 # To list all installed packages dpkg -l | less # To install package sudo dpkg -i dlocate_1.02+nmu3_all.deb # To Remove sudo dpkg --purge dlocate Apt \u00b6 # Sources file cat /etc/apt/sources.list sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade # Better # To Get dependences apt-cache depends apache2 | less","title":"Debian"},{"location":"platforms/unix/debian/#debian","text":"","title":"Debian"},{"location":"platforms/unix/debian/#know-errors","text":"","title":"Know Errors"},{"location":"platforms/unix/debian/#unable-to-lock-the-administration-directory","text":"Desc: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? Solution sudo rm /var/lib/apt/lists/lock","title":"Unable to lock the administration directory"},{"location":"platforms/unix/debian/#dpkg","text":"# To list all installed packages dpkg -l | less # To install package sudo dpkg -i dlocate_1.02+nmu3_all.deb # To Remove sudo dpkg --purge dlocate","title":"Dpkg"},{"location":"platforms/unix/debian/#apt","text":"# Sources file cat /etc/apt/sources.list sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade # Better # To Get dependences apt-cache depends apache2 | less","title":"Apt"},{"location":"platforms/unix/disks/","text":"Disks \u00b6 Mount Disk \u00b6 List Devices fdisk -l Create Partition # fdisk /dev/sdb Command ( m for help ) : n = > Create Partition Partition type: p primary ( 0 primary, 0 extended, 4 free ) e extended Command ( m for help ) : p = > Get partition Command ( m for help ) : w = > Write Format and Create Label # mkfs.xfs /dev/sdb1 -L elastic Mount # vi /etc/fstab LABEL = elastic /elastic xfs defaults 0 0 ## Add this line # mount -a Create ext4 Filesystem \u00b6 Can use parted command # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 88 .5M 1 loop /snap/core/7270 loop1 7 :1 0 18M 1 loop /snap/amazon-ssm-agent/1455 xvda 202 :0 0 8G 0 disk \u2514\u2500xvda1 202 :1 0 8G 0 part / xvdf 202 :80 0 8G 0 disk # fdisk /dev/xvdf Command ( m for help ) : n = > Create Partition Partition type: p primary ( 0 primary, 0 extended, 4 free ) e extended Command ( m for help ) : p = > Print partition Command ( m for help ) : w = > Write # mkfs -L projectA -t ext4 /dev/xvdf1 # Format Partition and create label # mkdir /mnt/ProjectA # mount /dev/xvdf1 /mnt/ProjectA/ # vi /etc/fstab LABEL = projectA /mnt/ProjectA/ ext4 defaults 0 0 # Add This Line umount /mnt/ProjectA/ Create Swap Partition \u00b6 Can use parted command # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 1 Hex code ( type L to list all codes ) : 82 Command ( m for help ) : w # mkswap /dev/xvdf1 # swapon /dev/xvdf1 # to shutoff -> swapoff /dev/xvdf2 # free -m # vi /etc/fstab UUID = 99fd52d5-e821-45a5-9366-30666046406f swap swap default 0 0 Create LVM Filesystem \u00b6 # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 2 Hex code ( type L to list all codes ) : 8e Changed type of partition 'Linux' to 'Linux LVM' . Command ( m for help ) : w The partition table has been altered. Syncing disks. # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 8e Linux LVM /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 5 Hex code ( type L to list all codes ) : 8e Changed type of partition 'Linux' to 'Linux LVM' . Command ( m for help ) : w The partition table has been altered. Syncing disks. # pvcreate /dev/xvdf2 Physical volume \"/dev/xvdf2\" successfully created. # pvcreate /dev/xvdf5 Physical volume \"/dev/xvdf5\" successfully created. # vgcreate VG1 /dev/xvdf2 /dev/xvdf5 Volume group \"VG1\" successfully created root@ip-172-31-81-196:~# vgdisplay /dev/VG1 --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 3 .99 GiB PE Size 4 .00 MiB Total PE 1022 Alloc PE / Size 0 / 0 Free PE / Size 1022 / 3 .99 GiB VG UUID ELopyA-2vai-XuiQ-vm7M-RF9q-k0ve-yUeXM6 # lvcreate VG1 -L +3.9G -n LV1 Rounding up size to full physical extent 3 .90 GiB Logical volume \"LV1\" created. root@ip-172-31-81-196:~# lvdisplay --- Logical volume --- LV Path /dev/VG1/LV1 LV Name LV1 VG Name VG1 LV UUID Q0mXgw-q9E7-ncuH-Vv2t-NTqx-epn9-3EK3DH LV Write Access read/write LV Creation host, time ip-172-31-81-196, 2019 -08-02 21 :45:49 +0000 LV Status available # open 0 LV Size 3 .90 GiB Current LE 999 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253 :0 # mkfs.ext4 /dev/VG1/LV1 mke2fs 1 .44.1 ( 24 -Mar-2018 ) Creating filesystem with 1022976 4k blocks and 256000 inodes Filesystem UUID: 3cc6d97e-a315-4051-a572-16e06bf9c9f9 Superblock backups stored on blocks: 32768 , 98304 , 163840 , 229376 , 294912 , 819200 , 884736 Allocating group tables: done Writing inode tables: done Creating journal ( 16384 blocks ) : done Writing superblocks and filesystem accounting information: done root@ip-172-31-81-196:~# mkdir /mnt/LV1-mount root@ip-172-31-81-196:~# mount /dev/VG1/LV1 /mnt/LV1-mount root@ip-172-31-81-196:~# vi /etc/fstab /dev/VG1/LV1 /mnt/LV1-mount ext4 defaults 0 0 # Add This Line # Can edit first the /etc/fsatab file and after do mount -a (mount everything in /etc/fstab file) instead mount command Mount Shared Disk \u00b6 Install NFS yum install -y nfs-utils systemctl enable nfs systemctl start nfs Master \u00b6 Configure Export # vi /etc/exports /snapshots * ( rw ) = > Add Line # exportfs /snapshots <world> Enable Service firewall-cmd --add-service nfs --permanent firewall-cmd --permanent --add-service = rpc-bind firewall-cmd --permanent --add-service = mountd firewall-cmd --permanent --add-port = 2049 /tcp firewall-cmd --permanent --add-port = 2049 /udp firewall-cmd --reload Slaves \u00b6 # vi /etc/fstab 10 .240.100.18:/snapshots /elastic/snapshots nfs _netdev,rw 0 0 # mount -a # mount | grep elastic Disk Quotas \u00b6 ```bash vi /etc/fstab \u00b6 LABEL=projectA /mnt/ProjectA/ ext4 defaults,usrquota 0 0 mount -a \u00b6 apt-get install quota \u00b6 cd /mnt/ProjectA \u00b6 quotacheck -avugc # Create filesystem to support quotas \u00b6 quotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. quotacheck: Scanning /dev/xvdf7 [/mnt/ProjectA] done quotacheck: Cannot stat old user quota file /mnt/ProjectA/aquota.user: No such file or directory. Usage will not be subtracted. quotacheck: Old group file name could not been determined. Usage will not be subtracted. quotacheck: Checked 3 directories and 0 files quotacheck: Old file not found. edquota -u fsantos # To edit quotas to user \u00b6 quotaon /mnt/Photos # Enable quotas \u00b6 quota -v # To see quotas usage \u00b6 sudo repquota /mnt/Photos # To see a resume from users \u00b6","title":"Disks"},{"location":"platforms/unix/disks/#disks","text":"","title":"Disks"},{"location":"platforms/unix/disks/#mount-disk","text":"List Devices fdisk -l Create Partition # fdisk /dev/sdb Command ( m for help ) : n = > Create Partition Partition type: p primary ( 0 primary, 0 extended, 4 free ) e extended Command ( m for help ) : p = > Get partition Command ( m for help ) : w = > Write Format and Create Label # mkfs.xfs /dev/sdb1 -L elastic Mount # vi /etc/fstab LABEL = elastic /elastic xfs defaults 0 0 ## Add this line # mount -a","title":"Mount Disk"},{"location":"platforms/unix/disks/#create-ext4-filesystem","text":"Can use parted command # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 88 .5M 1 loop /snap/core/7270 loop1 7 :1 0 18M 1 loop /snap/amazon-ssm-agent/1455 xvda 202 :0 0 8G 0 disk \u2514\u2500xvda1 202 :1 0 8G 0 part / xvdf 202 :80 0 8G 0 disk # fdisk /dev/xvdf Command ( m for help ) : n = > Create Partition Partition type: p primary ( 0 primary, 0 extended, 4 free ) e extended Command ( m for help ) : p = > Print partition Command ( m for help ) : w = > Write # mkfs -L projectA -t ext4 /dev/xvdf1 # Format Partition and create label # mkdir /mnt/ProjectA # mount /dev/xvdf1 /mnt/ProjectA/ # vi /etc/fstab LABEL = projectA /mnt/ProjectA/ ext4 defaults 0 0 # Add This Line umount /mnt/ProjectA/","title":"Create ext4 Filesystem"},{"location":"platforms/unix/disks/#create-swap-partition","text":"Can use parted command # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 1 Hex code ( type L to list all codes ) : 82 Command ( m for help ) : w # mkswap /dev/xvdf1 # swapon /dev/xvdf1 # to shutoff -> swapoff /dev/xvdf2 # free -m # vi /etc/fstab UUID = 99fd52d5-e821-45a5-9366-30666046406f swap swap default 0 0","title":"Create Swap Partition"},{"location":"platforms/unix/disks/#create-lvm-filesystem","text":"# fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 2 Hex code ( type L to list all codes ) : 8e Changed type of partition 'Linux' to 'Linux LVM' . Command ( m for help ) : w The partition table has been altered. Syncing disks. # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 8e Linux LVM /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 5 Hex code ( type L to list all codes ) : 8e Changed type of partition 'Linux' to 'Linux LVM' . Command ( m for help ) : w The partition table has been altered. Syncing disks. # pvcreate /dev/xvdf2 Physical volume \"/dev/xvdf2\" successfully created. # pvcreate /dev/xvdf5 Physical volume \"/dev/xvdf5\" successfully created. # vgcreate VG1 /dev/xvdf2 /dev/xvdf5 Volume group \"VG1\" successfully created root@ip-172-31-81-196:~# vgdisplay /dev/VG1 --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 3 .99 GiB PE Size 4 .00 MiB Total PE 1022 Alloc PE / Size 0 / 0 Free PE / Size 1022 / 3 .99 GiB VG UUID ELopyA-2vai-XuiQ-vm7M-RF9q-k0ve-yUeXM6 # lvcreate VG1 -L +3.9G -n LV1 Rounding up size to full physical extent 3 .90 GiB Logical volume \"LV1\" created. root@ip-172-31-81-196:~# lvdisplay --- Logical volume --- LV Path /dev/VG1/LV1 LV Name LV1 VG Name VG1 LV UUID Q0mXgw-q9E7-ncuH-Vv2t-NTqx-epn9-3EK3DH LV Write Access read/write LV Creation host, time ip-172-31-81-196, 2019 -08-02 21 :45:49 +0000 LV Status available # open 0 LV Size 3 .90 GiB Current LE 999 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253 :0 # mkfs.ext4 /dev/VG1/LV1 mke2fs 1 .44.1 ( 24 -Mar-2018 ) Creating filesystem with 1022976 4k blocks and 256000 inodes Filesystem UUID: 3cc6d97e-a315-4051-a572-16e06bf9c9f9 Superblock backups stored on blocks: 32768 , 98304 , 163840 , 229376 , 294912 , 819200 , 884736 Allocating group tables: done Writing inode tables: done Creating journal ( 16384 blocks ) : done Writing superblocks and filesystem accounting information: done root@ip-172-31-81-196:~# mkdir /mnt/LV1-mount root@ip-172-31-81-196:~# mount /dev/VG1/LV1 /mnt/LV1-mount root@ip-172-31-81-196:~# vi /etc/fstab /dev/VG1/LV1 /mnt/LV1-mount ext4 defaults 0 0 # Add This Line # Can edit first the /etc/fsatab file and after do mount -a (mount everything in /etc/fstab file) instead mount command","title":"Create LVM Filesystem"},{"location":"platforms/unix/disks/#mount-shared-disk","text":"Install NFS yum install -y nfs-utils systemctl enable nfs systemctl start nfs","title":"Mount Shared Disk"},{"location":"platforms/unix/disks/#master","text":"Configure Export # vi /etc/exports /snapshots * ( rw ) = > Add Line # exportfs /snapshots <world> Enable Service firewall-cmd --add-service nfs --permanent firewall-cmd --permanent --add-service = rpc-bind firewall-cmd --permanent --add-service = mountd firewall-cmd --permanent --add-port = 2049 /tcp firewall-cmd --permanent --add-port = 2049 /udp firewall-cmd --reload","title":"Master"},{"location":"platforms/unix/disks/#slaves","text":"# vi /etc/fstab 10 .240.100.18:/snapshots /elastic/snapshots nfs _netdev,rw 0 0 # mount -a # mount | grep elastic","title":"Slaves"},{"location":"platforms/unix/disks/#disk-quotas","text":"```bash","title":"Disk Quotas"},{"location":"platforms/unix/disks/#vi-etcfstab","text":"LABEL=projectA /mnt/ProjectA/ ext4 defaults,usrquota 0 0","title":"vi /etc/fstab"},{"location":"platforms/unix/disks/#mount-a","text":"","title":"mount -a"},{"location":"platforms/unix/disks/#apt-get-install-quota","text":"","title":"apt-get install quota"},{"location":"platforms/unix/disks/#cd-mntprojecta","text":"","title":"cd /mnt/ProjectA"},{"location":"platforms/unix/disks/#quotacheck-avugc-create-filesystem-to-support-quotas","text":"quotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. quotacheck: Scanning /dev/xvdf7 [/mnt/ProjectA] done quotacheck: Cannot stat old user quota file /mnt/ProjectA/aquota.user: No such file or directory. Usage will not be subtracted. quotacheck: Old group file name could not been determined. Usage will not be subtracted. quotacheck: Checked 3 directories and 0 files quotacheck: Old file not found.","title":"quotacheck -avugc # Create filesystem to support quotas"},{"location":"platforms/unix/disks/#edquota-u-fsantos-to-edit-quotas-to-user","text":"","title":"edquota -u fsantos # To edit quotas to user"},{"location":"platforms/unix/disks/#quotaon-mntphotos-enable-quotas","text":"","title":"quotaon /mnt/Photos # Enable quotas"},{"location":"platforms/unix/disks/#quota-v-to-see-quotas-usage","text":"","title":"quota -v # To see quotas usage"},{"location":"platforms/unix/disks/#sudo-repquota-mntphotos-to-see-a-resume-from-users","text":"","title":"sudo repquota /mnt/Photos # To see a resume from users"},{"location":"platforms/unix/networking/","text":"Networking \u00b6 Configuration \u00b6 ifconfig \u00b6 ip addr ip addr show ifconfig ifconfig -a ifdown enp0s3 ifup enp0s3 ifconfig enp0s3 192 .168.0.150/24 # Not Permanent Add Secondary IP \u00b6 Link ifconfig eth0:1 172 .31.81.196 netmask 255 .255.240.0 up # Not Permanent Remove Secondary Ip \u00b6 ip addr del 172 .31.81.196/20 dev eth0:1 # Not Permanent Network-Manager \u00b6 sudo apt install network-manager sudo service network-manager restart nmcli nmcli connection # Connection information # Add Static Route sudo nmcli connection add con-name STATIC ipv4.addresses 192 .168.58.1/24 ifname eth0 type ethernet nmcli connection show STATIC sudo nmcli connection modify STATIC +ipv4.routes \"172.16.0.0/16 192.168.58.254\" ipv4.dns 172 .16.58.254 Routes \u00b6 route route -n route add default gw 192 .168.0.254 Dns \u00b6 cat /etc/hosts cat /etc/nsswitch.conf # show how priority to will resolve dns cat /etc/resolv.conf cat /etc/dhcp/dhclient.conf sudo service networking restart File \u00b6 cat /etc/network/interfaces IP Tables \u00b6 Nat sudo iptables -t nat -L | grep 100 .66.20.131 sudo iptables -t nat -L | grep nginx-service Troubleshooting \u00b6 Netstat \u00b6 $ sudo netstat -nl -p tcp | grep 8123 $ sudo netstat -nl -p tcp | head Get open ports \u00b6 nmap localhost netstat -at SS \u00b6 ss -an | grep -i listen sudo ss -ntlp # To get process Who listen on Port \u00b6 sudo lsof -i :8000 Test Remote Connection to port \u00b6 nc -v 10 .240.100.18 2049 netcat -l 12345 # To listen a Port Get Ports than services are listen \u00b6 cat /etc/services MTR \u00b6 mtr acloud.com DNS \u00b6 host google.com dig google.com TCP Dump \u00b6 sudo tcpdump -v -l -i any 'host 100.65.17.226' tcpdump -n port 6443 tcpdump port 443 and '(tcp-syn|tcp-ack)!=0' VPN \u00b6 Permanent VPN Connection \u00b6 Article","title":"Networking"},{"location":"platforms/unix/networking/#networking","text":"","title":"Networking"},{"location":"platforms/unix/networking/#configuration","text":"","title":"Configuration"},{"location":"platforms/unix/networking/#ifconfig","text":"ip addr ip addr show ifconfig ifconfig -a ifdown enp0s3 ifup enp0s3 ifconfig enp0s3 192 .168.0.150/24 # Not Permanent","title":"ifconfig"},{"location":"platforms/unix/networking/#add-secondary-ip","text":"Link ifconfig eth0:1 172 .31.81.196 netmask 255 .255.240.0 up # Not Permanent","title":"Add Secondary IP"},{"location":"platforms/unix/networking/#remove-secondary-ip","text":"ip addr del 172 .31.81.196/20 dev eth0:1 # Not Permanent","title":"Remove Secondary Ip"},{"location":"platforms/unix/networking/#network-manager","text":"sudo apt install network-manager sudo service network-manager restart nmcli nmcli connection # Connection information # Add Static Route sudo nmcli connection add con-name STATIC ipv4.addresses 192 .168.58.1/24 ifname eth0 type ethernet nmcli connection show STATIC sudo nmcli connection modify STATIC +ipv4.routes \"172.16.0.0/16 192.168.58.254\" ipv4.dns 172 .16.58.254","title":"Network-Manager"},{"location":"platforms/unix/networking/#routes","text":"route route -n route add default gw 192 .168.0.254","title":"Routes"},{"location":"platforms/unix/networking/#dns","text":"cat /etc/hosts cat /etc/nsswitch.conf # show how priority to will resolve dns cat /etc/resolv.conf cat /etc/dhcp/dhclient.conf sudo service networking restart","title":"Dns"},{"location":"platforms/unix/networking/#file","text":"cat /etc/network/interfaces","title":"File"},{"location":"platforms/unix/networking/#ip-tables","text":"Nat sudo iptables -t nat -L | grep 100 .66.20.131 sudo iptables -t nat -L | grep nginx-service","title":"IP Tables"},{"location":"platforms/unix/networking/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"platforms/unix/networking/#netstat","text":"$ sudo netstat -nl -p tcp | grep 8123 $ sudo netstat -nl -p tcp | head","title":"Netstat"},{"location":"platforms/unix/networking/#get-open-ports","text":"nmap localhost netstat -at","title":"Get open ports"},{"location":"platforms/unix/networking/#ss","text":"ss -an | grep -i listen sudo ss -ntlp # To get process","title":"SS"},{"location":"platforms/unix/networking/#who-listen-on-port","text":"sudo lsof -i :8000","title":"Who listen on Port"},{"location":"platforms/unix/networking/#test-remote-connection-to-port","text":"nc -v 10 .240.100.18 2049 netcat -l 12345 # To listen a Port","title":"Test Remote Connection to port"},{"location":"platforms/unix/networking/#get-ports-than-services-are-listen","text":"cat /etc/services","title":"Get Ports than services are listen"},{"location":"platforms/unix/networking/#mtr","text":"mtr acloud.com","title":"MTR"},{"location":"platforms/unix/networking/#dns_1","text":"host google.com dig google.com","title":"DNS"},{"location":"platforms/unix/networking/#tcp-dump","text":"sudo tcpdump -v -l -i any 'host 100.65.17.226' tcpdump -n port 6443 tcpdump port 443 and '(tcp-syn|tcp-ack)!=0'","title":"TCP Dump"},{"location":"platforms/unix/networking/#vpn","text":"","title":"VPN"},{"location":"platforms/unix/networking/#permanent-vpn-connection","text":"Article","title":"Permanent VPN Connection"},{"location":"platforms/unix/redhat/","text":"Redhat \u00b6 Recover Root Password \u00b6 Na console send ctrl-alt-del press \"e\" = > TO Edit Na linha linux16 -- No final da linha adicionar rd.break press \"ctrl-x\" mount -o remount,rw /sysroot cd /sysroot chroot . touch .autorelabel passwd teste123ibm4 ctrl d ctrl d SSH \u00b6 Permit SSH Root Login (CentOs) vi /etc/ssh/sshd_config # Comment PasswordAuthentication no # Descomment PasswordAuthentication yes systemctl restart sshd Configure Network Interface (CentOs) \u00b6 vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR = 172 .16.1.180 GATEWAY = 172 .16.1.16 DNS1 = 172 .16.1.112 DNS2 = 172 .16.1.119 systemctl restart network Package Manager \u00b6 RPM \u00b6 sudo rpm -i wget-xxxxxx.rpm # To see which lib file belongs rpm -qf /etc/protocols # To see if some file is missing from lib sudo rpm --verify setup sudo rpm -Va # To verify entire machine YUM \u00b6 # Config file cat /etc/yum.conf # Repos dir cd /etc/yum.repos.d sudo yum update # To download but not install sudo yum install --downloadonly --downloaddir = /tmp wget sudo yum remove wget","title":"Redhat"},{"location":"platforms/unix/redhat/#redhat","text":"","title":"Redhat"},{"location":"platforms/unix/redhat/#recover-root-password","text":"Na console send ctrl-alt-del press \"e\" = > TO Edit Na linha linux16 -- No final da linha adicionar rd.break press \"ctrl-x\" mount -o remount,rw /sysroot cd /sysroot chroot . touch .autorelabel passwd teste123ibm4 ctrl d ctrl d","title":"Recover Root Password"},{"location":"platforms/unix/redhat/#ssh","text":"Permit SSH Root Login (CentOs) vi /etc/ssh/sshd_config # Comment PasswordAuthentication no # Descomment PasswordAuthentication yes systemctl restart sshd","title":"SSH"},{"location":"platforms/unix/redhat/#configure-network-interface-centos","text":"vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR = 172 .16.1.180 GATEWAY = 172 .16.1.16 DNS1 = 172 .16.1.112 DNS2 = 172 .16.1.119 systemctl restart network","title":"Configure Network Interface (CentOs)"},{"location":"platforms/unix/redhat/#package-manager","text":"","title":"Package Manager"},{"location":"platforms/unix/redhat/#rpm","text":"sudo rpm -i wget-xxxxxx.rpm # To see which lib file belongs rpm -qf /etc/protocols # To see if some file is missing from lib sudo rpm --verify setup sudo rpm -Va # To verify entire machine","title":"RPM"},{"location":"platforms/unix/redhat/#yum","text":"# Config file cat /etc/yum.conf # Repos dir cd /etc/yum.repos.d sudo yum update # To download but not install sudo yum install --downloadonly --downloaddir = /tmp wget sudo yum remove wget","title":"YUM"},{"location":"platforms/unix/security/","text":"Security \u00b6 Host \u00b6 cat /etc/hosts.allow # Hosts allowed to access cat /etc/hosts.deny # Hosts not allowed to access Nologin \u00b6 touch /etc/nologin # Denies login to all users (except root). Need remove the file SE Linux \u00b6 Ubuntu \u00b6 # Need remove apparmor in ubuntu systems # make sure you have the most up-to-date info apt-get update apt-get dist-upgrade #disable and remove apparmor /etc/init.d/apparmor stop apt-get remove apparmor #install SELinux apt-get install selinux # install the missing dependency apt-get install auditd # install the activate tool required to make it work apt-get install selinux-basics #missing manual step to actually make SELinux work (part of selinux-basics) selinux-activate Config \u00b6 # Config file cat /etc/selinux/config getenforce setenforce 1 sestatus ls -Z /etc/shadow ps -Z Contexts \u00b6 # List semanage fcontext -l semanage fcontext -l | grep httpd_sys_content_t # Change Context Type chcon -t user_home_dir_t /etc/shadow Booleans \u00b6 getsebool -a semanage boolean -l | sort | less semanage boolean -m -1 httpd_enable_homedirs # or setsebool -P httpd_enable_homedirs=1 Troubleshooting \u00b6 grep http /var/log/audit/audit.log","title":"Security"},{"location":"platforms/unix/security/#security","text":"","title":"Security"},{"location":"platforms/unix/security/#host","text":"cat /etc/hosts.allow # Hosts allowed to access cat /etc/hosts.deny # Hosts not allowed to access","title":"Host"},{"location":"platforms/unix/security/#nologin","text":"touch /etc/nologin # Denies login to all users (except root). Need remove the file","title":"Nologin"},{"location":"platforms/unix/security/#se-linux","text":"","title":"SE Linux"},{"location":"platforms/unix/security/#ubuntu","text":"# Need remove apparmor in ubuntu systems # make sure you have the most up-to-date info apt-get update apt-get dist-upgrade #disable and remove apparmor /etc/init.d/apparmor stop apt-get remove apparmor #install SELinux apt-get install selinux # install the missing dependency apt-get install auditd # install the activate tool required to make it work apt-get install selinux-basics #missing manual step to actually make SELinux work (part of selinux-basics) selinux-activate","title":"Ubuntu"},{"location":"platforms/unix/security/#config","text":"# Config file cat /etc/selinux/config getenforce setenforce 1 sestatus ls -Z /etc/shadow ps -Z","title":"Config"},{"location":"platforms/unix/security/#contexts","text":"# List semanage fcontext -l semanage fcontext -l | grep httpd_sys_content_t # Change Context Type chcon -t user_home_dir_t /etc/shadow","title":"Contexts"},{"location":"platforms/unix/security/#booleans","text":"getsebool -a semanage boolean -l | sort | less semanage boolean -m -1 httpd_enable_homedirs # or setsebool -P httpd_enable_homedirs=1","title":"Booleans"},{"location":"platforms/unix/security/#troubleshooting","text":"grep http /var/log/audit/audit.log","title":"Troubleshooting"},{"location":"platforms/unix/virsh/","text":"Snapshot \u00b6 $ ssh -l rgameiro 10 .242.12.23 $ virsh list --all Id Name State ---------------------------------------------------- 1 bm02vsc01 running 2 bm02alinternal01 running 3 architect running 4 bm02ifwinternal01 running 5 bm02vsd01 running 6 bm02sfinternal02 running 7 bm02acinternal01 running 8 bm02dirinternal01 running - bm02nes01 shut off $ virsh destroy bm02sfinternal02 $ virsh dumpxml bm02sfinternal02 # Find Disk File <domain type = 'kvm' > <name>bm02sfinternal02</name> <uuid>af7d246d-4578-4660-96fa-ac40c87c24eb</uuid> <memory unit = 'KiB' >2097152</memory> <currentMemory unit = 'KiB' >2097152</currentMemory> <vcpu placement = 'static' >4</vcpu> <os> < type arch = 'x86_64' machine = 'pc-i440fx-rhel7.3.0' >hvm</type> <boot dev = 'hd' /> </os> <features> <acpi/> <apic/> <vmport state = 'off' /> </features> <cpu mode = 'host-model' check = 'partial' > <model fallback = 'allow' /> </cpu> <clock offset = 'utc' > <timer name = 'rtc' tickpolicy = 'catchup' /> <timer name = 'pit' tickpolicy = 'delay' /> <timer name = 'hpet' present = 'no' /> </clock> <on_poweroff>destroy</on_poweroff> <on_reboot>restart</on_reboot> <on_crash>restart</on_crash> <pm> <suspend-to-mem enabled = 'no' /> <suspend-to-disk enabled = 'no' /> </pm> <devices> <emulator>/usr/libexec/qemu-kvm</emulator> <disk type = 'file' device = 'disk' > <driver name = 'qemu' type = 'qcow2' cache = 'none' io = 'native' /> < source file = '/var/lib/libvirt/images/bm02sfinternal02.qcow2' /> <target dev = 'vda' bus = 'virtio' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x06' function = '0x0' /> </disk> <disk type = 'file' device = 'cdrom' > <driver name = 'qemu' type = 'raw' /> <target dev = 'hda' bus = 'ide' /> <readonly/> <address type = 'drive' controller = '0' bus = '0' target = '0' unit = '0' /> </disk> <controller type = 'usb' index = '0' model = 'ich9-ehci1' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x7' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci1' > <master startport = '0' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x0' multifunction = 'on' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci2' > <master startport = '2' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x1' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci3' > <master startport = '4' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x2' /> </controller> <controller type = 'pci' index = '0' model = 'pci-root' /> <controller type = 'ide' index = '0' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x01' function = '0x1' /> </controller> <controller type = 'virtio-serial' index = '0' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x04' function = '0x0' /> </controller> <interface type = 'bridge' > <mac address = '52:54:00:84:7e:d0' /> < source bridge = 'br100' /> <model type = 'virtio' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x03' function = '0x0' /> </interface> <serial type = 'pty' > <target type = 'isa-serial' port = '0' > <model name = 'isa-serial' /> </target> </serial> <console type = 'pty' > <target type = 'serial' port = '0' /> </console> <channel type = 'unix' > <target type = 'virtio' name = 'org.qemu.guest_agent.0' /> <address type = 'virtio-serial' controller = '0' bus = '0' port = '1' /> </channel> <channel type = 'spicevmc' > <target type = 'virtio' name = 'com.redhat.spice.0' /> <address type = 'virtio-serial' controller = '0' bus = '0' port = '2' /> </channel> <input type = 'tablet' bus = 'usb' > <address type = 'usb' bus = '0' port = '1' /> </input> <input type = 'mouse' bus = 'ps2' /> <input type = 'keyboard' bus = 'ps2' /> <graphics type = 'spice' autoport = 'yes' > <listen type = 'address' /> </graphics> <video> <model type = 'qxl' ram = '65536' vram = '65536' vgamem = '16384' heads = '1' primary = 'yes' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x02' function = '0x0' /> </video> <redirdev bus = 'usb' type = 'spicevmc' > <address type = 'usb' bus = '0' port = '2' /> </redirdev> <redirdev bus = 'usb' type = 'spicevmc' > <address type = 'usb' bus = '0' port = '3' /> </redirdev> <memballoon model = 'virtio' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x07' function = '0x0' /> </memballoon> </devices> </domain> $ ls -la /var/lib/libvirt/images/bm02sfinternal02.qcow2 $ df -B 1g # Ensure we have space to copy disk $ cp /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ bg # Por o Processo em Brackground $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 rw------- 1 root root 17607884800 Oct 30 08 :21 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 # Terminou [ 1 ] + Done cp -i /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02* $ virsh start bm02sfinternal02","title":"Virsh"},{"location":"platforms/unix/virsh/#snapshot","text":"$ ssh -l rgameiro 10 .242.12.23 $ virsh list --all Id Name State ---------------------------------------------------- 1 bm02vsc01 running 2 bm02alinternal01 running 3 architect running 4 bm02ifwinternal01 running 5 bm02vsd01 running 6 bm02sfinternal02 running 7 bm02acinternal01 running 8 bm02dirinternal01 running - bm02nes01 shut off $ virsh destroy bm02sfinternal02 $ virsh dumpxml bm02sfinternal02 # Find Disk File <domain type = 'kvm' > <name>bm02sfinternal02</name> <uuid>af7d246d-4578-4660-96fa-ac40c87c24eb</uuid> <memory unit = 'KiB' >2097152</memory> <currentMemory unit = 'KiB' >2097152</currentMemory> <vcpu placement = 'static' >4</vcpu> <os> < type arch = 'x86_64' machine = 'pc-i440fx-rhel7.3.0' >hvm</type> <boot dev = 'hd' /> </os> <features> <acpi/> <apic/> <vmport state = 'off' /> </features> <cpu mode = 'host-model' check = 'partial' > <model fallback = 'allow' /> </cpu> <clock offset = 'utc' > <timer name = 'rtc' tickpolicy = 'catchup' /> <timer name = 'pit' tickpolicy = 'delay' /> <timer name = 'hpet' present = 'no' /> </clock> <on_poweroff>destroy</on_poweroff> <on_reboot>restart</on_reboot> <on_crash>restart</on_crash> <pm> <suspend-to-mem enabled = 'no' /> <suspend-to-disk enabled = 'no' /> </pm> <devices> <emulator>/usr/libexec/qemu-kvm</emulator> <disk type = 'file' device = 'disk' > <driver name = 'qemu' type = 'qcow2' cache = 'none' io = 'native' /> < source file = '/var/lib/libvirt/images/bm02sfinternal02.qcow2' /> <target dev = 'vda' bus = 'virtio' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x06' function = '0x0' /> </disk> <disk type = 'file' device = 'cdrom' > <driver name = 'qemu' type = 'raw' /> <target dev = 'hda' bus = 'ide' /> <readonly/> <address type = 'drive' controller = '0' bus = '0' target = '0' unit = '0' /> </disk> <controller type = 'usb' index = '0' model = 'ich9-ehci1' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x7' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci1' > <master startport = '0' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x0' multifunction = 'on' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci2' > <master startport = '2' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x1' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci3' > <master startport = '4' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x2' /> </controller> <controller type = 'pci' index = '0' model = 'pci-root' /> <controller type = 'ide' index = '0' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x01' function = '0x1' /> </controller> <controller type = 'virtio-serial' index = '0' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x04' function = '0x0' /> </controller> <interface type = 'bridge' > <mac address = '52:54:00:84:7e:d0' /> < source bridge = 'br100' /> <model type = 'virtio' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x03' function = '0x0' /> </interface> <serial type = 'pty' > <target type = 'isa-serial' port = '0' > <model name = 'isa-serial' /> </target> </serial> <console type = 'pty' > <target type = 'serial' port = '0' /> </console> <channel type = 'unix' > <target type = 'virtio' name = 'org.qemu.guest_agent.0' /> <address type = 'virtio-serial' controller = '0' bus = '0' port = '1' /> </channel> <channel type = 'spicevmc' > <target type = 'virtio' name = 'com.redhat.spice.0' /> <address type = 'virtio-serial' controller = '0' bus = '0' port = '2' /> </channel> <input type = 'tablet' bus = 'usb' > <address type = 'usb' bus = '0' port = '1' /> </input> <input type = 'mouse' bus = 'ps2' /> <input type = 'keyboard' bus = 'ps2' /> <graphics type = 'spice' autoport = 'yes' > <listen type = 'address' /> </graphics> <video> <model type = 'qxl' ram = '65536' vram = '65536' vgamem = '16384' heads = '1' primary = 'yes' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x02' function = '0x0' /> </video> <redirdev bus = 'usb' type = 'spicevmc' > <address type = 'usb' bus = '0' port = '2' /> </redirdev> <redirdev bus = 'usb' type = 'spicevmc' > <address type = 'usb' bus = '0' port = '3' /> </redirdev> <memballoon model = 'virtio' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x07' function = '0x0' /> </memballoon> </devices> </domain> $ ls -la /var/lib/libvirt/images/bm02sfinternal02.qcow2 $ df -B 1g # Ensure we have space to copy disk $ cp /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ bg # Por o Processo em Brackground $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 rw------- 1 root root 17607884800 Oct 30 08 :21 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 # Terminou [ 1 ] + Done cp -i /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02* $ virsh start bm02sfinternal02","title":"Snapshot"}]}