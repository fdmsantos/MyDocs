{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deploy Localhost python -m mkdocs serve Github Pages python -m mkdocs gh-deploy URL MyDocs Cheatseets Markdown Markdown","title":"Home"},{"location":"#deploy","text":"","title":"Deploy"},{"location":"#localhost","text":"python -m mkdocs serve","title":"Localhost"},{"location":"#github-pages","text":"python -m mkdocs gh-deploy","title":"Github Pages"},{"location":"#url","text":"MyDocs","title":"URL"},{"location":"#cheatseets","text":"","title":"Cheatseets"},{"location":"#markdown","text":"Markdown","title":"Markdown"},{"location":"aws/compute/ec2/","text":"AMI Create EC2 EBS Backed Launch an instance Connect to your instance and customize it Stop Instances (stopping the instance ensures data integrity of the instance; a snapshot will be taken of the instance) Create the image Bundle it (use bundle command in console) AWS automatically registers it for you EC2 Instance store Launch an instance Connect to your instance and customize it Bundle it (Consists of an image manifest -image.manifest.xml- and files -image.part.xx-) Upload the bundle to S3 bucket Register AMI Snapshots Restore Read all blocks to eliminate penalty in production sudo dd if=/dev/xvdf of=/dev/null bs=1M # Or sudo fio \u2013filename=/dev/xvdf \u2013rw=read \u2013bs=128k \u2013iodepth=32 \u2013ioengine=libaio \u2013direct=1 \u2013name=volume-initialize","title":"EC2"},{"location":"aws/compute/ec2/#ami","text":"","title":"AMI"},{"location":"aws/compute/ec2/#create","text":"EC2 EBS Backed Launch an instance Connect to your instance and customize it Stop Instances (stopping the instance ensures data integrity of the instance; a snapshot will be taken of the instance) Create the image Bundle it (use bundle command in console) AWS automatically registers it for you EC2 Instance store Launch an instance Connect to your instance and customize it Bundle it (Consists of an image manifest -image.manifest.xml- and files -image.part.xx-) Upload the bundle to S3 bucket Register AMI","title":"Create"},{"location":"aws/compute/ec2/#snapshots","text":"","title":"Snapshots"},{"location":"aws/compute/ec2/#restore","text":"Read all blocks to eliminate penalty in production sudo dd if=/dev/xvdf of=/dev/null bs=1M # Or sudo fio \u2013filename=/dev/xvdf \u2013rw=read \u2013bs=128k \u2013iodepth=32 \u2013ioengine=libaio \u2013direct=1 \u2013name=volume-initialize","title":"Restore"},{"location":"aws/deployment/cloudformation/","text":"Troubleshooting WaitCondition If WaitCondition times out or returns an error. There is an error in your cloudformation::init code Analyze cfn-init.log and cfn-wire.log for details collect logs via CloudWatch logs \u2013on-failure DO_NOTHING to keep instance from rolling back so you can log in and examine the logs Most common error: URLs for referenced resources (scripts, MSIs, etc.) are returning HTTP 403 or 404 errors","title":"Cloudformation"},{"location":"aws/deployment/cloudformation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"aws/deployment/cloudformation/#waitcondition","text":"If WaitCondition times out or returns an error. There is an error in your cloudformation::init code Analyze cfn-init.log and cfn-wire.log for details collect logs via CloudWatch logs \u2013on-failure DO_NOTHING to keep instance from rolling back so you can log in and examine the logs Most common error: URLs for referenced resources (scripts, MSIs, etc.) are returning HTTP 403 or 404 errors","title":"WaitCondition"},{"location":"aws/networking/vpc/","text":"Bastion Host High Availability Create 2 bastion hosts in different subnets. Create dns entry in Route53 which uses round robin dns and points to each instance. Tell your sysdamins to connect using the new dns entry EC2 Auto-recovery - If fails, will be recreate Autoscaling group of MAX =1 and MIN = 1. So if the bastion fails it is recreated automatically VPC Flow Logs Traffic Not Logged Traffic generated by instances when contact the Amazon DNS server (if use own dns, so all that traffic is logged) Traffic generated by a windows instance for Amazon windows license activation Traffic to and from 169.254.169.254 for instance metadata DHCP traffic Traffic to the reserved IP address for the default VPC router Troubleshooting Instances cannot communicate If instances in subnets cannot communicate with one another Verify that it is a network issue, not an instance issue Check network ACLs If enable, check VPC flow Logs Nat Configuration Nat Instance and Nat Gateway - Check Route table Association Nat Instance - Check to see if the source/Dest check is disabled Ensure that NAT has masquerade configured, Restart NAT, check the inbound security Group rules If you cannot reach resources in a peered network: Peering request approval Route table configuration Check network ACLs: are you forbidding all external traffic Check security Group configurations on resources Use Cidr block rules in VPC A to allow Access from VPC B If you are not seeing network traffic flow on the AWS side of your VPN connection to your Amazon VPC. Turn on route propagation in the Amazon VPC\u2019s main routing table.","title":"VPC"},{"location":"aws/networking/vpc/#bastion-host","text":"","title":"Bastion Host"},{"location":"aws/networking/vpc/#high-availability","text":"Create 2 bastion hosts in different subnets. Create dns entry in Route53 which uses round robin dns and points to each instance. Tell your sysdamins to connect using the new dns entry EC2 Auto-recovery - If fails, will be recreate Autoscaling group of MAX =1 and MIN = 1. So if the bastion fails it is recreated automatically","title":"High Availability"},{"location":"aws/networking/vpc/#vpc-flow-logs","text":"","title":"VPC Flow Logs"},{"location":"aws/networking/vpc/#traffic-not-logged","text":"Traffic generated by instances when contact the Amazon DNS server (if use own dns, so all that traffic is logged) Traffic generated by a windows instance for Amazon windows license activation Traffic to and from 169.254.169.254 for instance metadata DHCP traffic Traffic to the reserved IP address for the default VPC router","title":"Traffic Not Logged"},{"location":"aws/networking/vpc/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"aws/networking/vpc/#instances-cannot-communicate","text":"If instances in subnets cannot communicate with one another Verify that it is a network issue, not an instance issue Check network ACLs If enable, check VPC flow Logs","title":"Instances cannot communicate"},{"location":"aws/networking/vpc/#nat-configuration","text":"Nat Instance and Nat Gateway - Check Route table Association Nat Instance - Check to see if the source/Dest check is disabled Ensure that NAT has masquerade configured, Restart NAT, check the inbound security Group rules If you cannot reach resources in a peered network:","title":"Nat Configuration"},{"location":"aws/networking/vpc/#peering-request-approval","text":"Route table configuration Check network ACLs: are you forbidding all external traffic Check security Group configurations on resources Use Cidr block rules in VPC A to allow Access from VPC B If you are not seeing network traffic flow on the AWS side of your VPN connection to your Amazon VPC. Turn on route propagation in the Amazon VPC\u2019s main routing table.","title":"Peering request approval"},{"location":"aws/storage/ebs/","text":"Root Volumes Resize Manual method Create a snapshot of the current root volume Create a new volume from the snapshot with new storage specifications Select larger \"size\", thus increasing IOPS Must select the same availability zone as the source Stop the Instance Detach the original root volume Attach the new volume to the instance at same mount point (xvda) \"Automated\" method We can replace the launch configuration of an Auto Scaling Group In a true 3-tier application that is decoupled, we should then be able to terminate instances one by one to recreate them using new configuration Note: If the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity","title":"EBS"},{"location":"aws/storage/ebs/#root-volumes","text":"","title":"Root Volumes"},{"location":"aws/storage/ebs/#resize","text":"Manual method Create a snapshot of the current root volume Create a new volume from the snapshot with new storage specifications Select larger \"size\", thus increasing IOPS Must select the same availability zone as the source Stop the Instance Detach the original root volume Attach the new volume to the instance at same mount point (xvda) \"Automated\" method We can replace the launch configuration of an Auto Scaling Group In a true 3-tier application that is decoupled, we should then be able to terminate instances one by one to recreate them using new configuration Note: If the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity","title":"Resize"},{"location":"containers/kubernetes/admin/admin/","text":"Pre-Requisites Redhat Disable SELinux setenforce 0 sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux Enable the br_netfilter module for cluster communication modprobe br_netfilter echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables Disable swap to prevent memory allocation issues swapoff -a vi /etc/fstab -> Comment out the swap line Docker Redhat Install the Docker prerequisites yum install -y yum-utils device-mapper-persistent-data lvm2 Add the Docker repo and install Docker yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce Configure the Docker Cgroup Driver to systemd, enable and start Docker sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker --now Ubuntu curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce Amazon AMI Linux 2 amazon-linux-extras install -y docker cat > /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF systemctl daemon-reload systemctl enable docker systemctl start docker usermod -a -G docker ec2-user kubernetes Redhat cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl systemctl enable kubelet kubeadm init --pod-network-cidr=10.244.0.0/16 # Exit sudo user mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Ubuntu curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet=1.12.7-00 kubeadm=1.12.7-00 kubectl=1.12.7-00 sudo apt-mark hold kubelet kubeadm kubectl echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Upgrade Ubuntu kubectl version --short # Release the hold on versions of kubeadm and kubelet sudo apt-mark unhold kubeadm kubelet sudo apt install -y kubeadm=1.14.1-00 sudo apt-mark hold kubeadm kubeadm version sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.14.1 sudo apt-mark unhold kubectl sudo apt install -y kubectl=1.14.1-00 sudo apt-mark hold kubectl sudo apt-mark unhold kubelet sudo apt install -y kubelet=1.14.1-00 sudo apt-mark hold kubelet Maintenance Evict Pods # Evict the pods on a node kubectl drain [node_name] --ignore-daemonsets kubectl get nodes -w # Rollback - Schedule pods to the node after maintenance is complete kubectl uncordon [node_name] Delete Node kubectl delete node [node_name] sudo kubeadm token generate # Print the kubeadm join command to join a node to the cluster sudo kubeadm token create [token_name] --ttl 2h --print-join-command Autocomple Redhat # Enable Epel Repo sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel # Instal bash completion yum install bash-completion bash-completion-extras vi /.bashrc # Add - source <(kubectl completion bash)","title":"Admin"},{"location":"containers/kubernetes/admin/admin/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"containers/kubernetes/admin/admin/#redhat","text":"Disable SELinux setenforce 0 sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux Enable the br_netfilter module for cluster communication modprobe br_netfilter echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables Disable swap to prevent memory allocation issues swapoff -a vi /etc/fstab -> Comment out the swap line","title":"Redhat"},{"location":"containers/kubernetes/admin/admin/#docker","text":"","title":"Docker"},{"location":"containers/kubernetes/admin/admin/#redhat_1","text":"Install the Docker prerequisites yum install -y yum-utils device-mapper-persistent-data lvm2 Add the Docker repo and install Docker yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce Configure the Docker Cgroup Driver to systemd, enable and start Docker sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker --now","title":"Redhat"},{"location":"containers/kubernetes/admin/admin/#ubuntu","text":"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce","title":"Ubuntu"},{"location":"containers/kubernetes/admin/admin/#amazon-ami-linux-2","text":"amazon-linux-extras install -y docker cat > /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF systemctl daemon-reload systemctl enable docker systemctl start docker usermod -a -G docker ec2-user","title":"Amazon AMI Linux 2"},{"location":"containers/kubernetes/admin/admin/#kubernetes","text":"","title":"kubernetes"},{"location":"containers/kubernetes/admin/admin/#redhat_2","text":"cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl systemctl enable kubelet kubeadm init --pod-network-cidr=10.244.0.0/16 # Exit sudo user mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config","title":"Redhat"},{"location":"containers/kubernetes/admin/admin/#ubuntu_1","text":"curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet=1.12.7-00 kubeadm=1.12.7-00 kubectl=1.12.7-00 sudo apt-mark hold kubelet kubeadm kubectl echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config","title":"Ubuntu"},{"location":"containers/kubernetes/admin/admin/#upgrade","text":"","title":"Upgrade"},{"location":"containers/kubernetes/admin/admin/#ubuntu_2","text":"kubectl version --short # Release the hold on versions of kubeadm and kubelet sudo apt-mark unhold kubeadm kubelet sudo apt install -y kubeadm=1.14.1-00 sudo apt-mark hold kubeadm kubeadm version sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.14.1 sudo apt-mark unhold kubectl sudo apt install -y kubectl=1.14.1-00 sudo apt-mark hold kubectl sudo apt-mark unhold kubelet sudo apt install -y kubelet=1.14.1-00 sudo apt-mark hold kubelet","title":"Ubuntu"},{"location":"containers/kubernetes/admin/admin/#maintenance","text":"","title":"Maintenance"},{"location":"containers/kubernetes/admin/admin/#evict-pods","text":"# Evict the pods on a node kubectl drain [node_name] --ignore-daemonsets kubectl get nodes -w # Rollback - Schedule pods to the node after maintenance is complete kubectl uncordon [node_name]","title":"Evict Pods"},{"location":"containers/kubernetes/admin/admin/#delete-node","text":"kubectl delete node [node_name] sudo kubeadm token generate # Print the kubeadm join command to join a node to the cluster sudo kubeadm token create [token_name] --ttl 2h --print-join-command","title":"Delete Node"},{"location":"containers/kubernetes/admin/admin/#autocomple","text":"","title":"Autocomple"},{"location":"containers/kubernetes/admin/admin/#redhat_3","text":"# Enable Epel Repo sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel # Instal bash completion yum install bash-completion bash-completion-extras vi /.bashrc # Add - source <(kubectl completion bash)","title":"Redhat"},{"location":"containers/kubernetes/admin/etcd/","text":"etcdctl Install wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz tar xvf etcd-v3.3.12-linux-amd64.tar.gz sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin Snapshot sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View that the snapshot was successful ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db Using Docker Container Find Command Parameters ps -ef | grep etcd Get All Keys docker exec -it 3606376c1aba /bin/sh -c \"export ETCDCTL_API=3 && etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\"","title":"Etcd"},{"location":"containers/kubernetes/admin/etcd/#etcdctl","text":"","title":"etcdctl"},{"location":"containers/kubernetes/admin/etcd/#install","text":"wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz tar xvf etcd-v3.3.12-linux-amd64.tar.gz sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin","title":"Install"},{"location":"containers/kubernetes/admin/etcd/#snapshot","text":"sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View that the snapshot was successful ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db","title":"Snapshot"},{"location":"containers/kubernetes/admin/etcd/#using-docker-container","text":"","title":"Using Docker Container"},{"location":"containers/kubernetes/admin/etcd/#find-command-parameters","text":"ps -ef | grep etcd","title":"Find Command Parameters"},{"location":"containers/kubernetes/admin/etcd/#get-all-keys","text":"docker exec -it 3606376c1aba /bin/sh -c \"export ETCDCTL_API=3 && etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\"","title":"Get All Keys"},{"location":"containers/kubernetes/admin/monitoring/","text":"Monitoring Cluster Components Metrics Server git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/ # Get a response from the metrics server API: kubectl get --raw /apis/metrics.k8s.io/ kubectl top node kubectl top pods kubectl top pods --all-namespaces kubectl top pods -n kube-system kubectl top pod -l run=pod-with-defaults kubectl top pod pod-with-defaults # Get the CPU and memory of the containers inside the pod kubectl top pods group-context --containers Applications Liveness Probe apiVersion: v1 kind: Pod metadata: name: liveness spec: containers: - image: linuxacademycontent/kubeserve name: kubeserve livenessProbe: httpGet: path: / port: 80 Readiness Probe apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx --- apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Pod metadata: name: nginxpd labels: app: nginx spec: containers: - name: nginx image: nginx:191 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5 Logs Cluster Dirs The directory where the continainer logs reside ls /var/log/containers The directory where kubelet stores its logs ls /var/log SideCar Container The YAML for a sidecar container that will tail the logs for each type apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log'] volumeMounts: - name: varlog mountPath: /var/log - name: count-log-2 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log'] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} kubectl logs counter count-log-1 kubectl logs counter count-log-2 Application # Get the logs from a pod: kubectl logs nginx # Get the logs from a specific container on a pod: kubectl logs counter -c count-log-1 # Get the logs from all containers on the pod: kubectl logs counter --all-containers=true # Get the logs from containers with a certain label: kubectl logs -lapp=nginx # Get the logs from a previously terminated container within a pod: kubectl logs -p -c nginx nginx # Stream the logs from a container in a pod: kubectl logs -f -c count-log-1 counter # Tail the logs to only view a certain number of lines: kubectl logs --tail=20 nginx # View the logs from a previous time duration: kubectl logs --since=1h nginx # View the logs from a container within a pod within a deployment: kubectl logs deployment/nginx -c nginx # Redirect the output of the logs to a file: kubectl logs counter -c count-log-1 > count.log Troubleshooting Applications Use Termination Reason apiVersion: v1 kind: Pod metadata: name: pod2 spec: containers: - image: busybox name: main command: - sh - -c - 'echo \"I''ve had enough\" > /var/termination-reason ; exit 1' terminationMessagePath: /var/termination-reason Healthz Not all pods have healthz configured apiVersion: v1 kind: Pod metadata: name: liveness spec: containers: - image: linuxacademycontent/candy-service:2 name: kubeserve livenessProbe: httpGet: path: /healthz port: 8081 Steps kubectl describe po pod2 kubectl logs pod-with-defaults kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml Cluster # Check the events in the kube-system namespace for errors kubectl get events -n kube-system kubectl logs [kube_scheduler_pod_name] -n kube-system # Check the status of the Docker service: sudo systemctl status docker sudo systemctl enable docker && systemctl start docker # Check the status of the kubelet service: sudo systemctl status kubelet sudo systemctl enable kubelet && systemctl start kubelet # Turn off swap on your machine sudo su - swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # Check if you have a firewall running: sudo systemctl status firewalld sudo systemctl disable firewalld && systemctl stop firewalld Worker Node kubectl get nodes kubectl describe nodes chadcrowell2c.mylabserver.com # Create New Worker Server # Generate a new token after spinning up a new server: sudo kubeadm token generate # Create the kubeadm join command for your new worker node: # sudo kubeadm token create [token_name] --ttl 2h --print-join-command # View the journalctl logs: sudo journalctl -u kubelet # View the syslogs: sudo more syslog | tail -120 | grep kubelet Networking DNS # Run an interactive busybox pod: kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 sh # From the pod, check if DNS is resolving hostnames: nslookup hostnames # From the pod, cat out the /etc/resolv.conf file: cat /etc/resolv.conf # From the pod, look up the DNS name of the Kubernetes service: nslookup kubernetes.default nslookup kube-dns.kube-system.svc.cluster.loca # Look up a service in your Kubernetes cluster nslookup [pod-ip-address].default.pod.cluster.local # Logs Core Dns kubectl logs [coredns-pod-name] Kube-Proxy # View the endpoints for your service: kubectl get ep # Communicate with the pod directly (without the service): wget -qO- 10.244.1.6:9376 # Check if kube-proxy is running on the nodes: ps auxw | grep kube-proxy # Check if kube-proxy is writing iptables: kubectl get services -o wide iptables-save | grep hostnames sudo iptables-save | grep KUBE | grep <service-name> # View the list of kube-system pods: kubectl get pods -n kube-system # Connect to your kube-proxy pod in the kube-system namespace: kubectl exec -it kube-proxy-cqptg -n kube-system -- sh Change CNI Plugin # Delete the flannel CNI plugin: kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml # Apply the Weave Net CNI plugin: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"","title":"Monitoring"},{"location":"containers/kubernetes/admin/monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"containers/kubernetes/admin/monitoring/#cluster-components","text":"","title":"Cluster Components"},{"location":"containers/kubernetes/admin/monitoring/#metrics-server","text":"git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/ # Get a response from the metrics server API: kubectl get --raw /apis/metrics.k8s.io/ kubectl top node kubectl top pods kubectl top pods --all-namespaces kubectl top pods -n kube-system kubectl top pod -l run=pod-with-defaults kubectl top pod pod-with-defaults # Get the CPU and memory of the containers inside the pod kubectl top pods group-context --containers","title":"Metrics Server"},{"location":"containers/kubernetes/admin/monitoring/#applications","text":"","title":"Applications"},{"location":"containers/kubernetes/admin/monitoring/#liveness-probe","text":"apiVersion: v1 kind: Pod metadata: name: liveness spec: containers: - image: linuxacademycontent/kubeserve name: kubeserve livenessProbe: httpGet: path: / port: 80","title":"Liveness Probe"},{"location":"containers/kubernetes/admin/monitoring/#readiness-probe","text":"apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx --- apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Pod metadata: name: nginxpd labels: app: nginx spec: containers: - name: nginx image: nginx:191 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5","title":"Readiness Probe"},{"location":"containers/kubernetes/admin/monitoring/#logs","text":"","title":"Logs"},{"location":"containers/kubernetes/admin/monitoring/#cluster","text":"","title":"Cluster"},{"location":"containers/kubernetes/admin/monitoring/#dirs","text":"The directory where the continainer logs reside ls /var/log/containers The directory where kubelet stores its logs ls /var/log","title":"Dirs"},{"location":"containers/kubernetes/admin/monitoring/#sidecar-container","text":"The YAML for a sidecar container that will tail the logs for each type apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log'] volumeMounts: - name: varlog mountPath: /var/log - name: count-log-2 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log'] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} kubectl logs counter count-log-1 kubectl logs counter count-log-2","title":"SideCar Container"},{"location":"containers/kubernetes/admin/monitoring/#application","text":"# Get the logs from a pod: kubectl logs nginx # Get the logs from a specific container on a pod: kubectl logs counter -c count-log-1 # Get the logs from all containers on the pod: kubectl logs counter --all-containers=true # Get the logs from containers with a certain label: kubectl logs -lapp=nginx # Get the logs from a previously terminated container within a pod: kubectl logs -p -c nginx nginx # Stream the logs from a container in a pod: kubectl logs -f -c count-log-1 counter # Tail the logs to only view a certain number of lines: kubectl logs --tail=20 nginx # View the logs from a previous time duration: kubectl logs --since=1h nginx # View the logs from a container within a pod within a deployment: kubectl logs deployment/nginx -c nginx # Redirect the output of the logs to a file: kubectl logs counter -c count-log-1 > count.log","title":"Application"},{"location":"containers/kubernetes/admin/monitoring/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"containers/kubernetes/admin/monitoring/#applications_1","text":"","title":"Applications"},{"location":"containers/kubernetes/admin/monitoring/#use-termination-reason","text":"apiVersion: v1 kind: Pod metadata: name: pod2 spec: containers: - image: busybox name: main command: - sh - -c - 'echo \"I''ve had enough\" > /var/termination-reason ; exit 1' terminationMessagePath: /var/termination-reason","title":"Use Termination Reason"},{"location":"containers/kubernetes/admin/monitoring/#healthz","text":"Not all pods have healthz configured apiVersion: v1 kind: Pod metadata: name: liveness spec: containers: - image: linuxacademycontent/candy-service:2 name: kubeserve livenessProbe: httpGet: path: /healthz port: 8081","title":"Healthz"},{"location":"containers/kubernetes/admin/monitoring/#steps","text":"kubectl describe po pod2 kubectl logs pod-with-defaults kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml","title":"Steps"},{"location":"containers/kubernetes/admin/monitoring/#cluster_1","text":"# Check the events in the kube-system namespace for errors kubectl get events -n kube-system kubectl logs [kube_scheduler_pod_name] -n kube-system # Check the status of the Docker service: sudo systemctl status docker sudo systemctl enable docker && systemctl start docker # Check the status of the kubelet service: sudo systemctl status kubelet sudo systemctl enable kubelet && systemctl start kubelet # Turn off swap on your machine sudo su - swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # Check if you have a firewall running: sudo systemctl status firewalld sudo systemctl disable firewalld && systemctl stop firewalld","title":"Cluster"},{"location":"containers/kubernetes/admin/monitoring/#worker-node","text":"kubectl get nodes kubectl describe nodes chadcrowell2c.mylabserver.com # Create New Worker Server # Generate a new token after spinning up a new server: sudo kubeadm token generate # Create the kubeadm join command for your new worker node: # sudo kubeadm token create [token_name] --ttl 2h --print-join-command # View the journalctl logs: sudo journalctl -u kubelet # View the syslogs: sudo more syslog | tail -120 | grep kubelet","title":"Worker Node"},{"location":"containers/kubernetes/admin/monitoring/#networking","text":"","title":"Networking"},{"location":"containers/kubernetes/admin/monitoring/#dns","text":"# Run an interactive busybox pod: kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 sh # From the pod, check if DNS is resolving hostnames: nslookup hostnames # From the pod, cat out the /etc/resolv.conf file: cat /etc/resolv.conf # From the pod, look up the DNS name of the Kubernetes service: nslookup kubernetes.default nslookup kube-dns.kube-system.svc.cluster.loca # Look up a service in your Kubernetes cluster nslookup [pod-ip-address].default.pod.cluster.local # Logs Core Dns kubectl logs [coredns-pod-name]","title":"DNS"},{"location":"containers/kubernetes/admin/monitoring/#kube-proxy","text":"# View the endpoints for your service: kubectl get ep # Communicate with the pod directly (without the service): wget -qO- 10.244.1.6:9376 # Check if kube-proxy is running on the nodes: ps auxw | grep kube-proxy # Check if kube-proxy is writing iptables: kubectl get services -o wide iptables-save | grep hostnames sudo iptables-save | grep KUBE | grep <service-name> # View the list of kube-system pods: kubectl get pods -n kube-system # Connect to your kube-proxy pod in the kube-system namespace: kubectl exec -it kube-proxy-cqptg -n kube-system -- sh","title":"Kube-Proxy"},{"location":"containers/kubernetes/admin/monitoring/#change-cni-plugin","text":"# Delete the flannel CNI plugin: kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml # Apply the Weave Net CNI plugin: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"","title":"Change CNI Plugin"},{"location":"containers/kubernetes/admin/networking/","text":"Flannel Redhat kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Ubuntu On all nodes echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Master kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml Network Policies Plugin Canal wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml Deny All apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress Pod Selector apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - podSelector: matchLabels: app: web ports: - port: 5432 Namespace Policy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ns-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - namespaceSelector: matchLabels: tenant: web ports: - port: 5432 Block IP apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ipblock-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - ipBlock: cidr: 192.168.1.0/24 Egress Policy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: egress-netpol spec: podSelector: matchLabels: app: web egress: - to: - podSelector: matchLabels: app: db ports: - port: 5432 Custom DNS apiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsPolicy: \"None\" dnsConfig: nameservers: - 8.8.8.8 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: \"2\" - name: edns0","title":"Networking"},{"location":"containers/kubernetes/admin/networking/#flannel","text":"","title":"Flannel"},{"location":"containers/kubernetes/admin/networking/#redhat","text":"kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml","title":"Redhat"},{"location":"containers/kubernetes/admin/networking/#ubuntu","text":"On all nodes echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Master kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml","title":"Ubuntu"},{"location":"containers/kubernetes/admin/networking/#network-policies","text":"","title":"Network Policies"},{"location":"containers/kubernetes/admin/networking/#plugin-canal","text":"wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml","title":"Plugin Canal"},{"location":"containers/kubernetes/admin/networking/#deny-all","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress","title":"Deny All"},{"location":"containers/kubernetes/admin/networking/#pod-selector","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - podSelector: matchLabels: app: web ports: - port: 5432","title":"Pod Selector"},{"location":"containers/kubernetes/admin/networking/#namespace-policy","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ns-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - namespaceSelector: matchLabels: tenant: web ports: - port: 5432","title":"Namespace Policy"},{"location":"containers/kubernetes/admin/networking/#block-ip","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ipblock-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - ipBlock: cidr: 192.168.1.0/24","title":"Block IP"},{"location":"containers/kubernetes/admin/networking/#egress-policy","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: egress-netpol spec: podSelector: matchLabels: app: web egress: - to: - podSelector: matchLabels: app: db ports: - port: 5432","title":"Egress Policy"},{"location":"containers/kubernetes/admin/networking/#custom-dns","text":"apiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsPolicy: \"None\" dnsConfig: nameservers: - 8.8.8.8 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: \"2\" - name: edns0","title":"Custom DNS"},{"location":"containers/kubernetes/admin/nodes/","text":"Get kubectl get nodes Describe kubectl describe node/node1 kubectl describe node/node2 kubectl describe node/master Api resources kubectl api-resources -o wide Components Status kubectl get componentstatus Label kubectl label node node1 availability-zone=zone1 kubectl label node node2 share-type=dedicated","title":"Nodes"},{"location":"containers/kubernetes/admin/nodes/#get","text":"kubectl get nodes","title":"Get"},{"location":"containers/kubernetes/admin/nodes/#describe","text":"kubectl describe node/node1 kubectl describe node/node2 kubectl describe node/master","title":"Describe"},{"location":"containers/kubernetes/admin/nodes/#api-resources","text":"kubectl api-resources -o wide","title":"Api resources"},{"location":"containers/kubernetes/admin/nodes/#components-status","text":"kubectl get componentstatus","title":"Components Status"},{"location":"containers/kubernetes/admin/nodes/#label","text":"kubectl label node node1 availability-zone=zone1 kubectl label node node2 share-type=dedicated","title":"Label"},{"location":"containers/kubernetes/admin/persistentvolumes/","text":"PV HostPath apiVersion: v1 kind: PersistentVolume metadata: name: pv-hostpath spec: storageClassName: local-storage capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\" PV Claim apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mongodb-pvc spec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce storageClassName: \"local-storage\" Deploy Pod apiVersion: v1 kind: Pod metadata: name: mongodb spec: containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP volumes: - name: mongodb-data persistentVolumeClaim: claimName: mongodb-pvc Storage Class apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd Empty Dir apiVersion: v1 kind: Pod metadata: name: emptydir-pod spec: containers: - image: busybox name: busybox command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"] volumeMounts: - mountPath: /tmp/storage name: vol volumes: - name: vol emptyDir: {}","title":"PersistentVolumes"},{"location":"containers/kubernetes/admin/persistentvolumes/#pv","text":"","title":"PV"},{"location":"containers/kubernetes/admin/persistentvolumes/#hostpath","text":"apiVersion: v1 kind: PersistentVolume metadata: name: pv-hostpath spec: storageClassName: local-storage capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\"","title":"HostPath"},{"location":"containers/kubernetes/admin/persistentvolumes/#pv-claim","text":"apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mongodb-pvc spec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce storageClassName: \"local-storage\"","title":"PV Claim"},{"location":"containers/kubernetes/admin/persistentvolumes/#deploy-pod","text":"apiVersion: v1 kind: Pod metadata: name: mongodb spec: containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP volumes: - name: mongodb-data persistentVolumeClaim: claimName: mongodb-pvc","title":"Deploy Pod"},{"location":"containers/kubernetes/admin/persistentvolumes/#storage-class","text":"apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd","title":"Storage Class"},{"location":"containers/kubernetes/admin/persistentvolumes/#empty-dir","text":"apiVersion: v1 kind: Pod metadata: name: emptydir-pod spec: containers: - image: busybox name: busybox command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"] volumeMounts: - mountPath: /tmp/storage name: vol volumes: - name: vol emptyDir: {}","title":"Empty Dir"},{"location":"containers/kubernetes/admin/scheduler/","text":"Default Scheduler Does the node have adequate hardware resources? Is the node running out of resources? Does the pod request a specific node? Does the node have a matching label? If the pod requests a port, is it available? If the pod requests a volume, can it be mounted? Does the pod tolerate the taints of the node? Does the pod specify node or pod affinity? Custom Scheduler ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: csinodes-admin rules: - apiGroups: [\"storage.k8s.io\"] resources: [\"csinodes\"] verbs: [\"get\", \"watch\", \"list\"] ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-csinodes-global subjects: - kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: csinodes-admin apiGroup: rbac.authorization.k8s.io Role apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: system:serviceaccount:kube-system:my-scheduler namespace: kube-system rules: - apiGroups: - storage.k8s.io resources: - csinodes verbs: - get - list - watch RoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-csinodes namespace: kube-system subjects: - kind: User name: kubernetes-admin apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: system:serviceaccount:kube-system:my-scheduler apiGroup: rbac.authorization.k8s.io My-scheduler apiVersion: v1 kind: ServiceAccount metadata: name: my-scheduler namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: my-scheduler-as-kube-scheduler subjects: - kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-scheduler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: labels: component: scheduler tier: control-plane name: my-scheduler namespace: kube-system spec: selector: matchLabels: component: scheduler tier: control-plane replicas: 1 template: metadata: labels: component: scheduler tier: control-plane version: second spec: serviceAccountName: my-scheduler containers: - command: - /usr/local/bin/kube-scheduler - --address=0.0.0.0 - --leader-elect=false - --scheduler-name=my-scheduler image: chadmcrowell/custom-scheduler livenessProbe: httpGet: path: /healthz port: 10251 initialDelaySeconds: 15 name: kube-second-scheduler readinessProbe: httpGet: path: /healthz port: 10251 resources: requests: cpu: '0.1' securityContext: privileged: false volumeMounts: [] hostNetwork: false hostPID: false volumes: [] Deploy kubectl create -f clusterrole.yaml kubectl create -f clusterrolebinding.yaml kubectl create -f role.yaml kubectl create -f rolebinding.yaml kubectl edit clusterrole system:kube-scheduler # And add the following - apiGroups: - \"\" resourceNames: - kube-scheduler - my-scheduler resources: - endpoints verbs: - delete - get - patch - update - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - watch - list - get kubectl create -f my-scheduler.yaml kubectl get pods -n kube-system Get config kubectl get endpoints kube-scheduler -n kube-system -o yaml Troubleshooting kubectl describe pods [scheduler_pod_name] -n kube-system kubectl logs [kube_scheduler_pod_name] -n kube-system cat /var/log/kube-scheduler.log","title":"Scheduler"},{"location":"containers/kubernetes/admin/scheduler/#default-scheduler","text":"Does the node have adequate hardware resources? Is the node running out of resources? Does the pod request a specific node? Does the node have a matching label? If the pod requests a port, is it available? If the pod requests a volume, can it be mounted? Does the pod tolerate the taints of the node? Does the pod specify node or pod affinity?","title":"Default Scheduler"},{"location":"containers/kubernetes/admin/scheduler/#custom-scheduler","text":"","title":"Custom Scheduler"},{"location":"containers/kubernetes/admin/scheduler/#clusterrole","text":"apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: csinodes-admin rules: - apiGroups: [\"storage.k8s.io\"] resources: [\"csinodes\"] verbs: [\"get\", \"watch\", \"list\"]","title":"ClusterRole"},{"location":"containers/kubernetes/admin/scheduler/#clusterrolebinding","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-csinodes-global subjects: - kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: csinodes-admin apiGroup: rbac.authorization.k8s.io","title":"ClusterRoleBinding"},{"location":"containers/kubernetes/admin/scheduler/#role","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: system:serviceaccount:kube-system:my-scheduler namespace: kube-system rules: - apiGroups: - storage.k8s.io resources: - csinodes verbs: - get - list - watch","title":"Role"},{"location":"containers/kubernetes/admin/scheduler/#rolebinding","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-csinodes namespace: kube-system subjects: - kind: User name: kubernetes-admin apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: system:serviceaccount:kube-system:my-scheduler apiGroup: rbac.authorization.k8s.io","title":"RoleBinding"},{"location":"containers/kubernetes/admin/scheduler/#my-scheduler","text":"apiVersion: v1 kind: ServiceAccount metadata: name: my-scheduler namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: my-scheduler-as-kube-scheduler subjects: - kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-scheduler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: labels: component: scheduler tier: control-plane name: my-scheduler namespace: kube-system spec: selector: matchLabels: component: scheduler tier: control-plane replicas: 1 template: metadata: labels: component: scheduler tier: control-plane version: second spec: serviceAccountName: my-scheduler containers: - command: - /usr/local/bin/kube-scheduler - --address=0.0.0.0 - --leader-elect=false - --scheduler-name=my-scheduler image: chadmcrowell/custom-scheduler livenessProbe: httpGet: path: /healthz port: 10251 initialDelaySeconds: 15 name: kube-second-scheduler readinessProbe: httpGet: path: /healthz port: 10251 resources: requests: cpu: '0.1' securityContext: privileged: false volumeMounts: [] hostNetwork: false hostPID: false volumes: []","title":"My-scheduler"},{"location":"containers/kubernetes/admin/scheduler/#deploy","text":"kubectl create -f clusterrole.yaml kubectl create -f clusterrolebinding.yaml kubectl create -f role.yaml kubectl create -f rolebinding.yaml kubectl edit clusterrole system:kube-scheduler # And add the following - apiGroups: - \"\" resourceNames: - kube-scheduler - my-scheduler resources: - endpoints verbs: - delete - get - patch - update - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - watch - list - get kubectl create -f my-scheduler.yaml kubectl get pods -n kube-system","title":"Deploy"},{"location":"containers/kubernetes/admin/scheduler/#get-config","text":"kubectl get endpoints kube-scheduler -n kube-system -o yaml","title":"Get config"},{"location":"containers/kubernetes/admin/scheduler/#troubleshooting","text":"kubectl describe pods [scheduler_pod_name] -n kube-system kubectl logs [kube_scheduler_pod_name] -n kube-system cat /var/log/kube-scheduler.log","title":"Troubleshooting"},{"location":"containers/kubernetes/admin/security/","text":"End-To End Tests CheckList Deployments can run Pods can run Pods can be directly accessed Logs can be collected Commands run from pod Services can provide access Nodes are healthy Pods are healthy Service Accounts Get kubectl get serviceaccounts Create kubectl get serviceaccounts kubectl get serviceaccounts jenkins -o yaml Pod Example apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: serviceAccountName: jenkins containers: - image: busybox:1.28.4 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always View the token file from within a pod kubectl get pods -n my-ns kubectl exec -it <name-of-pod> -n my-ns sh cat /var/run/secrets/kubernetes.io/serviceaccount/token Users Create kubectl config view kubectl config set-credentials chad --username=chad --password=password # Create a role binding for anonymous users (not recommended in production): kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous # Need Copy /etc/kubernetes/pki/ca.crt to remote machine # Remote Machine (Install Kubectl) kubectl config set-cluster kubernetes --server=https://172.31.41.61:6443 --certificate-authority=ca.crt --embed-certs=true kubectl config set-credentials chad --username=chad --password=password kubectl config set-context kubernetes --cluster=kubernetes --user=chad --namespace=default kubectl config use-context kubernetes Roles Role apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: web name: service-reader rules: - apiGroups: [\"\"] verbs: [\"get\", \"list\"] resources: [\"services\"] RoleBinding kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web Cluster Roles Cluster Role kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes Cluster Role Binding kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=web:default Test apiVersion: v1 kind: Pod metadata: name: curlpod namespace: web spec: containers: - image: tutum/curl command: [\"sleep\", \"9999999\"] name: main - image: linuxacademycontent/kubectl-proxy name: proxy restartPolicy: Always kubectl apply -f curl-pod.yaml kubectl get pods -n web kubectl exec -it curlpod -n web -- sh curl localhost:8001/api/v1/persistentvolumes TLS Certficates Create # Find the CA certificate on a pod in your cluster: kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount # Download the binaries for the cfssl tool: wget -q --show-progress --https-only --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 # Make the binary files executable: chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson cfssl version # Create a CSR file cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"my-svc.my-namespace.svc.cluster.local\", \"my-pod.my-namespace.pod.cluster.local\", \"172.168.0.24\", \"10.0.34.2\" ], \"CN\": \"my-pod.my-namespace.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF # Create a CertificateSigningRequest API object: cat <<EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: pod-csr.web spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF # View the CSRs in the cluster: kubectl get csr # View additional details about the CSR: kubectl describe csr pod-csr.web # Approve the CSR: kubectl certificate approve pod-csr.web # View the certificate within your CSR: kubectl get csr pod-csr.web -o yaml # Extract and decode your certificate to use in a file: kubectl get csr pod-csr.web -o jsonpath='{.status.certificate}' \\ | base64 --decode > server.crt Container Registry Create # Create a new docker-registry secret: kubectl create secret docker-registry acr --docker-server=https://podofminerva.azurecr.io --docker-username=podofminerva --docker-password='otj701c9OucKZOCx5qrRblofcNRf3W+e' --docker-email=user@example.com # Modify the default service account to use your new docker-registry secret: kubectl patch sa default -p '{\"imagePullSecrets\": [{\"name\": \"acr\"}]}' apiVersion: v1 kind: Pod metadata: name: acr-pod labels: app: busybox spec: containers: - name: busybox image: podofminerva.azurecr.io/busybox:latest command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600'] imagePullPolicy: Always Security Contexts The YAML for a container that runs as a user apiVersion: v1 kind: Pod metadata: name: alpine-user-context spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 405 The YAML for a pod that runs the container as non-root apiVersion: v1 kind: Pod metadata: name: alpine-nonroot spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsNonRoot: true The YAML for a privileged container pod apiVersion: v1 kind: Pod metadata: name: privileged-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: privileged: true The YAML for a container that will allow you to change the time apiVersion: v1 kind: Pod metadata: name: kernelchange-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: capabilities: add: - SYS_TIME The YAML for a container that removes capabilities apiVersion: v1 kind: Pod metadata: name: remove-capabilities spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: capabilities: drop: - CHOWN The YAML for a pod container that can\u2019t write to the local filesystem apiVersion: v1 kind: Pod metadata: name: readonly-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: readOnlyRootFilesystem: true volumeMounts: - name: my-volume mountPath: /volume readOnly: false volumes: - name: my-volume emptyDir: The YAML for a pod that has different group permissions for different pods apiVersion: v1 kind: Pod metadata: name: group-context spec: securityContext: fsGroup: 555 supplementalGroups: [666, 777] containers: - name: first image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 1111 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false - name: second image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 2222 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false volumes: - name: shared-volume emptyDir: Persistent Key Value Store # Generate a key for your https server: openssl genrsa -out https.key 2048 # Generate a certificate for the https server: openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.example.com # Create an empty file to create the secret: touch file # Create a secret from your key, cert, and file: kubectl create secret generic example-https --from-file=https.key --from-file=https.cert --from-file=file Create the configMap that will mount to your pod apiVersion: v1 kind: ConfigMap metadata: name: config data: my-nginx-config.conf: | server { listen 80; listen 443 ssl; server_name www.example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval: | 25 The YAML for a pod using the new secret apiVersion: v1 kind: Pod metadata: name: example-https spec: containers: - image: linuxacademycontent/fortune name: html-web env: - name: INTERVAL valueFrom: configMapKeyRef: name: config key: sleep-interval volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: certs mountPath: /etc/nginx/certs/ readOnly: true ports: - containerPort: 80 - containerPort: 443 volumes: - name: html emptyDir: {} - name: config configMap: name: config items: - key: my-nginx-config.conf path: https.conf - name: certs secret: secretName: example-https # Use port forwarding on the pod to server traffic from 443: kubectl port-forward example-https 8443:443 & # Curl the web server to get a response: curl https://localhost:8443 -k","title":"Security"},{"location":"containers/kubernetes/admin/security/#end-to-end-tests","text":"","title":"End-To End Tests"},{"location":"containers/kubernetes/admin/security/#checklist","text":"Deployments can run Pods can run Pods can be directly accessed Logs can be collected Commands run from pod Services can provide access Nodes are healthy Pods are healthy","title":"CheckList"},{"location":"containers/kubernetes/admin/security/#service-accounts","text":"","title":"Service Accounts"},{"location":"containers/kubernetes/admin/security/#get","text":"kubectl get serviceaccounts","title":"Get"},{"location":"containers/kubernetes/admin/security/#create","text":"kubectl get serviceaccounts kubectl get serviceaccounts jenkins -o yaml","title":"Create"},{"location":"containers/kubernetes/admin/security/#pod-example","text":"apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: serviceAccountName: jenkins containers: - image: busybox:1.28.4 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always","title":"Pod Example"},{"location":"containers/kubernetes/admin/security/#view-the-token-file-from-within-a-pod","text":"kubectl get pods -n my-ns kubectl exec -it <name-of-pod> -n my-ns sh cat /var/run/secrets/kubernetes.io/serviceaccount/token","title":"View the token file from within a pod"},{"location":"containers/kubernetes/admin/security/#users","text":"","title":"Users"},{"location":"containers/kubernetes/admin/security/#create_1","text":"kubectl config view kubectl config set-credentials chad --username=chad --password=password # Create a role binding for anonymous users (not recommended in production): kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous # Need Copy /etc/kubernetes/pki/ca.crt to remote machine # Remote Machine (Install Kubectl) kubectl config set-cluster kubernetes --server=https://172.31.41.61:6443 --certificate-authority=ca.crt --embed-certs=true kubectl config set-credentials chad --username=chad --password=password kubectl config set-context kubernetes --cluster=kubernetes --user=chad --namespace=default kubectl config use-context kubernetes","title":"Create"},{"location":"containers/kubernetes/admin/security/#roles","text":"","title":"Roles"},{"location":"containers/kubernetes/admin/security/#role","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: web name: service-reader rules: - apiGroups: [\"\"] verbs: [\"get\", \"list\"] resources: [\"services\"]","title":"Role"},{"location":"containers/kubernetes/admin/security/#rolebinding","text":"kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web","title":"RoleBinding"},{"location":"containers/kubernetes/admin/security/#cluster-roles","text":"","title":"Cluster Roles"},{"location":"containers/kubernetes/admin/security/#cluster-role","text":"kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes","title":"Cluster Role"},{"location":"containers/kubernetes/admin/security/#cluster-role-binding","text":"kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=web:default","title":"Cluster Role Binding"},{"location":"containers/kubernetes/admin/security/#test","text":"apiVersion: v1 kind: Pod metadata: name: curlpod namespace: web spec: containers: - image: tutum/curl command: [\"sleep\", \"9999999\"] name: main - image: linuxacademycontent/kubectl-proxy name: proxy restartPolicy: Always kubectl apply -f curl-pod.yaml kubectl get pods -n web kubectl exec -it curlpod -n web -- sh curl localhost:8001/api/v1/persistentvolumes","title":"Test"},{"location":"containers/kubernetes/admin/security/#tls-certficates","text":"","title":"TLS Certficates"},{"location":"containers/kubernetes/admin/security/#create_2","text":"# Find the CA certificate on a pod in your cluster: kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount # Download the binaries for the cfssl tool: wget -q --show-progress --https-only --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 # Make the binary files executable: chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson cfssl version # Create a CSR file cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"my-svc.my-namespace.svc.cluster.local\", \"my-pod.my-namespace.pod.cluster.local\", \"172.168.0.24\", \"10.0.34.2\" ], \"CN\": \"my-pod.my-namespace.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF # Create a CertificateSigningRequest API object: cat <<EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: pod-csr.web spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF # View the CSRs in the cluster: kubectl get csr # View additional details about the CSR: kubectl describe csr pod-csr.web # Approve the CSR: kubectl certificate approve pod-csr.web # View the certificate within your CSR: kubectl get csr pod-csr.web -o yaml # Extract and decode your certificate to use in a file: kubectl get csr pod-csr.web -o jsonpath='{.status.certificate}' \\ | base64 --decode > server.crt","title":"Create"},{"location":"containers/kubernetes/admin/security/#container-registry","text":"","title":"Container Registry"},{"location":"containers/kubernetes/admin/security/#create_3","text":"# Create a new docker-registry secret: kubectl create secret docker-registry acr --docker-server=https://podofminerva.azurecr.io --docker-username=podofminerva --docker-password='otj701c9OucKZOCx5qrRblofcNRf3W+e' --docker-email=user@example.com # Modify the default service account to use your new docker-registry secret: kubectl patch sa default -p '{\"imagePullSecrets\": [{\"name\": \"acr\"}]}' apiVersion: v1 kind: Pod metadata: name: acr-pod labels: app: busybox spec: containers: - name: busybox image: podofminerva.azurecr.io/busybox:latest command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600'] imagePullPolicy: Always","title":"Create"},{"location":"containers/kubernetes/admin/security/#security-contexts","text":"The YAML for a container that runs as a user apiVersion: v1 kind: Pod metadata: name: alpine-user-context spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 405 The YAML for a pod that runs the container as non-root apiVersion: v1 kind: Pod metadata: name: alpine-nonroot spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsNonRoot: true The YAML for a privileged container pod apiVersion: v1 kind: Pod metadata: name: privileged-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: privileged: true The YAML for a container that will allow you to change the time apiVersion: v1 kind: Pod metadata: name: kernelchange-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: capabilities: add: - SYS_TIME The YAML for a container that removes capabilities apiVersion: v1 kind: Pod metadata: name: remove-capabilities spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: capabilities: drop: - CHOWN The YAML for a pod container that can\u2019t write to the local filesystem apiVersion: v1 kind: Pod metadata: name: readonly-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: readOnlyRootFilesystem: true volumeMounts: - name: my-volume mountPath: /volume readOnly: false volumes: - name: my-volume emptyDir: The YAML for a pod that has different group permissions for different pods apiVersion: v1 kind: Pod metadata: name: group-context spec: securityContext: fsGroup: 555 supplementalGroups: [666, 777] containers: - name: first image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 1111 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false - name: second image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 2222 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false volumes: - name: shared-volume emptyDir:","title":"Security Contexts"},{"location":"containers/kubernetes/admin/security/#persistent-key-value-store","text":"# Generate a key for your https server: openssl genrsa -out https.key 2048 # Generate a certificate for the https server: openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.example.com # Create an empty file to create the secret: touch file # Create a secret from your key, cert, and file: kubectl create secret generic example-https --from-file=https.key --from-file=https.cert --from-file=file Create the configMap that will mount to your pod apiVersion: v1 kind: ConfigMap metadata: name: config data: my-nginx-config.conf: | server { listen 80; listen 443 ssl; server_name www.example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval: | 25 The YAML for a pod using the new secret apiVersion: v1 kind: Pod metadata: name: example-https spec: containers: - image: linuxacademycontent/fortune name: html-web env: - name: INTERVAL valueFrom: configMapKeyRef: name: config key: sleep-interval volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: certs mountPath: /etc/nginx/certs/ readOnly: true ports: - containerPort: 80 - containerPort: 443 volumes: - name: html emptyDir: {} - name: config configMap: name: config items: - key: my-nginx-config.conf path: https.conf - name: certs secret: secretName: example-https # Use port forwarding on the pod to server traffic from 443: kubectl port-forward example-https 8443:443 & # Curl the web server to get a response: curl https://localhost:8443 -k","title":"Persistent Key Value Store"},{"location":"containers/kubernetes/admin/taint/","text":"Taint Node kubectl taint node <node_name> node-type=prod:NoSchedule Pod with Toleration apiVersion: apps/v1 kind: Deployment metadata: name: prod spec: replicas: 1 selector: matchLabels: app: prod template: metadata: labels: app: prod spec: containers: - args: - sleep - \"3600\" image: busybox name: main tolerations: - key: node-type operator: Equal value: prod effect: NoSchedule","title":"Taint"},{"location":"containers/kubernetes/admin/taint/#taint-node","text":"kubectl taint node <node_name> node-type=prod:NoSchedule","title":"Taint Node"},{"location":"containers/kubernetes/admin/taint/#pod-with-toleration","text":"apiVersion: apps/v1 kind: Deployment metadata: name: prod spec: replicas: 1 selector: matchLabels: app: prod template: metadata: labels: app: prod spec: containers: - args: - sleep - \"3600\" image: busybox name: main tolerations: - key: node-type operator: Equal value: prod effect: NoSchedule","title":"Pod with Toleration"},{"location":"containers/kubernetes/development/daemonsets/","text":"Example File apiVersion: apps/v1beta2 kind: DaemonSet metadata: name: ssd-monitor spec: selector: matchLabels: app: ssd-monitor template: metadata: labels: app: ssd-monitor spec: nodeSelector: disk: ssd containers: - name: main image: linuxacademycontent/ssd-monitor","title":"DaemonSets"},{"location":"containers/kubernetes/development/daemonsets/#example-file","text":"apiVersion: apps/v1beta2 kind: DaemonSet metadata: name: ssd-monitor spec: selector: matchLabels: app: ssd-monitor template: metadata: labels: app: ssd-monitor spec: nodeSelector: disk: ssd containers: - name: main image: linuxacademycontent/ssd-monitor","title":"Example File"},{"location":"containers/kubernetes/development/deployments/","text":"Example File Simple Example apiVersion: apps/v1 kind: Deployment metadata: name: example-deployment labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: darealmc/nginx-k8s:v1 ports: - containerPort: 80 Node affinity apiVersion: extensions/v1beta1 kind: Deployment metadata: name: pref spec: replicas: 5 template: metadata: labels: app: pref spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 preference: matchExpressions: - key: availability-zone operator: In values: - zone1 - weight: 20 preference: matchExpressions: - key: share-type operator: In values: - dedicated containers: - args: - sleep - \"99999\" image: busybox name: main Update Image kubectl set image deployment.v1.apps/example-deployment nginx=darealmc/nginx-k8s:v2 MicroServices Example cd ~/ git clone https://github.com/linuxacademy/robot-shop.git kubectl create namespace robot-shop kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors/ kubectl get pods -n robot-shop -w # Access in http://$kube_server_public_ip:30080 Application LifeCycle Manager Update kubectl apply -f kubeserve-deployment.yaml kubectl replace -f kubeserve-deployment.yaml Rolling Update kubectl set image deployments/kubeserve app=linuxacademycontent/kubeserve:v2 --v 6 Rollback # Use --record flag to create the deployment - kubectl create -f kubeserve-deployment.yaml --record kubectl rollout undo deployments kubeserve kubectl rollout history deployment kubeserve kubectl rollout undo deployment kubeserve --to-revision=2 Pause / Resume kubectl rollout undo deployment kubeserve --to-revision=2 kubectl rollout resume deployment kubeserve Readiness Probe apiVersion: apps/v1 kind: Deployment metadata: name: kubeserve spec: replicas: 3 selector: matchLabels: app: kubeserve minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubeserve labels: app: kubeserve spec: containers: - image: linuxacademycontent/kubeserve:v3 name: app readinessProbe: periodSeconds: 1 httpGet: path: / port: 80","title":"Deployments"},{"location":"containers/kubernetes/development/deployments/#example-file","text":"","title":"Example File"},{"location":"containers/kubernetes/development/deployments/#simple-example","text":"apiVersion: apps/v1 kind: Deployment metadata: name: example-deployment labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: darealmc/nginx-k8s:v1 ports: - containerPort: 80","title":"Simple Example"},{"location":"containers/kubernetes/development/deployments/#node-affinity","text":"apiVersion: extensions/v1beta1 kind: Deployment metadata: name: pref spec: replicas: 5 template: metadata: labels: app: pref spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 preference: matchExpressions: - key: availability-zone operator: In values: - zone1 - weight: 20 preference: matchExpressions: - key: share-type operator: In values: - dedicated containers: - args: - sleep - \"99999\" image: busybox name: main","title":"Node affinity"},{"location":"containers/kubernetes/development/deployments/#update-image","text":"kubectl set image deployment.v1.apps/example-deployment nginx=darealmc/nginx-k8s:v2","title":"Update Image"},{"location":"containers/kubernetes/development/deployments/#microservices-example","text":"cd ~/ git clone https://github.com/linuxacademy/robot-shop.git kubectl create namespace robot-shop kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors/ kubectl get pods -n robot-shop -w # Access in http://$kube_server_public_ip:30080","title":"MicroServices Example"},{"location":"containers/kubernetes/development/deployments/#application-lifecycle-manager","text":"","title":"Application LifeCycle Manager"},{"location":"containers/kubernetes/development/deployments/#update","text":"kubectl apply -f kubeserve-deployment.yaml kubectl replace -f kubeserve-deployment.yaml","title":"Update"},{"location":"containers/kubernetes/development/deployments/#rolling-update","text":"kubectl set image deployments/kubeserve app=linuxacademycontent/kubeserve:v2 --v 6","title":"Rolling Update"},{"location":"containers/kubernetes/development/deployments/#rollback","text":"# Use --record flag to create the deployment - kubectl create -f kubeserve-deployment.yaml --record kubectl rollout undo deployments kubeserve kubectl rollout history deployment kubeserve kubectl rollout undo deployment kubeserve --to-revision=2","title":"Rollback"},{"location":"containers/kubernetes/development/deployments/#pause-resume","text":"kubectl rollout undo deployment kubeserve --to-revision=2 kubectl rollout resume deployment kubeserve","title":"Pause / Resume"},{"location":"containers/kubernetes/development/deployments/#readiness-probe","text":"apiVersion: apps/v1 kind: Deployment metadata: name: kubeserve spec: replicas: 3 selector: matchLabels: app: kubeserve minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubeserve labels: app: kubeserve spec: containers: - image: linuxacademycontent/kubeserve:v3 name: app readinessProbe: periodSeconds: 1 httpGet: path: / port: 80","title":"Readiness Probe"},{"location":"containers/kubernetes/development/ingress/","text":"Example File apiVersion: extensions/v1beta1 kind: Ingress metadata: name: service-ingress spec: rules: - host: kubeserve.example.com http: paths: - backend: serviceName: kubeserve2 servicePort: 80 - host: app.example.com http: paths: - backend: serviceName: nginx servicePort: 80 - http: paths: - backend: serviceName: httpd servicePort: 80 Useful Commands kubectl edit ingress kubectl describe ingress","title":"Ingress"},{"location":"containers/kubernetes/development/ingress/#example-file","text":"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: service-ingress spec: rules: - host: kubeserve.example.com http: paths: - backend: serviceName: kubeserve2 servicePort: 80 - host: app.example.com http: paths: - backend: serviceName: nginx servicePort: 80 - http: paths: - backend: serviceName: httpd servicePort: 80","title":"Example File"},{"location":"containers/kubernetes/development/ingress/#useful-commands","text":"kubectl edit ingress kubectl describe ingress","title":"Useful Commands"},{"location":"containers/kubernetes/development/pods/","text":"Pods Get kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl get pods --namespace=podexample -o wide kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system Example File apiVersion: v1 kind: Pod metadata: name: examplepod namespace: pod-example spec: schedulerName: default-scheduler # To change scheduler - my-scheduler volumes: - name: html emptyDir: {} containers: - name: webcontainer image: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html - name: filecontainer image: debian volumeMounts: - name: html mountPath: /html command: [\"/bin/sh\", \"-c\"] args: - while true; do date >> /html/index.html; sleep 1; done Resources Requests and Limits apiVersion: v1 kind: Pod metadata: name: resource-pod2 spec: nodeSelector: kubernetes.io/hostname: \"chadcrowell3c.mylabserver.com\" containers: - image: busybox command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"] name: pod2 resources: requests: cpu: 1000m memory: 20Mi apiVersion: v1 kind: Pod metadata: name: limited-pod spec: containers: - image: busybox command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"] name: main resources: limits: cpu: 1 memory: 20Mi Create From File kubectl create -f ./pod-example.yaml Delete kubectl --namespace=podexample delete pod examplepod Get Containers Name inside a pod kubectl get pods examplepod -n podexample -o jsonpath='{.spec.containers[*].name}*' Exec in Container kubectl exec -ti examplepod -c webcontainer -n podexample /bin/bash Use port forwarding to access a pod directly kubectl port-forward $pod_name 8081:80 Namespaces Get kubectl get namespaces Create kubectl create namespace podexample kubectl get pods examplepod -n podexample -o jsonpath='{.spec.containers[ ].name} '","title":"Pods"},{"location":"containers/kubernetes/development/pods/#pods","text":"","title":"Pods"},{"location":"containers/kubernetes/development/pods/#get","text":"kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl get pods --namespace=podexample -o wide kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system","title":"Get"},{"location":"containers/kubernetes/development/pods/#example-file","text":"apiVersion: v1 kind: Pod metadata: name: examplepod namespace: pod-example spec: schedulerName: default-scheduler # To change scheduler - my-scheduler volumes: - name: html emptyDir: {} containers: - name: webcontainer image: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html - name: filecontainer image: debian volumeMounts: - name: html mountPath: /html command: [\"/bin/sh\", \"-c\"] args: - while true; do date >> /html/index.html; sleep 1; done","title":"Example File"},{"location":"containers/kubernetes/development/pods/#resources-requests-and-limits","text":"apiVersion: v1 kind: Pod metadata: name: resource-pod2 spec: nodeSelector: kubernetes.io/hostname: \"chadcrowell3c.mylabserver.com\" containers: - image: busybox command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"] name: pod2 resources: requests: cpu: 1000m memory: 20Mi apiVersion: v1 kind: Pod metadata: name: limited-pod spec: containers: - image: busybox command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"] name: main resources: limits: cpu: 1 memory: 20Mi","title":"Resources Requests and Limits"},{"location":"containers/kubernetes/development/pods/#create","text":"From File kubectl create -f ./pod-example.yaml","title":"Create"},{"location":"containers/kubernetes/development/pods/#delete","text":"kubectl --namespace=podexample delete pod examplepod","title":"Delete"},{"location":"containers/kubernetes/development/pods/#get-containers-name-inside-a-pod","text":"kubectl get pods examplepod -n podexample -o jsonpath='{.spec.containers[*].name}*'","title":"Get Containers Name inside a pod"},{"location":"containers/kubernetes/development/pods/#exec-in-container","text":"kubectl exec -ti examplepod -c webcontainer -n podexample /bin/bash","title":"Exec in Container"},{"location":"containers/kubernetes/development/pods/#use-port-forwarding-to-access-a-pod-directly","text":"kubectl port-forward $pod_name 8081:80","title":"Use port forwarding to access a pod directly"},{"location":"containers/kubernetes/development/pods/#namespaces","text":"","title":"Namespaces"},{"location":"containers/kubernetes/development/pods/#get_1","text":"kubectl get namespaces","title":"Get"},{"location":"containers/kubernetes/development/pods/#create_1","text":"kubectl create namespace podexample kubectl get pods examplepod -n podexample -o jsonpath='{.spec.containers[ ].name} '","title":"Create"},{"location":"containers/kubernetes/development/replicasets/","text":"Example file apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: nginx tier: frontend spec: replicas: 2 selector: matchLabels: tier: frontend matchExpressions: - {key: tier, operator: In, values: [frontend]} template: metadata: labels: app: nginx tier: frontend spec: containers: - name: nginx image: darealmc/nginx-k8s:v1 ports: - containerPort: 80 Describe kubectl describe rs/frontend Scale kubectl scale rs/frontend --replicas=4 Delete kubectl delete rs/frontend","title":"ReplicaSets"},{"location":"containers/kubernetes/development/replicasets/#example-file","text":"apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: nginx tier: frontend spec: replicas: 2 selector: matchLabels: tier: frontend matchExpressions: - {key: tier, operator: In, values: [frontend]} template: metadata: labels: app: nginx tier: frontend spec: containers: - name: nginx image: darealmc/nginx-k8s:v1 ports: - containerPort: 80","title":"Example file"},{"location":"containers/kubernetes/development/replicasets/#describe","text":"kubectl describe rs/frontend","title":"Describe"},{"location":"containers/kubernetes/development/replicasets/#scale","text":"kubectl scale rs/frontend --replicas=4","title":"Scale"},{"location":"containers/kubernetes/development/replicasets/#delete","text":"kubectl delete rs/frontend","title":"Delete"},{"location":"containers/kubernetes/development/secrets_configmaps/","text":"Config Maps Create kubectl create configmap appconfig --from-literal=key1=value1 --from-literal=key2=value2 Deploy Pods Env Vars apiVersion: v1 kind: Pod metadata: name: configmap-pod spec: containers: - name: app-container image: busybox:1.28 command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] env: - name: MY_VAR valueFrom: configMapKeyRef: name: appconfig key: key1 Volume apiVersion: v1 kind: Pod metadata: name: configmap-volume-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] volumeMounts: - name: configmapvolume mountPath: /etc/config volumes: - name: configmapvolume configMap: name: appconfig Get kubectl get configmaps --all-namespaces kubectl get configmaps -n kube-system/kube-flannel-cfg kubectl get configmaps/kube-flannel-cfg -n kube-system kubectl describe configmaps/kube-flannel-cfg -n kube-system kubectl get configmap appconfig -o yaml Secrets Create apiVersion: v1 kind: Secret metadata: name: appsecret stringData: cert: value key: value Deploy Pods Env Vars apiVersion: v1 kind: Pod metadata: name: secret-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo Hello, Kubernetes! && sleep 3600\"] env: - name: MY_CERT valueFrom: secretKeyRef: name: appsecret key: cert Volume apiVersion: v1 kind: Pod metadata: name: secret-volume-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] volumeMounts: - name: secretvolume mountPath: /etc/certs volumes: - name: secretvolume secret: secretName: appsecret","title":"Secrets&ConfigMaps"},{"location":"containers/kubernetes/development/secrets_configmaps/#config-maps","text":"","title":"Config Maps"},{"location":"containers/kubernetes/development/secrets_configmaps/#create","text":"kubectl create configmap appconfig --from-literal=key1=value1 --from-literal=key2=value2","title":"Create"},{"location":"containers/kubernetes/development/secrets_configmaps/#deploy-pods","text":"Env Vars apiVersion: v1 kind: Pod metadata: name: configmap-pod spec: containers: - name: app-container image: busybox:1.28 command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] env: - name: MY_VAR valueFrom: configMapKeyRef: name: appconfig key: key1 Volume apiVersion: v1 kind: Pod metadata: name: configmap-volume-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] volumeMounts: - name: configmapvolume mountPath: /etc/config volumes: - name: configmapvolume configMap: name: appconfig","title":"Deploy Pods"},{"location":"containers/kubernetes/development/secrets_configmaps/#get","text":"kubectl get configmaps --all-namespaces kubectl get configmaps -n kube-system/kube-flannel-cfg kubectl get configmaps/kube-flannel-cfg -n kube-system kubectl describe configmaps/kube-flannel-cfg -n kube-system kubectl get configmap appconfig -o yaml","title":"Get"},{"location":"containers/kubernetes/development/secrets_configmaps/#secrets","text":"","title":"Secrets"},{"location":"containers/kubernetes/development/secrets_configmaps/#create_1","text":"apiVersion: v1 kind: Secret metadata: name: appsecret stringData: cert: value key: value","title":"Create"},{"location":"containers/kubernetes/development/secrets_configmaps/#deploy-pods_1","text":"Env Vars apiVersion: v1 kind: Pod metadata: name: secret-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo Hello, Kubernetes! && sleep 3600\"] env: - name: MY_CERT valueFrom: secretKeyRef: name: appsecret key: cert Volume apiVersion: v1 kind: Pod metadata: name: secret-volume-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] volumeMounts: - name: secretvolume mountPath: /etc/certs volumes: - name: secretvolume secret: secretName: appsecret","title":"Deploy Pods"},{"location":"containers/kubernetes/development/services/","text":"Cluster IP kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: ClusterIP selector: app: nginx ports: - protocol: TCP port: 32768 targetPort: 80 NodePort kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 32768 # Service Port targetPort: 80 # Pod Port nodePort: 30080 # Node Port LoadBalancer apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx # Study Things # Set the annotation to route load balancer traffic local to the node: kubectl annotate service kubeserve2 externalTrafficPolicy=Local # https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip Headless Test Excert from Kubenertes in Action by Marco Luksa Each connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is? For a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it. apiVersion: v1 kind: Service metadata: name: kube-headless spec: clusterIP: None ports: - port: 80 targetPort: 8080 selector: app: kubserve2","title":"Services"},{"location":"containers/kubernetes/development/services/#cluster-ip","text":"kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: ClusterIP selector: app: nginx ports: - protocol: TCP port: 32768 targetPort: 80","title":"Cluster IP"},{"location":"containers/kubernetes/development/services/#nodeport","text":"kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 32768 # Service Port targetPort: 80 # Pod Port nodePort: 30080 # Node Port","title":"NodePort"},{"location":"containers/kubernetes/development/services/#loadbalancer","text":"apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx # Study Things # Set the annotation to route load balancer traffic local to the node: kubectl annotate service kubeserve2 externalTrafficPolicy=Local # https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip","title":"LoadBalancer"},{"location":"containers/kubernetes/development/services/#headless","text":"Test Excert from Kubenertes in Action by Marco Luksa Each connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is? For a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it. apiVersion: v1 kind: Service metadata: name: kube-headless spec: clusterIP: None ports: - port: 80 targetPort: 8080 selector: app: kubserve2","title":"Headless"},{"location":"containers/kubernetes/development/statefulsets/","text":"Example File apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi Commands kubectl get statefulsets kubectl describe statefulsets","title":"StatefulSets"},{"location":"containers/kubernetes/development/statefulsets/#example-file","text":"apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi","title":"Example File"},{"location":"containers/kubernetes/development/statefulsets/#commands","text":"kubectl get statefulsets kubectl describe statefulsets","title":"Commands"},{"location":"elk/admin/","text":"Deploy Docker-compose git clone https://github.com/maxyermayank/docker-compose-elasticsearch-kibana Know Errors Max virtual memory Desc: Max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] sudo sysctl -w vm.max_map_count=262144 Forbidden Index x-read-only-allow-delete-api Link PUT .kibana/_settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } }","title":"Administration"},{"location":"elk/admin/#deploy","text":"","title":"Deploy"},{"location":"elk/admin/#docker-compose","text":"git clone https://github.com/maxyermayank/docker-compose-elasticsearch-kibana","title":"Docker-compose"},{"location":"elk/admin/#know-errors","text":"","title":"Know Errors"},{"location":"elk/admin/#max-virtual-memory","text":"Desc: Max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] sudo sysctl -w vm.max_map_count=262144","title":"Max virtual memory"},{"location":"elk/admin/#forbidden-index-x-read-only-allow-delete-api","text":"Link PUT .kibana/_settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } }","title":"Forbidden Index x-read-only-allow-delete-api"},{"location":"elk/beats/","text":"Beats Creating Custom Creating New Beat Examples Know Errors bash: mage: command not found Use vendoring We recommend to use vendoring for your beat. This means the dependencies are put into your beat folder. The beats team currently uses govendor for vendoring. govendor init govendor update +e This will create a directory vendor inside your repository. To make sure all dependencies for the Makefile commands are loaded from the vendor directory, find the following line in your Makefile: ES_BEATS=${GOPATH}/src/github.com/elastic/beats Replace it with: ES_BEATS=./vendor/github.com/elastic/beats To Fetch: govendor fetch github.com/vmware/govmomi/^ +out","title":"Beats"},{"location":"elk/beats/#beats","text":"","title":"Beats"},{"location":"elk/beats/#creating-custom","text":"Creating New Beat Examples","title":"Creating Custom"},{"location":"elk/beats/#know-errors","text":"bash: mage: command not found","title":"Know Errors"},{"location":"elk/beats/#use-vendoring","text":"We recommend to use vendoring for your beat. This means the dependencies are put into your beat folder. The beats team currently uses govendor for vendoring. govendor init govendor update +e This will create a directory vendor inside your repository. To make sure all dependencies for the Makefile commands are loaded from the vendor directory, find the following line in your Makefile: ES_BEATS=${GOPATH}/src/github.com/elastic/beats Replace it with: ES_BEATS=./vendor/github.com/elastic/beats To Fetch: govendor fetch github.com/vmware/govmomi/^ +out","title":"Use vendoring"},{"location":"elk/kibana/","text":"Time Series Filters performancemanager.virtualmachines.metric.info.metric: \"cpu.usagemhz.average\" AND NOT performancemanager.virtualmachines.metric.sample.instance: \"*\" AND performancemanager.hosts.metric.sample.instance: \"*\" Timelion Expression .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metric.sample.value,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000).label(\"Disk Provisioned [TB]\").color(black).lines(fill=1,width=2).title(\"Capacity Assessment\"), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000000).label(\"Total Capacity [TB]\").color(yellow).lines(fill=2,width=2), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metric.sample.value,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.used.latest\").divide(1000000000).label(\"Disk Used [TB]\").color(green).lines(fill=3,width=1), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000000).multiply(1.1).label(\"Provisioning Threshold [TB]\").color(red).lines(fill=0,width=3),","title":"Kibana"},{"location":"elk/kibana/#time-series","text":"","title":"Time Series"},{"location":"elk/kibana/#filters","text":"performancemanager.virtualmachines.metric.info.metric: \"cpu.usagemhz.average\" AND NOT performancemanager.virtualmachines.metric.sample.instance: \"*\" AND performancemanager.hosts.metric.sample.instance: \"*\"","title":"Filters"},{"location":"elk/kibana/#timelion","text":"","title":"Timelion"},{"location":"elk/kibana/#expression","text":".es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metric.sample.value,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000).label(\"Disk Provisioned [TB]\").color(black).lines(fill=1,width=2).title(\"Capacity Assessment\"), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000000).label(\"Total Capacity [TB]\").color(yellow).lines(fill=2,width=2), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metric.sample.value,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.used.latest\").divide(1000000000).label(\"Disk Used [TB]\").color(green).lines(fill=3,width=1), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000000).multiply(1.1).label(\"Provisioning Threshold [TB]\").color(red).lines(fill=0,width=3),","title":"Expression"},{"location":"elk/queries/","text":"Cluster Health GET /_cluster/health Allocation Errors Explain GET /_cluster/allocation/explain Nodes Stats GET /_nodes/stats Indices List GET /_cat/indices?v List with selected Column GET /_cat/indices?h=creation.date.string Create POST /{index}/_open Close POST /_all/_close Search All GET /vspherebeat/_search { \"query\": { \"match_all\": {} } } Field Match string GET /vspherebeat/_search { \"query\": { \"match\" : { \"performancemanager.hosts.metaData.name\": \"nsvwsdv001.mngt.local\" } } } Field doesn't match string GET /vspherebeat/_search { \"query\": { \"bool\": { \"must_not\": [ { \"match\": { \"performancemanager.virtualmachines.metaData.name\": \"WSTPMNGT007\" } } ] } } } Bool Query (Match and Not Match) GET /vspherebeat/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"performancemanager.virtualmachines.metaData.name\": \"APM-Server\" } }, { \"match\": { \"performancemanager.virtualmachines.metric.info.metric\": \"cpu.usage.average\" } } ], \"must_not\": [ { \"match\": { \"performancemanager.virtualmachines.metric.sample.instance\": \"*\" } } ] } } } Exists Specified Field GET /vspherebeat/_search { \"query\": { \"exists\": { \"field\": \"performancemanager.hosts\" } } } Unique Values from a field GET vspherebeat/_search { \"size\":\"0\", \"aggs\" : { \"uniq_hotsr\" : { \"terms\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } } Total of unique values from a field GET /vspherebeat/_search { \"size\" : 0, \"aggs\" : { \"distinct_hots\" : { \"cardinality\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } } Help Get Indices Columns GET /_cat/indices?help","title":"ESQueries"},{"location":"elk/queries/#cluster","text":"","title":"Cluster"},{"location":"elk/queries/#health","text":"GET /_cluster/health","title":"Health"},{"location":"elk/queries/#allocation-errors-explain","text":"GET /_cluster/allocation/explain","title":"Allocation Errors Explain"},{"location":"elk/queries/#nodes","text":"","title":"Nodes"},{"location":"elk/queries/#stats","text":"GET /_nodes/stats","title":"Stats"},{"location":"elk/queries/#indices","text":"","title":"Indices"},{"location":"elk/queries/#list","text":"GET /_cat/indices?v","title":"List"},{"location":"elk/queries/#list-with-selected-column","text":"GET /_cat/indices?h=creation.date.string","title":"List with selected Column"},{"location":"elk/queries/#create","text":"POST /{index}/_open","title":"Create"},{"location":"elk/queries/#close","text":"POST /_all/_close","title":"Close"},{"location":"elk/queries/#search","text":"","title":"Search"},{"location":"elk/queries/#all","text":"GET /vspherebeat/_search { \"query\": { \"match_all\": {} } }","title":"All"},{"location":"elk/queries/#field-match-string","text":"GET /vspherebeat/_search { \"query\": { \"match\" : { \"performancemanager.hosts.metaData.name\": \"nsvwsdv001.mngt.local\" } } }","title":"Field Match string"},{"location":"elk/queries/#field-doesnt-match-string","text":"GET /vspherebeat/_search { \"query\": { \"bool\": { \"must_not\": [ { \"match\": { \"performancemanager.virtualmachines.metaData.name\": \"WSTPMNGT007\" } } ] } } }","title":"Field doesn't match string"},{"location":"elk/queries/#bool-query-match-and-not-match","text":"GET /vspherebeat/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"performancemanager.virtualmachines.metaData.name\": \"APM-Server\" } }, { \"match\": { \"performancemanager.virtualmachines.metric.info.metric\": \"cpu.usage.average\" } } ], \"must_not\": [ { \"match\": { \"performancemanager.virtualmachines.metric.sample.instance\": \"*\" } } ] } } }","title":"Bool Query (Match and Not Match)"},{"location":"elk/queries/#exists-specified-field","text":"GET /vspherebeat/_search { \"query\": { \"exists\": { \"field\": \"performancemanager.hosts\" } } }","title":"Exists Specified Field"},{"location":"elk/queries/#unique-values-from-a-field","text":"GET vspherebeat/_search { \"size\":\"0\", \"aggs\" : { \"uniq_hotsr\" : { \"terms\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Unique Values from a field"},{"location":"elk/queries/#total-of-unique-values-from-a-field","text":"GET /vspherebeat/_search { \"size\" : 0, \"aggs\" : { \"distinct_hots\" : { \"cardinality\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Total of unique values from a field"},{"location":"elk/queries/#help","text":"","title":"Help"},{"location":"elk/queries/#get-indices-columns","text":"GET /_cat/indices?help","title":"Get Indices Columns"},{"location":"elk/snapshotsAndRestore/","text":"Snapshot Config Change parameter in elastic search config file for all nodes The path.repo needs to be a shared folder beetween the cluster path.repo: [\"/usr/share/elasticsearch/snapshots\"] Create PUT /_snapshot/{repository}/{snapshot}?wait_for_completion=true List all Snapshots from a repo GET /_cat/snapshots/{repository}?v&s=id Delete DELETE /_snapshot/{repository}/{snapshot} Repositories Create PUT /_snapshot/my_backup { \"type\": \"fs\", \"settings\": { \"location\": \"/usr/share/elasticsearch/snapshots/backup\" } } List GET /_cat/repositories?v GET /_snapshot/_all Delete DELETE /_snapshot/{repository} Restore Restore - Official Doc Restore From Snapshot POST /_snapshot/{repository}/{snapshot}/_restore","title":"Snapshots&Restore"},{"location":"elk/snapshotsAndRestore/#snapshot","text":"","title":"Snapshot"},{"location":"elk/snapshotsAndRestore/#config","text":"Change parameter in elastic search config file for all nodes The path.repo needs to be a shared folder beetween the cluster path.repo: [\"/usr/share/elasticsearch/snapshots\"]","title":"Config"},{"location":"elk/snapshotsAndRestore/#create","text":"PUT /_snapshot/{repository}/{snapshot}?wait_for_completion=true","title":"Create"},{"location":"elk/snapshotsAndRestore/#list-all-snapshots-from-a-repo","text":"GET /_cat/snapshots/{repository}?v&s=id","title":"List all Snapshots from a repo"},{"location":"elk/snapshotsAndRestore/#delete","text":"DELETE /_snapshot/{repository}/{snapshot}","title":"Delete"},{"location":"elk/snapshotsAndRestore/#repositories","text":"","title":"Repositories"},{"location":"elk/snapshotsAndRestore/#create_1","text":"PUT /_snapshot/my_backup { \"type\": \"fs\", \"settings\": { \"location\": \"/usr/share/elasticsearch/snapshots/backup\" } }","title":"Create"},{"location":"elk/snapshotsAndRestore/#list","text":"GET /_cat/repositories?v GET /_snapshot/_all","title":"List"},{"location":"elk/snapshotsAndRestore/#delete_1","text":"DELETE /_snapshot/{repository}","title":"Delete"},{"location":"elk/snapshotsAndRestore/#restore","text":"Restore - Official Doc","title":"Restore"},{"location":"elk/snapshotsAndRestore/#restore-from-snapshot","text":"POST /_snapshot/{repository}/{snapshot}/_restore","title":"Restore From Snapshot"},{"location":"unix/commands/disks/","text":"Disks Lists block devices lsblk fdisk -l Create ext4 Filesystem Can use parted command # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 88.5M 1 loop /snap/core/7270 loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1455 xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 8G 0 disk # fdisk /dev/xvdf Command (m for help): n => Create Partition Partition type: p primary (0 primary, 0 extended, 4 free) e extended Command (m for help): p => Print partition Command (m for help): w => Write # mkfs -L projectA -t ext4 /dev/xvdf1 # Format Partition and create label # mkdir /mnt/ProjectA # mount /dev/xvdf1 /mnt/ProjectA/ # vi /etc/fstab LABEL=projectA /mnt/ProjectA/ ext4 defaults 0 0 # Add This Line umount /mnt/ProjectA/ Create Swap Partition Can use parted command # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 1 Hex code (type L to list all codes): 82 Command (m for help): w # mkswap /dev/xvdf1 # swapon /dev/xvdf1 # to shutoff -> swapoff /dev/xvdf2 # free -m # vi /etc/fstab UUID=99fd52d5-e821-45a5-9366-30666046406f swap swap default 0 0 Create LVM Filesystem # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 2 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Syncing disks. # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 8e Linux LVM /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 5 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Syncing disks. # pvcreate /dev/xvdf2 Physical volume \"/dev/xvdf2\" successfully created. # pvcreate /dev/xvdf5 Physical volume \"/dev/xvdf5\" successfully created. # vgcreate VG1 /dev/xvdf2 /dev/xvdf5 Volume group \"VG1\" successfully created root@ip-172-31-81-196:~# vgdisplay /dev/VG1 --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 3.99 GiB PE Size 4.00 MiB Total PE 1022 Alloc PE / Size 0 / 0 Free PE / Size 1022 / 3.99 GiB VG UUID ELopyA-2vai-XuiQ-vm7M-RF9q-k0ve-yUeXM6 # lvcreate VG1 -L +3.9G -n LV1 Rounding up size to full physical extent 3.90 GiB Logical volume \"LV1\" created. root@ip-172-31-81-196:~# lvdisplay --- Logical volume --- LV Path /dev/VG1/LV1 LV Name LV1 VG Name VG1 LV UUID Q0mXgw-q9E7-ncuH-Vv2t-NTqx-epn9-3EK3DH LV Write Access read/write LV Creation host, time ip-172-31-81-196, 2019-08-02 21:45:49 +0000 LV Status available # open 0 LV Size 3.90 GiB Current LE 999 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 # mkfs.ext4 /dev/VG1/LV1 mke2fs 1.44.1 (24-Mar-2018) Creating filesystem with 1022976 4k blocks and 256000 inodes Filesystem UUID: 3cc6d97e-a315-4051-a572-16e06bf9c9f9 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done root@ip-172-31-81-196:~# mkdir /mnt/LV1-mount root@ip-172-31-81-196:~# mount /dev/VG1/LV1 /mnt/LV1-mount root@ip-172-31-81-196:~# vi /etc/fstab /dev/VG1/LV1 /mnt/LV1-mount ext4 defaults 0 0 # Add This Line # Can edit first the /etc/fsatab file and after do mount -a (mount everything in /etc/fstab file) instead mount command Share Disk Install NFS yum install -y nfs-utils systemctl enable nfs systemctl start nfs Master Configure Export /snapshots is a filestem from nfs type # vi /etc/exports /snapshots *(rw) => Add Line # exportfs /snapshots <world> Enable Service firewall-cmd --add-service nfs --permanent firewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-service=mountd firewall-cmd --permanent --add-port=2049/tcp firewall-cmd --permanent --add-port=2049/udp firewall-cmd --reload Slaves # vi /etc/fstab 10.240.100.18:/snapshots /elastic/snapshots nfs _netdev,rw 0 0 # mount -a # mount | grep elastic","title":"Disks"},{"location":"unix/commands/disks/#disks","text":"Lists block devices lsblk fdisk -l","title":"Disks"},{"location":"unix/commands/disks/#create-ext4-filesystem","text":"Can use parted command # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 88.5M 1 loop /snap/core/7270 loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1455 xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 8G 0 disk # fdisk /dev/xvdf Command (m for help): n => Create Partition Partition type: p primary (0 primary, 0 extended, 4 free) e extended Command (m for help): p => Print partition Command (m for help): w => Write # mkfs -L projectA -t ext4 /dev/xvdf1 # Format Partition and create label # mkdir /mnt/ProjectA # mount /dev/xvdf1 /mnt/ProjectA/ # vi /etc/fstab LABEL=projectA /mnt/ProjectA/ ext4 defaults 0 0 # Add This Line umount /mnt/ProjectA/","title":"Create ext4 Filesystem"},{"location":"unix/commands/disks/#create-swap-partition","text":"Can use parted command # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 1 Hex code (type L to list all codes): 82 Command (m for help): w # mkswap /dev/xvdf1 # swapon /dev/xvdf1 # to shutoff -> swapoff /dev/xvdf2 # free -m # vi /etc/fstab UUID=99fd52d5-e821-45a5-9366-30666046406f swap swap default 0 0","title":"Create Swap Partition"},{"location":"unix/commands/disks/#create-lvm-filesystem","text":"# fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 2 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Syncing disks. # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 8e Linux LVM /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 5 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Syncing disks. # pvcreate /dev/xvdf2 Physical volume \"/dev/xvdf2\" successfully created. # pvcreate /dev/xvdf5 Physical volume \"/dev/xvdf5\" successfully created. # vgcreate VG1 /dev/xvdf2 /dev/xvdf5 Volume group \"VG1\" successfully created root@ip-172-31-81-196:~# vgdisplay /dev/VG1 --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 3.99 GiB PE Size 4.00 MiB Total PE 1022 Alloc PE / Size 0 / 0 Free PE / Size 1022 / 3.99 GiB VG UUID ELopyA-2vai-XuiQ-vm7M-RF9q-k0ve-yUeXM6 # lvcreate VG1 -L +3.9G -n LV1 Rounding up size to full physical extent 3.90 GiB Logical volume \"LV1\" created. root@ip-172-31-81-196:~# lvdisplay --- Logical volume --- LV Path /dev/VG1/LV1 LV Name LV1 VG Name VG1 LV UUID Q0mXgw-q9E7-ncuH-Vv2t-NTqx-epn9-3EK3DH LV Write Access read/write LV Creation host, time ip-172-31-81-196, 2019-08-02 21:45:49 +0000 LV Status available # open 0 LV Size 3.90 GiB Current LE 999 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 # mkfs.ext4 /dev/VG1/LV1 mke2fs 1.44.1 (24-Mar-2018) Creating filesystem with 1022976 4k blocks and 256000 inodes Filesystem UUID: 3cc6d97e-a315-4051-a572-16e06bf9c9f9 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done root@ip-172-31-81-196:~# mkdir /mnt/LV1-mount root@ip-172-31-81-196:~# mount /dev/VG1/LV1 /mnt/LV1-mount root@ip-172-31-81-196:~# vi /etc/fstab /dev/VG1/LV1 /mnt/LV1-mount ext4 defaults 0 0 # Add This Line # Can edit first the /etc/fsatab file and after do mount -a (mount everything in /etc/fstab file) instead mount command","title":"Create LVM Filesystem"},{"location":"unix/commands/disks/#share-disk","text":"","title":"Share Disk"},{"location":"unix/commands/disks/#install-nfs","text":"yum install -y nfs-utils systemctl enable nfs systemctl start nfs","title":"Install NFS"},{"location":"unix/commands/disks/#master","text":"","title":"Master"},{"location":"unix/commands/disks/#configure-export","text":"/snapshots is a filestem from nfs type # vi /etc/exports /snapshots *(rw) => Add Line # exportfs /snapshots <world>","title":"Configure Export"},{"location":"unix/commands/disks/#enable-service","text":"firewall-cmd --add-service nfs --permanent firewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-service=mountd firewall-cmd --permanent --add-port=2049/tcp firewall-cmd --permanent --add-port=2049/udp firewall-cmd --reload","title":"Enable Service"},{"location":"unix/commands/disks/#slaves","text":"# vi /etc/fstab 10.240.100.18:/snapshots /elastic/snapshots nfs _netdev,rw 0 0 # mount -a # mount | grep elastic","title":"Slaves"},{"location":"unix/commands/filesystem/","text":"FileSystem Dir Function Extras /dev (udev) Device manager. Contain device files Config in /etc/udev /sys (sysfs) Virtual file system. Info about Hardware devices, drivers /proc (procfs)Similiar to sysfs, but with info about processes and system info Can be used to interface with the kernel. Change parameters on the fly File System space usage df -h Commands Dirs Create Multiple Dirs mkdir -p {networking,compute,storage} Multiple Files in Multiple Dirs touch {networking,compute,storage}/{main.tf,variables.tf,outputs.tf} Find sudo find / -type f -name .gitconfig Inodes Get File Inode ls -i /etc/passwd Find File by Inode find / -inum 55116 /etc/passwd Get Inodes Available df -i Disk Space du -h du -s # To sum the total du -s / du -h --max-depth=1 # Show the disk usage by folder, and not the sub dirs because of Depth = 1 Filesystem fsck (filesystem check) To check errors from filesystem Don't do in mounted filesystem sudo fsck /dev/sdb1 sudo umount /dev/sdb1 dumpe2fs Get info about filesystem sudo dumpe2fs -h /dev/sdb1 tune2fs To get info and tune filesystem tune2fs -l /dev/xvda1 Set Volume Name tune2fs -L Photos /dev/xvda1 Set mount counts will be filesystem checked tune2fs -c 10 /dev/xvda1 fuser Show which process is using the directory # fuser /mount/Photos /mount/Photos: 3157c # ps aux | grep 3157 -- Will see the process, probably /bin/bash # ps auxf -> Show which command executed and the login session called Disk Quotas # vi /etc/fstab LABEL=projectA /mnt/ProjectA/ ext4 defaults,usrquota 0 0 # mount -a # apt-get install quota # cd /mnt/ProjectA # quotacheck -avugc # Create filesystem to support quotas quotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. quotacheck: Scanning /dev/xvdf7 [/mnt/ProjectA] done quotacheck: Cannot stat old user quota file /mnt/ProjectA/aquota.user: No such file or directory. Usage will not be subtracted. quotacheck: Old group file name could not been determined. Usage will not be subtracted. quotacheck: Checked 3 directories and 0 files quotacheck: Old file not found. # edquota -u fsantos # To edit quotas to user # quotaon /mnt/Photos # Enable quotas # quota -v # To see quotas usage # sudo repquota /mnt/Photos # To see a resume from users Tar # List files without extract tar -tvf etcd-v3.3.13-linux-amd64.tar.gz # Extract only one file tar -xvf etcd-v3.3.13-linux-amd64.tar.gz etcd-v3.3.13-linux-amd64/etcdctl","title":"Filesystem"},{"location":"unix/commands/filesystem/#filesystem","text":"Dir Function Extras /dev (udev) Device manager. Contain device files Config in /etc/udev /sys (sysfs) Virtual file system. Info about Hardware devices, drivers /proc (procfs)Similiar to sysfs, but with info about processes and system info Can be used to interface with the kernel. Change parameters on the fly File System space usage df -h","title":"FileSystem"},{"location":"unix/commands/filesystem/#commands","text":"","title":"Commands"},{"location":"unix/commands/filesystem/#dirs","text":"","title":"Dirs"},{"location":"unix/commands/filesystem/#create","text":"Multiple Dirs mkdir -p {networking,compute,storage} Multiple Files in Multiple Dirs touch {networking,compute,storage}/{main.tf,variables.tf,outputs.tf}","title":"Create"},{"location":"unix/commands/filesystem/#find","text":"sudo find / -type f -name .gitconfig","title":"Find"},{"location":"unix/commands/filesystem/#inodes","text":"Get File Inode ls -i /etc/passwd Find File by Inode find / -inum 55116 /etc/passwd Get Inodes Available df -i","title":"Inodes"},{"location":"unix/commands/filesystem/#disk-space","text":"du -h du -s # To sum the total du -s / du -h --max-depth=1 # Show the disk usage by folder, and not the sub dirs because of Depth = 1","title":"Disk Space"},{"location":"unix/commands/filesystem/#filesystem_1","text":"","title":"Filesystem"},{"location":"unix/commands/filesystem/#fsck-filesystem-check","text":"To check errors from filesystem Don't do in mounted filesystem sudo fsck /dev/sdb1 sudo umount /dev/sdb1","title":"fsck (filesystem check)"},{"location":"unix/commands/filesystem/#dumpe2fs","text":"Get info about filesystem sudo dumpe2fs -h /dev/sdb1","title":"dumpe2fs"},{"location":"unix/commands/filesystem/#tune2fs","text":"To get info and tune filesystem tune2fs -l /dev/xvda1 Set Volume Name tune2fs -L Photos /dev/xvda1 Set mount counts will be filesystem checked tune2fs -c 10 /dev/xvda1","title":"tune2fs"},{"location":"unix/commands/filesystem/#fuser","text":"Show which process is using the directory # fuser /mount/Photos /mount/Photos: 3157c # ps aux | grep 3157 -- Will see the process, probably /bin/bash # ps auxf -> Show which command executed and the login session called","title":"fuser"},{"location":"unix/commands/filesystem/#disk-quotas","text":"# vi /etc/fstab LABEL=projectA /mnt/ProjectA/ ext4 defaults,usrquota 0 0 # mount -a # apt-get install quota # cd /mnt/ProjectA # quotacheck -avugc # Create filesystem to support quotas quotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. quotacheck: Scanning /dev/xvdf7 [/mnt/ProjectA] done quotacheck: Cannot stat old user quota file /mnt/ProjectA/aquota.user: No such file or directory. Usage will not be subtracted. quotacheck: Old group file name could not been determined. Usage will not be subtracted. quotacheck: Checked 3 directories and 0 files quotacheck: Old file not found. # edquota -u fsantos # To edit quotas to user # quotaon /mnt/Photos # Enable quotas # quota -v # To see quotas usage # sudo repquota /mnt/Photos # To see a resume from users","title":"Disk Quotas"},{"location":"unix/commands/filesystem/#tar","text":"# List files without extract tar -tvf etcd-v3.3.13-linux-amd64.tar.gz # Extract only one file tar -xvf etcd-v3.3.13-linux-amd64.tar.gz etcd-v3.3.13-linux-amd64/etcdctl","title":"Tar"},{"location":"unix/commands/kernel/","text":"Kernel Modules # To view who use the modules lsmod # To remove sudo rmmod video # To enable sudo modprobe video Libs Shared # Get Shared libs location cat /etc/ld.so.conf # To load new lib ldconfig # Change Library Path temporary export LD_LIBRARY_PATH=/home/nick/lib # Print shared libs from app ldd /bin/ls Hardware PCI Devices # To list pci Devices lspci lspci -v lspci -vvv","title":"Kernel"},{"location":"unix/commands/kernel/#kernel-modules","text":"# To view who use the modules lsmod # To remove sudo rmmod video # To enable sudo modprobe video","title":"Kernel Modules"},{"location":"unix/commands/kernel/#libs","text":"","title":"Libs"},{"location":"unix/commands/kernel/#shared","text":"# Get Shared libs location cat /etc/ld.so.conf # To load new lib ldconfig # Change Library Path temporary export LD_LIBRARY_PATH=/home/nick/lib # Print shared libs from app ldd /bin/ls","title":"Shared"},{"location":"unix/commands/kernel/#hardware","text":"","title":"Hardware"},{"location":"unix/commands/kernel/#pci-devices","text":"# To list pci Devices lspci lspci -v lspci -vvv","title":"PCI Devices"},{"location":"unix/commands/logging/","text":"Syslog Config Files cd /etc/rsyslog.d/ Rotate Files Conf vi /etc/logrotate.conf Journalctl **Log For Services journalctl # All journalctl -b # Last Boot journalctl -b 1 # Before Last Boot journalctl --since \"2 days ago\" journalctl -u kubelet.service vi /etc/system.d/journald.conf # Config Files","title":"Logging"},{"location":"unix/commands/logging/#syslog","text":"","title":"Syslog"},{"location":"unix/commands/logging/#config-files","text":"cd /etc/rsyslog.d/","title":"Config Files"},{"location":"unix/commands/logging/#rotate-files-conf","text":"vi /etc/logrotate.conf","title":"Rotate Files Conf"},{"location":"unix/commands/logging/#journalctl","text":"**Log For Services journalctl # All journalctl -b # Last Boot journalctl -b 1 # Before Last Boot journalctl --since \"2 days ago\" journalctl -u kubelet.service vi /etc/system.d/journald.conf # Config Files","title":"Journalctl"},{"location":"unix/commands/networking/","text":"Config ifconfig ip addr ifconfig ifconfig -a ifdown enp0s3 ifup enp0s3 ifconfig enp0s3 192.168.0.150/24 # Not Permanent Add Secondary IP Link ifconfig eth0:1 172.31.81.196 netmask 255.255.240.0 up # Not Permanent ip ip a ip addr show Remove Secondary Ip ip addr del 172.31.81.196/20 dev eth0:1 # Not Permanent Network-Manager sudo apt install network-manager sudo service network-manager restart nmcli nmcli connection # Connection information # Add Static Route sudo nmcli connection add con-name STATIC ipv4.addresses 192.168.58.1/24 ifname eth0 type ethernet nmcli connection show STATIC sudo nmcli connection modify STATIC +ipv4.routes \"172.16.0.0/16 192.168.58.254\" ipv4.dns 172.16.58.254 route route route -n route add default gw 192.168.0.254 Dns cat /etc/hosts cat /etc/nsswitch.conf # show how priority to will resolve dns cat /etc/resolv.conf cat /etc/dhcp/dhclient.conf sudo service networking restart File cat /etc/network/interfaces Connectivity Netstat $ sudo netstat -nl -p tcp | grep 8123 $ sudo netstat -nl -p tcp | head Get open ports nmap localhost netstat -at SS ss -an | grep -i listen Who listen on Port sudo lsof -i :8000 Test Remote Connection to port nc -v 10.240.100.18 2049 netcat -l 12345 # To listen a Port Get Ports than services are listen cat /etc/services MTR mtr acloud.com DNS host google.com dig google.com","title":"Networking"},{"location":"unix/commands/networking/#config","text":"","title":"Config"},{"location":"unix/commands/networking/#ifconfig","text":"ip addr ifconfig ifconfig -a ifdown enp0s3 ifup enp0s3 ifconfig enp0s3 192.168.0.150/24 # Not Permanent","title":"ifconfig"},{"location":"unix/commands/networking/#add-secondary-ip","text":"Link ifconfig eth0:1 172.31.81.196 netmask 255.255.240.0 up # Not Permanent","title":"Add Secondary IP"},{"location":"unix/commands/networking/#ip","text":"ip a ip addr show","title":"ip"},{"location":"unix/commands/networking/#remove-secondary-ip","text":"ip addr del 172.31.81.196/20 dev eth0:1 # Not Permanent","title":"Remove Secondary Ip"},{"location":"unix/commands/networking/#network-manager","text":"sudo apt install network-manager sudo service network-manager restart nmcli nmcli connection # Connection information # Add Static Route sudo nmcli connection add con-name STATIC ipv4.addresses 192.168.58.1/24 ifname eth0 type ethernet nmcli connection show STATIC sudo nmcli connection modify STATIC +ipv4.routes \"172.16.0.0/16 192.168.58.254\" ipv4.dns 172.16.58.254","title":"Network-Manager"},{"location":"unix/commands/networking/#route","text":"route route -n route add default gw 192.168.0.254","title":"route"},{"location":"unix/commands/networking/#dns","text":"cat /etc/hosts cat /etc/nsswitch.conf # show how priority to will resolve dns cat /etc/resolv.conf cat /etc/dhcp/dhclient.conf sudo service networking restart","title":"Dns"},{"location":"unix/commands/networking/#file","text":"cat /etc/network/interfaces","title":"File"},{"location":"unix/commands/networking/#connectivity","text":"","title":"Connectivity"},{"location":"unix/commands/networking/#netstat","text":"$ sudo netstat -nl -p tcp | grep 8123 $ sudo netstat -nl -p tcp | head","title":"Netstat"},{"location":"unix/commands/networking/#get-open-ports","text":"nmap localhost netstat -at","title":"Get open ports"},{"location":"unix/commands/networking/#ss","text":"ss -an | grep -i listen","title":"SS"},{"location":"unix/commands/networking/#who-listen-on-port","text":"sudo lsof -i :8000","title":"Who listen on Port"},{"location":"unix/commands/networking/#test-remote-connection-to-port","text":"nc -v 10.240.100.18 2049 netcat -l 12345 # To listen a Port","title":"Test Remote Connection to port"},{"location":"unix/commands/networking/#get-ports-than-services-are-listen","text":"cat /etc/services","title":"Get Ports than services are listen"},{"location":"unix/commands/networking/#mtr","text":"mtr acloud.com","title":"MTR"},{"location":"unix/commands/networking/#dns_1","text":"host google.com dig google.com","title":"DNS"},{"location":"unix/commands/security/","text":"Security Host cat /etc/hosts.allow # Hosts allowed to access cat /etc/hosts.deny # Hosts not allowed to access Nologin touch /etc/nologin # Denies login to all users (except root). Need remove the file SE Linux Install Ubuntu # Need remove apparmor in ubuntu systems # make sure you have the most up-to-date info apt-get update apt-get dist-upgrade #disable and remove apparmor /etc/init.d/apparmor stop apt-get remove apparmor #install SELinux apt-get install selinux # install the missing dependency apt-get install auditd # install the activate tool required to make it work apt-get install selinux-basics #missing manual step to actually make SELinux work (part of selinux-basics) selinux-activate Config # Config file cat /etc/selinux/config getenforce setenforce 1 sestatus ls -Z /etc/shadow ps -Z Contexts # List semanage fcontext -l semanage fcontext -l | grep httpd_sys_content_t # Change Context Type chcon -t user_home_dir_t /etc/shadow Booleans getsebool -a semanage boolean -l | sort | less semanage boolean -m -1 httpd_enable_homedirs # or setsebool -P httpd_enable_homedirs=1 Troubleshooting grep http /var/log/audit/audit.log","title":"Security"},{"location":"unix/commands/security/#security","text":"","title":"Security"},{"location":"unix/commands/security/#host","text":"cat /etc/hosts.allow # Hosts allowed to access cat /etc/hosts.deny # Hosts not allowed to access","title":"Host"},{"location":"unix/commands/security/#nologin","text":"touch /etc/nologin # Denies login to all users (except root). Need remove the file","title":"Nologin"},{"location":"unix/commands/security/#se-linux","text":"","title":"SE Linux"},{"location":"unix/commands/security/#install-ubuntu","text":"# Need remove apparmor in ubuntu systems # make sure you have the most up-to-date info apt-get update apt-get dist-upgrade #disable and remove apparmor /etc/init.d/apparmor stop apt-get remove apparmor #install SELinux apt-get install selinux # install the missing dependency apt-get install auditd # install the activate tool required to make it work apt-get install selinux-basics #missing manual step to actually make SELinux work (part of selinux-basics) selinux-activate","title":"Install Ubuntu"},{"location":"unix/commands/security/#config","text":"# Config file cat /etc/selinux/config getenforce setenforce 1 sestatus ls -Z /etc/shadow ps -Z","title":"Config"},{"location":"unix/commands/security/#contexts","text":"# List semanage fcontext -l semanage fcontext -l | grep httpd_sys_content_t # Change Context Type chcon -t user_home_dir_t /etc/shadow","title":"Contexts"},{"location":"unix/commands/security/#booleans","text":"getsebool -a semanage boolean -l | sort | less semanage boolean -m -1 httpd_enable_homedirs # or setsebool -P httpd_enable_homedirs=1","title":"Booleans"},{"location":"unix/commands/security/#troubleshooting","text":"grep http /var/log/audit/audit.log","title":"Troubleshooting"},{"location":"unix/os/archlinux/","text":"Packages Packages Update Package sudo pacman -U <link - see above> Update corrupted or invalid database pacman-key --delete 91FFE0700E80619CEB73235CA88E23E377514E00 pacman-key --populate archlinux Resolve ICU Package Problem Link Download and Install old \"icu\" package; wget https://archive.archlinux.org/packages/i/icu/icu-62.1-1-x86_64.pkg.tar.xz sudo pacman -U icu-62.1-1-x86_64.pkg.tar.xz Copy all \"icu\" files to a backup directory; sudo mkdir /usr/lib/backup sudo cp -r /usr/lib/libicu* /usr/lib/backup/ Install new version of \"icu\" again; sudo pacman -U /var/cache/pacman/pkg/icu-62.1-1-x86_64.pkg.tar.xz Copy this three files back to /var/lib/ direcotry; sudo cp /var/lib/libicui18n.so.61 /usr/lib/ sudo cp /var/lib/libicuuc.so.61 /usr/lib/ sudo cp /var/lib/libicudata.so.61 /usr/lib/ You can now cleanup backup files; sudo rm -rf /var/lib/backup","title":"Archlinux"},{"location":"unix/os/archlinux/#packages","text":"Packages","title":"Packages"},{"location":"unix/os/archlinux/#update-package","text":"sudo pacman -U <link - see above>","title":"Update Package"},{"location":"unix/os/archlinux/#update-corrupted-or-invalid-database","text":"pacman-key --delete 91FFE0700E80619CEB73235CA88E23E377514E00 pacman-key --populate archlinux","title":"Update corrupted or invalid database"},{"location":"unix/os/archlinux/#resolve-icu-package-problem","text":"Link Download and Install old \"icu\" package; wget https://archive.archlinux.org/packages/i/icu/icu-62.1-1-x86_64.pkg.tar.xz sudo pacman -U icu-62.1-1-x86_64.pkg.tar.xz Copy all \"icu\" files to a backup directory; sudo mkdir /usr/lib/backup sudo cp -r /usr/lib/libicu* /usr/lib/backup/ Install new version of \"icu\" again; sudo pacman -U /var/cache/pacman/pkg/icu-62.1-1-x86_64.pkg.tar.xz Copy this three files back to /var/lib/ direcotry; sudo cp /var/lib/libicui18n.so.61 /usr/lib/ sudo cp /var/lib/libicuuc.so.61 /usr/lib/ sudo cp /var/lib/libicudata.so.61 /usr/lib/ You can now cleanup backup files; sudo rm -rf /var/lib/backup","title":"Resolve ICU Package Problem"},{"location":"unix/os/debian/","text":"Know Errors (Ubuntu) Unable to lock the administration directory Desc: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? Solution sudo rm /var/lib/apt/lists/lock dpkg # To list all installed packages dpkg -l | less # To install package sudo dpkg -i dlocate_1.02+nmu3_all.deb # To Remove sudo dpkg --purge dlocate Apt # Sources file cat /etc/apt/sources.list sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade # Better # To Get dependences apt-cache depends apache2 | less","title":"Debian"},{"location":"unix/os/debian/#know-errors-ubuntu","text":"","title":"Know Errors (Ubuntu)"},{"location":"unix/os/debian/#unable-to-lock-the-administration-directory","text":"Desc: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? Solution sudo rm /var/lib/apt/lists/lock","title":"Unable to lock the administration directory"},{"location":"unix/os/debian/#dpkg","text":"# To list all installed packages dpkg -l | less # To install package sudo dpkg -i dlocate_1.02+nmu3_all.deb # To Remove sudo dpkg --purge dlocate","title":"dpkg"},{"location":"unix/os/debian/#apt","text":"# Sources file cat /etc/apt/sources.list sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade # Better # To Get dependences apt-cache depends apache2 | less","title":"Apt"},{"location":"unix/os/redhat/","text":"Recover Root Password Na console send ctrl-alt-del press \"e\" => TO Edit Na linha linux16 -- No final da linha adicionar rd.break press \"ctrl-x\" mount -o remount,rw /sysroot cd /sysroot chroot . touch .autorelabel passwd teste123ibm4 ctrl d ctrl d Permit SSH Root Login vi /etc/ssh/sshd_config # Comment PasswordAuthentication no # Descomment PasswordAuthentication yes systemctl restart sshd Configure Network Interface vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR=172.16.1.180 GATEWAY=172.16.1.16 DNS1=172.16.1.112 DNS2=172.16.1.119 systemctl restart network Package Manager RPM sudo rpm -i wget-xxxxxx.rpm # To see which lib file belongs rpm -qf /etc/protocols # To see if some file is missing from lib sudo rpm --verify setup sudo rpm -Va # To verify entire machine YUM # Config file cat /etc/yum.conf # Repos dir cd /etc/yum.repos.d sudo yum update # To download but not install sudo yum install --downloadonly --downloaddir=/tmp wget sudo yum remove wget","title":"Redhat"},{"location":"unix/os/redhat/#recover-root-password","text":"Na console send ctrl-alt-del press \"e\" => TO Edit Na linha linux16 -- No final da linha adicionar rd.break press \"ctrl-x\" mount -o remount,rw /sysroot cd /sysroot chroot . touch .autorelabel passwd teste123ibm4 ctrl d ctrl d","title":"Recover Root Password"},{"location":"unix/os/redhat/#permit-ssh-root-login","text":"vi /etc/ssh/sshd_config # Comment PasswordAuthentication no # Descomment PasswordAuthentication yes systemctl restart sshd","title":"Permit SSH Root Login"},{"location":"unix/os/redhat/#configure-network-interface","text":"vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR=172.16.1.180 GATEWAY=172.16.1.16 DNS1=172.16.1.112 DNS2=172.16.1.119 systemctl restart network","title":"Configure Network Interface"},{"location":"unix/os/redhat/#package-manager","text":"","title":"Package Manager"},{"location":"unix/os/redhat/#rpm","text":"sudo rpm -i wget-xxxxxx.rpm # To see which lib file belongs rpm -qf /etc/protocols # To see if some file is missing from lib sudo rpm --verify setup sudo rpm -Va # To verify entire machine","title":"RPM"},{"location":"unix/os/redhat/#yum","text":"# Config file cat /etc/yum.conf # Repos dir cd /etc/yum.repos.d sudo yum update # To download but not install sudo yum install --downloadonly --downloaddir=/tmp wget sudo yum remove wget","title":"YUM"},{"location":"unix/software/mail/postfix/","text":"Install sudo apt-get install postfix mutt Aliases # vi /etc/aliases cloudgurus: root, guru Log cat /var/log/mail.log","title":"Postfix"},{"location":"unix/software/mail/postfix/#install","text":"sudo apt-get install postfix mutt","title":"Install"},{"location":"unix/software/mail/postfix/#aliases","text":"# vi /etc/aliases cloudgurus: root, guru","title":"Aliases"},{"location":"unix/software/mail/postfix/#log","text":"cat /var/log/mail.log","title":"Log"},{"location":"unix/software/programming/php/","text":"Install Lamp Server https://www.howtoforge.com/tutorial/centos-lamp-server-apache-mysql-php/ Ioncube Loader https://www.howtoforge.com/tutorial/how-to-install-ioncube-loader/ Cli Get Modules php -m","title":"PHP"},{"location":"unix/software/programming/php/#install","text":"","title":"Install"},{"location":"unix/software/programming/php/#lamp-server","text":"https://www.howtoforge.com/tutorial/centos-lamp-server-apache-mysql-php/","title":"Lamp Server"},{"location":"unix/software/programming/php/#ioncube-loader","text":"https://www.howtoforge.com/tutorial/how-to-install-ioncube-loader/","title":"Ioncube Loader"},{"location":"unix/software/programming/php/#cli","text":"","title":"Cli"},{"location":"unix/software/programming/php/#get-modules","text":"php -m","title":"Get Modules"},{"location":"unix/software/virtualization/virsh/","text":"VM Snapshot $ ssh -l rgameiro 10.242.12.23 $ virsh list --all Id Name State ---------------------------------------------------- 1 bm02vsc01 running 2 bm02alinternal01 running 3 architect running 4 bm02ifwinternal01 running 5 bm02vsd01 running 6 bm02sfinternal02 running 7 bm02acinternal01 running 8 bm02dirinternal01 running - bm02nes01 shut off $ virsh destroy bm02sfinternal02 $ virsh dumpxml bm02sfinternal02 # Find Disk File <domain type='kvm'> <name>bm02sfinternal02</name> <uuid>af7d246d-4578-4660-96fa-ac40c87c24eb</uuid> <memory unit='KiB'>2097152</memory> <currentMemory unit='KiB'>2097152</currentMemory> <vcpu placement='static'>4</vcpu> <os> <type arch='x86_64' machine='pc-i440fx-rhel7.3.0'>hvm</type> <boot dev='hd'/> </os> <features> <acpi/> <apic/> <vmport state='off'/> </features> <cpu mode='host-model' check='partial'> <model fallback='allow'/> </cpu> <clock offset='utc'> <timer name='rtc' tickpolicy='catchup'/> <timer name='pit' tickpolicy='delay'/> <timer name='hpet' present='no'/> </clock> <on_poweroff>destroy</on_poweroff> <on_reboot>restart</on_reboot> <on_crash>restart</on_crash> <pm> <suspend-to-mem enabled='no'/> <suspend-to-disk enabled='no'/> </pm> <devices> <emulator>/usr/libexec/qemu-kvm</emulator> <disk type='file' device='disk'> <driver name='qemu' type='qcow2' cache='none' io='native'/> <source file='/var/lib/libvirt/images/bm02sfinternal02.qcow2'/> <target dev='vda' bus='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/> </disk> <disk type='file' device='cdrom'> <driver name='qemu' type='raw'/> <target dev='hda' bus='ide'/> <readonly/> <address type='drive' controller='0' bus='0' target='0' unit='0'/> </disk> <controller type='usb' index='0' model='ich9-ehci1'> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x7'/> </controller> <controller type='usb' index='0' model='ich9-uhci1'> <master startport='0'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0' multifunction='on'/> </controller> <controller type='usb' index='0' model='ich9-uhci2'> <master startport='2'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x1'/> </controller> <controller type='usb' index='0' model='ich9-uhci3'> <master startport='4'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x2'/> </controller> <controller type='pci' index='0' model='pci-root'/> <controller type='ide' index='0'> <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/> </controller> <controller type='virtio-serial' index='0'> <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/> </controller> <interface type='bridge'> <mac address='52:54:00:84:7e:d0'/> <source bridge='br100'/> <model type='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/> </interface> <serial type='pty'> <target type='isa-serial' port='0'> <model name='isa-serial'/> </target> </serial> <console type='pty'> <target type='serial' port='0'/> </console> <channel type='unix'> <target type='virtio' name='org.qemu.guest_agent.0'/> <address type='virtio-serial' controller='0' bus='0' port='1'/> </channel> <channel type='spicevmc'> <target type='virtio' name='com.redhat.spice.0'/> <address type='virtio-serial' controller='0' bus='0' port='2'/> </channel> <input type='tablet' bus='usb'> <address type='usb' bus='0' port='1'/> </input> <input type='mouse' bus='ps2'/> <input type='keyboard' bus='ps2'/> <graphics type='spice' autoport='yes'> <listen type='address'/> </graphics> <video> <model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1' primary='yes'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/> </video> <redirdev bus='usb' type='spicevmc'> <address type='usb' bus='0' port='2'/> </redirdev> <redirdev bus='usb' type='spicevmc'> <address type='usb' bus='0' port='3'/> </redirdev> <memballoon model='virtio'> <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/> </memballoon> </devices> </domain> $ ls -la /var/lib/libvirt/images/bm02sfinternal02.qcow2 $ df -B 1g # Ensure we have space to copy disk $ cp /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ bg # Por o Processo em Brackground $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 rw------- 1 root root 17607884800 Oct 30 08:21 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 # Terminou [1]+ Done cp -i /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02* $ virsh start bm02sfinternal02","title":"Virsh"},{"location":"unix/software/virtualization/virsh/#vm","text":"","title":"VM"},{"location":"unix/software/virtualization/virsh/#snapshot","text":"$ ssh -l rgameiro 10.242.12.23 $ virsh list --all Id Name State ---------------------------------------------------- 1 bm02vsc01 running 2 bm02alinternal01 running 3 architect running 4 bm02ifwinternal01 running 5 bm02vsd01 running 6 bm02sfinternal02 running 7 bm02acinternal01 running 8 bm02dirinternal01 running - bm02nes01 shut off $ virsh destroy bm02sfinternal02 $ virsh dumpxml bm02sfinternal02 # Find Disk File <domain type='kvm'> <name>bm02sfinternal02</name> <uuid>af7d246d-4578-4660-96fa-ac40c87c24eb</uuid> <memory unit='KiB'>2097152</memory> <currentMemory unit='KiB'>2097152</currentMemory> <vcpu placement='static'>4</vcpu> <os> <type arch='x86_64' machine='pc-i440fx-rhel7.3.0'>hvm</type> <boot dev='hd'/> </os> <features> <acpi/> <apic/> <vmport state='off'/> </features> <cpu mode='host-model' check='partial'> <model fallback='allow'/> </cpu> <clock offset='utc'> <timer name='rtc' tickpolicy='catchup'/> <timer name='pit' tickpolicy='delay'/> <timer name='hpet' present='no'/> </clock> <on_poweroff>destroy</on_poweroff> <on_reboot>restart</on_reboot> <on_crash>restart</on_crash> <pm> <suspend-to-mem enabled='no'/> <suspend-to-disk enabled='no'/> </pm> <devices> <emulator>/usr/libexec/qemu-kvm</emulator> <disk type='file' device='disk'> <driver name='qemu' type='qcow2' cache='none' io='native'/> <source file='/var/lib/libvirt/images/bm02sfinternal02.qcow2'/> <target dev='vda' bus='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/> </disk> <disk type='file' device='cdrom'> <driver name='qemu' type='raw'/> <target dev='hda' bus='ide'/> <readonly/> <address type='drive' controller='0' bus='0' target='0' unit='0'/> </disk> <controller type='usb' index='0' model='ich9-ehci1'> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x7'/> </controller> <controller type='usb' index='0' model='ich9-uhci1'> <master startport='0'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0' multifunction='on'/> </controller> <controller type='usb' index='0' model='ich9-uhci2'> <master startport='2'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x1'/> </controller> <controller type='usb' index='0' model='ich9-uhci3'> <master startport='4'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x2'/> </controller> <controller type='pci' index='0' model='pci-root'/> <controller type='ide' index='0'> <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/> </controller> <controller type='virtio-serial' index='0'> <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/> </controller> <interface type='bridge'> <mac address='52:54:00:84:7e:d0'/> <source bridge='br100'/> <model type='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/> </interface> <serial type='pty'> <target type='isa-serial' port='0'> <model name='isa-serial'/> </target> </serial> <console type='pty'> <target type='serial' port='0'/> </console> <channel type='unix'> <target type='virtio' name='org.qemu.guest_agent.0'/> <address type='virtio-serial' controller='0' bus='0' port='1'/> </channel> <channel type='spicevmc'> <target type='virtio' name='com.redhat.spice.0'/> <address type='virtio-serial' controller='0' bus='0' port='2'/> </channel> <input type='tablet' bus='usb'> <address type='usb' bus='0' port='1'/> </input> <input type='mouse' bus='ps2'/> <input type='keyboard' bus='ps2'/> <graphics type='spice' autoport='yes'> <listen type='address'/> </graphics> <video> <model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1' primary='yes'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/> </video> <redirdev bus='usb' type='spicevmc'> <address type='usb' bus='0' port='2'/> </redirdev> <redirdev bus='usb' type='spicevmc'> <address type='usb' bus='0' port='3'/> </redirdev> <memballoon model='virtio'> <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/> </memballoon> </devices> </domain> $ ls -la /var/lib/libvirt/images/bm02sfinternal02.qcow2 $ df -B 1g # Ensure we have space to copy disk $ cp /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ bg # Por o Processo em Brackground $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 rw------- 1 root root 17607884800 Oct 30 08:21 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 # Terminou [1]+ Done cp -i /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02* $ virsh start bm02sfinternal02","title":"Snapshot"}]}