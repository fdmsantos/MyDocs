{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deploy Localhost python -m mkdocs serve Github Pages python -m mkdocs gh-deploy URL MyDocs Cheatseets Markdown Markdown","title":"Home"},{"location":"#deploy","text":"","title":"Deploy"},{"location":"#localhost","text":"python -m mkdocs serve","title":"Localhost"},{"location":"#github-pages","text":"python -m mkdocs gh-deploy","title":"Github Pages"},{"location":"#url","text":"MyDocs","title":"URL"},{"location":"#cheatseets","text":"","title":"Cheatseets"},{"location":"#markdown","text":"Markdown","title":"Markdown"},{"location":"aws/applicationIntegration/stepfunctions/","text":"Task Submit Batch Job \"Submit Batch Job\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::batch:submitJob.sync\", \"Parameters\": { \"JobName\": \"BatchJobNotification\", \"JobQueue\": \"<BATCH_QUEUE_ARN>\", \"JobDefinition\": \"<BATCH_JOB_DEFINITION_ARN>\" }, \"Next\": \"Notify Success\", \"Catch\": [ { \"ErrorEquals\": [ \"States.ALL\" ], \"Next\": \"Notify Failure\" } ] } Notify \"Notify Success\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::sns:publish\", \"Parameters\": { \"Message\": \"Batch job submitted through Step Functions succeeded\", \"TopicArn\": \"<SNS_TOPIC_ARN>\" }, \"End\": true }, Fargate Task \"Run Fargate Task\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::ecs:runTask.sync\", \"Parameters\": { \"LaunchType\": \"FARGATE\", \"Cluster\": \"<ECS_CLUSTER_ARN>\", \"TaskDefinition\": \"<ECS_TASK_DEFINITION_ARN>\", \"NetworkConfiguration\": { \"AwsvpcConfiguration\": { \"Subnets\": [ \"${subnetAz1}\", \"${subnetAz2}\" ], \"AssignPublicIp\": \"ENABLED\" } } }, \"Next\": \"Notify Success\", \"Catch\": [ { \"ErrorEquals\": [ \"States.ALL\" ], \"Next\": \"Notify Failure\" } ] }, Read Next Message from Dynamo DB \"Read Next Message from DynamoDB\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::dynamodb:getItem\", \"Parameters\": { \"TableName\": \"<DYNAMO_DB_TABLE_NAME>\", \"Key\": { \"MessageId\": {\"S.$\": \"$.List[0]\"} } }, \"ResultPath\": \"$.DynamoDB\", \"Next\": \"Send Message to SQS\" }, Send Message to SQS \"Send Message to SQS\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::sqs:sendMessage\", \"Parameters\": { \"MessageBody.$\": \"$.DynamoDB.Item.Message.S\", \"QueueUrl\": \"<SQS_QUEUE_URL>\" }, \"ResultPath\": \"$.SQS\", \"Next\": \"Pop Element from List\" }, Retry \"Submit Job\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:lambda:::function:SubmitJob\", \"ResultPath\": \"$.guid\", \"Next\": \"Wait X Seconds\", \"Retry\": [ { \"ErrorEquals\": [\"States.ALL\"], \"IntervalSeconds\": 1, \"MaxAttempts\": 3, \"BackoffRate\": 2 } ] } Start an execution of another state machine and continue \"Start new workflow and continue\": { \"Comment\": \"Start an execution of another state machine and continue\", \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::states:startExecution\", \"Parameters\": { \"StateMachineArn\": \"<STATE_MACHINE_ARN>\", \"Input\": { \"NeedCallback\": false, \"AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID.$\": \"$$.Execution.Id\" } }, \"Next\": \"Start in parallel\" } Callback Pattern \"Start Task And Wait For Callback\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::sqs:sendMessage.waitForTaskToken\", \"Parameters\": { \"QueueUrl\": \"<SQS_QUEUE_URL>\", \"MessageBody\": { \"MessageTitle\": \"Task started by Step Functions. Waiting for callback with task token.\", \"TaskToken.$\": \"$$.Task.Token\" } }, \"Next\": \"Notify Success\", \"Catch\": [ { \"ErrorEquals\": [ \"States.ALL\" ], \"Next\": \"Notify Failure\" } ] } Catch \"HelloWorld\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME\", \"Catch\": [ { \"ErrorEquals\": [\"CustomError\"], \"Next\": \"CustomErrorFallback\" }, { \"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ReservedTypeFallback\" }, { \"ErrorEquals\": [\"States.ALL\"], \"Next\": \"CatchAllFallback\" } ], \"End\": true } Parallel \"Start in parallel\": { \"Comment\": \"Start two executions of the same state machine in parallel\", \"Type\": \"Parallel\", \"End\": true, \"Branches\": [ { \"StartAt\": \"Start new workflow and wait for completion\", \"States\": { \"Start new workflow and wait for completion\": { \"Comment\": \"Start an execution and wait for it to complete\", \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::states:startExecution.sync\", \"Parameters\": { \"StateMachineArn\": \"<STATE_MACHINE_ARN>\", \"Input\": { \"NeedCallback\": false, \"AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID.$\": \"$$.Execution.Id\" } }, \"OutputPath\": \"$.Output\", \"End\": true } } }, { \"StartAt\": \"Start new workflow and wait for callback\", \"States\": { \"Start new workflow and wait for callback\": { \"Comment\": \"Start an execution and wait for it to call back with a task token\", \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::states:startExecution.waitForTaskToken\", \"Parameters\": { \"StateMachineArn\": \"<STATE_MACHINE_ARN>\", \"Input\": { \"NeedCallback\": true, \"AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID.$\": \"$$.Execution.Id\", \"TaskToken.$\": \"$$.Task.Token\" } }, \"End\": true } } } ] } Choice If \"Job Complete?\": { \"Type\": \"Choice\", \"Choices\": [ { \"Variable\": \"$.status\", \"StringEquals\": \"FAILED\", \"Next\": \"Job Failed\" }, { \"Variable\": \"$.status\", \"StringEquals\": \"SUCCEEDED\", \"Next\": \"Get Final Job Status\" } ], \"Default\": \"Wait X Seconds\" } For Loop \"For Loop Condition\": { \"Type\": \"Choice\", \"Choices\": [ { \"Not\": { \"Variable\": \"$.List[0]\", \"StringEquals\": \"DONE\" }, \"Next\": \"Read Next Message from DynamoDB\" } ], \"Default\": \"Succeed\" } Pass Hello World { \"Comment\": \"A Hello World example of the Amazon States Language using a Pass state\", \"StartAt\": \"HelloWorld\", \"States\": { \"HelloWorld\": { \"Type\": \"Pass\", \"Result\": \"Hello World!\", \"End\": true } } } Pop Element from List \"Pop Element from List\": { \"Type\": \"Pass\", \"Parameters\": { \"List.$\": \"$.List[1:]\" }, \"Next\": \"For Loop Condition\" } Succeed \"Succeed\": { \"Type\": \"Succeed\" } Fail \"Job Failed\": { \"Type\": \"Fail\", \"Cause\": \"AWS Batch Job Failed\", \"Error\": \"DescribeJob returned FAILED\" } Wait \"Wait X Seconds\": { \"Type\": \"Wait\", \"SecondsPath\": \"$.wait_time\", \"Next\": \"Get Job Status\" }","title":"Step Functions"},{"location":"aws/applicationIntegration/stepfunctions/#task","text":"","title":"Task"},{"location":"aws/applicationIntegration/stepfunctions/#submit-batch-job","text":"\"Submit Batch Job\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::batch:submitJob.sync\", \"Parameters\": { \"JobName\": \"BatchJobNotification\", \"JobQueue\": \"<BATCH_QUEUE_ARN>\", \"JobDefinition\": \"<BATCH_JOB_DEFINITION_ARN>\" }, \"Next\": \"Notify Success\", \"Catch\": [ { \"ErrorEquals\": [ \"States.ALL\" ], \"Next\": \"Notify Failure\" } ] }","title":"Submit Batch Job"},{"location":"aws/applicationIntegration/stepfunctions/#notify","text":"\"Notify Success\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::sns:publish\", \"Parameters\": { \"Message\": \"Batch job submitted through Step Functions succeeded\", \"TopicArn\": \"<SNS_TOPIC_ARN>\" }, \"End\": true },","title":"Notify"},{"location":"aws/applicationIntegration/stepfunctions/#fargate-task","text":"\"Run Fargate Task\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::ecs:runTask.sync\", \"Parameters\": { \"LaunchType\": \"FARGATE\", \"Cluster\": \"<ECS_CLUSTER_ARN>\", \"TaskDefinition\": \"<ECS_TASK_DEFINITION_ARN>\", \"NetworkConfiguration\": { \"AwsvpcConfiguration\": { \"Subnets\": [ \"${subnetAz1}\", \"${subnetAz2}\" ], \"AssignPublicIp\": \"ENABLED\" } } }, \"Next\": \"Notify Success\", \"Catch\": [ { \"ErrorEquals\": [ \"States.ALL\" ], \"Next\": \"Notify Failure\" } ] },","title":"Fargate Task"},{"location":"aws/applicationIntegration/stepfunctions/#read-next-message-from-dynamo-db","text":"\"Read Next Message from DynamoDB\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::dynamodb:getItem\", \"Parameters\": { \"TableName\": \"<DYNAMO_DB_TABLE_NAME>\", \"Key\": { \"MessageId\": {\"S.$\": \"$.List[0]\"} } }, \"ResultPath\": \"$.DynamoDB\", \"Next\": \"Send Message to SQS\" },","title":"Read Next Message from Dynamo DB"},{"location":"aws/applicationIntegration/stepfunctions/#send-message-to-sqs","text":"\"Send Message to SQS\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::sqs:sendMessage\", \"Parameters\": { \"MessageBody.$\": \"$.DynamoDB.Item.Message.S\", \"QueueUrl\": \"<SQS_QUEUE_URL>\" }, \"ResultPath\": \"$.SQS\", \"Next\": \"Pop Element from List\" },","title":"Send Message to SQS"},{"location":"aws/applicationIntegration/stepfunctions/#retry","text":"\"Submit Job\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:lambda:::function:SubmitJob\", \"ResultPath\": \"$.guid\", \"Next\": \"Wait X Seconds\", \"Retry\": [ { \"ErrorEquals\": [\"States.ALL\"], \"IntervalSeconds\": 1, \"MaxAttempts\": 3, \"BackoffRate\": 2 } ] }","title":"Retry"},{"location":"aws/applicationIntegration/stepfunctions/#start-an-execution-of-another-state-machine-and-continue","text":"\"Start new workflow and continue\": { \"Comment\": \"Start an execution of another state machine and continue\", \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::states:startExecution\", \"Parameters\": { \"StateMachineArn\": \"<STATE_MACHINE_ARN>\", \"Input\": { \"NeedCallback\": false, \"AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID.$\": \"$$.Execution.Id\" } }, \"Next\": \"Start in parallel\" }","title":"Start an execution of another state machine and continue"},{"location":"aws/applicationIntegration/stepfunctions/#callback-pattern","text":"\"Start Task And Wait For Callback\": { \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::sqs:sendMessage.waitForTaskToken\", \"Parameters\": { \"QueueUrl\": \"<SQS_QUEUE_URL>\", \"MessageBody\": { \"MessageTitle\": \"Task started by Step Functions. Waiting for callback with task token.\", \"TaskToken.$\": \"$$.Task.Token\" } }, \"Next\": \"Notify Success\", \"Catch\": [ { \"ErrorEquals\": [ \"States.ALL\" ], \"Next\": \"Notify Failure\" } ] }","title":"Callback Pattern"},{"location":"aws/applicationIntegration/stepfunctions/#catch","text":"\"HelloWorld\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME\", \"Catch\": [ { \"ErrorEquals\": [\"CustomError\"], \"Next\": \"CustomErrorFallback\" }, { \"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ReservedTypeFallback\" }, { \"ErrorEquals\": [\"States.ALL\"], \"Next\": \"CatchAllFallback\" } ], \"End\": true }","title":"Catch"},{"location":"aws/applicationIntegration/stepfunctions/#parallel","text":"\"Start in parallel\": { \"Comment\": \"Start two executions of the same state machine in parallel\", \"Type\": \"Parallel\", \"End\": true, \"Branches\": [ { \"StartAt\": \"Start new workflow and wait for completion\", \"States\": { \"Start new workflow and wait for completion\": { \"Comment\": \"Start an execution and wait for it to complete\", \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::states:startExecution.sync\", \"Parameters\": { \"StateMachineArn\": \"<STATE_MACHINE_ARN>\", \"Input\": { \"NeedCallback\": false, \"AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID.$\": \"$$.Execution.Id\" } }, \"OutputPath\": \"$.Output\", \"End\": true } } }, { \"StartAt\": \"Start new workflow and wait for callback\", \"States\": { \"Start new workflow and wait for callback\": { \"Comment\": \"Start an execution and wait for it to call back with a task token\", \"Type\": \"Task\", \"Resource\": \"arn:<PARTITION>:states:::states:startExecution.waitForTaskToken\", \"Parameters\": { \"StateMachineArn\": \"<STATE_MACHINE_ARN>\", \"Input\": { \"NeedCallback\": true, \"AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID.$\": \"$$.Execution.Id\", \"TaskToken.$\": \"$$.Task.Token\" } }, \"End\": true } } } ] }","title":"Parallel"},{"location":"aws/applicationIntegration/stepfunctions/#choice","text":"","title":"Choice"},{"location":"aws/applicationIntegration/stepfunctions/#if","text":"\"Job Complete?\": { \"Type\": \"Choice\", \"Choices\": [ { \"Variable\": \"$.status\", \"StringEquals\": \"FAILED\", \"Next\": \"Job Failed\" }, { \"Variable\": \"$.status\", \"StringEquals\": \"SUCCEEDED\", \"Next\": \"Get Final Job Status\" } ], \"Default\": \"Wait X Seconds\" }","title":"If"},{"location":"aws/applicationIntegration/stepfunctions/#for-loop","text":"\"For Loop Condition\": { \"Type\": \"Choice\", \"Choices\": [ { \"Not\": { \"Variable\": \"$.List[0]\", \"StringEquals\": \"DONE\" }, \"Next\": \"Read Next Message from DynamoDB\" } ], \"Default\": \"Succeed\" }","title":"For Loop"},{"location":"aws/applicationIntegration/stepfunctions/#pass","text":"","title":"Pass"},{"location":"aws/applicationIntegration/stepfunctions/#hello-world","text":"{ \"Comment\": \"A Hello World example of the Amazon States Language using a Pass state\", \"StartAt\": \"HelloWorld\", \"States\": { \"HelloWorld\": { \"Type\": \"Pass\", \"Result\": \"Hello World!\", \"End\": true } } }","title":"Hello World"},{"location":"aws/applicationIntegration/stepfunctions/#pop-element-from-list","text":"\"Pop Element from List\": { \"Type\": \"Pass\", \"Parameters\": { \"List.$\": \"$.List[1:]\" }, \"Next\": \"For Loop Condition\" }","title":"Pop Element from List"},{"location":"aws/applicationIntegration/stepfunctions/#succeed","text":"\"Succeed\": { \"Type\": \"Succeed\" }","title":"Succeed"},{"location":"aws/applicationIntegration/stepfunctions/#fail","text":"\"Job Failed\": { \"Type\": \"Fail\", \"Cause\": \"AWS Batch Job Failed\", \"Error\": \"DescribeJob returned FAILED\" }","title":"Fail"},{"location":"aws/applicationIntegration/stepfunctions/#wait","text":"\"Wait X Seconds\": { \"Type\": \"Wait\", \"SecondsPath\": \"$.wait_time\", \"Next\": \"Get Job Status\" }","title":"Wait"},{"location":"aws/compute/ebs/","text":"Root Volumes Resize Manual method Create a snapshot of the current root volume Create a new volume from the snapshot with new storage specifications Select larger \"size\", thus increasing IOPS Must select the same availability zone as the source Stop the Instance Detach the original root volume Attach the new volume to the instance at same mount point (xvda) \"Automated\" method We can replace the launch configuration of an Auto Scaling Group In a true 3-tier application that is decoupled, we should then be able to terminate instances one by one to recreate them using new configuration Note: If the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity","title":"EBS"},{"location":"aws/compute/ebs/#root-volumes","text":"","title":"Root Volumes"},{"location":"aws/compute/ebs/#resize","text":"Manual method Create a snapshot of the current root volume Create a new volume from the snapshot with new storage specifications Select larger \"size\", thus increasing IOPS Must select the same availability zone as the source Stop the Instance Detach the original root volume Attach the new volume to the instance at same mount point (xvda) \"Automated\" method We can replace the launch configuration of an Auto Scaling Group In a true 3-tier application that is decoupled, we should then be able to terminate instances one by one to recreate them using new configuration Note: If the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity","title":"Resize"},{"location":"aws/compute/ec2/","text":"AMI Create EC2 EBS Backed Launch an instance Connect to your instance and customize it Stop Instances (stopping the instance ensures data integrity of the instance; a snapshot will be taken of the instance) Create the image Bundle it (use bundle command in console) AWS automatically registers it for you EC2 Instance store Launch an instance Connect to your instance and customize it Bundle it (Consists of an image manifest -image.manifest.xml- and files -image.part.xx-) Upload the bundle to S3 bucket Register AMI Snapshots Restore Read all blocks to eliminate penalty in production sudo dd if=/dev/xvdf of=/dev/null bs=1M # Or sudo fio \u2013filename=/dev/xvdf \u2013rw=read \u2013bs=128k \u2013iodepth=32 \u2013ioengine=libaio \u2013direct=1 \u2013name=volume-initialize","title":"EC2"},{"location":"aws/compute/ec2/#ami","text":"","title":"AMI"},{"location":"aws/compute/ec2/#create","text":"EC2 EBS Backed Launch an instance Connect to your instance and customize it Stop Instances (stopping the instance ensures data integrity of the instance; a snapshot will be taken of the instance) Create the image Bundle it (use bundle command in console) AWS automatically registers it for you EC2 Instance store Launch an instance Connect to your instance and customize it Bundle it (Consists of an image manifest -image.manifest.xml- and files -image.part.xx-) Upload the bundle to S3 bucket Register AMI","title":"Create"},{"location":"aws/compute/ec2/#snapshots","text":"","title":"Snapshots"},{"location":"aws/compute/ec2/#restore","text":"Read all blocks to eliminate penalty in production sudo dd if=/dev/xvdf of=/dev/null bs=1M # Or sudo fio \u2013filename=/dev/xvdf \u2013rw=read \u2013bs=128k \u2013iodepth=32 \u2013ioengine=libaio \u2013direct=1 \u2013name=volume-initialize","title":"Restore"},{"location":"aws/compute/lambda/","text":"SAM sam local invoke \"myapp\" -e event.json sam package --template-file template.yaml --s3-bucket mybucket --output-template-file packaged.yaml sam deploy --template-file packaged.yaml --stack-name mySafeDeployStack --capabilities CAPABILITY_IAM Runtimes Go Link import ( \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/service/lambda\" \"fmt\" \"os\" ) sess := session.Must(session.NewSessionWithOptions(session.Options{ SharedConfigState: session.SharedConfigEnable, })) svc := lambda.New(sess, &aws.Config{Region: aws.String(\"eu-west-1\")}) result, err := svc.ListFunctions(nil) if err != nil { fmt.Println(\"Cannot list functions\") return false, nil } fmt.Println(result) return true, nil","title":"Lambda"},{"location":"aws/compute/lambda/#sam","text":"sam local invoke \"myapp\" -e event.json sam package --template-file template.yaml --s3-bucket mybucket --output-template-file packaged.yaml sam deploy --template-file packaged.yaml --stack-name mySafeDeployStack --capabilities CAPABILITY_IAM","title":"SAM"},{"location":"aws/compute/lambda/#runtimes","text":"","title":"Runtimes"},{"location":"aws/compute/lambda/#go","text":"Link import ( \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/service/lambda\" \"fmt\" \"os\" ) sess := session.Must(session.NewSessionWithOptions(session.Options{ SharedConfigState: session.SharedConfigEnable, })) svc := lambda.New(sess, &aws.Config{Region: aws.String(\"eu-west-1\")}) result, err := svc.ListFunctions(nil) if err != nil { fmt.Println(\"Cannot list functions\") return false, nil } fmt.Println(result) return true, nil","title":"Go"},{"location":"aws/developer/codebuild/","text":"buildspec.yml --- version: 0.2 phases: install: runtime-versions: python: 3.7 commands: - echo \"Installing HashiCorp Packer...\" - curl -qL -o packer.zip https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip && unzip packer.zip pre_build: commands: - echo \"Validating Packer Template\" - ./packer validate packer.json build: commands: - ./packer build -color=false -var 'subnet_id=subnet-0cbb310334e31c9c1' packer.json | tee build.log post_build: commands: - egrep \"${AWS_REGION}\\:\\sami\\-\" build.log | cut -d' ' -f2 > ami_id.txt # Packer doesn't return non-zero status; we must do that if Packer build failed - test -s ami_id.txt || exit # Create Event to CloudWatch # - sed -i.bak \"s/<<AMI-ID>>/$(cat ami_id.txt)/g\" ami_builder_event.json # - aws events put-events --entries file://ami_builder_event.json - echo \"build completed on `date`\" artifacts: files: - ami_builder_event.json - build.log discard-paths: yes","title":"CodeBuild"},{"location":"aws/developer/codebuild/#buildspecyml","text":"--- version: 0.2 phases: install: runtime-versions: python: 3.7 commands: - echo \"Installing HashiCorp Packer...\" - curl -qL -o packer.zip https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip && unzip packer.zip pre_build: commands: - echo \"Validating Packer Template\" - ./packer validate packer.json build: commands: - ./packer build -color=false -var 'subnet_id=subnet-0cbb310334e31c9c1' packer.json | tee build.log post_build: commands: - egrep \"${AWS_REGION}\\:\\sami\\-\" build.log | cut -d' ' -f2 > ami_id.txt # Packer doesn't return non-zero status; we must do that if Packer build failed - test -s ami_id.txt || exit # Create Event to CloudWatch # - sed -i.bak \"s/<<AMI-ID>>/$(cat ami_id.txt)/g\" ami_builder_event.json # - aws events put-events --entries file://ami_builder_event.json - echo \"build completed on `date`\" artifacts: files: - ami_builder_event.json - build.log discard-paths: yes","title":"buildspec.yml"},{"location":"aws/developer/codecommit/","text":"Configure Git to Use AWS Cli Credentials git config --global user.name \"developer\" git config --global user.email developer@example.com git config --global credential.helper '!aws codecommit credential-helper $@' git config --global credential.UseHttpPath true","title":"CodeCommit"},{"location":"aws/developer/codecommit/#configure-git-to-use-aws-cli-credentials","text":"git config --global user.name \"developer\" git config --global user.email developer@example.com git config --global credential.helper '!aws codecommit credential-helper $@' git config --global credential.UseHttpPath true","title":"Configure Git to Use AWS Cli Credentials"},{"location":"aws/developer/codedeploy/","text":"appspec.yml version: 0.0 os: linux files: - source: files/newpage.html destination: /var/www/html permissions: - object: /var/www/html pattern: newpage.html mode: 755 type: - file hooks: BeforeInstall: - location: scripts/webserver-stop.sh AfterInstall: - location: scripts/webserver-start.sh","title":"CodeDeploy"},{"location":"aws/developer/codedeploy/#appspecyml","text":"version: 0.0 os: linux files: - source: files/newpage.html destination: /var/www/html permissions: - object: /var/www/html pattern: newpage.html mode: 755 type: - file hooks: BeforeInstall: - location: scripts/webserver-stop.sh AfterInstall: - location: scripts/webserver-start.sh","title":"appspec.yml"},{"location":"aws/management/cloudformation/","text":"Macro Dynamic Resources Github Example Macro Template AWSTemplateFormatVersion: 2010-09-09 Resources: Macro: Type: AWS::CloudFormation::Macro Properties: Name: !Sub '${AWS::StackName}' Description: Transform the original template performing a for loop that adds the requested number of Users resources. FunctionName: !Ref MacroProcessor MacroProcessor: Type: \"AWS::Lambda::Function\" Properties: FunctionName: MacroProcessor Runtime: \"python3.6\" Description: IAM Users Macro processor function Handler: \"index.lambda_handler\" Code: ZipFile: | import json def lambda_handler(event, context): FinalFragment= event[\"fragment\"] Number = FinalFragment[\"Parameters\"][\"NumberOfUsers\"] #I declare a new dictionary to have the resources object NewUser = {} NewUser = FinalFragment[\"Resources\"] for i in range(len(Number)): #I modify the IAM User resource logical id in order to add it later to \"Resources\" object the number of times requested NewUser[\"myuser\"+str(i+2)] = NewUser[\"myuser\"].copy() FinalFragment[\"Resources\"].update(NewUser) NewUser[\"EC2Instance\"+str(i+2)] = NewUser[\"EC2Instance\"].copy() FinalFragment[\"Resources\"].update(NewUser) response = {} response[\"requestId\"] = event[\"requestId\"] response[\"status\"] = \"success\" response[\"fragment\"] = FinalFragment print (FinalFragment[\"Resources\"]) return response Timeout: \"100\" MemorySize: 128 Role: !GetAtt LambdaExecutionRole.Arn LambdaExecutionRole: Type: AWS::IAM::Role Properties: RoleName: !Sub MacroProcessor-lambda-role-${AWS::Region} AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - sts:AssumeRole Path: / LambdaPolicy: Type: AWS::IAM::Policy Properties: PolicyName: !Sub MacroProcessor-lambda-policy-${AWS::Region} PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: \"cloudformation:*\" Resource: \"*\" - Effect: Allow Action: \"logs:*\" Resource: \"*\" Roles: - !Ref LambdaExecutionRole CFN Template AWSTemplateFormatVersion: 2010-09-09 Description: This template generates the requested number of IAM users that will have the same custom EC2 policy for EC2 resources tagged with Owner:devteam. The user will be prompted to reset their passwords on next sign-in. Also an EC2 instance will be created for each user simulating a classroom. Transform: \"Macro\" Parameters: NumberOfUsers: Type: String Description: Enter the number of users to create for the Dev Team KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access to the web server Type: 'AWS::EC2::KeyPair::KeyName' ConstraintDescription: must be the name of an existing EC2 KeyPair. InstanceType: Description: WebServer EC2 instance type Type: String Default: t2.small AllowedValues: - t1.micro - t2.nano - t2.micro - t2.small - t2.medium - t2.large ConstraintDescription: must be a T2 EC2 instance type. SSHLocation: Description: >- Lockdown SSH access to the bastion host (default can be accessed from anywhere) Type: String MinLength: '9' MaxLength: '18' Default: 0.0.0.0/0 AllowedPattern: '(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})' ConstraintDescription: must be a valid CIDR range of the form x.x.x.x/x. Mappings: AWSRegion2AMI: us-east-1: AMI: \"ami-97785bed\" us-west-2: AMI: \"ami-f2d3638a\" us-west-1: AMI: \"ami-824c4ee2\" eu-west-1: AMI: \"ami-d834aba1\" eu-west-2: AMI: \"ami-403e2524\" eu-west-3: AMI: \"ami-8ee056f3\" eu-central-1: AMI: \"ami-5652ce39\" ap-northeast-1: AMI: \"ami-ceafcba8\" ap-northeast-2: AMI: \"ami-863090e8\" ap-northeast-3: AMI: \"ami-83444afe\" ap-southeast-1: AMI: \"ami-68097514\" ap-southeast-2: AMI: \"ami-942dd1f6\" ap-south-1: AMI: \"ami-531a4c3c\" us-east-2: AMI: \"ami-f63b1193\" ca-central-1: AMI: \"ami-a954d1cd\" sa-east-1: AMI: \"ami-84175ae8\" cn-north-1: AMI: \"ami-cb19c4a6\" cn-northwest-1: AMI: \"ami-3e60745c\" Resources: myuser: Type: AWS::IAM::User Properties: Path: \"/\" LoginProfile: Password: myP@ssW0rd PasswordResetRequired: yes ManagedPolicyArns: - !Ref DevTeamEC2Policy EC2Instance: Type: 'AWS::EC2::Instance' Properties: KeyName: !Ref KeyName InstanceType: !Ref InstanceType ImageId: !FindInMap [ AWSRegion2AMI, !Ref \"AWS::Region\", \"AMI\" ] SecurityGroups: - !Ref EC2SecurityGroup Tags: - Key: Owner Value: devteam EC2SecurityGroup: Type: 'AWS::EC2::SecurityGroup' Properties: GroupDescription: SSH access SecurityGroupIngress: - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: !Ref SSHLocation DevTeamEC2Policy: Type: AWS::IAM::ManagedPolicy Properties: Description: \"Policy for EC2\" Path: \"/\" PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - ec2:StartInstances - ec2:StopInstances - ec2:Describe* - ec2:Get* Resource: !Sub \"arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:instance/*\" Condition: StringEquals: ec2:ResourceTag/Owner: \"devteam\" Nested Stacks Nested Template Must be stored in S3 AWSTemplateFormatVersion: \"2010-09-09\" Description: Template to Create the Code Commit Repositories to DEV Env Parameters: Name: Type: String Description: Repository Name Description: Type: String Description: Repository Description Resources: Repository: Type: AWS::CodeCommit::Repository Properties: RepositoryDescription: Ref: Description RepositoryName: Ref: Name Main Template CodeCommitRepo: Type: AWS::CloudFormation::Stack Properties: Parameters: Name: \"\" Description: \"\" TemplateURL: https://dev-env-cfn-templates.s3-eu-west-1.amazonaws.com/codecommit-repos.yml Troubleshooting WaitCondition If WaitCondition times out or returns an error. There is an error in your cloudformation::init code Analyze cfn-init.log and cfn-wire.log for details collect logs via CloudWatch logs \u2013on-failure DO_NOTHING to keep instance from rolling back so you can log in and examine the logs Most common error: URLs for referenced resources (scripts, MSIs, etc.) are returning HTTP 403 or 404 errors","title":"Cloudformation"},{"location":"aws/management/cloudformation/#macro","text":"","title":"Macro"},{"location":"aws/management/cloudformation/#dynamic-resources","text":"Github Example","title":"Dynamic Resources"},{"location":"aws/management/cloudformation/#macro-template","text":"AWSTemplateFormatVersion: 2010-09-09 Resources: Macro: Type: AWS::CloudFormation::Macro Properties: Name: !Sub '${AWS::StackName}' Description: Transform the original template performing a for loop that adds the requested number of Users resources. FunctionName: !Ref MacroProcessor MacroProcessor: Type: \"AWS::Lambda::Function\" Properties: FunctionName: MacroProcessor Runtime: \"python3.6\" Description: IAM Users Macro processor function Handler: \"index.lambda_handler\" Code: ZipFile: | import json def lambda_handler(event, context): FinalFragment= event[\"fragment\"] Number = FinalFragment[\"Parameters\"][\"NumberOfUsers\"] #I declare a new dictionary to have the resources object NewUser = {} NewUser = FinalFragment[\"Resources\"] for i in range(len(Number)): #I modify the IAM User resource logical id in order to add it later to \"Resources\" object the number of times requested NewUser[\"myuser\"+str(i+2)] = NewUser[\"myuser\"].copy() FinalFragment[\"Resources\"].update(NewUser) NewUser[\"EC2Instance\"+str(i+2)] = NewUser[\"EC2Instance\"].copy() FinalFragment[\"Resources\"].update(NewUser) response = {} response[\"requestId\"] = event[\"requestId\"] response[\"status\"] = \"success\" response[\"fragment\"] = FinalFragment print (FinalFragment[\"Resources\"]) return response Timeout: \"100\" MemorySize: 128 Role: !GetAtt LambdaExecutionRole.Arn LambdaExecutionRole: Type: AWS::IAM::Role Properties: RoleName: !Sub MacroProcessor-lambda-role-${AWS::Region} AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - sts:AssumeRole Path: / LambdaPolicy: Type: AWS::IAM::Policy Properties: PolicyName: !Sub MacroProcessor-lambda-policy-${AWS::Region} PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: \"cloudformation:*\" Resource: \"*\" - Effect: Allow Action: \"logs:*\" Resource: \"*\" Roles: - !Ref LambdaExecutionRole","title":"Macro Template"},{"location":"aws/management/cloudformation/#cfn-template","text":"AWSTemplateFormatVersion: 2010-09-09 Description: This template generates the requested number of IAM users that will have the same custom EC2 policy for EC2 resources tagged with Owner:devteam. The user will be prompted to reset their passwords on next sign-in. Also an EC2 instance will be created for each user simulating a classroom. Transform: \"Macro\" Parameters: NumberOfUsers: Type: String Description: Enter the number of users to create for the Dev Team KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access to the web server Type: 'AWS::EC2::KeyPair::KeyName' ConstraintDescription: must be the name of an existing EC2 KeyPair. InstanceType: Description: WebServer EC2 instance type Type: String Default: t2.small AllowedValues: - t1.micro - t2.nano - t2.micro - t2.small - t2.medium - t2.large ConstraintDescription: must be a T2 EC2 instance type. SSHLocation: Description: >- Lockdown SSH access to the bastion host (default can be accessed from anywhere) Type: String MinLength: '9' MaxLength: '18' Default: 0.0.0.0/0 AllowedPattern: '(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})' ConstraintDescription: must be a valid CIDR range of the form x.x.x.x/x. Mappings: AWSRegion2AMI: us-east-1: AMI: \"ami-97785bed\" us-west-2: AMI: \"ami-f2d3638a\" us-west-1: AMI: \"ami-824c4ee2\" eu-west-1: AMI: \"ami-d834aba1\" eu-west-2: AMI: \"ami-403e2524\" eu-west-3: AMI: \"ami-8ee056f3\" eu-central-1: AMI: \"ami-5652ce39\" ap-northeast-1: AMI: \"ami-ceafcba8\" ap-northeast-2: AMI: \"ami-863090e8\" ap-northeast-3: AMI: \"ami-83444afe\" ap-southeast-1: AMI: \"ami-68097514\" ap-southeast-2: AMI: \"ami-942dd1f6\" ap-south-1: AMI: \"ami-531a4c3c\" us-east-2: AMI: \"ami-f63b1193\" ca-central-1: AMI: \"ami-a954d1cd\" sa-east-1: AMI: \"ami-84175ae8\" cn-north-1: AMI: \"ami-cb19c4a6\" cn-northwest-1: AMI: \"ami-3e60745c\" Resources: myuser: Type: AWS::IAM::User Properties: Path: \"/\" LoginProfile: Password: myP@ssW0rd PasswordResetRequired: yes ManagedPolicyArns: - !Ref DevTeamEC2Policy EC2Instance: Type: 'AWS::EC2::Instance' Properties: KeyName: !Ref KeyName InstanceType: !Ref InstanceType ImageId: !FindInMap [ AWSRegion2AMI, !Ref \"AWS::Region\", \"AMI\" ] SecurityGroups: - !Ref EC2SecurityGroup Tags: - Key: Owner Value: devteam EC2SecurityGroup: Type: 'AWS::EC2::SecurityGroup' Properties: GroupDescription: SSH access SecurityGroupIngress: - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: !Ref SSHLocation DevTeamEC2Policy: Type: AWS::IAM::ManagedPolicy Properties: Description: \"Policy for EC2\" Path: \"/\" PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - ec2:StartInstances - ec2:StopInstances - ec2:Describe* - ec2:Get* Resource: !Sub \"arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:instance/*\" Condition: StringEquals: ec2:ResourceTag/Owner: \"devteam\"","title":"CFN Template"},{"location":"aws/management/cloudformation/#nested-stacks","text":"","title":"Nested Stacks"},{"location":"aws/management/cloudformation/#nested-template","text":"Must be stored in S3 AWSTemplateFormatVersion: \"2010-09-09\" Description: Template to Create the Code Commit Repositories to DEV Env Parameters: Name: Type: String Description: Repository Name Description: Type: String Description: Repository Description Resources: Repository: Type: AWS::CodeCommit::Repository Properties: RepositoryDescription: Ref: Description RepositoryName: Ref: Name","title":"Nested Template"},{"location":"aws/management/cloudformation/#main-template","text":"CodeCommitRepo: Type: AWS::CloudFormation::Stack Properties: Parameters: Name: \"\" Description: \"\" TemplateURL: https://dev-env-cfn-templates.s3-eu-west-1.amazonaws.com/codecommit-repos.yml","title":"Main Template"},{"location":"aws/management/cloudformation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"aws/management/cloudformation/#waitcondition","text":"If WaitCondition times out or returns an error. There is an error in your cloudformation::init code Analyze cfn-init.log and cfn-wire.log for details collect logs via CloudWatch logs \u2013on-failure DO_NOTHING to keep instance from rolling back so you can log in and examine the logs Most common error: URLs for referenced resources (scripts, MSIs, etc.) are returning HTTP 403 or 404 errors","title":"WaitCondition"},{"location":"aws/networking/vpc/","text":"Bastion Host High Availability Create 2 bastion hosts in different subnets. Create dns entry in Route53 which uses round robin dns and points to each instance. Tell your sysdamins to connect using the new dns entry EC2 Auto-recovery - If fails, will be recreate Autoscaling group of MAX =1 and MIN = 1. So if the bastion fails it is recreated automatically VPC Flow Logs Traffic Not Logged Traffic generated by instances when contact the Amazon DNS server (if use own dns, so all that traffic is logged) Traffic generated by a windows instance for Amazon windows license activation Traffic to and from 169.254.169.254 for instance metadata DHCP traffic Traffic to the reserved IP address for the default VPC router Troubleshooting Instances cannot communicate If instances in subnets cannot communicate with one another Verify that it is a network issue, not an instance issue Check network ACLs If enable, check VPC flow Logs Nat Configuration Nat Instance and Nat Gateway - Check Route table Association Nat Instance - Check to see if the source/Dest check is disabled Ensure that NAT has masquerade configured, Restart NAT, check the inbound security Group rules If you cannot reach resources in a peered network: Peering request approval Route table configuration Check network ACLs: are you forbidding all external traffic Check security Group configurations on resources Use Cidr block rules in VPC A to allow Access from VPC B If you are not seeing network traffic flow on the AWS side of your VPN connection to your Amazon VPC. Turn on route propagation in the Amazon VPC\u2019s main routing table.","title":"VPC"},{"location":"aws/networking/vpc/#bastion-host","text":"","title":"Bastion Host"},{"location":"aws/networking/vpc/#high-availability","text":"Create 2 bastion hosts in different subnets. Create dns entry in Route53 which uses round robin dns and points to each instance. Tell your sysdamins to connect using the new dns entry EC2 Auto-recovery - If fails, will be recreate Autoscaling group of MAX =1 and MIN = 1. So if the bastion fails it is recreated automatically","title":"High Availability"},{"location":"aws/networking/vpc/#vpc-flow-logs","text":"","title":"VPC Flow Logs"},{"location":"aws/networking/vpc/#traffic-not-logged","text":"Traffic generated by instances when contact the Amazon DNS server (if use own dns, so all that traffic is logged) Traffic generated by a windows instance for Amazon windows license activation Traffic to and from 169.254.169.254 for instance metadata DHCP traffic Traffic to the reserved IP address for the default VPC router","title":"Traffic Not Logged"},{"location":"aws/networking/vpc/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"aws/networking/vpc/#instances-cannot-communicate","text":"If instances in subnets cannot communicate with one another Verify that it is a network issue, not an instance issue Check network ACLs If enable, check VPC flow Logs","title":"Instances cannot communicate"},{"location":"aws/networking/vpc/#nat-configuration","text":"Nat Instance and Nat Gateway - Check Route table Association Nat Instance - Check to see if the source/Dest check is disabled Ensure that NAT has masquerade configured, Restart NAT, check the inbound security Group rules If you cannot reach resources in a peered network:","title":"Nat Configuration"},{"location":"aws/networking/vpc/#peering-request-approval","text":"Route table configuration Check network ACLs: are you forbidding all external traffic Check security Group configurations on resources Use Cidr block rules in VPC A to allow Access from VPC B If you are not seeing network traffic flow on the AWS side of your VPN connection to your Amazon VPC. Turn on route propagation in the Amazon VPC\u2019s main routing table.","title":"Peering request approval"},{"location":"containers/kubernetes/admin/admin/","text":"Pre-Requisites Redhat Disable SELinux setenforce 0 sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux Enable the br_netfilter module for cluster communication modprobe br_netfilter echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables Disable swap to prevent memory allocation issues swapoff -a vi /etc/fstab -> Comment out the swap line Docker Redhat Install the Docker prerequisites yum install -y yum-utils device-mapper-persistent-data lvm2 Add the Docker repo and install Docker yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce Configure the Docker Cgroup Driver to systemd, enable and start Docker sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker --now Ubuntu curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce Amazon AMI Linux 2 sudo amazon-linux-extras install -y docker sudo cat > /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo systemctl daemon-reload sudo systemctl enable docker sudo systemctl start docker sudo usermod -a -G docker ec2-user kubernetes Redhat cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl systemctl enable kubelet kubeadm init --pod-network-cidr=10.244.0.0/16 # Exit sudo user mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Ubuntu curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet=1.12.7-00 kubeadm=1.12.7-00 kubectl=1.12.7-00 sudo apt-mark hold kubelet kubeadm kubectl echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Upgrade Ubuntu kubectl version --short # Release the hold on versions of kubeadm and kubelet sudo apt-mark unhold kubeadm kubelet sudo apt install -y kubeadm=1.14.1-00 sudo apt-mark hold kubeadm kubeadm version sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.14.1 sudo apt-mark unhold kubectl sudo apt install -y kubectl=1.14.1-00 sudo apt-mark hold kubectl sudo apt-mark unhold kubelet sudo apt install -y kubelet=1.14.1-00 sudo apt-mark hold kubelet Maintenance Evict Pods # Evict the pods on a node kubectl drain [node_name] --ignore-daemonsets kubectl get nodes -w # Rollback - Schedule pods to the node after maintenance is complete kubectl uncordon [node_name] Delete Node kubectl delete node [node_name] sudo kubeadm token generate # Print the kubeadm join command to join a node to the cluster sudo kubeadm token create [token_name] --ttl 2h --print-join-command Autocomple Redhat # Enable Epel Repo sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel # Instal bash completion yum install bash-completion bash-completion-extras vi /.bashrc # Add Following lines # alias k=kubectl # source <(kubectl completion bash | sed s/kubectl/k/g) Geral Commands kubectl api-resources -o name Kubectl Install wget https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/ kubectl version --client Set Remote kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://localhost:6443 kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster=kubernetes-the-hard-way \\ --user=admin kubectl config use-context kubernetes-the-hard-way Kube Configs Generate Kube Configs See How Generate TLS Certificates # Create an environment variable to store the address of the Kubernetes API, and set it to the private IP of your load balancer cloud server: KUBERNETES_ADDRESS=<load balancer private ip> # Generate a kubelet kubeconfig for each worker node: for instance in <worker 1 hostname> <worker 2 hostname>; do kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done # Generate a kube-proxy kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig } # Generate a kube-controller-manager kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig } # Generate a kube-scheduler kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig } # Generate an admin kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig } Distributing the Kubeconfig Files Move kubeconfig files to the worker nodes: scp <worker 1 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 1 public IP>:~/ scp <worker 2 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 2 public IP>:~/ Move kubeconfig files to the master nodes: scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 1 public IP>:~/ scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 2 public IP>:~/ Data Encryption Config # Generate the Kubernetes Data encrpytion config file containing the encrpytion key: export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat > encryption-config.yaml << EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF # Copy the file to both master servers: scp encryption-config.yaml user@<master 1 public ip>:~/ scp encryption-config.yaml user@<master 2 public ip>:~/ LoadBalancer Setting UP # Here are the commands you can use to set up the nginx load balancer. Run these on the server that you have designated as your load balancer server: sudo apt-get install -y nginx sudo systemctl enable nginx sudo mkdir -p /etc/nginx/tcpconf.d sudo vi /etc/nginx/nginx.conf # Add the following to the end of nginx.conf: include /etc/nginx/tcpconf.d/*; # Set up some environment variables for the lead balancer config file: export CONTROLLER0_IP=<controller 0 private ip> export CONTROLLER1_IP=<controller 1 private ip> # Create the load balancer nginx config file: cat << EOF | sudo tee /etc/nginx/tcpconf.d/kubernetes.conf stream { upstream kubernetes { server $CONTROLLER0_IP:6443; server $CONTROLLER1_IP:6443; } server { listen 6443; listen 443; proxy_pass kubernetes; } } EOF # Reload the nginx configuration: sudo nginx -s reload # You can verify that the load balancer is working like so: curl -k https://localhost:6443/version","title":"Admin"},{"location":"containers/kubernetes/admin/admin/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"containers/kubernetes/admin/admin/#redhat","text":"Disable SELinux setenforce 0 sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux Enable the br_netfilter module for cluster communication modprobe br_netfilter echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables Disable swap to prevent memory allocation issues swapoff -a vi /etc/fstab -> Comment out the swap line","title":"Redhat"},{"location":"containers/kubernetes/admin/admin/#docker","text":"","title":"Docker"},{"location":"containers/kubernetes/admin/admin/#redhat_1","text":"Install the Docker prerequisites yum install -y yum-utils device-mapper-persistent-data lvm2 Add the Docker repo and install Docker yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce Configure the Docker Cgroup Driver to systemd, enable and start Docker sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker --now","title":"Redhat"},{"location":"containers/kubernetes/admin/admin/#ubuntu","text":"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce","title":"Ubuntu"},{"location":"containers/kubernetes/admin/admin/#amazon-ami-linux-2","text":"sudo amazon-linux-extras install -y docker sudo cat > /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo systemctl daemon-reload sudo systemctl enable docker sudo systemctl start docker sudo usermod -a -G docker ec2-user","title":"Amazon AMI Linux 2"},{"location":"containers/kubernetes/admin/admin/#kubernetes","text":"","title":"kubernetes"},{"location":"containers/kubernetes/admin/admin/#redhat_2","text":"cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl systemctl enable kubelet kubeadm init --pod-network-cidr=10.244.0.0/16 # Exit sudo user mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config","title":"Redhat"},{"location":"containers/kubernetes/admin/admin/#ubuntu_1","text":"curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet=1.12.7-00 kubeadm=1.12.7-00 kubectl=1.12.7-00 sudo apt-mark hold kubelet kubeadm kubectl echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config","title":"Ubuntu"},{"location":"containers/kubernetes/admin/admin/#upgrade","text":"","title":"Upgrade"},{"location":"containers/kubernetes/admin/admin/#ubuntu_2","text":"kubectl version --short # Release the hold on versions of kubeadm and kubelet sudo apt-mark unhold kubeadm kubelet sudo apt install -y kubeadm=1.14.1-00 sudo apt-mark hold kubeadm kubeadm version sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.14.1 sudo apt-mark unhold kubectl sudo apt install -y kubectl=1.14.1-00 sudo apt-mark hold kubectl sudo apt-mark unhold kubelet sudo apt install -y kubelet=1.14.1-00 sudo apt-mark hold kubelet","title":"Ubuntu"},{"location":"containers/kubernetes/admin/admin/#maintenance","text":"","title":"Maintenance"},{"location":"containers/kubernetes/admin/admin/#evict-pods","text":"# Evict the pods on a node kubectl drain [node_name] --ignore-daemonsets kubectl get nodes -w # Rollback - Schedule pods to the node after maintenance is complete kubectl uncordon [node_name]","title":"Evict Pods"},{"location":"containers/kubernetes/admin/admin/#delete-node","text":"kubectl delete node [node_name] sudo kubeadm token generate # Print the kubeadm join command to join a node to the cluster sudo kubeadm token create [token_name] --ttl 2h --print-join-command","title":"Delete Node"},{"location":"containers/kubernetes/admin/admin/#autocomple","text":"","title":"Autocomple"},{"location":"containers/kubernetes/admin/admin/#redhat_3","text":"# Enable Epel Repo sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel # Instal bash completion yum install bash-completion bash-completion-extras vi /.bashrc # Add Following lines # alias k=kubectl # source <(kubectl completion bash | sed s/kubectl/k/g)","title":"Redhat"},{"location":"containers/kubernetes/admin/admin/#geral-commands","text":"kubectl api-resources -o name","title":"Geral Commands"},{"location":"containers/kubernetes/admin/admin/#kubectl","text":"","title":"Kubectl"},{"location":"containers/kubernetes/admin/admin/#install","text":"wget https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/ kubectl version --client","title":"Install"},{"location":"containers/kubernetes/admin/admin/#set-remote","text":"kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://localhost:6443 kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster=kubernetes-the-hard-way \\ --user=admin kubectl config use-context kubernetes-the-hard-way","title":"Set Remote"},{"location":"containers/kubernetes/admin/admin/#kube-configs","text":"","title":"Kube Configs"},{"location":"containers/kubernetes/admin/admin/#generate-kube-configs","text":"See How Generate TLS Certificates # Create an environment variable to store the address of the Kubernetes API, and set it to the private IP of your load balancer cloud server: KUBERNETES_ADDRESS=<load balancer private ip> # Generate a kubelet kubeconfig for each worker node: for instance in <worker 1 hostname> <worker 2 hostname>; do kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done # Generate a kube-proxy kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig } # Generate a kube-controller-manager kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig } # Generate a kube-scheduler kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig } # Generate an admin kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig }","title":"Generate Kube Configs"},{"location":"containers/kubernetes/admin/admin/#distributing-the-kubeconfig-files","text":"","title":"Distributing the Kubeconfig Files"},{"location":"containers/kubernetes/admin/admin/#move-kubeconfig-files-to-the-worker-nodes","text":"scp <worker 1 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 1 public IP>:~/ scp <worker 2 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 2 public IP>:~/","title":"Move kubeconfig files to the worker nodes:"},{"location":"containers/kubernetes/admin/admin/#move-kubeconfig-files-to-the-master-nodes","text":"scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 1 public IP>:~/ scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 2 public IP>:~/","title":"Move kubeconfig files to the master nodes:"},{"location":"containers/kubernetes/admin/admin/#data-encryption-config","text":"# Generate the Kubernetes Data encrpytion config file containing the encrpytion key: export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat > encryption-config.yaml << EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF # Copy the file to both master servers: scp encryption-config.yaml user@<master 1 public ip>:~/ scp encryption-config.yaml user@<master 2 public ip>:~/","title":"Data Encryption Config"},{"location":"containers/kubernetes/admin/admin/#loadbalancer","text":"","title":"LoadBalancer"},{"location":"containers/kubernetes/admin/admin/#setting-up","text":"# Here are the commands you can use to set up the nginx load balancer. Run these on the server that you have designated as your load balancer server: sudo apt-get install -y nginx sudo systemctl enable nginx sudo mkdir -p /etc/nginx/tcpconf.d sudo vi /etc/nginx/nginx.conf # Add the following to the end of nginx.conf: include /etc/nginx/tcpconf.d/*; # Set up some environment variables for the lead balancer config file: export CONTROLLER0_IP=<controller 0 private ip> export CONTROLLER1_IP=<controller 1 private ip> # Create the load balancer nginx config file: cat << EOF | sudo tee /etc/nginx/tcpconf.d/kubernetes.conf stream { upstream kubernetes { server $CONTROLLER0_IP:6443; server $CONTROLLER1_IP:6443; } server { listen 6443; listen 443; proxy_pass kubernetes; } } EOF # Reload the nginx configuration: sudo nginx -s reload # You can verify that the load balancer is working like so: curl -k https://localhost:6443/version","title":"Setting UP"},{"location":"containers/kubernetes/admin/controlplane/","text":"Master Install Binaries # You can install the control plane binaries on each master node like this: sudo mkdir -p /etc/kubernetes/config wget -q --timestamping \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-apiserver\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-controller-manager\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-scheduler\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl\" chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/ API-Server Setting Up # You can configure the Kubernetes API server like so: sudo mkdir -p /var/lib/kubernetes/ sudo cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/ # Set some environment variables that will be used to create the systemd unit file. Make sure you replace the placeholders with their actual values: export INTERNAL_IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4) export CONTROLLER0_IP=<private ip of controller 0> export CONTROLLER1_IP=<private ip of controller 1> # Generate the kube-apiserver unit file for systemd: cat << EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://$CONTROLLER0_IP:2379,https://$CONTROLLER1_IP:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 \\\\ --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-apiserver sudo systemctl start kube-apiserver sudo systemctl status kube-apiserver Controller Manager Setting Up # You can configure the Kubernetes Controller Manager like so: sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/ # Generate the kube-controller-manager systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-controller-manager sudo systemctl start kube-controller-manager sudo systemctl status kube-controller-manager Scheduler Setting Up # Copy kube-scheduler.kubeconfig into the proper location: sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/ # Generate the kube-scheduler yaml config file. cat << EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: componentconfig/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: true EOF # Create the kube-scheduler systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-scheduler sudo systemctl start kube-scheduler sudo systemctl status kube-scheduler Default Scheduler Does the node have adequate hardware resources? Is the node running out of resources? Does the pod request a specific node? Does the node have a matching label? If the pod requests a port, is it available? If the pod requests a volume, can it be mounted? Does the pod tolerate the taints of the node? Does the pod specify node or pod affinity? Custom Scheduler ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: csinodes-admin rules: - apiGroups: [\"storage.k8s.io\"] resources: [\"csinodes\"] verbs: [\"get\", \"watch\", \"list\"] ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-csinodes-global subjects: - kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: csinodes-admin apiGroup: rbac.authorization.k8s.io Role apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: system:serviceaccount:kube-system:my-scheduler namespace: kube-system rules: - apiGroups: - storage.k8s.io resources: - csinodes verbs: - get - list - watch RoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-csinodes namespace: kube-system subjects: - kind: User name: kubernetes-admin apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: system:serviceaccount:kube-system:my-scheduler apiGroup: rbac.authorization.k8s.io My-scheduler apiVersion: v1 kind: ServiceAccount metadata: name: my-scheduler namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: my-scheduler-as-kube-scheduler subjects: - kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-scheduler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: labels: component: scheduler tier: control-plane name: my-scheduler namespace: kube-system spec: selector: matchLabels: component: scheduler tier: control-plane replicas: 1 template: metadata: labels: component: scheduler tier: control-plane version: second spec: serviceAccountName: my-scheduler containers: - command: - /usr/local/bin/kube-scheduler - --address=0.0.0.0 - --leader-elect=false - --scheduler-name=my-scheduler image: chadmcrowell/custom-scheduler livenessProbe: httpGet: path: /healthz port: 10251 initialDelaySeconds: 15 name: kube-second-scheduler readinessProbe: httpGet: path: /healthz port: 10251 resources: requests: cpu: '0.1' securityContext: privileged: false volumeMounts: [] hostNetwork: false hostPID: false volumes: [] Deploy kubectl create -f clusterrole.yaml kubectl create -f clusterrolebinding.yaml kubectl create -f role.yaml kubectl create -f rolebinding.yaml kubectl edit clusterrole system:kube-scheduler # And add the following - apiGroups: - \"\" resourceNames: - kube-scheduler - my-scheduler resources: - endpoints verbs: - delete - get - patch - update - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - watch - list - get kubectl create -f my-scheduler.yaml kubectl get pods -n kube-system Get config kubectl get endpoints kube-scheduler -n kube-system -o yaml Troubleshooting kubectl describe pods [scheduler_pod_name] -n kube-system kubectl logs [kube_scheduler_pod_name] -n kube-system cat /var/log/kube-scheduler.log RBAC Setting UP # Create a role with the necessary permissions: cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF # Bind the role to the kubernetes user cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF Nodes Install Binaries # You can install the worker binaries like so. Run these commands on both worker nodes: sudo apt-get -y install socat conntrack ipset wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.0/containerd-1.1.0.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.0.linux-amd64.tar.gz -C / ContainerD # You can configure the containerd service like so. Run these commands on both worker nodes: sudo mkdir -p /etc/containerd/ # Create the containerd config.toml: cat << EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\" [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runsc\" runtime_root = \"/run/containerd/runsc\" EOF # Create the containerd unit file: cat << EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/bin/containerd Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable containerd sudo systemctl start containerd sudo systemctl status containerd Kubelet # Set a HOSTNAME environment variable that will be used to generate your config files. Make sure you set the HOSTNAME appropriately for each worker node: export HOSTNAME=$(hostname) sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/ sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ # Create the kubelet config file: cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" authorization: mode: Webhook clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${HOSTNAME}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${HOSTNAME}-key.pem\" EOF # Create the kubelet unit file: cat << EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 \\\\ --hostname-override=${HOSTNAME} \\\\ --allow-privileged=true Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kubelet sudo systemctl start kubelet sudo systemctl status kubelet Kube-Proxy # You can configure the kube-proxy service like so. Run these commands on both worker nodes: sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig # Create the kube-proxy config file: cat << EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"10.200.0.0/16\" EOF # Create the kube-proxy unit file: cat << EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kube-proxy sudo systemctl start kube-proxy sudo systemctl status kube-proxy Commands Components Status # Use kubectl to check componentstatuses: kubectl get componentstatuses --kubeconfig admin.kubeconfig # You should get output that looks like this: NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} Enable HTTP Health Checks Using Nginx Load Balancer On Master Nodes # You can set up a basic nginx proxy for the healthz endpoint by first installing nginx\" sudo apt-get install -y nginx # Create an nginx configuration for the health check proxy: cat > kubernetes.default.svc.cluster.local << EOF server { listen 80; server_name kubernetes.default.svc.cluster.local; location /healthz { proxy_pass https://127.0.0.1:6443/healthz; proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem; } } EOF # Set up the proxy configuration so that it is loaded by nginx: sudo mv kubernetes.default.svc.cluster.local /etc/nginx/sites-available/kubernetes.default.svc.cluster.local sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/ sudo systemctl restart nginx sudo systemctl enable nginx # You can verify that everything is working like so: curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz","title":"Control Plane"},{"location":"containers/kubernetes/admin/controlplane/#master","text":"","title":"Master"},{"location":"containers/kubernetes/admin/controlplane/#install-binaries","text":"# You can install the control plane binaries on each master node like this: sudo mkdir -p /etc/kubernetes/config wget -q --timestamping \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-apiserver\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-controller-manager\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-scheduler\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl\" chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/","title":"Install Binaries"},{"location":"containers/kubernetes/admin/controlplane/#api-server","text":"","title":"API-Server"},{"location":"containers/kubernetes/admin/controlplane/#setting-up","text":"# You can configure the Kubernetes API server like so: sudo mkdir -p /var/lib/kubernetes/ sudo cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/ # Set some environment variables that will be used to create the systemd unit file. Make sure you replace the placeholders with their actual values: export INTERNAL_IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4) export CONTROLLER0_IP=<private ip of controller 0> export CONTROLLER1_IP=<private ip of controller 1> # Generate the kube-apiserver unit file for systemd: cat << EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://$CONTROLLER0_IP:2379,https://$CONTROLLER1_IP:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 \\\\ --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-apiserver sudo systemctl start kube-apiserver sudo systemctl status kube-apiserver","title":"Setting Up"},{"location":"containers/kubernetes/admin/controlplane/#controller-manager","text":"","title":"Controller Manager"},{"location":"containers/kubernetes/admin/controlplane/#setting-up_1","text":"# You can configure the Kubernetes Controller Manager like so: sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/ # Generate the kube-controller-manager systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-controller-manager sudo systemctl start kube-controller-manager sudo systemctl status kube-controller-manager","title":"Setting Up"},{"location":"containers/kubernetes/admin/controlplane/#scheduler","text":"","title":"Scheduler"},{"location":"containers/kubernetes/admin/controlplane/#setting-up_2","text":"# Copy kube-scheduler.kubeconfig into the proper location: sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/ # Generate the kube-scheduler yaml config file. cat << EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: componentconfig/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: true EOF # Create the kube-scheduler systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-scheduler sudo systemctl start kube-scheduler sudo systemctl status kube-scheduler","title":"Setting Up"},{"location":"containers/kubernetes/admin/controlplane/#default-scheduler","text":"Does the node have adequate hardware resources? Is the node running out of resources? Does the pod request a specific node? Does the node have a matching label? If the pod requests a port, is it available? If the pod requests a volume, can it be mounted? Does the pod tolerate the taints of the node? Does the pod specify node or pod affinity?","title":"Default Scheduler"},{"location":"containers/kubernetes/admin/controlplane/#custom-scheduler","text":"","title":"Custom Scheduler"},{"location":"containers/kubernetes/admin/controlplane/#clusterrole","text":"apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: csinodes-admin rules: - apiGroups: [\"storage.k8s.io\"] resources: [\"csinodes\"] verbs: [\"get\", \"watch\", \"list\"]","title":"ClusterRole"},{"location":"containers/kubernetes/admin/controlplane/#clusterrolebinding","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-csinodes-global subjects: - kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: csinodes-admin apiGroup: rbac.authorization.k8s.io","title":"ClusterRoleBinding"},{"location":"containers/kubernetes/admin/controlplane/#role","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: system:serviceaccount:kube-system:my-scheduler namespace: kube-system rules: - apiGroups: - storage.k8s.io resources: - csinodes verbs: - get - list - watch","title":"Role"},{"location":"containers/kubernetes/admin/controlplane/#rolebinding","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-csinodes namespace: kube-system subjects: - kind: User name: kubernetes-admin apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: system:serviceaccount:kube-system:my-scheduler apiGroup: rbac.authorization.k8s.io","title":"RoleBinding"},{"location":"containers/kubernetes/admin/controlplane/#my-scheduler","text":"apiVersion: v1 kind: ServiceAccount metadata: name: my-scheduler namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: my-scheduler-as-kube-scheduler subjects: - kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-scheduler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: labels: component: scheduler tier: control-plane name: my-scheduler namespace: kube-system spec: selector: matchLabels: component: scheduler tier: control-plane replicas: 1 template: metadata: labels: component: scheduler tier: control-plane version: second spec: serviceAccountName: my-scheduler containers: - command: - /usr/local/bin/kube-scheduler - --address=0.0.0.0 - --leader-elect=false - --scheduler-name=my-scheduler image: chadmcrowell/custom-scheduler livenessProbe: httpGet: path: /healthz port: 10251 initialDelaySeconds: 15 name: kube-second-scheduler readinessProbe: httpGet: path: /healthz port: 10251 resources: requests: cpu: '0.1' securityContext: privileged: false volumeMounts: [] hostNetwork: false hostPID: false volumes: []","title":"My-scheduler"},{"location":"containers/kubernetes/admin/controlplane/#deploy","text":"kubectl create -f clusterrole.yaml kubectl create -f clusterrolebinding.yaml kubectl create -f role.yaml kubectl create -f rolebinding.yaml kubectl edit clusterrole system:kube-scheduler # And add the following - apiGroups: - \"\" resourceNames: - kube-scheduler - my-scheduler resources: - endpoints verbs: - delete - get - patch - update - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - watch - list - get kubectl create -f my-scheduler.yaml kubectl get pods -n kube-system","title":"Deploy"},{"location":"containers/kubernetes/admin/controlplane/#get-config","text":"kubectl get endpoints kube-scheduler -n kube-system -o yaml","title":"Get config"},{"location":"containers/kubernetes/admin/controlplane/#troubleshooting","text":"kubectl describe pods [scheduler_pod_name] -n kube-system kubectl logs [kube_scheduler_pod_name] -n kube-system cat /var/log/kube-scheduler.log","title":"Troubleshooting"},{"location":"containers/kubernetes/admin/controlplane/#rbac","text":"","title":"RBAC"},{"location":"containers/kubernetes/admin/controlplane/#setting-up_3","text":"# Create a role with the necessary permissions: cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF # Bind the role to the kubernetes user cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF","title":"Setting UP"},{"location":"containers/kubernetes/admin/controlplane/#nodes","text":"","title":"Nodes"},{"location":"containers/kubernetes/admin/controlplane/#install-binaries_1","text":"# You can install the worker binaries like so. Run these commands on both worker nodes: sudo apt-get -y install socat conntrack ipset wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.0/containerd-1.1.0.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.0.linux-amd64.tar.gz -C /","title":"Install Binaries"},{"location":"containers/kubernetes/admin/controlplane/#containerd","text":"# You can configure the containerd service like so. Run these commands on both worker nodes: sudo mkdir -p /etc/containerd/ # Create the containerd config.toml: cat << EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\" [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runsc\" runtime_root = \"/run/containerd/runsc\" EOF # Create the containerd unit file: cat << EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/bin/containerd Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable containerd sudo systemctl start containerd sudo systemctl status containerd","title":"ContainerD"},{"location":"containers/kubernetes/admin/controlplane/#kubelet","text":"# Set a HOSTNAME environment variable that will be used to generate your config files. Make sure you set the HOSTNAME appropriately for each worker node: export HOSTNAME=$(hostname) sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/ sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ # Create the kubelet config file: cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" authorization: mode: Webhook clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${HOSTNAME}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${HOSTNAME}-key.pem\" EOF # Create the kubelet unit file: cat << EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 \\\\ --hostname-override=${HOSTNAME} \\\\ --allow-privileged=true Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kubelet sudo systemctl start kubelet sudo systemctl status kubelet","title":"Kubelet"},{"location":"containers/kubernetes/admin/controlplane/#kube-proxy","text":"# You can configure the kube-proxy service like so. Run these commands on both worker nodes: sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig # Create the kube-proxy config file: cat << EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"10.200.0.0/16\" EOF # Create the kube-proxy unit file: cat << EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kube-proxy sudo systemctl start kube-proxy sudo systemctl status kube-proxy","title":"Kube-Proxy"},{"location":"containers/kubernetes/admin/controlplane/#commands","text":"","title":"Commands"},{"location":"containers/kubernetes/admin/controlplane/#components-status","text":"# Use kubectl to check componentstatuses: kubectl get componentstatuses --kubeconfig admin.kubeconfig # You should get output that looks like this: NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"}","title":"Components Status"},{"location":"containers/kubernetes/admin/controlplane/#enable-http-health-checks","text":"","title":"Enable HTTP Health Checks"},{"location":"containers/kubernetes/admin/controlplane/#using-nginx-load-balancer","text":"On Master Nodes # You can set up a basic nginx proxy for the healthz endpoint by first installing nginx\" sudo apt-get install -y nginx # Create an nginx configuration for the health check proxy: cat > kubernetes.default.svc.cluster.local << EOF server { listen 80; server_name kubernetes.default.svc.cluster.local; location /healthz { proxy_pass https://127.0.0.1:6443/healthz; proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem; } } EOF # Set up the proxy configuration so that it is loaded by nginx: sudo mv kubernetes.default.svc.cluster.local /etc/nginx/sites-available/kubernetes.default.svc.cluster.local sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/ sudo systemctl restart nginx sudo systemctl enable nginx # You can verify that everything is working like so: curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz","title":"Using Nginx Load Balancer"},{"location":"containers/kubernetes/admin/etcd/","text":"etcdctl Install wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz tar xvf etcd-v3.3.12-linux-amd64.tar.gz sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin Snapshot sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View that the snapshot was successful ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db Using Docker Container Find Command Parameters ps -ef | grep etcd Get All Keys docker exec -it 3606376c1aba /bin/sh -c \"export ETCDCTL_API=3 && etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\" etcd Create Etcd Cluster # Here are the commands used in the demo (note that these have to be run on both controller servers, with a few differences between them): wget -q --show-progress --https-only --timestamping \\ \"https://github.com/coreos/etcd/releases/download/v3.3.5/etcd-v3.3.5-linux-amd64.tar.gz\" tar -xvf etcd-v3.3.5-linux-amd64.tar.gz sudo mv etcd-v3.3.5-linux-amd64/etcd* /usr/local/bin/ sudo mkdir -p /etc/etcd /var/lib/etcd sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ # Set up the following environment variables. Be sure you replace all of the <placeholder values> with their corresponding real values: export ETCD_NAME=<cloud server hostname> export INTERNAL_IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4) export INITIAL_CLUSTER=<controller 1 hostname>=https://<controller 1 private ip>:2380,<controller 2 hostname>=https://<controller 2 private ip>:2380 # Create the systemd unit file for etcd using this command. Note that this command uses the environment variables that were set earlier: cat << EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster ${INITIAL_CLUSTER} \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the etcd service: sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd # You can verify that the etcd service started up successfully like so: sudo systemctl status etcd # Use this command to verify that etcd is working correctly. The output should list your two etcd nodes: sudo ETCDCTL_API=3 etcdctl member list \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pem","title":"Etcd"},{"location":"containers/kubernetes/admin/etcd/#etcdctl","text":"","title":"etcdctl"},{"location":"containers/kubernetes/admin/etcd/#install","text":"wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz tar xvf etcd-v3.3.12-linux-amd64.tar.gz sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin","title":"Install"},{"location":"containers/kubernetes/admin/etcd/#snapshot","text":"sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View that the snapshot was successful ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db","title":"Snapshot"},{"location":"containers/kubernetes/admin/etcd/#using-docker-container","text":"","title":"Using Docker Container"},{"location":"containers/kubernetes/admin/etcd/#find-command-parameters","text":"ps -ef | grep etcd","title":"Find Command Parameters"},{"location":"containers/kubernetes/admin/etcd/#get-all-keys","text":"docker exec -it 3606376c1aba /bin/sh -c \"export ETCDCTL_API=3 && etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\"","title":"Get All Keys"},{"location":"containers/kubernetes/admin/etcd/#etcd","text":"","title":"etcd"},{"location":"containers/kubernetes/admin/etcd/#create-etcd-cluster","text":"# Here are the commands used in the demo (note that these have to be run on both controller servers, with a few differences between them): wget -q --show-progress --https-only --timestamping \\ \"https://github.com/coreos/etcd/releases/download/v3.3.5/etcd-v3.3.5-linux-amd64.tar.gz\" tar -xvf etcd-v3.3.5-linux-amd64.tar.gz sudo mv etcd-v3.3.5-linux-amd64/etcd* /usr/local/bin/ sudo mkdir -p /etc/etcd /var/lib/etcd sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ # Set up the following environment variables. Be sure you replace all of the <placeholder values> with their corresponding real values: export ETCD_NAME=<cloud server hostname> export INTERNAL_IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4) export INITIAL_CLUSTER=<controller 1 hostname>=https://<controller 1 private ip>:2380,<controller 2 hostname>=https://<controller 2 private ip>:2380 # Create the systemd unit file for etcd using this command. Note that this command uses the environment variables that were set earlier: cat << EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster ${INITIAL_CLUSTER} \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the etcd service: sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd # You can verify that the etcd service started up successfully like so: sudo systemctl status etcd # Use this command to verify that etcd is working correctly. The output should list your two etcd nodes: sudo ETCDCTL_API=3 etcdctl member list \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pem","title":"Create Etcd Cluster"},{"location":"containers/kubernetes/admin/monitoring/","text":"Monitoring Cluster Components Metrics Server git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/ # Get a response from the metrics server API: kubectl get --raw /apis/metrics.k8s.io/ kubectl top node kubectl top pods kubectl top pods --all-namespaces kubectl top pods -n kube-system kubectl top pod -l run=pod-with-defaults kubectl top pod pod-with-defaults # Get the CPU and memory of the containers inside the pod kubectl top pods group-context --containers Applications Liveness Probe apiVersion: v1 kind: Pod metadata: name: liveness spec: containers: - image: linuxacademycontent/kubeserve name: kubeserve livenessProbe: httpGet: path: / port: 80 apiVersion: v1 kind: Pod metadata: name: my-liveness-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \"echo Hello, Kubernetes! && sleep 3600\"] livenessProbe: exec: command: - echo - testing initialDelaySeconds: 5 periodSeconds: 5 Readiness Probe apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx --- apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Pod metadata: name: nginxpd labels: app: nginx spec: containers: - name: nginx image: nginx:191 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5 Logs Cluster Dirs The directory where the continainer logs reside ls /var/log/containers The directory where kubelet stores its logs ls /var/log SideCar Container The YAML for a sidecar container that will tail the logs for each type apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log'] volumeMounts: - name: varlog mountPath: /var/log - name: count-log-2 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log'] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} kubectl logs counter count-log-1 kubectl logs counter count-log-2 Application # Get the logs from a pod: kubectl logs nginx # Get the logs from a specific container on a pod: kubectl logs counter -c count-log-1 # Get the logs from all containers on the pod: kubectl logs counter --all-containers=true # Get the logs from containers with a certain label: kubectl logs -lapp=nginx # Get the logs from a previously terminated container within a pod: kubectl logs -p -c nginx nginx # Stream the logs from a container in a pod: kubectl logs -f -c count-log-1 counter # Tail the logs to only view a certain number of lines: kubectl logs --tail=20 nginx # View the logs from a previous time duration: kubectl logs --since=1h nginx # View the logs from a container within a pod within a deployment: kubectl logs deployment/nginx -c nginx # Redirect the output of the logs to a file: kubectl logs counter -c count-log-1 > count.log Troubleshooting Applications Use Termination Reason apiVersion: v1 kind: Pod metadata: name: pod2 spec: containers: - image: busybox name: main command: - sh - -c - 'echo \"I''ve had enough\" > /var/termination-reason ; exit 1' terminationMessagePath: /var/termination-reason Healthz Not all pods have healthz configured apiVersion: v1 kind: Pod metadata: name: liveness spec: containers: - image: linuxacademycontent/candy-service:2 name: kubeserve livenessProbe: httpGet: path: /healthz port: 8081 Steps kubectl describe po pod2 kubectl logs pod-with-defaults kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml Cluster # Check the events in the kube-system namespace for errors kubectl get events -n kube-system kubectl logs [kube_scheduler_pod_name] -n kube-system # Check the status of the Docker service: sudo systemctl status docker sudo systemctl enable docker && systemctl start docker # Check the status of the kubelet service: sudo systemctl status kubelet sudo systemctl enable kubelet && systemctl start kubelet # Turn off swap on your machine sudo su - swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # Check if you have a firewall running: sudo systemctl status firewalld sudo systemctl disable firewalld && systemctl stop firewalld Worker Node kubectl get nodes kubectl describe nodes chadcrowell2c.mylabserver.com # Create New Worker Server # Generate a new token after spinning up a new server: sudo kubeadm token generate # Create the kubeadm join command for your new worker node: # sudo kubeadm token create [token_name] --ttl 2h --print-join-command # View the journalctl logs: sudo journalctl -u kubelet # View the syslogs: sudo more syslog | tail -120 | grep kubelet Networking DNS # Run an interactive busybox pod: kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 sh # From the pod, check if DNS is resolving hostnames: nslookup hostnames # From the pod, cat out the /etc/resolv.conf file: cat /etc/resolv.conf # From the pod, look up the DNS name of the Kubernetes service: nslookup kubernetes.default nslookup kube-dns.kube-system.svc.cluster.loca # Look up a service in your Kubernetes cluster nslookup [pod-ip-address].default.pod.cluster.local # Logs Core Dns kubectl logs [coredns-pod-name] Kube-Proxy # View the endpoints for your service: kubectl get ep # Communicate with the pod directly (without the service): wget -qO- 10.244.1.6:9376 # Check if kube-proxy is running on the nodes: ps auxw | grep kube-proxy # Check if kube-proxy is writing iptables: kubectl get services -o wide iptables-save | grep hostnames sudo iptables-save | grep KUBE | grep <service-name> # View the list of kube-system pods: kubectl get pods -n kube-system # Connect to your kube-proxy pod in the kube-system namespace: kubectl exec -it kube-proxy-cqptg -n kube-system -- sh Change CNI Plugin # Delete the flannel CNI plugin: kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml # Apply the Weave Net CNI plugin: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Smoke Testing Data Encryption # Create a test secret: kubectl create secret generic kubernetes-the-hard-way --from-literal=\"mykey=mydata\" # Log in to one of your master servers, and get the raw data for the test secret from etcd: sudo ETCDCTL_API=3 etcdctl get \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pem\\ /registry/secrets/default/kubernetes-the-hard-way | hexdump -C # Your output should look something like this: 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret| 00000010 73 2f 64 65 66 61 75 6c 74 2f 6b 75 62 65 72 6e |s/default/kubern| 00000020 65 74 65 73 2d 74 68 65 2d 68 61 72 64 2d 77 61 |etes-the-hard-wa| 00000030 79 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 |y.k8s:enc:aescbc| 00000040 3a 76 31 3a 6b 65 79 31 3a fc 21 ee dc e5 84 8a |:v1:key1:.!.....| 00000050 53 8e fd a9 72 a8 75 25 65 30 55 0e 72 43 1f 20 |S...r.u%e0U.rC. | 00000060 9f 07 15 4f 69 8a 79 a4 70 62 e9 ab f9 14 93 2e |...Oi.y.pb......| 00000070 e5 59 3f ab a7 b2 d8 d6 05 84 84 aa c3 6f 8d 5c |.Y?..........o.\\| 00000080 09 7a 2f 82 81 b5 d5 ec ba c7 23 34 46 d9 43 02 |.z/.......#4F.C.| 00000090 88 93 57 26 66 da 4e 8e 5c 24 44 6e 3e ec 9c 8e |..W&f.N.\\$Dn>...| 000000a0 83 ff 40 9a fb 94 07 3c 08 52 0e 77 50 81 c9 d0 |..@....<.R.wP...| 000000b0 b7 30 68 ba b1 b3 26 eb b1 9f 3f f1 d7 76 86 09 |.0h...&...?..v..| 000000c0 d8 14 02 12 09 30 b0 60 b2 ad dc bb cf f5 77 e0 |.....0.`......w.| 000000d0 4f 0b 1f 74 79 c1 e7 20 1d 32 b2 68 01 19 93 fc |O..ty.. .2.h....| 000000e0 f5 c8 8b 0b 16 7b 4f c2 6a 0a |.....{O.j.| 000000ea # Look for k8s:enc:aescbc:v1:key1 on the right of the output to verify that the data is stored in an encrypted format! Deployments # Create a a simple nginx deployment: kubectl run nginx --image=nginx # Verify that the deployment created a pod and that the pod is running: kubectl get pods -l run=nginx # Verify that the output looks something like this: NAME READY STATUS RESTARTS AGE nginx-65899c769f-9xnqm 1/1 Running 0 30s Port Forwarding # First, get the pod name of the nginx pod and store it an an environment variable: POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\") # Forward port 8081 to the nginx pod: kubectl port-forward $POD_NAME 8081:80 # Open up a new terminal on the same machine running the kubectl port-forward command and verify that the port forward works. curl --head http://127.0.0.1:8081 # You should get an http 200 OK response from the nginx pod. Logs # First, let's set an environment variable to the name of the nginx pod: POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\") # Get the logs from the nginx pod: kubectl logs $POD_NAME # This command should return the nginx pod's logs. It will look something like this (but there could be more lines): 127.0.0.1 - - [10/Sep/2018:19:29:01 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\" Exec # First, let's set an environment variable to the name of the nginx pod: POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\") # To test kubectl exec, execute a simple nginx -v command inside the nginx pod: kubectl exec -ti $POD_NAME -- nginx -v # This command should return the nginx version output, which should look like this: nginx version: nginx/1.15.3 Services # First, create a service to expose the nginx deployment: kubectl expose deployment nginx --port 80 --type NodePort # Get the node port assigned to the newly-created service and assign it to an environment variable: kubectl get svc # The output should look something like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 20d nginx NodePort 10.32.0.81 <none> 80:32642/TCP 2m # Look for the service called nginx in that output. Under PORT(S), look for the second port, listed after 80:. In the example above, it is 32642. That is the node port, so make note of that value since you will need it in a moment. # Next, log in to one of your worker servers and make a request to the service using the node port. Be sure to replace the placeholder with the actual node port: curl -I localhost:<node port> # You should get an http 200 OK response. Untrusted Workloads # First, create an untrusted pod: cat << EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: untrusted annotations: io.kubernetes.cri.untrusted-workload: \"true\" spec: containers: - name: webserver image: gcr.io/hightowerlabs/helloworld:2.0.0 EOF # Make sure that the untrusted pod is running: kubectl get pods untrusted -o wide # Take note of which worker node the untrusted pod is running on, then log into that worker node. # On the worker node, list all of the containers running under gVisor: sudo runsc --root /run/containerd/runsc/k8s.io list # Get the pod ID of the untrusted pod and store it in an environment variable: POD_ID=$(sudo crictl -r unix:///var/run/containerd/containerd.sock \\ pods --name untrusted -q) # Get the container ID of the container running in the untrusted pod and store it in an environment variable: CONTAINER_ID=$(sudo crictl -r unix:///var/run/containerd/containerd.sock \\ ps -p ${POD_ID} -q) # Get information about the process running in the container: sudo runsc --root /run/containerd/runsc/k8s.io ps ${CONTAINER_ID} # Since we were able to get the process info using runsc, we know that the untrusted container is running securely as expected.","title":"Monitoring"},{"location":"containers/kubernetes/admin/monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"containers/kubernetes/admin/monitoring/#cluster-components","text":"","title":"Cluster Components"},{"location":"containers/kubernetes/admin/monitoring/#metrics-server","text":"git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/ # Get a response from the metrics server API: kubectl get --raw /apis/metrics.k8s.io/ kubectl top node kubectl top pods kubectl top pods --all-namespaces kubectl top pods -n kube-system kubectl top pod -l run=pod-with-defaults kubectl top pod pod-with-defaults # Get the CPU and memory of the containers inside the pod kubectl top pods group-context --containers","title":"Metrics Server"},{"location":"containers/kubernetes/admin/monitoring/#applications","text":"","title":"Applications"},{"location":"containers/kubernetes/admin/monitoring/#liveness-probe","text":"apiVersion: v1 kind: Pod metadata: name: liveness spec: containers: - image: linuxacademycontent/kubeserve name: kubeserve livenessProbe: httpGet: path: / port: 80 apiVersion: v1 kind: Pod metadata: name: my-liveness-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \"echo Hello, Kubernetes! && sleep 3600\"] livenessProbe: exec: command: - echo - testing initialDelaySeconds: 5 periodSeconds: 5","title":"Liveness Probe"},{"location":"containers/kubernetes/admin/monitoring/#readiness-probe","text":"apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx --- apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Pod metadata: name: nginxpd labels: app: nginx spec: containers: - name: nginx image: nginx:191 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5","title":"Readiness Probe"},{"location":"containers/kubernetes/admin/monitoring/#logs","text":"","title":"Logs"},{"location":"containers/kubernetes/admin/monitoring/#cluster","text":"","title":"Cluster"},{"location":"containers/kubernetes/admin/monitoring/#dirs","text":"The directory where the continainer logs reside ls /var/log/containers The directory where kubelet stores its logs ls /var/log","title":"Dirs"},{"location":"containers/kubernetes/admin/monitoring/#sidecar-container","text":"The YAML for a sidecar container that will tail the logs for each type apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log'] volumeMounts: - name: varlog mountPath: /var/log - name: count-log-2 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log'] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} kubectl logs counter count-log-1 kubectl logs counter count-log-2","title":"SideCar Container"},{"location":"containers/kubernetes/admin/monitoring/#application","text":"# Get the logs from a pod: kubectl logs nginx # Get the logs from a specific container on a pod: kubectl logs counter -c count-log-1 # Get the logs from all containers on the pod: kubectl logs counter --all-containers=true # Get the logs from containers with a certain label: kubectl logs -lapp=nginx # Get the logs from a previously terminated container within a pod: kubectl logs -p -c nginx nginx # Stream the logs from a container in a pod: kubectl logs -f -c count-log-1 counter # Tail the logs to only view a certain number of lines: kubectl logs --tail=20 nginx # View the logs from a previous time duration: kubectl logs --since=1h nginx # View the logs from a container within a pod within a deployment: kubectl logs deployment/nginx -c nginx # Redirect the output of the logs to a file: kubectl logs counter -c count-log-1 > count.log","title":"Application"},{"location":"containers/kubernetes/admin/monitoring/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"containers/kubernetes/admin/monitoring/#applications_1","text":"","title":"Applications"},{"location":"containers/kubernetes/admin/monitoring/#use-termination-reason","text":"apiVersion: v1 kind: Pod metadata: name: pod2 spec: containers: - image: busybox name: main command: - sh - -c - 'echo \"I''ve had enough\" > /var/termination-reason ; exit 1' terminationMessagePath: /var/termination-reason","title":"Use Termination Reason"},{"location":"containers/kubernetes/admin/monitoring/#healthz","text":"Not all pods have healthz configured apiVersion: v1 kind: Pod metadata: name: liveness spec: containers: - image: linuxacademycontent/candy-service:2 name: kubeserve livenessProbe: httpGet: path: /healthz port: 8081","title":"Healthz"},{"location":"containers/kubernetes/admin/monitoring/#steps","text":"kubectl describe po pod2 kubectl logs pod-with-defaults kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml","title":"Steps"},{"location":"containers/kubernetes/admin/monitoring/#cluster_1","text":"# Check the events in the kube-system namespace for errors kubectl get events -n kube-system kubectl logs [kube_scheduler_pod_name] -n kube-system # Check the status of the Docker service: sudo systemctl status docker sudo systemctl enable docker && systemctl start docker # Check the status of the kubelet service: sudo systemctl status kubelet sudo systemctl enable kubelet && systemctl start kubelet # Turn off swap on your machine sudo su - swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # Check if you have a firewall running: sudo systemctl status firewalld sudo systemctl disable firewalld && systemctl stop firewalld","title":"Cluster"},{"location":"containers/kubernetes/admin/monitoring/#worker-node","text":"kubectl get nodes kubectl describe nodes chadcrowell2c.mylabserver.com # Create New Worker Server # Generate a new token after spinning up a new server: sudo kubeadm token generate # Create the kubeadm join command for your new worker node: # sudo kubeadm token create [token_name] --ttl 2h --print-join-command # View the journalctl logs: sudo journalctl -u kubelet # View the syslogs: sudo more syslog | tail -120 | grep kubelet","title":"Worker Node"},{"location":"containers/kubernetes/admin/monitoring/#networking","text":"","title":"Networking"},{"location":"containers/kubernetes/admin/monitoring/#dns","text":"# Run an interactive busybox pod: kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 sh # From the pod, check if DNS is resolving hostnames: nslookup hostnames # From the pod, cat out the /etc/resolv.conf file: cat /etc/resolv.conf # From the pod, look up the DNS name of the Kubernetes service: nslookup kubernetes.default nslookup kube-dns.kube-system.svc.cluster.loca # Look up a service in your Kubernetes cluster nslookup [pod-ip-address].default.pod.cluster.local # Logs Core Dns kubectl logs [coredns-pod-name]","title":"DNS"},{"location":"containers/kubernetes/admin/monitoring/#kube-proxy","text":"# View the endpoints for your service: kubectl get ep # Communicate with the pod directly (without the service): wget -qO- 10.244.1.6:9376 # Check if kube-proxy is running on the nodes: ps auxw | grep kube-proxy # Check if kube-proxy is writing iptables: kubectl get services -o wide iptables-save | grep hostnames sudo iptables-save | grep KUBE | grep <service-name> # View the list of kube-system pods: kubectl get pods -n kube-system # Connect to your kube-proxy pod in the kube-system namespace: kubectl exec -it kube-proxy-cqptg -n kube-system -- sh","title":"Kube-Proxy"},{"location":"containers/kubernetes/admin/monitoring/#change-cni-plugin","text":"# Delete the flannel CNI plugin: kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml # Apply the Weave Net CNI plugin: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"","title":"Change CNI Plugin"},{"location":"containers/kubernetes/admin/monitoring/#smoke-testing","text":"","title":"Smoke Testing"},{"location":"containers/kubernetes/admin/monitoring/#data-encryption","text":"# Create a test secret: kubectl create secret generic kubernetes-the-hard-way --from-literal=\"mykey=mydata\" # Log in to one of your master servers, and get the raw data for the test secret from etcd: sudo ETCDCTL_API=3 etcdctl get \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pem\\ /registry/secrets/default/kubernetes-the-hard-way | hexdump -C # Your output should look something like this: 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret| 00000010 73 2f 64 65 66 61 75 6c 74 2f 6b 75 62 65 72 6e |s/default/kubern| 00000020 65 74 65 73 2d 74 68 65 2d 68 61 72 64 2d 77 61 |etes-the-hard-wa| 00000030 79 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 |y.k8s:enc:aescbc| 00000040 3a 76 31 3a 6b 65 79 31 3a fc 21 ee dc e5 84 8a |:v1:key1:.!.....| 00000050 53 8e fd a9 72 a8 75 25 65 30 55 0e 72 43 1f 20 |S...r.u%e0U.rC. | 00000060 9f 07 15 4f 69 8a 79 a4 70 62 e9 ab f9 14 93 2e |...Oi.y.pb......| 00000070 e5 59 3f ab a7 b2 d8 d6 05 84 84 aa c3 6f 8d 5c |.Y?..........o.\\| 00000080 09 7a 2f 82 81 b5 d5 ec ba c7 23 34 46 d9 43 02 |.z/.......#4F.C.| 00000090 88 93 57 26 66 da 4e 8e 5c 24 44 6e 3e ec 9c 8e |..W&f.N.\\$Dn>...| 000000a0 83 ff 40 9a fb 94 07 3c 08 52 0e 77 50 81 c9 d0 |..@....<.R.wP...| 000000b0 b7 30 68 ba b1 b3 26 eb b1 9f 3f f1 d7 76 86 09 |.0h...&...?..v..| 000000c0 d8 14 02 12 09 30 b0 60 b2 ad dc bb cf f5 77 e0 |.....0.`......w.| 000000d0 4f 0b 1f 74 79 c1 e7 20 1d 32 b2 68 01 19 93 fc |O..ty.. .2.h....| 000000e0 f5 c8 8b 0b 16 7b 4f c2 6a 0a |.....{O.j.| 000000ea # Look for k8s:enc:aescbc:v1:key1 on the right of the output to verify that the data is stored in an encrypted format!","title":"Data Encryption"},{"location":"containers/kubernetes/admin/monitoring/#deployments","text":"# Create a a simple nginx deployment: kubectl run nginx --image=nginx # Verify that the deployment created a pod and that the pod is running: kubectl get pods -l run=nginx # Verify that the output looks something like this: NAME READY STATUS RESTARTS AGE nginx-65899c769f-9xnqm 1/1 Running 0 30s","title":"Deployments"},{"location":"containers/kubernetes/admin/monitoring/#port-forwarding","text":"# First, get the pod name of the nginx pod and store it an an environment variable: POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\") # Forward port 8081 to the nginx pod: kubectl port-forward $POD_NAME 8081:80 # Open up a new terminal on the same machine running the kubectl port-forward command and verify that the port forward works. curl --head http://127.0.0.1:8081 # You should get an http 200 OK response from the nginx pod.","title":"Port Forwarding"},{"location":"containers/kubernetes/admin/monitoring/#logs_1","text":"# First, let's set an environment variable to the name of the nginx pod: POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\") # Get the logs from the nginx pod: kubectl logs $POD_NAME # This command should return the nginx pod's logs. It will look something like this (but there could be more lines): 127.0.0.1 - - [10/Sep/2018:19:29:01 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\"","title":"Logs"},{"location":"containers/kubernetes/admin/monitoring/#exec","text":"# First, let's set an environment variable to the name of the nginx pod: POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\") # To test kubectl exec, execute a simple nginx -v command inside the nginx pod: kubectl exec -ti $POD_NAME -- nginx -v # This command should return the nginx version output, which should look like this: nginx version: nginx/1.15.3","title":"Exec"},{"location":"containers/kubernetes/admin/monitoring/#services","text":"# First, create a service to expose the nginx deployment: kubectl expose deployment nginx --port 80 --type NodePort # Get the node port assigned to the newly-created service and assign it to an environment variable: kubectl get svc # The output should look something like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 20d nginx NodePort 10.32.0.81 <none> 80:32642/TCP 2m # Look for the service called nginx in that output. Under PORT(S), look for the second port, listed after 80:. In the example above, it is 32642. That is the node port, so make note of that value since you will need it in a moment. # Next, log in to one of your worker servers and make a request to the service using the node port. Be sure to replace the placeholder with the actual node port: curl -I localhost:<node port> # You should get an http 200 OK response.","title":"Services"},{"location":"containers/kubernetes/admin/monitoring/#untrusted-workloads","text":"# First, create an untrusted pod: cat << EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: untrusted annotations: io.kubernetes.cri.untrusted-workload: \"true\" spec: containers: - name: webserver image: gcr.io/hightowerlabs/helloworld:2.0.0 EOF # Make sure that the untrusted pod is running: kubectl get pods untrusted -o wide # Take note of which worker node the untrusted pod is running on, then log into that worker node. # On the worker node, list all of the containers running under gVisor: sudo runsc --root /run/containerd/runsc/k8s.io list # Get the pod ID of the untrusted pod and store it in an environment variable: POD_ID=$(sudo crictl -r unix:///var/run/containerd/containerd.sock \\ pods --name untrusted -q) # Get the container ID of the container running in the untrusted pod and store it in an environment variable: CONTAINER_ID=$(sudo crictl -r unix:///var/run/containerd/containerd.sock \\ ps -p ${POD_ID} -q) # Get information about the process running in the container: sudo runsc --root /run/containerd/runsc/k8s.io ps ${CONTAINER_ID} # Since we were able to get the process info using runsc, we know that the untrusted container is running securely as expected.","title":"Untrusted Workloads"},{"location":"containers/kubernetes/admin/networking/","text":"Flannel Redhat kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Ubuntu On all nodes echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Master kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml Weave Net Set up # First, log in to both worker nodes and enable IP forwarding: sudo sysctl net.ipv4.conf.all.forwarding=1 echo \"net.ipv4.conf.all.forwarding=1\" | sudo tee -a /etc/sysctl.conf # The remaining commands can be done using kubectl. To connect with kubectl, you can either log in to one of the control nodes and run kubectl there or open an SSH tunnel for port 6443 to the load balancer server and use kubectl locally. # You can open the SSH tunnel by running this in a separate terminal. Leave the session open while you are working to keep the tunnel active: ssh -L 6443:localhost:6443 user@<your Load balancer cloud server public IP> kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')&env.IPALLOC_RANGE=10.200.0.0/16\" # Now Weave Net is installed, but we need to test our network to make sure everything is working. # First, make sure the Weave Net pods are up and running: kubectl get pods -n kube-system #cThis should return two Weave Net pods, and look something like this: NAME READY STATUS RESTARTS AGE weave-net-m69xq 2/2 Running 0 11s weave-net-vmb2n 2/2 Running 0 11s # Next, we want to test that pods can connect to each other and that they can connect to services. We will set up two Nginx pods and a service for those two pods. Then, we will create a busybox pod and use it to test connectivity to both Nginx pods and the service. # First, create an Nginx deployment with 2 replicas: cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: run: nginx replicas: 2 template: metadata: labels: run: nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EOF # Next, create a service for that deployment so that we can test connectivity to services as well: # kubectl expose deployment/nginx # Now let's start up another pod. We will use this pod to test our networking. We will test whether we can connect to the other pods and services from this pod. kubectl run busybox --image=radial/busyboxplus:curl --command -- sleep 3600 POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\"{.items[0].metadata.name}\") # Now let's get the IP addresses of our two Nginx pods: kubectl get ep nginx # There should be two IP addresses listed under ENDPOINTS, for example: NAME ENDPOINTS AGE nginx 10.200.0.2:80,10.200.128.1:80 50m # Now let's make sure the busybox pod can connect to the Nginx pods on both of those IP addresses. kubectl exec $POD_NAME -- curl <first nginx pod IP address> kubectl exec $POD_NAME -- curl <second nginx pod IP address> # Both commands should return some HTML with the title \"Welcome to Nginx!\" This means that we can successfully connect to other pods. # Now let's verify that we can connect to services. kubectl get svc # This should display the IP address for our Nginx service. For example, in this case, the IP is 10.32.0.54: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 1h nginx ClusterIP 10.32.0.54 <none> 80/TCP 53m # Let's see if we can access the service from the busybox pod! kubectl exec $POD_NAME -- curl <nginx service IP address> Network Policies Plugin Canal wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml Deny All apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress Pod Selector apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - podSelector: matchLabels: app: web ports: - port: 5432 Namespace Policy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ns-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - namespaceSelector: matchLabels: tenant: web ports: - port: 5432 Block IP apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ipblock-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - ipBlock: cidr: 192.168.1.0/24 Egress Policy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: egress-netpol spec: podSelector: matchLabels: app: web egress: - to: - podSelector: matchLabels: app: db ports: - port: 5432 DNS Kube-Dns kubectl create -f https://storage.googleapis.com/kubernetes-the-hard-way/kube-dns.yaml # Verify that the kube-dns pod starts up correctly: kubectl get pods -l k8s-app=kube-dns -n kube-system # You should get output showing the kube-dns pod. It should look something like this: NAME READY STATUS RESTARTS AGE kube-dns-598d7bf7d4-spbmj 3/3 Running 0 36s # Make sure that 3/3 containers are ready, and that the pod has a status of Running. It may take a moment for the pod to be fully up and running, so if READY is not 3/3 at first, check again after a few moments. # Now let's test our kube-dns installation by doing a DNS lookup from within a pod. First, we need to start up a pod that we can use for testing: kubectl run busybox --image=busybox:1.28 --command -- sleep 3600 POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\"{.items[0].metadata.name}\") # Next, run an nslookup from inside the busybox container: kubectl exec -ti $POD_NAME -- nslookup kubernetes # You should get output that looks something like this: Server: 10.32.0.10 Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local Custom DNS apiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsPolicy: \"None\" dnsConfig: nameservers: - 8.8.8.8 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: \"2\" - name: edns0","title":"Networking"},{"location":"containers/kubernetes/admin/networking/#flannel","text":"","title":"Flannel"},{"location":"containers/kubernetes/admin/networking/#redhat","text":"kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml","title":"Redhat"},{"location":"containers/kubernetes/admin/networking/#ubuntu","text":"On all nodes echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Master kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml","title":"Ubuntu"},{"location":"containers/kubernetes/admin/networking/#weave-net","text":"","title":"Weave Net"},{"location":"containers/kubernetes/admin/networking/#set-up","text":"# First, log in to both worker nodes and enable IP forwarding: sudo sysctl net.ipv4.conf.all.forwarding=1 echo \"net.ipv4.conf.all.forwarding=1\" | sudo tee -a /etc/sysctl.conf # The remaining commands can be done using kubectl. To connect with kubectl, you can either log in to one of the control nodes and run kubectl there or open an SSH tunnel for port 6443 to the load balancer server and use kubectl locally. # You can open the SSH tunnel by running this in a separate terminal. Leave the session open while you are working to keep the tunnel active: ssh -L 6443:localhost:6443 user@<your Load balancer cloud server public IP> kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')&env.IPALLOC_RANGE=10.200.0.0/16\" # Now Weave Net is installed, but we need to test our network to make sure everything is working. # First, make sure the Weave Net pods are up and running: kubectl get pods -n kube-system #cThis should return two Weave Net pods, and look something like this: NAME READY STATUS RESTARTS AGE weave-net-m69xq 2/2 Running 0 11s weave-net-vmb2n 2/2 Running 0 11s # Next, we want to test that pods can connect to each other and that they can connect to services. We will set up two Nginx pods and a service for those two pods. Then, we will create a busybox pod and use it to test connectivity to both Nginx pods and the service. # First, create an Nginx deployment with 2 replicas: cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: run: nginx replicas: 2 template: metadata: labels: run: nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EOF # Next, create a service for that deployment so that we can test connectivity to services as well: # kubectl expose deployment/nginx # Now let's start up another pod. We will use this pod to test our networking. We will test whether we can connect to the other pods and services from this pod. kubectl run busybox --image=radial/busyboxplus:curl --command -- sleep 3600 POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\"{.items[0].metadata.name}\") # Now let's get the IP addresses of our two Nginx pods: kubectl get ep nginx # There should be two IP addresses listed under ENDPOINTS, for example: NAME ENDPOINTS AGE nginx 10.200.0.2:80,10.200.128.1:80 50m # Now let's make sure the busybox pod can connect to the Nginx pods on both of those IP addresses. kubectl exec $POD_NAME -- curl <first nginx pod IP address> kubectl exec $POD_NAME -- curl <second nginx pod IP address> # Both commands should return some HTML with the title \"Welcome to Nginx!\" This means that we can successfully connect to other pods. # Now let's verify that we can connect to services. kubectl get svc # This should display the IP address for our Nginx service. For example, in this case, the IP is 10.32.0.54: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 1h nginx ClusterIP 10.32.0.54 <none> 80/TCP 53m # Let's see if we can access the service from the busybox pod! kubectl exec $POD_NAME -- curl <nginx service IP address>","title":"Set up"},{"location":"containers/kubernetes/admin/networking/#network-policies","text":"","title":"Network Policies"},{"location":"containers/kubernetes/admin/networking/#plugin-canal","text":"wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml","title":"Plugin Canal"},{"location":"containers/kubernetes/admin/networking/#deny-all","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress","title":"Deny All"},{"location":"containers/kubernetes/admin/networking/#pod-selector","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - podSelector: matchLabels: app: web ports: - port: 5432","title":"Pod Selector"},{"location":"containers/kubernetes/admin/networking/#namespace-policy","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ns-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - namespaceSelector: matchLabels: tenant: web ports: - port: 5432","title":"Namespace Policy"},{"location":"containers/kubernetes/admin/networking/#block-ip","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ipblock-netpolicy spec: podSelector: matchLabels: app: db ingress: - from: - ipBlock: cidr: 192.168.1.0/24","title":"Block IP"},{"location":"containers/kubernetes/admin/networking/#egress-policy","text":"apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: egress-netpol spec: podSelector: matchLabels: app: web egress: - to: - podSelector: matchLabels: app: db ports: - port: 5432","title":"Egress Policy"},{"location":"containers/kubernetes/admin/networking/#dns","text":"","title":"DNS"},{"location":"containers/kubernetes/admin/networking/#kube-dns","text":"kubectl create -f https://storage.googleapis.com/kubernetes-the-hard-way/kube-dns.yaml # Verify that the kube-dns pod starts up correctly: kubectl get pods -l k8s-app=kube-dns -n kube-system # You should get output showing the kube-dns pod. It should look something like this: NAME READY STATUS RESTARTS AGE kube-dns-598d7bf7d4-spbmj 3/3 Running 0 36s # Make sure that 3/3 containers are ready, and that the pod has a status of Running. It may take a moment for the pod to be fully up and running, so if READY is not 3/3 at first, check again after a few moments. # Now let's test our kube-dns installation by doing a DNS lookup from within a pod. First, we need to start up a pod that we can use for testing: kubectl run busybox --image=busybox:1.28 --command -- sleep 3600 POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\"{.items[0].metadata.name}\") # Next, run an nslookup from inside the busybox container: kubectl exec -ti $POD_NAME -- nslookup kubernetes # You should get output that looks something like this: Server: 10.32.0.10 Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local","title":"Kube-Dns"},{"location":"containers/kubernetes/admin/networking/#custom-dns","text":"apiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsPolicy: \"None\" dnsConfig: nameservers: - 8.8.8.8 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: \"2\" - name: edns0","title":"Custom DNS"},{"location":"containers/kubernetes/admin/nodes/","text":"Get kubectl get nodes Describe kubectl describe node/node1 kubectl describe node/node2 kubectl describe node/master Api resources kubectl api-resources -o wide Label kubectl label node node1 availability-zone=zone1 kubectl label node node2 share-type=dedicated","title":"Nodes"},{"location":"containers/kubernetes/admin/nodes/#get","text":"kubectl get nodes","title":"Get"},{"location":"containers/kubernetes/admin/nodes/#describe","text":"kubectl describe node/node1 kubectl describe node/node2 kubectl describe node/master","title":"Describe"},{"location":"containers/kubernetes/admin/nodes/#api-resources","text":"kubectl api-resources -o wide","title":"Api resources"},{"location":"containers/kubernetes/admin/nodes/#label","text":"kubectl label node node1 availability-zone=zone1 kubectl label node node2 share-type=dedicated","title":"Label"},{"location":"containers/kubernetes/admin/security/","text":"Service Accounts Get kubectl get serviceaccounts Create kubectl get serviceaccounts kubectl get serviceaccounts jenkins -o yaml Pod Example apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: serviceAccountName: jenkins containers: - image: busybox:1.28.4 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always View the token file from within a pod kubectl get pods -n my-ns kubectl exec -it <name-of-pod> -n my-ns sh cat /var/run/secrets/kubernetes.io/serviceaccount/token Users Create kubectl config view kubectl config set-credentials chad --username=chad --password=password # Create a role binding for anonymous users (not recommended in production): kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous # Need Copy /etc/kubernetes/pki/ca.crt to remote machine # Remote Machine (Install Kubectl) kubectl config set-cluster kubernetes --server=https://172.31.41.61:6443 --certificate-authority=ca.crt --embed-certs=true kubectl config set-credentials chad --username=chad --password=password kubectl config set-context kubernetes --cluster=kubernetes --user=chad --namespace=default kubectl config use-context kubernetes Roles Role apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: web name: service-reader rules: - apiGroups: [\"\"] verbs: [\"get\", \"list\"] resources: [\"services\"] RoleBinding kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web Cluster Roles Cluster Role kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes Cluster Role Binding kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=web:default Test apiVersion: v1 kind: Pod metadata: name: curlpod namespace: web spec: containers: - image: tutum/curl command: [\"sleep\", \"9999999\"] name: main - image: linuxacademycontent/kubectl-proxy name: proxy restartPolicy: Always kubectl apply -f curl-pod.yaml kubectl get pods -n web kubectl exec -it curlpod -n web -- sh curl localhost:8001/api/v1/persistentvolumes TLS Certficates Install cfssl # Download the binaries for the cfssl tool: wget -q --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 # Make the binary files executable: chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson Create Certificate Authority to Kubernetes cd ~/ mkdir kthw cd kthw/ { cat > ca-config.json << EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json << EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca } Generating Client Certificates will generate the following client certificates: admin, kubelet (one for each worker node), kube-controller-manager, kube-proxy, and kube-scheduler Admin Client Certificate { cat > admin-csr.json << EOF { \"CN\": \"admin\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin } Kubelet Client certificates export WORKER0_HOST=<Public hostname of your first worker node cloud server> export WORKER0_IP=<Private IP of your first worker node cloud server> export WORKER1_HOST=<Public hostname of your second worker node cloud server> export WORKER1_IP=<Private IP of your second worker node cloud server> { cat > ${WORKER0_HOST}-csr.json << EOF { \"CN\": \"system:node:${WORKER0_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${WORKER0_IP},${WORKER0_HOST} \\ -profile=kubernetes \\ ${WORKER0_HOST}-csr.json | cfssljson -bare ${WORKER0_HOST} cat > ${WORKER1_HOST}-csr.json << EOF { \"CN\": \"system:node:${WORKER1_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${WORKER1_IP},${WORKER1_HOST} \\ -profile=kubernetes \\ ${WORKER1_HOST}-csr.json | cfssljson -bare ${WORKER1_HOST} } Controller Manager Client certificate { cat > kube-controller-manager-csr.json << EOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager } Kube-proxy Client certificate { cat > kube-proxy-csr.json << EOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy } Kube Scheduler Client Certificate { cat > kube-scheduler-csr.json << EOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler } Generating the Kubernetes API Server Certificate Note: 10.32.0.1 - Common use this IP. Can be used by the pods in some scenarios cd ~/kthw export CERT_HOSTNAME=10.32.0.1,<controller node 1 Private IP>,<controller node 1 hostname>,<controller node 2 Private IP>,<controller node 2 hostname>,<API load balancer Private IP>,<API load balancer hostname>,127.0.0.1,localhost,kubernetes.default { cat > kubernetes-csr.json << EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${CERT_HOSTNAME} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes } Generating the Service Account Key Pair cd ~/kthw { cat > service-account-csr.json << EOF { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account } Distributing the Certificate Files Move certificate files to the worker nodes: scp ca.pem <worker 1 hostname>-key.pem <worker 1 hostname>.pem user@<worker 1 public IP>:~/ scp ca.pem <worker 2 hostname>-key.pem <worker 2 hostname>.pem user@<worker 2 public IP>:~/ Move certificate files to the Master nodes: scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 1 public IP>:~/ scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 2 public IP>:~/ Create TLS For Applications (Not Cluster, only same pods) # Find the CA certificate on a pod in your cluster: kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount cfssl version # Create a CSR file - Need instal cfssl tool cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"my-svc.my-namespace.svc.cluster.local\", \"my-pod.my-namespace.pod.cluster.local\", \"172.168.0.24\", \"10.0.34.2\" ], \"CN\": \"my-pod.my-namespace.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF # Create a CertificateSigningRequest API object: cat <<EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: pod-csr.web spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF # View the CSRs in the cluster: kubectl get csr # View additional details about the CSR: kubectl describe csr pod-csr.web # Approve the CSR: kubectl certificate approve pod-csr.web # View the certificate within your CSR: kubectl get csr pod-csr.web -o yaml # Extract and decode your certificate to use in a file: kubectl get csr pod-csr.web -o jsonpath='{.status.certificate}' \\ | base64 --decode > server.crt Container Registry Create # Create a new docker-registry secret: kubectl create secret docker-registry acr --docker-server=https://podofminerva.azurecr.io --docker-username=podofminerva --docker-password='otj701c9OucKZOCx5qrRblofcNRf3W+e' --docker-email=user@example.com # Modify the default service account to use your new docker-registry secret: kubectl patch sa default -p '{\"imagePullSecrets\": [{\"name\": \"acr\"}]}' apiVersion: v1 kind: Pod metadata: name: acr-pod labels: app: busybox spec: containers: - name: busybox image: podofminerva.azurecr.io/busybox:latest command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600'] imagePullPolicy: Always Security Contexts The YAML for a container that runs as a user apiVersion: v1 kind: Pod metadata: name: alpine-user-context spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 405 The YAML for a pod that runs the container as non-root apiVersion: v1 kind: Pod metadata: name: alpine-nonroot spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsNonRoot: true The YAML for a privileged container pod apiVersion: v1 kind: Pod metadata: name: privileged-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: privileged: true The YAML for a container that will allow you to change the time apiVersion: v1 kind: Pod metadata: name: kernelchange-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: capabilities: add: - SYS_TIME The YAML for a container that removes capabilities apiVersion: v1 kind: Pod metadata: name: remove-capabilities spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: capabilities: drop: - CHOWN The YAML for a pod container that can\u2019t write to the local filesystem apiVersion: v1 kind: Pod metadata: name: readonly-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: readOnlyRootFilesystem: true volumeMounts: - name: my-volume mountPath: /volume readOnly: false volumes: - name: my-volume emptyDir: The YAML for a pod that has different group permissions for different pods apiVersion: v1 kind: Pod metadata: name: group-context spec: securityContext: fsGroup: 555 supplementalGroups: [666, 777] containers: - name: first image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 1111 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false - name: second image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 2222 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false volumes: - name: shared-volume emptyDir: Persistent Key Value Store # Generate a key for your https server: openssl genrsa -out https.key 2048 # Generate a certificate for the https server: openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.example.com # Create an empty file to create the secret: touch file # Create a secret from your key, cert, and file: kubectl create secret generic example-https --from-file=https.key --from-file=https.cert --from-file=file Create the configMap that will mount to your pod apiVersion: v1 kind: ConfigMap metadata: name: config data: my-nginx-config.conf: | server { listen 80; listen 443 ssl; server_name www.example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval: | 25 The YAML for a pod using the new secret apiVersion: v1 kind: Pod metadata: name: example-https spec: containers: - image: linuxacademycontent/fortune name: html-web env: - name: INTERVAL valueFrom: configMapKeyRef: name: config key: sleep-interval volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: certs mountPath: /etc/nginx/certs/ readOnly: true ports: - containerPort: 80 - containerPort: 443 volumes: - name: html emptyDir: {} - name: config configMap: name: config items: - key: my-nginx-config.conf path: https.conf - name: certs secret: secretName: example-https # Use port forwarding on the pod to server traffic from 443: kubectl port-forward example-https 8443:443 & # Curl the web server to get a response: curl https://localhost:8443 -k","title":"Security"},{"location":"containers/kubernetes/admin/security/#service-accounts","text":"","title":"Service Accounts"},{"location":"containers/kubernetes/admin/security/#get","text":"kubectl get serviceaccounts","title":"Get"},{"location":"containers/kubernetes/admin/security/#create","text":"kubectl get serviceaccounts kubectl get serviceaccounts jenkins -o yaml","title":"Create"},{"location":"containers/kubernetes/admin/security/#pod-example","text":"apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: serviceAccountName: jenkins containers: - image: busybox:1.28.4 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always","title":"Pod Example"},{"location":"containers/kubernetes/admin/security/#view-the-token-file-from-within-a-pod","text":"kubectl get pods -n my-ns kubectl exec -it <name-of-pod> -n my-ns sh cat /var/run/secrets/kubernetes.io/serviceaccount/token","title":"View the token file from within a pod"},{"location":"containers/kubernetes/admin/security/#users","text":"","title":"Users"},{"location":"containers/kubernetes/admin/security/#create_1","text":"kubectl config view kubectl config set-credentials chad --username=chad --password=password # Create a role binding for anonymous users (not recommended in production): kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous # Need Copy /etc/kubernetes/pki/ca.crt to remote machine # Remote Machine (Install Kubectl) kubectl config set-cluster kubernetes --server=https://172.31.41.61:6443 --certificate-authority=ca.crt --embed-certs=true kubectl config set-credentials chad --username=chad --password=password kubectl config set-context kubernetes --cluster=kubernetes --user=chad --namespace=default kubectl config use-context kubernetes","title":"Create"},{"location":"containers/kubernetes/admin/security/#roles","text":"","title":"Roles"},{"location":"containers/kubernetes/admin/security/#role","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: web name: service-reader rules: - apiGroups: [\"\"] verbs: [\"get\", \"list\"] resources: [\"services\"]","title":"Role"},{"location":"containers/kubernetes/admin/security/#rolebinding","text":"kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web","title":"RoleBinding"},{"location":"containers/kubernetes/admin/security/#cluster-roles","text":"","title":"Cluster Roles"},{"location":"containers/kubernetes/admin/security/#cluster-role","text":"kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes","title":"Cluster Role"},{"location":"containers/kubernetes/admin/security/#cluster-role-binding","text":"kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=web:default","title":"Cluster Role Binding"},{"location":"containers/kubernetes/admin/security/#test","text":"apiVersion: v1 kind: Pod metadata: name: curlpod namespace: web spec: containers: - image: tutum/curl command: [\"sleep\", \"9999999\"] name: main - image: linuxacademycontent/kubectl-proxy name: proxy restartPolicy: Always kubectl apply -f curl-pod.yaml kubectl get pods -n web kubectl exec -it curlpod -n web -- sh curl localhost:8001/api/v1/persistentvolumes","title":"Test"},{"location":"containers/kubernetes/admin/security/#tls-certficates","text":"","title":"TLS Certficates"},{"location":"containers/kubernetes/admin/security/#install-cfssl","text":"# Download the binaries for the cfssl tool: wget -q --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 # Make the binary files executable: chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson","title":"Install cfssl"},{"location":"containers/kubernetes/admin/security/#create-certificate-authority-to-kubernetes","text":"cd ~/ mkdir kthw cd kthw/ { cat > ca-config.json << EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json << EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca }","title":"Create Certificate Authority to Kubernetes"},{"location":"containers/kubernetes/admin/security/#generating-client-certificates","text":"will generate the following client certificates: admin, kubelet (one for each worker node), kube-controller-manager, kube-proxy, and kube-scheduler Admin Client Certificate { cat > admin-csr.json << EOF { \"CN\": \"admin\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin } Kubelet Client certificates export WORKER0_HOST=<Public hostname of your first worker node cloud server> export WORKER0_IP=<Private IP of your first worker node cloud server> export WORKER1_HOST=<Public hostname of your second worker node cloud server> export WORKER1_IP=<Private IP of your second worker node cloud server> { cat > ${WORKER0_HOST}-csr.json << EOF { \"CN\": \"system:node:${WORKER0_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${WORKER0_IP},${WORKER0_HOST} \\ -profile=kubernetes \\ ${WORKER0_HOST}-csr.json | cfssljson -bare ${WORKER0_HOST} cat > ${WORKER1_HOST}-csr.json << EOF { \"CN\": \"system:node:${WORKER1_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${WORKER1_IP},${WORKER1_HOST} \\ -profile=kubernetes \\ ${WORKER1_HOST}-csr.json | cfssljson -bare ${WORKER1_HOST} } Controller Manager Client certificate { cat > kube-controller-manager-csr.json << EOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager } Kube-proxy Client certificate { cat > kube-proxy-csr.json << EOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy } Kube Scheduler Client Certificate { cat > kube-scheduler-csr.json << EOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler }","title":"Generating Client Certificates"},{"location":"containers/kubernetes/admin/security/#generating-the-kubernetes-api-server-certificate","text":"Note: 10.32.0.1 - Common use this IP. Can be used by the pods in some scenarios cd ~/kthw export CERT_HOSTNAME=10.32.0.1,<controller node 1 Private IP>,<controller node 1 hostname>,<controller node 2 Private IP>,<controller node 2 hostname>,<API load balancer Private IP>,<API load balancer hostname>,127.0.0.1,localhost,kubernetes.default { cat > kubernetes-csr.json << EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${CERT_HOSTNAME} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes }","title":"Generating the Kubernetes API Server Certificate"},{"location":"containers/kubernetes/admin/security/#generating-the-service-account-key-pair","text":"cd ~/kthw { cat > service-account-csr.json << EOF { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account }","title":"Generating the Service Account Key Pair"},{"location":"containers/kubernetes/admin/security/#distributing-the-certificate-files","text":"","title":"Distributing the Certificate Files"},{"location":"containers/kubernetes/admin/security/#move-certificate-files-to-the-worker-nodes","text":"scp ca.pem <worker 1 hostname>-key.pem <worker 1 hostname>.pem user@<worker 1 public IP>:~/ scp ca.pem <worker 2 hostname>-key.pem <worker 2 hostname>.pem user@<worker 2 public IP>:~/","title":"Move certificate files to the worker nodes:"},{"location":"containers/kubernetes/admin/security/#move-certificate-files-to-the-master-nodes","text":"scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 1 public IP>:~/ scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 2 public IP>:~/","title":"Move certificate files to the Master nodes:"},{"location":"containers/kubernetes/admin/security/#create-tls-for-applications-not-cluster-only-same-pods","text":"# Find the CA certificate on a pod in your cluster: kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount cfssl version # Create a CSR file - Need instal cfssl tool cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"my-svc.my-namespace.svc.cluster.local\", \"my-pod.my-namespace.pod.cluster.local\", \"172.168.0.24\", \"10.0.34.2\" ], \"CN\": \"my-pod.my-namespace.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF # Create a CertificateSigningRequest API object: cat <<EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: pod-csr.web spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF # View the CSRs in the cluster: kubectl get csr # View additional details about the CSR: kubectl describe csr pod-csr.web # Approve the CSR: kubectl certificate approve pod-csr.web # View the certificate within your CSR: kubectl get csr pod-csr.web -o yaml # Extract and decode your certificate to use in a file: kubectl get csr pod-csr.web -o jsonpath='{.status.certificate}' \\ | base64 --decode > server.crt","title":"Create TLS For Applications (Not Cluster, only same pods)"},{"location":"containers/kubernetes/admin/security/#container-registry","text":"","title":"Container Registry"},{"location":"containers/kubernetes/admin/security/#create_2","text":"# Create a new docker-registry secret: kubectl create secret docker-registry acr --docker-server=https://podofminerva.azurecr.io --docker-username=podofminerva --docker-password='otj701c9OucKZOCx5qrRblofcNRf3W+e' --docker-email=user@example.com # Modify the default service account to use your new docker-registry secret: kubectl patch sa default -p '{\"imagePullSecrets\": [{\"name\": \"acr\"}]}' apiVersion: v1 kind: Pod metadata: name: acr-pod labels: app: busybox spec: containers: - name: busybox image: podofminerva.azurecr.io/busybox:latest command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600'] imagePullPolicy: Always","title":"Create"},{"location":"containers/kubernetes/admin/security/#security-contexts","text":"The YAML for a container that runs as a user apiVersion: v1 kind: Pod metadata: name: alpine-user-context spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 405 The YAML for a pod that runs the container as non-root apiVersion: v1 kind: Pod metadata: name: alpine-nonroot spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsNonRoot: true The YAML for a privileged container pod apiVersion: v1 kind: Pod metadata: name: privileged-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: privileged: true The YAML for a container that will allow you to change the time apiVersion: v1 kind: Pod metadata: name: kernelchange-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: capabilities: add: - SYS_TIME The YAML for a container that removes capabilities apiVersion: v1 kind: Pod metadata: name: remove-capabilities spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: capabilities: drop: - CHOWN The YAML for a pod container that can\u2019t write to the local filesystem apiVersion: v1 kind: Pod metadata: name: readonly-pod spec: containers: - name: main image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: readOnlyRootFilesystem: true volumeMounts: - name: my-volume mountPath: /volume readOnly: false volumes: - name: my-volume emptyDir: The YAML for a pod that has different group permissions for different pods apiVersion: v1 kind: Pod metadata: name: group-context spec: securityContext: fsGroup: 555 supplementalGroups: [666, 777] containers: - name: first image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 1111 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false - name: second image: alpine command: [\"/bin/sleep\", \"999999\"] securityContext: runAsUser: 2222 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false volumes: - name: shared-volume emptyDir:","title":"Security Contexts"},{"location":"containers/kubernetes/admin/security/#persistent-key-value-store","text":"# Generate a key for your https server: openssl genrsa -out https.key 2048 # Generate a certificate for the https server: openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.example.com # Create an empty file to create the secret: touch file # Create a secret from your key, cert, and file: kubectl create secret generic example-https --from-file=https.key --from-file=https.cert --from-file=file Create the configMap that will mount to your pod apiVersion: v1 kind: ConfigMap metadata: name: config data: my-nginx-config.conf: | server { listen 80; listen 443 ssl; server_name www.example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval: | 25 The YAML for a pod using the new secret apiVersion: v1 kind: Pod metadata: name: example-https spec: containers: - image: linuxacademycontent/fortune name: html-web env: - name: INTERVAL valueFrom: configMapKeyRef: name: config key: sleep-interval volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: certs mountPath: /etc/nginx/certs/ readOnly: true ports: - containerPort: 80 - containerPort: 443 volumes: - name: html emptyDir: {} - name: config configMap: name: config items: - key: my-nginx-config.conf path: https.conf - name: certs secret: secretName: example-https # Use port forwarding on the pod to server traffic from 443: kubectl port-forward example-https 8443:443 & # Curl the web server to get a response: curl https://localhost:8443 -k","title":"Persistent Key Value Store"},{"location":"containers/kubernetes/admin/taint/","text":"Taint Node kubectl taint node <node_name> node-type=prod:NoSchedule Pod with Toleration apiVersion: apps/v1 kind: Deployment metadata: name: prod spec: replicas: 1 selector: matchLabels: app: prod template: metadata: labels: app: prod spec: containers: - args: - sleep - \"3600\" image: busybox name: main tolerations: - key: node-type operator: Equal value: prod effect: NoSchedule","title":"Taint"},{"location":"containers/kubernetes/admin/taint/#taint-node","text":"kubectl taint node <node_name> node-type=prod:NoSchedule","title":"Taint Node"},{"location":"containers/kubernetes/admin/taint/#pod-with-toleration","text":"apiVersion: apps/v1 kind: Deployment metadata: name: prod spec: replicas: 1 selector: matchLabels: app: prod template: metadata: labels: app: prod spec: containers: - args: - sleep - \"3600\" image: busybox name: main tolerations: - key: node-type operator: Equal value: prod effect: NoSchedule","title":"Pod with Toleration"},{"location":"containers/kubernetes/development/daemonsets/","text":"Example File apiVersion: apps/v1beta2 kind: DaemonSet metadata: name: ssd-monitor spec: selector: matchLabels: app: ssd-monitor template: metadata: labels: app: ssd-monitor spec: nodeSelector: disk: ssd containers: - name: main image: linuxacademycontent/ssd-monitor","title":"DaemonSets"},{"location":"containers/kubernetes/development/daemonsets/#example-file","text":"apiVersion: apps/v1beta2 kind: DaemonSet metadata: name: ssd-monitor spec: selector: matchLabels: app: ssd-monitor template: metadata: labels: app: ssd-monitor spec: nodeSelector: disk: ssd containers: - name: main image: linuxacademycontent/ssd-monitor","title":"Example File"},{"location":"containers/kubernetes/development/deployments/","text":"Examples Simple Example apiVersion: apps/v1 kind: Deployment metadata: name: example-deployment labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: darealmc/nginx-k8s:v1 ports: - containerPort: 80 Node affinity apiVersion: extensions/v1beta1 kind: Deployment metadata: name: pref spec: replicas: 5 template: metadata: labels: app: pref spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 preference: matchExpressions: - key: availability-zone operator: In values: - zone1 - weight: 20 preference: matchExpressions: - key: share-type operator: In values: - dedicated containers: - args: - sleep - \"99999\" image: busybox name: main Update Image kubectl set image deployment.v1.apps/example-deployment nginx=darealmc/nginx-k8s:v2 MicroServices Example cd ~/ git clone https://github.com/linuxacademy/robot-shop.git kubectl create namespace robot-shop kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors/ kubectl get pods -n robot-shop -w # Access in http://$kube_server_public_ip:30080 Application LifeCycle Manager Update kubectl apply -f kubeserve-deployment.yaml kubectl replace -f kubeserve-deployment.yaml Rolling Update kubectl set image deployments/kubeserve app=linuxacademycontent/kubeserve:v2 --v 6 Rollback # Use --record flag to create the deployment - kubectl create -f kubeserve-deployment.yaml --record kubectl rollout undo deployments kubeserve kubectl rollout history deployment kubeserve kubectl rollout undo deployment kubeserve --to-revision=2 Pause / Resume kubectl rollout undo deployment kubeserve --to-revision=2 kubectl rollout resume deployment kubeserve Readiness Probe apiVersion: apps/v1 kind: Deployment metadata: name: kubeserve spec: replicas: 3 selector: matchLabels: app: kubeserve minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubeserve labels: app: kubeserve spec: containers: - image: linuxacademycontent/kubeserve:v3 name: app readinessProbe: periodSeconds: 1 httpGet: path: / port: 80","title":"Deployments"},{"location":"containers/kubernetes/development/deployments/#examples","text":"","title":"Examples"},{"location":"containers/kubernetes/development/deployments/#simple-example","text":"apiVersion: apps/v1 kind: Deployment metadata: name: example-deployment labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: darealmc/nginx-k8s:v1 ports: - containerPort: 80","title":"Simple Example"},{"location":"containers/kubernetes/development/deployments/#node-affinity","text":"apiVersion: extensions/v1beta1 kind: Deployment metadata: name: pref spec: replicas: 5 template: metadata: labels: app: pref spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 preference: matchExpressions: - key: availability-zone operator: In values: - zone1 - weight: 20 preference: matchExpressions: - key: share-type operator: In values: - dedicated containers: - args: - sleep - \"99999\" image: busybox name: main","title":"Node affinity"},{"location":"containers/kubernetes/development/deployments/#update-image","text":"kubectl set image deployment.v1.apps/example-deployment nginx=darealmc/nginx-k8s:v2","title":"Update Image"},{"location":"containers/kubernetes/development/deployments/#microservices-example","text":"cd ~/ git clone https://github.com/linuxacademy/robot-shop.git kubectl create namespace robot-shop kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors/ kubectl get pods -n robot-shop -w # Access in http://$kube_server_public_ip:30080","title":"MicroServices Example"},{"location":"containers/kubernetes/development/deployments/#application-lifecycle-manager","text":"","title":"Application LifeCycle Manager"},{"location":"containers/kubernetes/development/deployments/#update","text":"kubectl apply -f kubeserve-deployment.yaml kubectl replace -f kubeserve-deployment.yaml","title":"Update"},{"location":"containers/kubernetes/development/deployments/#rolling-update","text":"kubectl set image deployments/kubeserve app=linuxacademycontent/kubeserve:v2 --v 6","title":"Rolling Update"},{"location":"containers/kubernetes/development/deployments/#rollback","text":"# Use --record flag to create the deployment - kubectl create -f kubeserve-deployment.yaml --record kubectl rollout undo deployments kubeserve kubectl rollout history deployment kubeserve kubectl rollout undo deployment kubeserve --to-revision=2","title":"Rollback"},{"location":"containers/kubernetes/development/deployments/#pause-resume","text":"kubectl rollout undo deployment kubeserve --to-revision=2 kubectl rollout resume deployment kubeserve","title":"Pause / Resume"},{"location":"containers/kubernetes/development/deployments/#readiness-probe","text":"apiVersion: apps/v1 kind: Deployment metadata: name: kubeserve spec: replicas: 3 selector: matchLabels: app: kubeserve minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubeserve labels: app: kubeserve spec: containers: - image: linuxacademycontent/kubeserve:v3 name: app readinessProbe: periodSeconds: 1 httpGet: path: / port: 80","title":"Readiness Probe"},{"location":"containers/kubernetes/development/ingress/","text":"Example File apiVersion: extensions/v1beta1 kind: Ingress metadata: name: service-ingress spec: rules: - host: kubeserve.example.com http: paths: - backend: serviceName: kubeserve2 servicePort: 80 - host: app.example.com http: paths: - backend: serviceName: nginx servicePort: 80 - http: paths: - backend: serviceName: httpd servicePort: 80 Useful Commands kubectl edit ingress kubectl describe ingress","title":"Ingress"},{"location":"containers/kubernetes/development/ingress/#example-file","text":"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: service-ingress spec: rules: - host: kubeserve.example.com http: paths: - backend: serviceName: kubeserve2 servicePort: 80 - host: app.example.com http: paths: - backend: serviceName: nginx servicePort: 80 - http: paths: - backend: serviceName: httpd servicePort: 80","title":"Example File"},{"location":"containers/kubernetes/development/ingress/#useful-commands","text":"kubectl edit ingress kubectl describe ingress","title":"Useful Commands"},{"location":"containers/kubernetes/development/jobs/","text":"Job apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never backoffLimit: 4 CronJob apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure","title":"Jobs"},{"location":"containers/kubernetes/development/jobs/#job","text":"apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never backoffLimit: 4","title":"Job"},{"location":"containers/kubernetes/development/jobs/#cronjob","text":"apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure","title":"CronJob"},{"location":"containers/kubernetes/development/persistentvolumes/","text":"PV HostPath apiVersion: v1 kind: PersistentVolume metadata: name: pv-hostpath spec: storageClassName: local-storage capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\" PV Claim apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mongodb-pvc spec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce storageClassName: \"local-storage\" Deploy Pod apiVersion: v1 kind: Pod metadata: name: mongodb spec: containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP volumes: - name: mongodb-data persistentVolumeClaim: claimName: mongodb-pvc Storage Class apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd Empty Dir apiVersion: v1 kind: Pod metadata: name: emptydir-pod spec: containers: - image: busybox name: busybox command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"] volumeMounts: - mountPath: /tmp/storage name: vol volumes: - name: vol emptyDir: {}","title":"PersistentVolumes"},{"location":"containers/kubernetes/development/persistentvolumes/#pv","text":"","title":"PV"},{"location":"containers/kubernetes/development/persistentvolumes/#hostpath","text":"apiVersion: v1 kind: PersistentVolume metadata: name: pv-hostpath spec: storageClassName: local-storage capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\"","title":"HostPath"},{"location":"containers/kubernetes/development/persistentvolumes/#pv-claim","text":"apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mongodb-pvc spec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce storageClassName: \"local-storage\"","title":"PV Claim"},{"location":"containers/kubernetes/development/persistentvolumes/#deploy-pod","text":"apiVersion: v1 kind: Pod metadata: name: mongodb spec: containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP volumes: - name: mongodb-data persistentVolumeClaim: claimName: mongodb-pvc","title":"Deploy Pod"},{"location":"containers/kubernetes/development/persistentvolumes/#storage-class","text":"apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd","title":"Storage Class"},{"location":"containers/kubernetes/development/persistentvolumes/#empty-dir","text":"apiVersion: v1 kind: Pod metadata: name: emptydir-pod spec: containers: - image: busybox name: busybox command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"] volumeMounts: - mountPath: /tmp/storage name: vol volumes: - name: vol emptyDir: {}","title":"Empty Dir"},{"location":"containers/kubernetes/development/pods/","text":"Pods Get kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl get pods --namespace=podexample -o wide kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system Example File apiVersion: v1 kind: Pod metadata: name: examplepod namespace: pod-example spec: schedulerName: default-scheduler # To change scheduler - my-scheduler volumes: - name: html emptyDir: {} containers: - name: webcontainer image: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html - name: filecontainer image: debian volumeMounts: - name: html mountPath: /html command: [\"/bin/sh\", \"-c\"] args: - while true; do date >> /html/index.html; sleep 1; done Resources Requests and Limits apiVersion: v1 kind: Pod metadata: name: resource-pod2 spec: nodeSelector: kubernetes.io/hostname: \"chadcrowell3c.mylabserver.com\" containers: - image: busybox command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"] name: pod2 resources: requests: cpu: 1000m memory: 20Mi apiVersion: v1 kind: Pod metadata: name: limited-pod spec: containers: - image: busybox command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"] name: main resources: limits: cpu: 1 memory: 20Mi Create From File kubectl create -f ./pod-example.yaml Delete kubectl --namespace=podexample delete pod examplepod Get Containers Name inside a pod kubectl get pods examplepod -n podexample -o jsonpath='{.spec.containers[*].name}*' Exec in Container kubectl exec -ti examplepod -c webcontainer -n podexample /bin/bash Use port forwarding to access a pod directly kubectl port-forward $pod_name 8081:80 Namespaces Get kubectl get namespaces Create kubectl create namespace podexample kubectl get pods examplepod -n podexample -o jsonpath='{.spec.containers[ ].name} '","title":"Pods"},{"location":"containers/kubernetes/development/pods/#pods","text":"","title":"Pods"},{"location":"containers/kubernetes/development/pods/#get","text":"kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl get pods --namespace=podexample -o wide kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system","title":"Get"},{"location":"containers/kubernetes/development/pods/#example-file","text":"apiVersion: v1 kind: Pod metadata: name: examplepod namespace: pod-example spec: schedulerName: default-scheduler # To change scheduler - my-scheduler volumes: - name: html emptyDir: {} containers: - name: webcontainer image: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html - name: filecontainer image: debian volumeMounts: - name: html mountPath: /html command: [\"/bin/sh\", \"-c\"] args: - while true; do date >> /html/index.html; sleep 1; done","title":"Example File"},{"location":"containers/kubernetes/development/pods/#resources-requests-and-limits","text":"apiVersion: v1 kind: Pod metadata: name: resource-pod2 spec: nodeSelector: kubernetes.io/hostname: \"chadcrowell3c.mylabserver.com\" containers: - image: busybox command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"] name: pod2 resources: requests: cpu: 1000m memory: 20Mi apiVersion: v1 kind: Pod metadata: name: limited-pod spec: containers: - image: busybox command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"] name: main resources: limits: cpu: 1 memory: 20Mi","title":"Resources Requests and Limits"},{"location":"containers/kubernetes/development/pods/#create","text":"From File kubectl create -f ./pod-example.yaml","title":"Create"},{"location":"containers/kubernetes/development/pods/#delete","text":"kubectl --namespace=podexample delete pod examplepod","title":"Delete"},{"location":"containers/kubernetes/development/pods/#get-containers-name-inside-a-pod","text":"kubectl get pods examplepod -n podexample -o jsonpath='{.spec.containers[*].name}*'","title":"Get Containers Name inside a pod"},{"location":"containers/kubernetes/development/pods/#exec-in-container","text":"kubectl exec -ti examplepod -c webcontainer -n podexample /bin/bash","title":"Exec in Container"},{"location":"containers/kubernetes/development/pods/#use-port-forwarding-to-access-a-pod-directly","text":"kubectl port-forward $pod_name 8081:80","title":"Use port forwarding to access a pod directly"},{"location":"containers/kubernetes/development/pods/#namespaces","text":"","title":"Namespaces"},{"location":"containers/kubernetes/development/pods/#get_1","text":"kubectl get namespaces","title":"Get"},{"location":"containers/kubernetes/development/pods/#create_1","text":"kubectl create namespace podexample kubectl get pods examplepod -n podexample -o jsonpath='{.spec.containers[ ].name} '","title":"Create"},{"location":"containers/kubernetes/development/replicasets/","text":"Example file apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: nginx tier: frontend spec: replicas: 2 selector: matchLabels: tier: frontend matchExpressions: - {key: tier, operator: In, values: [frontend]} template: metadata: labels: app: nginx tier: frontend spec: containers: - name: nginx image: darealmc/nginx-k8s:v1 ports: - containerPort: 80 Describe kubectl describe rs/frontend Scale kubectl scale rs/frontend --replicas=4 Delete kubectl delete rs/frontend","title":"ReplicaSets"},{"location":"containers/kubernetes/development/replicasets/#example-file","text":"apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: nginx tier: frontend spec: replicas: 2 selector: matchLabels: tier: frontend matchExpressions: - {key: tier, operator: In, values: [frontend]} template: metadata: labels: app: nginx tier: frontend spec: containers: - name: nginx image: darealmc/nginx-k8s:v1 ports: - containerPort: 80","title":"Example file"},{"location":"containers/kubernetes/development/replicasets/#describe","text":"kubectl describe rs/frontend","title":"Describe"},{"location":"containers/kubernetes/development/replicasets/#scale","text":"kubectl scale rs/frontend --replicas=4","title":"Scale"},{"location":"containers/kubernetes/development/replicasets/#delete","text":"kubectl delete rs/frontend","title":"Delete"},{"location":"containers/kubernetes/development/secrets_configmaps/","text":"Config Maps Create kubectl create configmap appconfig --from-literal=key1=value1 --from-literal=key2=value2 Deploy Pods Env Vars apiVersion: v1 kind: Pod metadata: name: configmap-pod spec: containers: - name: app-container image: busybox:1.28 command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] env: - name: MY_VAR valueFrom: configMapKeyRef: name: appconfig key: key1 Volume apiVersion: v1 kind: Pod metadata: name: configmap-volume-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] volumeMounts: - name: configmapvolume mountPath: /etc/config volumes: - name: configmapvolume configMap: name: appconfig Get kubectl get configmaps --all-namespaces kubectl get configmaps -n kube-system/kube-flannel-cfg kubectl get configmaps/kube-flannel-cfg -n kube-system kubectl describe configmaps/kube-flannel-cfg -n kube-system kubectl get configmap appconfig -o yaml Secrets Create apiVersion: v1 kind: Secret metadata: name: appsecret stringData: cert: value key: value Deploy Pods Env Vars apiVersion: v1 kind: Pod metadata: name: secret-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo Hello, Kubernetes! && sleep 3600\"] env: - name: MY_CERT valueFrom: secretKeyRef: name: appsecret key: cert Volume apiVersion: v1 kind: Pod metadata: name: secret-volume-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] volumeMounts: - name: secretvolume mountPath: /etc/certs volumes: - name: secretvolume secret: secretName: appsecret","title":"Secrets&ConfigMaps"},{"location":"containers/kubernetes/development/secrets_configmaps/#config-maps","text":"","title":"Config Maps"},{"location":"containers/kubernetes/development/secrets_configmaps/#create","text":"kubectl create configmap appconfig --from-literal=key1=value1 --from-literal=key2=value2","title":"Create"},{"location":"containers/kubernetes/development/secrets_configmaps/#deploy-pods","text":"Env Vars apiVersion: v1 kind: Pod metadata: name: configmap-pod spec: containers: - name: app-container image: busybox:1.28 command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] env: - name: MY_VAR valueFrom: configMapKeyRef: name: appconfig key: key1 Volume apiVersion: v1 kind: Pod metadata: name: configmap-volume-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] volumeMounts: - name: configmapvolume mountPath: /etc/config volumes: - name: configmapvolume configMap: name: appconfig","title":"Deploy Pods"},{"location":"containers/kubernetes/development/secrets_configmaps/#get","text":"kubectl get configmaps --all-namespaces kubectl get configmaps -n kube-system/kube-flannel-cfg kubectl get configmaps/kube-flannel-cfg -n kube-system kubectl describe configmaps/kube-flannel-cfg -n kube-system kubectl get configmap appconfig -o yaml","title":"Get"},{"location":"containers/kubernetes/development/secrets_configmaps/#secrets","text":"","title":"Secrets"},{"location":"containers/kubernetes/development/secrets_configmaps/#create_1","text":"apiVersion: v1 kind: Secret metadata: name: appsecret stringData: cert: value key: value","title":"Create"},{"location":"containers/kubernetes/development/secrets_configmaps/#deploy-pods_1","text":"Env Vars apiVersion: v1 kind: Pod metadata: name: secret-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo Hello, Kubernetes! && sleep 3600\"] env: - name: MY_CERT valueFrom: secretKeyRef: name: appsecret key: cert Volume apiVersion: v1 kind: Pod metadata: name: secret-volume-pod spec: containers: - name: app-container image: busybox command: ['sh', '-c', \"echo $(MY_VAR) && sleep 3600\"] volumeMounts: - name: secretvolume mountPath: /etc/certs volumes: - name: secretvolume secret: secretName: appsecret","title":"Deploy Pods"},{"location":"containers/kubernetes/development/services/","text":"Cluster IP kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: ClusterIP selector: app: nginx ports: - protocol: TCP port: 32768 targetPort: 80 NodePort kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 32768 # Service Port targetPort: 80 # Pod Port nodePort: 30080 # Node Port LoadBalancer apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx # Study Things # Set the annotation to route load balancer traffic local to the node: kubectl annotate service kubeserve2 externalTrafficPolicy=Local # https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip Headless Test Excert from Kubenertes in Action by Marco Luksa Each connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is? For a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it. apiVersion: v1 kind: Service metadata: name: kube-headless spec: clusterIP: None ports: - port: 80 targetPort: 8080 selector: app: kubserve2","title":"Services"},{"location":"containers/kubernetes/development/services/#cluster-ip","text":"kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: ClusterIP selector: app: nginx ports: - protocol: TCP port: 32768 targetPort: 80","title":"Cluster IP"},{"location":"containers/kubernetes/development/services/#nodeport","text":"kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 32768 # Service Port targetPort: 80 # Pod Port nodePort: 30080 # Node Port","title":"NodePort"},{"location":"containers/kubernetes/development/services/#loadbalancer","text":"apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx # Study Things # Set the annotation to route load balancer traffic local to the node: kubectl annotate service kubeserve2 externalTrafficPolicy=Local # https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip","title":"LoadBalancer"},{"location":"containers/kubernetes/development/services/#headless","text":"Test Excert from Kubenertes in Action by Marco Luksa Each connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is? For a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it. apiVersion: v1 kind: Service metadata: name: kube-headless spec: clusterIP: None ports: - port: 80 targetPort: 8080 selector: app: kubserve2","title":"Headless"},{"location":"containers/kubernetes/development/statefulsets/","text":"Example File apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi Commands kubectl get statefulsets kubectl describe statefulsets","title":"StatefulSets"},{"location":"containers/kubernetes/development/statefulsets/#example-file","text":"apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi","title":"Example File"},{"location":"containers/kubernetes/development/statefulsets/#commands","text":"kubectl get statefulsets kubectl describe statefulsets","title":"Commands"},{"location":"devops/iaas/packer/","text":"Install wget https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip unzip packer_1.4.2_linux_amd64.zip rm packer_1.4.2_linux_amd64.zip sudo mv packer /usr/bin/ packer -v Template packer.json AMI { \"variables\": { \"subnet_id\": \"\", \"instance_size\": \"t2.micro\", \"ami_name\": \"bastion\", \"ssh_username\": \"ec2-user\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"instance_type\": \"{{user `instance_size`}}\", \"ssh_username\": \"{{user `ssh_username`}}\", \"ssh_timeout\": \"20m\", \"ssh_pty\": \"true\", \"ami_name\": \"{{user `ami_name`}}\", \"subnet_id\": \"{{user `subnet_id`}}\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"amzn2-ami-hvm-2.0.*-x86_64-gp2*\", \"root-device-type\": \"ebs\" }, \"owners\": [\"amazon\"], \"most_recent\": true }, \"tags\": { \"Name\": \"{{user `ami_name`}}\", \"BuiltBy\": \"Packer\" } } ], \"description\": \"AWS Bastion AMI\", \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"sudo yum update -y\", \"sudo hostnamectl set-hostname bastion\" ] } ] } Docker { \"variables\": { \"repository\": \"la/express\", \"tag\": \"0.1.0\" }, \"builders\": [ { \"type\": \"docker\", \"author\": \"Fabio Santos\", \"image\": \"node\", \"commit\": \"true\", \"changes\": [ \"EXPOSE 3000\" ] } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"apt-get update -y && apt-get install curl -y\", \"mkdir -p /var/code\", \"cd /root\", \"curl -L https://github.com/linuxacademy/content-nodejs-hello-world/archive/v1.0.tar.gz -o code.tar.gz\", \"tar zxvf code.tar.gz -C /var/code --strip-components=1\", \"cd /var/code\", \"npm install\" ] } ], \"post-processors\": [ { \"type\": \"docker-tag\", \"repository\": \"{{user `repository`}}\", \"tag\": \"{{user `tag`}}\" } ] } Commands packer validate packer.json packer build -var 'tag=0.0.1' -var 'another=212' packer.json # Not Tested it packer fix packer validate","title":"Packer"},{"location":"devops/iaas/packer/#install","text":"wget https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip unzip packer_1.4.2_linux_amd64.zip rm packer_1.4.2_linux_amd64.zip sudo mv packer /usr/bin/ packer -v","title":"Install"},{"location":"devops/iaas/packer/#template","text":"packer.json","title":"Template"},{"location":"devops/iaas/packer/#ami","text":"{ \"variables\": { \"subnet_id\": \"\", \"instance_size\": \"t2.micro\", \"ami_name\": \"bastion\", \"ssh_username\": \"ec2-user\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"instance_type\": \"{{user `instance_size`}}\", \"ssh_username\": \"{{user `ssh_username`}}\", \"ssh_timeout\": \"20m\", \"ssh_pty\": \"true\", \"ami_name\": \"{{user `ami_name`}}\", \"subnet_id\": \"{{user `subnet_id`}}\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"amzn2-ami-hvm-2.0.*-x86_64-gp2*\", \"root-device-type\": \"ebs\" }, \"owners\": [\"amazon\"], \"most_recent\": true }, \"tags\": { \"Name\": \"{{user `ami_name`}}\", \"BuiltBy\": \"Packer\" } } ], \"description\": \"AWS Bastion AMI\", \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"sudo yum update -y\", \"sudo hostnamectl set-hostname bastion\" ] } ] }","title":"AMI"},{"location":"devops/iaas/packer/#docker","text":"{ \"variables\": { \"repository\": \"la/express\", \"tag\": \"0.1.0\" }, \"builders\": [ { \"type\": \"docker\", \"author\": \"Fabio Santos\", \"image\": \"node\", \"commit\": \"true\", \"changes\": [ \"EXPOSE 3000\" ] } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"apt-get update -y && apt-get install curl -y\", \"mkdir -p /var/code\", \"cd /root\", \"curl -L https://github.com/linuxacademy/content-nodejs-hello-world/archive/v1.0.tar.gz -o code.tar.gz\", \"tar zxvf code.tar.gz -C /var/code --strip-components=1\", \"cd /var/code\", \"npm install\" ] } ], \"post-processors\": [ { \"type\": \"docker-tag\", \"repository\": \"{{user `repository`}}\", \"tag\": \"{{user `tag`}}\" } ] }","title":"Docker"},{"location":"devops/iaas/packer/#commands","text":"packer validate packer.json packer build -var 'tag=0.0.1' -var 'another=212' packer.json # Not Tested it packer fix packer validate","title":"Commands"},{"location":"elk/admin/","text":"Deploy Docker-compose git clone https://github.com/maxyermayank/docker-compose-elasticsearch-kibana Know Errors Max virtual memory Desc: Max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] sudo sysctl -w vm.max_map_count=262144 Forbidden Index x-read-only-allow-delete-api Link PUT .kibana/_settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } }","title":"Administration"},{"location":"elk/admin/#deploy","text":"","title":"Deploy"},{"location":"elk/admin/#docker-compose","text":"git clone https://github.com/maxyermayank/docker-compose-elasticsearch-kibana","title":"Docker-compose"},{"location":"elk/admin/#know-errors","text":"","title":"Know Errors"},{"location":"elk/admin/#max-virtual-memory","text":"Desc: Max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] sudo sysctl -w vm.max_map_count=262144","title":"Max virtual memory"},{"location":"elk/admin/#forbidden-index-x-read-only-allow-delete-api","text":"Link PUT .kibana/_settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } }","title":"Forbidden Index x-read-only-allow-delete-api"},{"location":"elk/beats/","text":"Beats Creating Custom Creating New Beat Examples Know Errors bash: mage: command not found Use vendoring We recommend to use vendoring for your beat. This means the dependencies are put into your beat folder. The beats team currently uses govendor for vendoring. govendor init govendor update +e This will create a directory vendor inside your repository. To make sure all dependencies for the Makefile commands are loaded from the vendor directory, find the following line in your Makefile: ES_BEATS=${GOPATH}/src/github.com/elastic/beats Replace it with: ES_BEATS=./vendor/github.com/elastic/beats To Fetch: govendor fetch github.com/vmware/govmomi/^ +out","title":"Beats"},{"location":"elk/beats/#beats","text":"","title":"Beats"},{"location":"elk/beats/#creating-custom","text":"Creating New Beat Examples","title":"Creating Custom"},{"location":"elk/beats/#know-errors","text":"bash: mage: command not found","title":"Know Errors"},{"location":"elk/beats/#use-vendoring","text":"We recommend to use vendoring for your beat. This means the dependencies are put into your beat folder. The beats team currently uses govendor for vendoring. govendor init govendor update +e This will create a directory vendor inside your repository. To make sure all dependencies for the Makefile commands are loaded from the vendor directory, find the following line in your Makefile: ES_BEATS=${GOPATH}/src/github.com/elastic/beats Replace it with: ES_BEATS=./vendor/github.com/elastic/beats To Fetch: govendor fetch github.com/vmware/govmomi/^ +out","title":"Use vendoring"},{"location":"elk/kibana/","text":"Time Series Filters performancemanager.virtualmachines.metric.info.metric: \"cpu.usagemhz.average\" AND NOT performancemanager.virtualmachines.metric.sample.instance: \"*\" AND performancemanager.hosts.metric.sample.instance: \"*\" Timelion Expression .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metric.sample.value,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000).label(\"Disk Provisioned [TB]\").color(black).lines(fill=1,width=2).title(\"Capacity Assessment\"), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000000).label(\"Total Capacity [TB]\").color(yellow).lines(fill=2,width=2), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metric.sample.value,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.used.latest\").divide(1000000000).label(\"Disk Used [TB]\").color(green).lines(fill=3,width=1), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000000).multiply(1.1).label(\"Provisioning Threshold [TB]\").color(red).lines(fill=0,width=3),","title":"Kibana"},{"location":"elk/kibana/#time-series","text":"","title":"Time Series"},{"location":"elk/kibana/#filters","text":"performancemanager.virtualmachines.metric.info.metric: \"cpu.usagemhz.average\" AND NOT performancemanager.virtualmachines.metric.sample.instance: \"*\" AND performancemanager.hosts.metric.sample.instance: \"*\"","title":"Filters"},{"location":"elk/kibana/#timelion","text":"","title":"Timelion"},{"location":"elk/kibana/#expression","text":".es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metric.sample.value,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000).label(\"Disk Provisioned [TB]\").color(black).lines(fill=1,width=2).title(\"Capacity Assessment\"), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000000).label(\"Total Capacity [TB]\").color(yellow).lines(fill=2,width=2), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metric.sample.value,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.used.latest\").divide(1000000000).label(\"Disk Used [TB]\").color(green).lines(fill=3,width=1), .es(index=vspherebeat-emp-imopolis-*, timefield=performancemanager.datastoresclusters.metric.sample.timestamp, metric=sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q=\"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\").divide(1000000000000).multiply(1.1).label(\"Provisioning Threshold [TB]\").color(red).lines(fill=0,width=3),","title":"Expression"},{"location":"elk/queries/","text":"Cluster Health GET /_cluster/health Allocation Errors Explain GET /_cluster/allocation/explain Nodes Stats GET /_nodes/stats Indices List GET /_cat/indices?v List with selected Column GET /_cat/indices?h=creation.date.string Create POST /{index}/_open Close POST /_all/_close Search All GET /vspherebeat/_search { \"query\": { \"match_all\": {} } } Field Match string GET /vspherebeat/_search { \"query\": { \"match\" : { \"performancemanager.hosts.metaData.name\": \"nsvwsdv001.mngt.local\" } } } Field doesn't match string GET /vspherebeat/_search { \"query\": { \"bool\": { \"must_not\": [ { \"match\": { \"performancemanager.virtualmachines.metaData.name\": \"WSTPMNGT007\" } } ] } } } Bool Query (Match and Not Match) GET /vspherebeat/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"performancemanager.virtualmachines.metaData.name\": \"APM-Server\" } }, { \"match\": { \"performancemanager.virtualmachines.metric.info.metric\": \"cpu.usage.average\" } } ], \"must_not\": [ { \"match\": { \"performancemanager.virtualmachines.metric.sample.instance\": \"*\" } } ] } } } Exists Specified Field GET /vspherebeat/_search { \"query\": { \"exists\": { \"field\": \"performancemanager.hosts\" } } } Unique Values from a field GET vspherebeat/_search { \"size\":\"0\", \"aggs\" : { \"uniq_hotsr\" : { \"terms\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } } Total of unique values from a field GET /vspherebeat/_search { \"size\" : 0, \"aggs\" : { \"distinct_hots\" : { \"cardinality\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } } Help Get Indices Columns GET /_cat/indices?help","title":"ESQueries"},{"location":"elk/queries/#cluster","text":"","title":"Cluster"},{"location":"elk/queries/#health","text":"GET /_cluster/health","title":"Health"},{"location":"elk/queries/#allocation-errors-explain","text":"GET /_cluster/allocation/explain","title":"Allocation Errors Explain"},{"location":"elk/queries/#nodes","text":"","title":"Nodes"},{"location":"elk/queries/#stats","text":"GET /_nodes/stats","title":"Stats"},{"location":"elk/queries/#indices","text":"","title":"Indices"},{"location":"elk/queries/#list","text":"GET /_cat/indices?v","title":"List"},{"location":"elk/queries/#list-with-selected-column","text":"GET /_cat/indices?h=creation.date.string","title":"List with selected Column"},{"location":"elk/queries/#create","text":"POST /{index}/_open","title":"Create"},{"location":"elk/queries/#close","text":"POST /_all/_close","title":"Close"},{"location":"elk/queries/#search","text":"","title":"Search"},{"location":"elk/queries/#all","text":"GET /vspherebeat/_search { \"query\": { \"match_all\": {} } }","title":"All"},{"location":"elk/queries/#field-match-string","text":"GET /vspherebeat/_search { \"query\": { \"match\" : { \"performancemanager.hosts.metaData.name\": \"nsvwsdv001.mngt.local\" } } }","title":"Field Match string"},{"location":"elk/queries/#field-doesnt-match-string","text":"GET /vspherebeat/_search { \"query\": { \"bool\": { \"must_not\": [ { \"match\": { \"performancemanager.virtualmachines.metaData.name\": \"WSTPMNGT007\" } } ] } } }","title":"Field doesn't match string"},{"location":"elk/queries/#bool-query-match-and-not-match","text":"GET /vspherebeat/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"performancemanager.virtualmachines.metaData.name\": \"APM-Server\" } }, { \"match\": { \"performancemanager.virtualmachines.metric.info.metric\": \"cpu.usage.average\" } } ], \"must_not\": [ { \"match\": { \"performancemanager.virtualmachines.metric.sample.instance\": \"*\" } } ] } } }","title":"Bool Query (Match and Not Match)"},{"location":"elk/queries/#exists-specified-field","text":"GET /vspherebeat/_search { \"query\": { \"exists\": { \"field\": \"performancemanager.hosts\" } } }","title":"Exists Specified Field"},{"location":"elk/queries/#unique-values-from-a-field","text":"GET vspherebeat/_search { \"size\":\"0\", \"aggs\" : { \"uniq_hotsr\" : { \"terms\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Unique Values from a field"},{"location":"elk/queries/#total-of-unique-values-from-a-field","text":"GET /vspherebeat/_search { \"size\" : 0, \"aggs\" : { \"distinct_hots\" : { \"cardinality\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Total of unique values from a field"},{"location":"elk/queries/#help","text":"","title":"Help"},{"location":"elk/queries/#get-indices-columns","text":"GET /_cat/indices?help","title":"Get Indices Columns"},{"location":"elk/snapshotsAndRestore/","text":"Snapshot Config Change parameter in elastic search config file for all nodes The path.repo needs to be a shared folder beetween the cluster path.repo: [\"/usr/share/elasticsearch/snapshots\"] Create PUT /_snapshot/{repository}/{snapshot}?wait_for_completion=true List all Snapshots from a repo GET /_cat/snapshots/{repository}?v&s=id Delete DELETE /_snapshot/{repository}/{snapshot} Repositories Create PUT /_snapshot/my_backup { \"type\": \"fs\", \"settings\": { \"location\": \"/usr/share/elasticsearch/snapshots/backup\" } } List GET /_cat/repositories?v GET /_snapshot/_all Delete DELETE /_snapshot/{repository} Restore Restore - Official Doc Restore From Snapshot POST /_snapshot/{repository}/{snapshot}/_restore","title":"Snapshots&Restore"},{"location":"elk/snapshotsAndRestore/#snapshot","text":"","title":"Snapshot"},{"location":"elk/snapshotsAndRestore/#config","text":"Change parameter in elastic search config file for all nodes The path.repo needs to be a shared folder beetween the cluster path.repo: [\"/usr/share/elasticsearch/snapshots\"]","title":"Config"},{"location":"elk/snapshotsAndRestore/#create","text":"PUT /_snapshot/{repository}/{snapshot}?wait_for_completion=true","title":"Create"},{"location":"elk/snapshotsAndRestore/#list-all-snapshots-from-a-repo","text":"GET /_cat/snapshots/{repository}?v&s=id","title":"List all Snapshots from a repo"},{"location":"elk/snapshotsAndRestore/#delete","text":"DELETE /_snapshot/{repository}/{snapshot}","title":"Delete"},{"location":"elk/snapshotsAndRestore/#repositories","text":"","title":"Repositories"},{"location":"elk/snapshotsAndRestore/#create_1","text":"PUT /_snapshot/my_backup { \"type\": \"fs\", \"settings\": { \"location\": \"/usr/share/elasticsearch/snapshots/backup\" } }","title":"Create"},{"location":"elk/snapshotsAndRestore/#list","text":"GET /_cat/repositories?v GET /_snapshot/_all","title":"List"},{"location":"elk/snapshotsAndRestore/#delete_1","text":"DELETE /_snapshot/{repository}","title":"Delete"},{"location":"elk/snapshotsAndRestore/#restore","text":"Restore - Official Doc","title":"Restore"},{"location":"elk/snapshotsAndRestore/#restore-from-snapshot","text":"POST /_snapshot/{repository}/{snapshot}/_restore","title":"Restore From Snapshot"},{"location":"unix/commands/disks/","text":"Disks Lists block devices lsblk fdisk -l Create ext4 Filesystem Can use parted command # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 88.5M 1 loop /snap/core/7270 loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1455 xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 8G 0 disk # fdisk /dev/xvdf Command (m for help): n => Create Partition Partition type: p primary (0 primary, 0 extended, 4 free) e extended Command (m for help): p => Print partition Command (m for help): w => Write # mkfs -L projectA -t ext4 /dev/xvdf1 # Format Partition and create label # mkdir /mnt/ProjectA # mount /dev/xvdf1 /mnt/ProjectA/ # vi /etc/fstab LABEL=projectA /mnt/ProjectA/ ext4 defaults 0 0 # Add This Line umount /mnt/ProjectA/ Create Swap Partition Can use parted command # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 1 Hex code (type L to list all codes): 82 Command (m for help): w # mkswap /dev/xvdf1 # swapon /dev/xvdf1 # to shutoff -> swapoff /dev/xvdf2 # free -m # vi /etc/fstab UUID=99fd52d5-e821-45a5-9366-30666046406f swap swap default 0 0 Create LVM Filesystem # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 2 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Syncing disks. # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 8e Linux LVM /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 5 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Syncing disks. # pvcreate /dev/xvdf2 Physical volume \"/dev/xvdf2\" successfully created. # pvcreate /dev/xvdf5 Physical volume \"/dev/xvdf5\" successfully created. # vgcreate VG1 /dev/xvdf2 /dev/xvdf5 Volume group \"VG1\" successfully created root@ip-172-31-81-196:~# vgdisplay /dev/VG1 --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 3.99 GiB PE Size 4.00 MiB Total PE 1022 Alloc PE / Size 0 / 0 Free PE / Size 1022 / 3.99 GiB VG UUID ELopyA-2vai-XuiQ-vm7M-RF9q-k0ve-yUeXM6 # lvcreate VG1 -L +3.9G -n LV1 Rounding up size to full physical extent 3.90 GiB Logical volume \"LV1\" created. root@ip-172-31-81-196:~# lvdisplay --- Logical volume --- LV Path /dev/VG1/LV1 LV Name LV1 VG Name VG1 LV UUID Q0mXgw-q9E7-ncuH-Vv2t-NTqx-epn9-3EK3DH LV Write Access read/write LV Creation host, time ip-172-31-81-196, 2019-08-02 21:45:49 +0000 LV Status available # open 0 LV Size 3.90 GiB Current LE 999 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 # mkfs.ext4 /dev/VG1/LV1 mke2fs 1.44.1 (24-Mar-2018) Creating filesystem with 1022976 4k blocks and 256000 inodes Filesystem UUID: 3cc6d97e-a315-4051-a572-16e06bf9c9f9 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done root@ip-172-31-81-196:~# mkdir /mnt/LV1-mount root@ip-172-31-81-196:~# mount /dev/VG1/LV1 /mnt/LV1-mount root@ip-172-31-81-196:~# vi /etc/fstab /dev/VG1/LV1 /mnt/LV1-mount ext4 defaults 0 0 # Add This Line # Can edit first the /etc/fsatab file and after do mount -a (mount everything in /etc/fstab file) instead mount command Share Disk Install NFS yum install -y nfs-utils systemctl enable nfs systemctl start nfs Master Configure Export /snapshots is a filestem from nfs type # vi /etc/exports /snapshots *(rw) => Add Line # exportfs /snapshots <world> Enable Service firewall-cmd --add-service nfs --permanent firewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-service=mountd firewall-cmd --permanent --add-port=2049/tcp firewall-cmd --permanent --add-port=2049/udp firewall-cmd --reload Slaves # vi /etc/fstab 10.240.100.18:/snapshots /elastic/snapshots nfs _netdev,rw 0 0 # mount -a # mount | grep elastic","title":"Disks"},{"location":"unix/commands/disks/#disks","text":"Lists block devices lsblk fdisk -l","title":"Disks"},{"location":"unix/commands/disks/#create-ext4-filesystem","text":"Can use parted command # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 88.5M 1 loop /snap/core/7270 loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1455 xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 8G 0 disk # fdisk /dev/xvdf Command (m for help): n => Create Partition Partition type: p primary (0 primary, 0 extended, 4 free) e extended Command (m for help): p => Print partition Command (m for help): w => Write # mkfs -L projectA -t ext4 /dev/xvdf1 # Format Partition and create label # mkdir /mnt/ProjectA # mount /dev/xvdf1 /mnt/ProjectA/ # vi /etc/fstab LABEL=projectA /mnt/ProjectA/ ext4 defaults 0 0 # Add This Line umount /mnt/ProjectA/","title":"Create ext4 Filesystem"},{"location":"unix/commands/disks/#create-swap-partition","text":"Can use parted command # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 1 Hex code (type L to list all codes): 82 Command (m for help): w # mkswap /dev/xvdf1 # swapon /dev/xvdf1 # to shutoff -> swapoff /dev/xvdf2 # free -m # vi /etc/fstab UUID=99fd52d5-e821-45a5-9366-30666046406f swap swap default 0 0","title":"Create Swap Partition"},{"location":"unix/commands/disks/#create-lvm-filesystem","text":"# fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 2 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Syncing disks. # fdisk /dev/xvdf Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 8e Linux LVM /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command (m for help): t Partition number (1-3,5-7, default 7): 5 Hex code (type L to list all codes): 8e Changed type of partition 'Linux' to 'Linux LVM'. Command (m for help): w The partition table has been altered. Syncing disks. # pvcreate /dev/xvdf2 Physical volume \"/dev/xvdf2\" successfully created. # pvcreate /dev/xvdf5 Physical volume \"/dev/xvdf5\" successfully created. # vgcreate VG1 /dev/xvdf2 /dev/xvdf5 Volume group \"VG1\" successfully created root@ip-172-31-81-196:~# vgdisplay /dev/VG1 --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 3.99 GiB PE Size 4.00 MiB Total PE 1022 Alloc PE / Size 0 / 0 Free PE / Size 1022 / 3.99 GiB VG UUID ELopyA-2vai-XuiQ-vm7M-RF9q-k0ve-yUeXM6 # lvcreate VG1 -L +3.9G -n LV1 Rounding up size to full physical extent 3.90 GiB Logical volume \"LV1\" created. root@ip-172-31-81-196:~# lvdisplay --- Logical volume --- LV Path /dev/VG1/LV1 LV Name LV1 VG Name VG1 LV UUID Q0mXgw-q9E7-ncuH-Vv2t-NTqx-epn9-3EK3DH LV Write Access read/write LV Creation host, time ip-172-31-81-196, 2019-08-02 21:45:49 +0000 LV Status available # open 0 LV Size 3.90 GiB Current LE 999 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 # mkfs.ext4 /dev/VG1/LV1 mke2fs 1.44.1 (24-Mar-2018) Creating filesystem with 1022976 4k blocks and 256000 inodes Filesystem UUID: 3cc6d97e-a315-4051-a572-16e06bf9c9f9 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done root@ip-172-31-81-196:~# mkdir /mnt/LV1-mount root@ip-172-31-81-196:~# mount /dev/VG1/LV1 /mnt/LV1-mount root@ip-172-31-81-196:~# vi /etc/fstab /dev/VG1/LV1 /mnt/LV1-mount ext4 defaults 0 0 # Add This Line # Can edit first the /etc/fsatab file and after do mount -a (mount everything in /etc/fstab file) instead mount command","title":"Create LVM Filesystem"},{"location":"unix/commands/disks/#share-disk","text":"","title":"Share Disk"},{"location":"unix/commands/disks/#install-nfs","text":"yum install -y nfs-utils systemctl enable nfs systemctl start nfs","title":"Install NFS"},{"location":"unix/commands/disks/#master","text":"","title":"Master"},{"location":"unix/commands/disks/#configure-export","text":"/snapshots is a filestem from nfs type # vi /etc/exports /snapshots *(rw) => Add Line # exportfs /snapshots <world>","title":"Configure Export"},{"location":"unix/commands/disks/#enable-service","text":"firewall-cmd --add-service nfs --permanent firewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-service=mountd firewall-cmd --permanent --add-port=2049/tcp firewall-cmd --permanent --add-port=2049/udp firewall-cmd --reload","title":"Enable Service"},{"location":"unix/commands/disks/#slaves","text":"# vi /etc/fstab 10.240.100.18:/snapshots /elastic/snapshots nfs _netdev,rw 0 0 # mount -a # mount | grep elastic","title":"Slaves"},{"location":"unix/commands/filesystem/","text":"FileSystem Dir Function Extras /dev (udev) Device manager. Contain device files Config in /etc/udev /sys (sysfs) Virtual file system. Info about Hardware devices, drivers /proc (procfs)Similiar to sysfs, but with info about processes and system info Can be used to interface with the kernel. Change parameters on the fly File System space usage df -h Commands Dirs Create Multiple Dirs mkdir -p {networking,compute,storage} Multiple Files in Multiple Dirs touch {networking,compute,storage}/{main.tf,variables.tf,outputs.tf} Find sudo find / -type f -name .gitconfig Inodes Get File Inode ls -i /etc/passwd Find File by Inode find / -inum 55116 /etc/passwd Get Inodes Available df -i Disk Space du -h du -s # To sum the total du -s / du -h --max-depth=1 # Show the disk usage by folder, and not the sub dirs because of Depth = 1 Filesystem fsck (filesystem check) To check errors from filesystem Don't do in mounted filesystem sudo fsck /dev/sdb1 sudo umount /dev/sdb1 dumpe2fs Get info about filesystem sudo dumpe2fs -h /dev/sdb1 tune2fs To get info and tune filesystem tune2fs -l /dev/xvda1 Set Volume Name tune2fs -L Photos /dev/xvda1 Set mount counts will be filesystem checked tune2fs -c 10 /dev/xvda1 fuser Show which process is using the directory # fuser /mount/Photos /mount/Photos: 3157c # ps aux | grep 3157 -- Will see the process, probably /bin/bash # ps auxf -> Show which command executed and the login session called Disk Quotas # vi /etc/fstab LABEL=projectA /mnt/ProjectA/ ext4 defaults,usrquota 0 0 # mount -a # apt-get install quota # cd /mnt/ProjectA # quotacheck -avugc # Create filesystem to support quotas quotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. quotacheck: Scanning /dev/xvdf7 [/mnt/ProjectA] done quotacheck: Cannot stat old user quota file /mnt/ProjectA/aquota.user: No such file or directory. Usage will not be subtracted. quotacheck: Old group file name could not been determined. Usage will not be subtracted. quotacheck: Checked 3 directories and 0 files quotacheck: Old file not found. # edquota -u fsantos # To edit quotas to user # quotaon /mnt/Photos # Enable quotas # quota -v # To see quotas usage # sudo repquota /mnt/Photos # To see a resume from users Tar # List files without extract tar -tvf etcd-v3.3.13-linux-amd64.tar.gz # Extract only one file tar -xvf etcd-v3.3.13-linux-amd64.tar.gz etcd-v3.3.13-linux-amd64/etcdctl","title":"Filesystem"},{"location":"unix/commands/filesystem/#filesystem","text":"Dir Function Extras /dev (udev) Device manager. Contain device files Config in /etc/udev /sys (sysfs) Virtual file system. Info about Hardware devices, drivers /proc (procfs)Similiar to sysfs, but with info about processes and system info Can be used to interface with the kernel. Change parameters on the fly File System space usage df -h","title":"FileSystem"},{"location":"unix/commands/filesystem/#commands","text":"","title":"Commands"},{"location":"unix/commands/filesystem/#dirs","text":"","title":"Dirs"},{"location":"unix/commands/filesystem/#create","text":"Multiple Dirs mkdir -p {networking,compute,storage} Multiple Files in Multiple Dirs touch {networking,compute,storage}/{main.tf,variables.tf,outputs.tf}","title":"Create"},{"location":"unix/commands/filesystem/#find","text":"sudo find / -type f -name .gitconfig","title":"Find"},{"location":"unix/commands/filesystem/#inodes","text":"Get File Inode ls -i /etc/passwd Find File by Inode find / -inum 55116 /etc/passwd Get Inodes Available df -i","title":"Inodes"},{"location":"unix/commands/filesystem/#disk-space","text":"du -h du -s # To sum the total du -s / du -h --max-depth=1 # Show the disk usage by folder, and not the sub dirs because of Depth = 1","title":"Disk Space"},{"location":"unix/commands/filesystem/#filesystem_1","text":"","title":"Filesystem"},{"location":"unix/commands/filesystem/#fsck-filesystem-check","text":"To check errors from filesystem Don't do in mounted filesystem sudo fsck /dev/sdb1 sudo umount /dev/sdb1","title":"fsck (filesystem check)"},{"location":"unix/commands/filesystem/#dumpe2fs","text":"Get info about filesystem sudo dumpe2fs -h /dev/sdb1","title":"dumpe2fs"},{"location":"unix/commands/filesystem/#tune2fs","text":"To get info and tune filesystem tune2fs -l /dev/xvda1 Set Volume Name tune2fs -L Photos /dev/xvda1 Set mount counts will be filesystem checked tune2fs -c 10 /dev/xvda1","title":"tune2fs"},{"location":"unix/commands/filesystem/#fuser","text":"Show which process is using the directory # fuser /mount/Photos /mount/Photos: 3157c # ps aux | grep 3157 -- Will see the process, probably /bin/bash # ps auxf -> Show which command executed and the login session called","title":"fuser"},{"location":"unix/commands/filesystem/#disk-quotas","text":"# vi /etc/fstab LABEL=projectA /mnt/ProjectA/ ext4 defaults,usrquota 0 0 # mount -a # apt-get install quota # cd /mnt/ProjectA # quotacheck -avugc # Create filesystem to support quotas quotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. quotacheck: Scanning /dev/xvdf7 [/mnt/ProjectA] done quotacheck: Cannot stat old user quota file /mnt/ProjectA/aquota.user: No such file or directory. Usage will not be subtracted. quotacheck: Old group file name could not been determined. Usage will not be subtracted. quotacheck: Checked 3 directories and 0 files quotacheck: Old file not found. # edquota -u fsantos # To edit quotas to user # quotaon /mnt/Photos # Enable quotas # quota -v # To see quotas usage # sudo repquota /mnt/Photos # To see a resume from users","title":"Disk Quotas"},{"location":"unix/commands/filesystem/#tar","text":"# List files without extract tar -tvf etcd-v3.3.13-linux-amd64.tar.gz # Extract only one file tar -xvf etcd-v3.3.13-linux-amd64.tar.gz etcd-v3.3.13-linux-amd64/etcdctl","title":"Tar"},{"location":"unix/commands/kernel/","text":"Kernel Modules # To view who use the modules lsmod # To remove sudo rmmod video # To enable sudo modprobe video Libs Shared # Get Shared libs location cat /etc/ld.so.conf # To load new lib ldconfig # Change Library Path temporary export LD_LIBRARY_PATH=/home/nick/lib # Print shared libs from app ldd /bin/ls Hardware PCI Devices # To list pci Devices lspci lspci -v lspci -vvv","title":"Kernel"},{"location":"unix/commands/kernel/#kernel-modules","text":"# To view who use the modules lsmod # To remove sudo rmmod video # To enable sudo modprobe video","title":"Kernel Modules"},{"location":"unix/commands/kernel/#libs","text":"","title":"Libs"},{"location":"unix/commands/kernel/#shared","text":"# Get Shared libs location cat /etc/ld.so.conf # To load new lib ldconfig # Change Library Path temporary export LD_LIBRARY_PATH=/home/nick/lib # Print shared libs from app ldd /bin/ls","title":"Shared"},{"location":"unix/commands/kernel/#hardware","text":"","title":"Hardware"},{"location":"unix/commands/kernel/#pci-devices","text":"# To list pci Devices lspci lspci -v lspci -vvv","title":"PCI Devices"},{"location":"unix/commands/logging/","text":"Syslog Config Files cd /etc/rsyslog.d/ Rotate Files Conf vi /etc/logrotate.conf Journalctl **Log For Services journalctl # All journalctl -b # Last Boot journalctl -b 1 # Before Last Boot journalctl --since \"2 days ago\" journalctl -u kubelet.service vi /etc/system.d/journald.conf # Config Files","title":"Logging"},{"location":"unix/commands/logging/#syslog","text":"","title":"Syslog"},{"location":"unix/commands/logging/#config-files","text":"cd /etc/rsyslog.d/","title":"Config Files"},{"location":"unix/commands/logging/#rotate-files-conf","text":"vi /etc/logrotate.conf","title":"Rotate Files Conf"},{"location":"unix/commands/logging/#journalctl","text":"**Log For Services journalctl # All journalctl -b # Last Boot journalctl -b 1 # Before Last Boot journalctl --since \"2 days ago\" journalctl -u kubelet.service vi /etc/system.d/journald.conf # Config Files","title":"Journalctl"},{"location":"unix/commands/networking/","text":"Config ifconfig ip addr ifconfig ifconfig -a ifdown enp0s3 ifup enp0s3 ifconfig enp0s3 192.168.0.150/24 # Not Permanent Add Secondary IP Link ifconfig eth0:1 172.31.81.196 netmask 255.255.240.0 up # Not Permanent ip ip a ip addr show Remove Secondary Ip ip addr del 172.31.81.196/20 dev eth0:1 # Not Permanent Network-Manager sudo apt install network-manager sudo service network-manager restart nmcli nmcli connection # Connection information # Add Static Route sudo nmcli connection add con-name STATIC ipv4.addresses 192.168.58.1/24 ifname eth0 type ethernet nmcli connection show STATIC sudo nmcli connection modify STATIC +ipv4.routes \"172.16.0.0/16 192.168.58.254\" ipv4.dns 172.16.58.254 route route route -n route add default gw 192.168.0.254 Dns cat /etc/hosts cat /etc/nsswitch.conf # show how priority to will resolve dns cat /etc/resolv.conf cat /etc/dhcp/dhclient.conf sudo service networking restart File cat /etc/network/interfaces Connectivity Netstat $ sudo netstat -nl -p tcp | grep 8123 $ sudo netstat -nl -p tcp | head Get open ports nmap localhost netstat -at SS ss -an | grep -i listen Who listen on Port sudo lsof -i :8000 Test Remote Connection to port nc -v 10.240.100.18 2049 netcat -l 12345 # To listen a Port Get Ports than services are listen cat /etc/services MTR mtr acloud.com DNS host google.com dig google.com","title":"Networking"},{"location":"unix/commands/networking/#config","text":"","title":"Config"},{"location":"unix/commands/networking/#ifconfig","text":"ip addr ifconfig ifconfig -a ifdown enp0s3 ifup enp0s3 ifconfig enp0s3 192.168.0.150/24 # Not Permanent","title":"ifconfig"},{"location":"unix/commands/networking/#add-secondary-ip","text":"Link ifconfig eth0:1 172.31.81.196 netmask 255.255.240.0 up # Not Permanent","title":"Add Secondary IP"},{"location":"unix/commands/networking/#ip","text":"ip a ip addr show","title":"ip"},{"location":"unix/commands/networking/#remove-secondary-ip","text":"ip addr del 172.31.81.196/20 dev eth0:1 # Not Permanent","title":"Remove Secondary Ip"},{"location":"unix/commands/networking/#network-manager","text":"sudo apt install network-manager sudo service network-manager restart nmcli nmcli connection # Connection information # Add Static Route sudo nmcli connection add con-name STATIC ipv4.addresses 192.168.58.1/24 ifname eth0 type ethernet nmcli connection show STATIC sudo nmcli connection modify STATIC +ipv4.routes \"172.16.0.0/16 192.168.58.254\" ipv4.dns 172.16.58.254","title":"Network-Manager"},{"location":"unix/commands/networking/#route","text":"route route -n route add default gw 192.168.0.254","title":"route"},{"location":"unix/commands/networking/#dns","text":"cat /etc/hosts cat /etc/nsswitch.conf # show how priority to will resolve dns cat /etc/resolv.conf cat /etc/dhcp/dhclient.conf sudo service networking restart","title":"Dns"},{"location":"unix/commands/networking/#file","text":"cat /etc/network/interfaces","title":"File"},{"location":"unix/commands/networking/#connectivity","text":"","title":"Connectivity"},{"location":"unix/commands/networking/#netstat","text":"$ sudo netstat -nl -p tcp | grep 8123 $ sudo netstat -nl -p tcp | head","title":"Netstat"},{"location":"unix/commands/networking/#get-open-ports","text":"nmap localhost netstat -at","title":"Get open ports"},{"location":"unix/commands/networking/#ss","text":"ss -an | grep -i listen","title":"SS"},{"location":"unix/commands/networking/#who-listen-on-port","text":"sudo lsof -i :8000","title":"Who listen on Port"},{"location":"unix/commands/networking/#test-remote-connection-to-port","text":"nc -v 10.240.100.18 2049 netcat -l 12345 # To listen a Port","title":"Test Remote Connection to port"},{"location":"unix/commands/networking/#get-ports-than-services-are-listen","text":"cat /etc/services","title":"Get Ports than services are listen"},{"location":"unix/commands/networking/#mtr","text":"mtr acloud.com","title":"MTR"},{"location":"unix/commands/networking/#dns_1","text":"host google.com dig google.com","title":"DNS"},{"location":"unix/commands/security/","text":"Security Host cat /etc/hosts.allow # Hosts allowed to access cat /etc/hosts.deny # Hosts not allowed to access Nologin touch /etc/nologin # Denies login to all users (except root). Need remove the file SE Linux Install Ubuntu # Need remove apparmor in ubuntu systems # make sure you have the most up-to-date info apt-get update apt-get dist-upgrade #disable and remove apparmor /etc/init.d/apparmor stop apt-get remove apparmor #install SELinux apt-get install selinux # install the missing dependency apt-get install auditd # install the activate tool required to make it work apt-get install selinux-basics #missing manual step to actually make SELinux work (part of selinux-basics) selinux-activate Config # Config file cat /etc/selinux/config getenforce setenforce 1 sestatus ls -Z /etc/shadow ps -Z Contexts # List semanage fcontext -l semanage fcontext -l | grep httpd_sys_content_t # Change Context Type chcon -t user_home_dir_t /etc/shadow Booleans getsebool -a semanage boolean -l | sort | less semanage boolean -m -1 httpd_enable_homedirs # or setsebool -P httpd_enable_homedirs=1 Troubleshooting grep http /var/log/audit/audit.log","title":"Security"},{"location":"unix/commands/security/#security","text":"","title":"Security"},{"location":"unix/commands/security/#host","text":"cat /etc/hosts.allow # Hosts allowed to access cat /etc/hosts.deny # Hosts not allowed to access","title":"Host"},{"location":"unix/commands/security/#nologin","text":"touch /etc/nologin # Denies login to all users (except root). Need remove the file","title":"Nologin"},{"location":"unix/commands/security/#se-linux","text":"","title":"SE Linux"},{"location":"unix/commands/security/#install-ubuntu","text":"# Need remove apparmor in ubuntu systems # make sure you have the most up-to-date info apt-get update apt-get dist-upgrade #disable and remove apparmor /etc/init.d/apparmor stop apt-get remove apparmor #install SELinux apt-get install selinux # install the missing dependency apt-get install auditd # install the activate tool required to make it work apt-get install selinux-basics #missing manual step to actually make SELinux work (part of selinux-basics) selinux-activate","title":"Install Ubuntu"},{"location":"unix/commands/security/#config","text":"# Config file cat /etc/selinux/config getenforce setenforce 1 sestatus ls -Z /etc/shadow ps -Z","title":"Config"},{"location":"unix/commands/security/#contexts","text":"# List semanage fcontext -l semanage fcontext -l | grep httpd_sys_content_t # Change Context Type chcon -t user_home_dir_t /etc/shadow","title":"Contexts"},{"location":"unix/commands/security/#booleans","text":"getsebool -a semanage boolean -l | sort | less semanage boolean -m -1 httpd_enable_homedirs # or setsebool -P httpd_enable_homedirs=1","title":"Booleans"},{"location":"unix/commands/security/#troubleshooting","text":"grep http /var/log/audit/audit.log","title":"Troubleshooting"},{"location":"unix/os/archlinux/","text":"Packages Packages Update Package sudo pacman -U <link - see above> Update corrupted or invalid database pacman-key --delete 91FFE0700E80619CEB73235CA88E23E377514E00 pacman-key --populate archlinux Resolve ICU Package Problem Link Download and Install old \"icu\" package; wget https://archive.archlinux.org/packages/i/icu/icu-62.1-1-x86_64.pkg.tar.xz sudo pacman -U icu-62.1-1-x86_64.pkg.tar.xz Copy all \"icu\" files to a backup directory; sudo mkdir /usr/lib/backup sudo cp -r /usr/lib/libicu* /usr/lib/backup/ Install new version of \"icu\" again; sudo pacman -U /var/cache/pacman/pkg/icu-62.1-1-x86_64.pkg.tar.xz Copy this three files back to /var/lib/ direcotry; sudo cp /var/lib/libicui18n.so.61 /usr/lib/ sudo cp /var/lib/libicuuc.so.61 /usr/lib/ sudo cp /var/lib/libicudata.so.61 /usr/lib/ You can now cleanup backup files; sudo rm -rf /var/lib/backup","title":"Archlinux"},{"location":"unix/os/archlinux/#packages","text":"Packages","title":"Packages"},{"location":"unix/os/archlinux/#update-package","text":"sudo pacman -U <link - see above>","title":"Update Package"},{"location":"unix/os/archlinux/#update-corrupted-or-invalid-database","text":"pacman-key --delete 91FFE0700E80619CEB73235CA88E23E377514E00 pacman-key --populate archlinux","title":"Update corrupted or invalid database"},{"location":"unix/os/archlinux/#resolve-icu-package-problem","text":"Link Download and Install old \"icu\" package; wget https://archive.archlinux.org/packages/i/icu/icu-62.1-1-x86_64.pkg.tar.xz sudo pacman -U icu-62.1-1-x86_64.pkg.tar.xz Copy all \"icu\" files to a backup directory; sudo mkdir /usr/lib/backup sudo cp -r /usr/lib/libicu* /usr/lib/backup/ Install new version of \"icu\" again; sudo pacman -U /var/cache/pacman/pkg/icu-62.1-1-x86_64.pkg.tar.xz Copy this three files back to /var/lib/ direcotry; sudo cp /var/lib/libicui18n.so.61 /usr/lib/ sudo cp /var/lib/libicuuc.so.61 /usr/lib/ sudo cp /var/lib/libicudata.so.61 /usr/lib/ You can now cleanup backup files; sudo rm -rf /var/lib/backup","title":"Resolve ICU Package Problem"},{"location":"unix/os/debian/","text":"Know Errors (Ubuntu) Unable to lock the administration directory Desc: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? Solution sudo rm /var/lib/apt/lists/lock dpkg # To list all installed packages dpkg -l | less # To install package sudo dpkg -i dlocate_1.02+nmu3_all.deb # To Remove sudo dpkg --purge dlocate Apt # Sources file cat /etc/apt/sources.list sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade # Better # To Get dependences apt-cache depends apache2 | less","title":"Debian"},{"location":"unix/os/debian/#know-errors-ubuntu","text":"","title":"Know Errors (Ubuntu)"},{"location":"unix/os/debian/#unable-to-lock-the-administration-directory","text":"Desc: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? Solution sudo rm /var/lib/apt/lists/lock","title":"Unable to lock the administration directory"},{"location":"unix/os/debian/#dpkg","text":"# To list all installed packages dpkg -l | less # To install package sudo dpkg -i dlocate_1.02+nmu3_all.deb # To Remove sudo dpkg --purge dlocate","title":"dpkg"},{"location":"unix/os/debian/#apt","text":"# Sources file cat /etc/apt/sources.list sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade # Better # To Get dependences apt-cache depends apache2 | less","title":"Apt"},{"location":"unix/os/redhat/","text":"Recover Root Password Na console send ctrl-alt-del press \"e\" => TO Edit Na linha linux16 -- No final da linha adicionar rd.break press \"ctrl-x\" mount -o remount,rw /sysroot cd /sysroot chroot . touch .autorelabel passwd teste123ibm4 ctrl d ctrl d Permit SSH Root Login vi /etc/ssh/sshd_config # Comment PasswordAuthentication no # Descomment PasswordAuthentication yes systemctl restart sshd Configure Network Interface vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR=172.16.1.180 GATEWAY=172.16.1.16 DNS1=172.16.1.112 DNS2=172.16.1.119 systemctl restart network Package Manager RPM sudo rpm -i wget-xxxxxx.rpm # To see which lib file belongs rpm -qf /etc/protocols # To see if some file is missing from lib sudo rpm --verify setup sudo rpm -Va # To verify entire machine YUM # Config file cat /etc/yum.conf # Repos dir cd /etc/yum.repos.d sudo yum update # To download but not install sudo yum install --downloadonly --downloaddir=/tmp wget sudo yum remove wget","title":"Redhat"},{"location":"unix/os/redhat/#recover-root-password","text":"Na console send ctrl-alt-del press \"e\" => TO Edit Na linha linux16 -- No final da linha adicionar rd.break press \"ctrl-x\" mount -o remount,rw /sysroot cd /sysroot chroot . touch .autorelabel passwd teste123ibm4 ctrl d ctrl d","title":"Recover Root Password"},{"location":"unix/os/redhat/#permit-ssh-root-login","text":"vi /etc/ssh/sshd_config # Comment PasswordAuthentication no # Descomment PasswordAuthentication yes systemctl restart sshd","title":"Permit SSH Root Login"},{"location":"unix/os/redhat/#configure-network-interface","text":"vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR=172.16.1.180 GATEWAY=172.16.1.16 DNS1=172.16.1.112 DNS2=172.16.1.119 systemctl restart network","title":"Configure Network Interface"},{"location":"unix/os/redhat/#package-manager","text":"","title":"Package Manager"},{"location":"unix/os/redhat/#rpm","text":"sudo rpm -i wget-xxxxxx.rpm # To see which lib file belongs rpm -qf /etc/protocols # To see if some file is missing from lib sudo rpm --verify setup sudo rpm -Va # To verify entire machine","title":"RPM"},{"location":"unix/os/redhat/#yum","text":"# Config file cat /etc/yum.conf # Repos dir cd /etc/yum.repos.d sudo yum update # To download but not install sudo yum install --downloadonly --downloaddir=/tmp wget sudo yum remove wget","title":"YUM"},{"location":"unix/software/mail/postfix/","text":"Install sudo apt-get install postfix mutt Aliases # vi /etc/aliases cloudgurus: root, guru Log cat /var/log/mail.log","title":"Postfix"},{"location":"unix/software/mail/postfix/#install","text":"sudo apt-get install postfix mutt","title":"Install"},{"location":"unix/software/mail/postfix/#aliases","text":"# vi /etc/aliases cloudgurus: root, guru","title":"Aliases"},{"location":"unix/software/mail/postfix/#log","text":"cat /var/log/mail.log","title":"Log"},{"location":"unix/software/programming/php/","text":"Install Lamp Server https://www.howtoforge.com/tutorial/centos-lamp-server-apache-mysql-php/ Ioncube Loader https://www.howtoforge.com/tutorial/how-to-install-ioncube-loader/ Cli Get Modules php -m","title":"PHP"},{"location":"unix/software/programming/php/#install","text":"","title":"Install"},{"location":"unix/software/programming/php/#lamp-server","text":"https://www.howtoforge.com/tutorial/centos-lamp-server-apache-mysql-php/","title":"Lamp Server"},{"location":"unix/software/programming/php/#ioncube-loader","text":"https://www.howtoforge.com/tutorial/how-to-install-ioncube-loader/","title":"Ioncube Loader"},{"location":"unix/software/programming/php/#cli","text":"","title":"Cli"},{"location":"unix/software/programming/php/#get-modules","text":"php -m","title":"Get Modules"},{"location":"unix/software/virtualization/virsh/","text":"VM Snapshot $ ssh -l rgameiro 10.242.12.23 $ virsh list --all Id Name State ---------------------------------------------------- 1 bm02vsc01 running 2 bm02alinternal01 running 3 architect running 4 bm02ifwinternal01 running 5 bm02vsd01 running 6 bm02sfinternal02 running 7 bm02acinternal01 running 8 bm02dirinternal01 running - bm02nes01 shut off $ virsh destroy bm02sfinternal02 $ virsh dumpxml bm02sfinternal02 # Find Disk File <domain type='kvm'> <name>bm02sfinternal02</name> <uuid>af7d246d-4578-4660-96fa-ac40c87c24eb</uuid> <memory unit='KiB'>2097152</memory> <currentMemory unit='KiB'>2097152</currentMemory> <vcpu placement='static'>4</vcpu> <os> <type arch='x86_64' machine='pc-i440fx-rhel7.3.0'>hvm</type> <boot dev='hd'/> </os> <features> <acpi/> <apic/> <vmport state='off'/> </features> <cpu mode='host-model' check='partial'> <model fallback='allow'/> </cpu> <clock offset='utc'> <timer name='rtc' tickpolicy='catchup'/> <timer name='pit' tickpolicy='delay'/> <timer name='hpet' present='no'/> </clock> <on_poweroff>destroy</on_poweroff> <on_reboot>restart</on_reboot> <on_crash>restart</on_crash> <pm> <suspend-to-mem enabled='no'/> <suspend-to-disk enabled='no'/> </pm> <devices> <emulator>/usr/libexec/qemu-kvm</emulator> <disk type='file' device='disk'> <driver name='qemu' type='qcow2' cache='none' io='native'/> <source file='/var/lib/libvirt/images/bm02sfinternal02.qcow2'/> <target dev='vda' bus='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/> </disk> <disk type='file' device='cdrom'> <driver name='qemu' type='raw'/> <target dev='hda' bus='ide'/> <readonly/> <address type='drive' controller='0' bus='0' target='0' unit='0'/> </disk> <controller type='usb' index='0' model='ich9-ehci1'> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x7'/> </controller> <controller type='usb' index='0' model='ich9-uhci1'> <master startport='0'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0' multifunction='on'/> </controller> <controller type='usb' index='0' model='ich9-uhci2'> <master startport='2'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x1'/> </controller> <controller type='usb' index='0' model='ich9-uhci3'> <master startport='4'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x2'/> </controller> <controller type='pci' index='0' model='pci-root'/> <controller type='ide' index='0'> <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/> </controller> <controller type='virtio-serial' index='0'> <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/> </controller> <interface type='bridge'> <mac address='52:54:00:84:7e:d0'/> <source bridge='br100'/> <model type='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/> </interface> <serial type='pty'> <target type='isa-serial' port='0'> <model name='isa-serial'/> </target> </serial> <console type='pty'> <target type='serial' port='0'/> </console> <channel type='unix'> <target type='virtio' name='org.qemu.guest_agent.0'/> <address type='virtio-serial' controller='0' bus='0' port='1'/> </channel> <channel type='spicevmc'> <target type='virtio' name='com.redhat.spice.0'/> <address type='virtio-serial' controller='0' bus='0' port='2'/> </channel> <input type='tablet' bus='usb'> <address type='usb' bus='0' port='1'/> </input> <input type='mouse' bus='ps2'/> <input type='keyboard' bus='ps2'/> <graphics type='spice' autoport='yes'> <listen type='address'/> </graphics> <video> <model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1' primary='yes'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/> </video> <redirdev bus='usb' type='spicevmc'> <address type='usb' bus='0' port='2'/> </redirdev> <redirdev bus='usb' type='spicevmc'> <address type='usb' bus='0' port='3'/> </redirdev> <memballoon model='virtio'> <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/> </memballoon> </devices> </domain> $ ls -la /var/lib/libvirt/images/bm02sfinternal02.qcow2 $ df -B 1g # Ensure we have space to copy disk $ cp /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ bg # Por o Processo em Brackground $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 rw------- 1 root root 17607884800 Oct 30 08:21 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 # Terminou [1]+ Done cp -i /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02* $ virsh start bm02sfinternal02","title":"Virsh"},{"location":"unix/software/virtualization/virsh/#vm","text":"","title":"VM"},{"location":"unix/software/virtualization/virsh/#snapshot","text":"$ ssh -l rgameiro 10.242.12.23 $ virsh list --all Id Name State ---------------------------------------------------- 1 bm02vsc01 running 2 bm02alinternal01 running 3 architect running 4 bm02ifwinternal01 running 5 bm02vsd01 running 6 bm02sfinternal02 running 7 bm02acinternal01 running 8 bm02dirinternal01 running - bm02nes01 shut off $ virsh destroy bm02sfinternal02 $ virsh dumpxml bm02sfinternal02 # Find Disk File <domain type='kvm'> <name>bm02sfinternal02</name> <uuid>af7d246d-4578-4660-96fa-ac40c87c24eb</uuid> <memory unit='KiB'>2097152</memory> <currentMemory unit='KiB'>2097152</currentMemory> <vcpu placement='static'>4</vcpu> <os> <type arch='x86_64' machine='pc-i440fx-rhel7.3.0'>hvm</type> <boot dev='hd'/> </os> <features> <acpi/> <apic/> <vmport state='off'/> </features> <cpu mode='host-model' check='partial'> <model fallback='allow'/> </cpu> <clock offset='utc'> <timer name='rtc' tickpolicy='catchup'/> <timer name='pit' tickpolicy='delay'/> <timer name='hpet' present='no'/> </clock> <on_poweroff>destroy</on_poweroff> <on_reboot>restart</on_reboot> <on_crash>restart</on_crash> <pm> <suspend-to-mem enabled='no'/> <suspend-to-disk enabled='no'/> </pm> <devices> <emulator>/usr/libexec/qemu-kvm</emulator> <disk type='file' device='disk'> <driver name='qemu' type='qcow2' cache='none' io='native'/> <source file='/var/lib/libvirt/images/bm02sfinternal02.qcow2'/> <target dev='vda' bus='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/> </disk> <disk type='file' device='cdrom'> <driver name='qemu' type='raw'/> <target dev='hda' bus='ide'/> <readonly/> <address type='drive' controller='0' bus='0' target='0' unit='0'/> </disk> <controller type='usb' index='0' model='ich9-ehci1'> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x7'/> </controller> <controller type='usb' index='0' model='ich9-uhci1'> <master startport='0'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0' multifunction='on'/> </controller> <controller type='usb' index='0' model='ich9-uhci2'> <master startport='2'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x1'/> </controller> <controller type='usb' index='0' model='ich9-uhci3'> <master startport='4'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x2'/> </controller> <controller type='pci' index='0' model='pci-root'/> <controller type='ide' index='0'> <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/> </controller> <controller type='virtio-serial' index='0'> <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/> </controller> <interface type='bridge'> <mac address='52:54:00:84:7e:d0'/> <source bridge='br100'/> <model type='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/> </interface> <serial type='pty'> <target type='isa-serial' port='0'> <model name='isa-serial'/> </target> </serial> <console type='pty'> <target type='serial' port='0'/> </console> <channel type='unix'> <target type='virtio' name='org.qemu.guest_agent.0'/> <address type='virtio-serial' controller='0' bus='0' port='1'/> </channel> <channel type='spicevmc'> <target type='virtio' name='com.redhat.spice.0'/> <address type='virtio-serial' controller='0' bus='0' port='2'/> </channel> <input type='tablet' bus='usb'> <address type='usb' bus='0' port='1'/> </input> <input type='mouse' bus='ps2'/> <input type='keyboard' bus='ps2'/> <graphics type='spice' autoport='yes'> <listen type='address'/> </graphics> <video> <model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1' primary='yes'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/> </video> <redirdev bus='usb' type='spicevmc'> <address type='usb' bus='0' port='2'/> </redirdev> <redirdev bus='usb' type='spicevmc'> <address type='usb' bus='0' port='3'/> </redirdev> <memballoon model='virtio'> <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/> </memballoon> </devices> </domain> $ ls -la /var/lib/libvirt/images/bm02sfinternal02.qcow2 $ df -B 1g # Ensure we have space to copy disk $ cp /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ bg # Por o Processo em Brackground $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 rw------- 1 root root 17607884800 Oct 30 08:21 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 # Terminou [1]+ Done cp -i /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02* $ virsh start bm02sfinternal02","title":"Snapshot"}]}