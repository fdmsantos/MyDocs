{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation \u00b6 Topic Link Markdown https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Mkdocs https://www.mkdocs.org/ Material https://squidfunk.github.io/mkdocs-material/ Mermaid https://mermaidjs.github.io/ phpdoc http://phpdoc2cheatsheet.com/#mult_types Commands \u00b6 py -m mkdocs new [dir-name] - Create a new project. py -m mkdocs serve - Start the live-reloading docs server. py -m mkdocs build - Build the documentation site. py -m mkdocs help - Print this help message.","title":"Index"},{"location":"#documentation","text":"Topic Link Markdown https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Mkdocs https://www.mkdocs.org/ Material https://squidfunk.github.io/mkdocs-material/ Mermaid https://mermaidjs.github.io/ phpdoc http://phpdoc2cheatsheet.com/#mult_types","title":"Documentation"},{"location":"#commands","text":"py -m mkdocs new [dir-name] - Create a new project. py -m mkdocs serve - Start the live-reloading docs server. py -m mkdocs build - Build the documentation site. py -m mkdocs help - Print this help message.","title":"Commands"},{"location":"certifications/aws/bigdata-specialty/","text":"AWS Certified Big Data Specialty \u00b6 Recommended for study: Big Data Technology Fundamentals, Big Data Instructor led training class, Big Data AWS whitepappers and documentation Amazon Kinesis \u00b6 kinesis Streams \u00b6 Collect and process large streams of data in real time Create data-processing applications Read data from stream Process records Send to a dashboard Generate an alert Dynamically change price Dynamically change advertising strategy Scenarios Fast log and data feed intake and processing Real-time metrics and reporting Real-time data analytics Complex stream processing Benefits Real-time aggregation of data Loading the aggregate data into a data warehouse/map reduce cluster Durability and Elasticity Parallel application readers Methods to load/get data Kinesis Producer Library (KPL) Kinesis Client Library (KCL) Kinesis Agent to collect and send Kinesis Rest API Core Concepts \u00b6 Shard Uniquely identified group of data records in a stream Single Shard Capacity 1 MB/Sec data input 2 MB/sec data output 5 transactions/sec for reads 1000 records/sec for writes Can create multiple shards in a stream Stream with 2 shards 2 MB/Sec data input 4 MB/sec data output 10 transactions/sec for reads 2000 records/sec for writes Resharding - Dynamically add or remove shards as data throughput changes Shard Split Increase capacity and costs Shard merge Decrease capacity and costs Records Unit of data stored in a stream A record consists of Partition Key Group the data by shard Tells you which shard the data belongs to Partition Key is specified by the applications putting the data into a stream Sequence Number Unique identifiers for records inserted into a shard Think of its as a unique key that identifies a data blob Assigned when a producer calls PutRecord or PutRecords operations to add data to a stream You can't use sequence numbers to logically separate data in terms of what shards they have come from. You can only do this using partition keys Data Blob Data blob is the data your data producer adds to a stream The maximum size of data blob (the data payload after Base64-decoding) is 1MB Retention Period 24 hours by default Increase this to 7 days if required Change retention period through CLI Data Producers Amazon Kinesis Streams API AWS SDK for Java PutRecord PutRecords Amazon Kinesis Producer Library (KPL) Configurable library to create producer applications that allow developers to achieve high write thoughput to a kinesis stream Write to one of more Kinesis Streams with an auto-retry configurable mechanism Collects records to write multiple records to multiple shards per request Aggregate user records kinesis Client Library integration (de-aggregate records) Monitoring (CloudWatch) Do Not use KPL if..... Your producer application/use case cannot incur an additional processing delay RecordMaxBufferedTime Maximum amount of time a record spends being buffered Larger values result in better performance, but can delay records Batching Aggregation User records and streams records Aggregation allows you to combine multiple users records into a single streams records, helping improve per shard throughput Single Shard capacity is 1 MB/sec (write rate of 1000 records per second) Example: 1000 records (500 bytes each) = 0.5 MB/Sec => Shard not efficient Collection Multiple Streams records are batched and sent in a single HTTP request with a call to the PutRecords API operation Helps increase throughput due to reduced overhead of not making separate HTTP requests Collection Vs Aggregation Collection is working with groups of streams records and batching them to reduce HTTP requests Aggregation allows you to combine multiple user records into a single streams records in order to efficiently use shards Error Handling PutRecords operation sends multiple records to your stream per HTTP request (recommended approach for applications that require high throughput) Single record failure does not stop the processing of subsquent records Despite the failure of a single record, if other records in the PutRecords APi are successful, HTTP status code of 200 is still returned For partial failures, the PutRecordsResult method in the KPL can be used detect individual record failures and retry the PUT based on the HTTP status code Amazon Kinesis Agent Web Servers, Log Servers, Database Servers Download and install the agent Monitor multiple directories and write to multiple streams Pre-process data Multi-line record to single line Convert from delimiter to json format Data consumer Kinesis Streams Applications using Kinesis Client Library (KCL) KCL consume and process data KCL handles complex tasks Java, Node.js, .NET, Python and Ruby Handles Checkpoint Keeps track of records that have already been processed in a shard If a worker fails, KCL will restart the processing of the shard at the last know processed record Run your consumer application by running it on EC2 instance under an Auto Scaling group in order to replace failed instances or handle additional load Automatically load balances record processing across many instances Supports de-aggregation of records that were aggregated with the KPL Uses unique DynamoDB table for each application to track application state KCL uses the name of the streams application to create DynamoDB table, so use unique application names in KCL Each row in the DynaoDB table represents a shard Hash key for the DynamoDB table is the shard ID DynamoDB table throughput 10 read capacity units and 10 write capacity units Provisioned throughput exceptions Your application does frequent checkpointing Your stream has too many shards Consider adding more provisioned throughput to the DynamoDB table For the exam Any kind of scenario where you are streaming large amounts of data that needs to be processed quickly and you have a requirement to build a custom application to process and analyze streaming data Shards Record and it's components Retention Period Data Producers ( KPL, Kinesis Agent, Kinesis Streams API) Data Consumers (KCL) Additional throughput for DynamoDB application state table if you get provisioned throughput exception errors Emitting Data to AWS Services \u00b6 Use Cases S3 - Archiving data DynamoDB - Metrics Elasticsearch - Search and index Redshift - Micro batch loading EMR - Process and Analyze Data Lambda - Automate emitting data Kinesis Connector Library Java based Used with the Kinesis Client Library Connectors for DynamoDB Redshift S3 Elasticsearch Connector Library Pipeline Kinesis Stream iTransformer Defines the transformation of records from the Amazon Kinesis stream in order to suit the user-defined data model iFilter Excludes irrelevant records from the processing IBuffer Buffers the set of records to processed by specifying size limit (# of records)& total byte count IEmitter Makes client calls to other AWS services and persists the records stored in the buffer S3Emitter Class Writes buffer contents into a single file in S3 Requires the configuration of a S3 bucket and endpoint S3/DynamoDB/ElasticSearch/Redshift For the exam Data can be emitted to S3, DynamoDBm Elasticsearch and Redshift from Kinesis Streams using the Kinesis connector Library Lambda functions can automatically read records from a Kinesis stream, process them and send the records to S3, DynamoDB or Redshift Kinesis Analytics \u00b6 Kinesis Firehose \u00b6","title":"Big-Data"},{"location":"certifications/aws/bigdata-specialty/#aws-certified-big-data-specialty","text":"Recommended for study: Big Data Technology Fundamentals, Big Data Instructor led training class, Big Data AWS whitepappers and documentation","title":"AWS Certified Big Data Specialty"},{"location":"certifications/aws/bigdata-specialty/#amazon-kinesis","text":"","title":"Amazon Kinesis"},{"location":"certifications/aws/bigdata-specialty/#kinesis-streams","text":"Collect and process large streams of data in real time Create data-processing applications Read data from stream Process records Send to a dashboard Generate an alert Dynamically change price Dynamically change advertising strategy Scenarios Fast log and data feed intake and processing Real-time metrics and reporting Real-time data analytics Complex stream processing Benefits Real-time aggregation of data Loading the aggregate data into a data warehouse/map reduce cluster Durability and Elasticity Parallel application readers Methods to load/get data Kinesis Producer Library (KPL) Kinesis Client Library (KCL) Kinesis Agent to collect and send Kinesis Rest API","title":"kinesis Streams"},{"location":"certifications/aws/bigdata-specialty/#core-concepts","text":"Shard Uniquely identified group of data records in a stream Single Shard Capacity 1 MB/Sec data input 2 MB/sec data output 5 transactions/sec for reads 1000 records/sec for writes Can create multiple shards in a stream Stream with 2 shards 2 MB/Sec data input 4 MB/sec data output 10 transactions/sec for reads 2000 records/sec for writes Resharding - Dynamically add or remove shards as data throughput changes Shard Split Increase capacity and costs Shard merge Decrease capacity and costs Records Unit of data stored in a stream A record consists of Partition Key Group the data by shard Tells you which shard the data belongs to Partition Key is specified by the applications putting the data into a stream Sequence Number Unique identifiers for records inserted into a shard Think of its as a unique key that identifies a data blob Assigned when a producer calls PutRecord or PutRecords operations to add data to a stream You can't use sequence numbers to logically separate data in terms of what shards they have come from. You can only do this using partition keys Data Blob Data blob is the data your data producer adds to a stream The maximum size of data blob (the data payload after Base64-decoding) is 1MB Retention Period 24 hours by default Increase this to 7 days if required Change retention period through CLI Data Producers Amazon Kinesis Streams API AWS SDK for Java PutRecord PutRecords Amazon Kinesis Producer Library (KPL) Configurable library to create producer applications that allow developers to achieve high write thoughput to a kinesis stream Write to one of more Kinesis Streams with an auto-retry configurable mechanism Collects records to write multiple records to multiple shards per request Aggregate user records kinesis Client Library integration (de-aggregate records) Monitoring (CloudWatch) Do Not use KPL if..... Your producer application/use case cannot incur an additional processing delay RecordMaxBufferedTime Maximum amount of time a record spends being buffered Larger values result in better performance, but can delay records Batching Aggregation User records and streams records Aggregation allows you to combine multiple users records into a single streams records, helping improve per shard throughput Single Shard capacity is 1 MB/sec (write rate of 1000 records per second) Example: 1000 records (500 bytes each) = 0.5 MB/Sec => Shard not efficient Collection Multiple Streams records are batched and sent in a single HTTP request with a call to the PutRecords API operation Helps increase throughput due to reduced overhead of not making separate HTTP requests Collection Vs Aggregation Collection is working with groups of streams records and batching them to reduce HTTP requests Aggregation allows you to combine multiple user records into a single streams records in order to efficiently use shards Error Handling PutRecords operation sends multiple records to your stream per HTTP request (recommended approach for applications that require high throughput) Single record failure does not stop the processing of subsquent records Despite the failure of a single record, if other records in the PutRecords APi are successful, HTTP status code of 200 is still returned For partial failures, the PutRecordsResult method in the KPL can be used detect individual record failures and retry the PUT based on the HTTP status code Amazon Kinesis Agent Web Servers, Log Servers, Database Servers Download and install the agent Monitor multiple directories and write to multiple streams Pre-process data Multi-line record to single line Convert from delimiter to json format Data consumer Kinesis Streams Applications using Kinesis Client Library (KCL) KCL consume and process data KCL handles complex tasks Java, Node.js, .NET, Python and Ruby Handles Checkpoint Keeps track of records that have already been processed in a shard If a worker fails, KCL will restart the processing of the shard at the last know processed record Run your consumer application by running it on EC2 instance under an Auto Scaling group in order to replace failed instances or handle additional load Automatically load balances record processing across many instances Supports de-aggregation of records that were aggregated with the KPL Uses unique DynamoDB table for each application to track application state KCL uses the name of the streams application to create DynamoDB table, so use unique application names in KCL Each row in the DynaoDB table represents a shard Hash key for the DynamoDB table is the shard ID DynamoDB table throughput 10 read capacity units and 10 write capacity units Provisioned throughput exceptions Your application does frequent checkpointing Your stream has too many shards Consider adding more provisioned throughput to the DynamoDB table For the exam Any kind of scenario where you are streaming large amounts of data that needs to be processed quickly and you have a requirement to build a custom application to process and analyze streaming data Shards Record and it's components Retention Period Data Producers ( KPL, Kinesis Agent, Kinesis Streams API) Data Consumers (KCL) Additional throughput for DynamoDB application state table if you get provisioned throughput exception errors","title":"Core Concepts"},{"location":"certifications/aws/bigdata-specialty/#emitting-data-to-aws-services","text":"Use Cases S3 - Archiving data DynamoDB - Metrics Elasticsearch - Search and index Redshift - Micro batch loading EMR - Process and Analyze Data Lambda - Automate emitting data Kinesis Connector Library Java based Used with the Kinesis Client Library Connectors for DynamoDB Redshift S3 Elasticsearch Connector Library Pipeline Kinesis Stream iTransformer Defines the transformation of records from the Amazon Kinesis stream in order to suit the user-defined data model iFilter Excludes irrelevant records from the processing IBuffer Buffers the set of records to processed by specifying size limit (# of records)& total byte count IEmitter Makes client calls to other AWS services and persists the records stored in the buffer S3Emitter Class Writes buffer contents into a single file in S3 Requires the configuration of a S3 bucket and endpoint S3/DynamoDB/ElasticSearch/Redshift For the exam Data can be emitted to S3, DynamoDBm Elasticsearch and Redshift from Kinesis Streams using the Kinesis connector Library Lambda functions can automatically read records from a Kinesis stream, process them and send the records to S3, DynamoDB or Redshift","title":"Emitting Data to AWS Services"},{"location":"certifications/aws/bigdata-specialty/#kinesis-analytics","text":"","title":"Kinesis Analytics"},{"location":"certifications/aws/bigdata-specialty/#kinesis-firehose","text":"","title":"Kinesis Firehose"},{"location":"certifications/aws/security-specialty/","text":"AWS Certified Security Specialty \u00b6 Resources \u00b6 AWS Security White Paper: Introduction to AWS Security Processes Linux academy Guide WhitePapers KMS best Practices KMS Cryptographic Details DDOS Best Practices Logging in AWS Well-Architected framework - Security Pillar Re:invent Videos (2017) - NET 3XX - Medium level , NET 4XX - Advanced KMS best Practices AWS Encryption Deepdive Become an IAM Policy Master DDOS Best Practices VPC fundamentals & Connectivity options Logging in AWS Advanced security best practices masterclass Advanced VPC Design and New captabilities for Amazon VPC FAQs https://aws.amazon.com/faqs/ Security, identity & Compliance Exams \u00b6 Exam Guide AWS Practice Exam AWS Practice Exam 2 Quiz 1 Quiz 2 Quiz 3 Quiz 4 Exam Sample Questions ExamTopics Cloud Guru Linux Academy Introduction \u00b6 Security 101 \u00b6 Security Basics - Models \u00b6 CIA Confidentiality - IAM, MFA Integrity - Certificate Manager, IAM, Bucket Policies Availability - Auto-Scaling, Multi-AZ AAA Authentication - IAM Authorization - Policies Accounting - Cloudtrail Non-repudiation (Can't deny you did something) Shared Responsibility Model \u00b6 Infrastructure (EC2, EBS, VPC) Container (RDS, EMR, Elastic Beanstalk) Abstracted (S3, Glacier, DynamoDB, SQS, SES) Security IN AWS \u00b6 Visibility AWS Config Auditability AWS CloudTrail Controllability AWS KMS - Multi-Tenant AWS CloudHSM - Dedicated - FIPS 140-2 Compliance Agility AWS Cloudformation AWS Elastic Beanstalk Automation AWS OpsWorks AWS CodeDeploy IAM, S3 & Security Policies \u00b6 Resetting Root Users \u00b6 Create a new root user password and strong password policy Delete previous 2 factor authentication and re-create Check if the root user has an Access Key Id and Secret Access Key. If so delete these immediately Check other user accounts. Verify they are legitimate and if not, delete these IAM Policies \u00b6 IAM is global. Applies to all areas of AWS, not just S3 Three different types os IAM Policies AWS Managed Policies Customer Managed Policies Inline Policies Components Version: current lastest is 2012-10-17 - check this is correct Statement: Enclosed {}, and deliminated by commas . make sure the formatting is good Effect: Needs to be Allow or Deny Principal: The entity which the statement applies to, invalid for IAM user policies since it's implied - be careful to make sure it exists and is correct for resource policies Action: The api Actions which the statement refers to. Make sure the actions and resources match. Resource: One or more ARNs or wildcards which refert to AWS objects Condition (optional): Conditions for the statement. Check for validity. sourceIP won't work with endpoints Permissions Boundaries \u00b6 A permissions policy allows or denies, actions on resources. Policies are applied to identities (Users, Groups, Or Roles) in the case of identity policies, or resources in the case of resources policies Permissions boundary is a set of access which an entity (user, role, organisation) can never exceed It can act as a safety net to ensure adherence to organizational policies, or it can act as delegation tools A permissions boundary on its own grants no permissions, it only restricts Policy Evaluation \u00b6 With least-privilege, decisions ALWAYS default to DENY ALSO an explicit DENY ALWAYS trumps an ALLOW If no method specifies an ALLOW, then the request will be denied by default Only if no method specifies as DENY and one or more methods specify an ALLOW will the request be allowed Evaluation Order Boundaries are always processed first, starting with organizational and then identity (User or Role) Then AWS checks if you have chosen a subset of permissions for sts:AssumeRole Final effective permissions are a merge of identity, resource, and ACL Security Token Service (STS) \u00b6 Grants users limited and temporary access to AWS resources. Users can come from three sources: Federation (typically Active Directory) Uses security assertion markup language (SAML) Grants temporary access based off the users Active Directory credentials. Does not need to be a user in IAM Single sign on allows users to log in to AWS console without assigning IAM credentials Federation with Mobile Apps Use Facebook/Amazon/Google or other OpenID providers to log in Cross Account Access Let's users from one AWS account access resources in another Key Terms Federation: combining or joining a list of users in one domain (such as IAM) with a list of users in another domain (such as Active Directory, Facebook etc) Identity Broker: a service that allows you to take an identity from point A and join it (federate it) to point B Identity store: services like Active Directory, Facebook, Google etc Identities: a user of a service like Facebook etc Use Case 1. Employee enters their username and password 2. The application calls an identity broker. The broker captures the username and password 3. The identity broker uses the organization's LDAP directory to validate the employee's identity 4. The identity broker calls the new GetFederationToken function using IAM credentials. The call must include an IAM policy and a duration (1 to 36 hours), along with a policy tha specifies the permissions to be granted to the temporary security credentials 5. The STS confirms that the policy of the IAM user making the call to GetFederationToken gives the permission to create new tokens and then returns four values to the application: An Access key, a secret access key, a token, and a duration (the token's lifetime) 6. The identity broker returns the temporary security credentials to the reporting application 7. The data storage application uses the temporary security credentials (including the token) to make requests to S3 8. S3 uses IAM to verify that the credentials allow the requested operation on the given S3 bucket and key 9. IAM provides S3 with the go-ahead to perform the requested operation Cognito \u00b6 User Pools User pools are user directories used to manage sign-up and sign-in functionallity for mobile and web applications Users can sign-in directly to the user Pool, or indirectly via an identity provider like Facebook, Amazon, Or Google. Cognito acts as an Identity broker, handling all interaction with Web Identity Providers Successfull authentication generates a number of JSON Web Tokens (JWTs) Groups Collection of users in a user Pool, which is often done to set the permissions for those users Identity Pools Enable you to create unique identities for your users and authenticate them with identity providers. With an identity, you can obtain temporary, limited-privilege AWS credentials to access other AWS Services Glacier Vault \u00b6 Glacier is low-cost storage for archiving and long-term backup Files are stored as Archives (Single of Multiple files a .tar or .zip file), Archives are stored in Vaults A Vault Lock Policy allows you to configure and enforce compliance controls for Glacier Vaults, using Vault Lock Policy Similiar to an IAM Policy Configure WORM (write once read many) archives Create data retentions policys, ex: 5 year retention 2 Steps to configuring a Vault Policy Initiate the lock by attaching a vault policy to your vault, this sets the lock to an in-progress state You have 24 hours to validate the lock policy, once validated, Lock policies are immutable If the policy doesn't work as expected you can abort Enforce regulatory and compliance controls AWS Organizations \u00b6 Allows you to organize your accounts into groups / OUs for access control and centralized billing Attach policy based controls - Service Control Policies Centrally manage permissions for OUs - groups of accounts or individual accounts Access to accounts created in Organizations is initially via a role OrganizationAccountAccessRole, which is created automatically SCP is used to centrally control the use of AWS Serices across multiple accounts Like a filter which restricts access to AWS services The SCP applies to all OUs and accounts below the OU to which it is attached Can be used to create a Permissions Boundary Restricting the actions the users, groups, roles in those accounts can do - including root SCPs can deny access only, then cannot allow IAM Credential Report \u00b6 Credential Report List all users in your account and the status of their various credentials, including password, access keys, and MFA devices Console Head to IAM section, find credential report on the left and download CSV containing the report CLI aws iam generate-credential-report aws iam get-credential-report Domain 1 - Incident Response \u00b6 Given an AWS abuse notice, evaluate the suspected compromised instance or exposed access keys \u00b6 AWS Acceptable Use Policy Compromised Resources and Abuse Abuse Activities: Externally observed behavior of AWS customer instances or resources that are malicious, offensive, illegal, or cloud harm other internet sites AWS will shut down malicious abusers, but many of the abuse complaints are about customers conducting legitimate business on AWS Example causes of abuse that are not intentional Compromised Resource: EC2 instance becoming part of a botnet then attacking other hosts on the internet. This traffic could be going to other AWS Accounts as well Secondary abuse One of your end-users posts an infected file on your resources. When that file calls \"home\", it is going to appear to be traffic generated in your account Application Function If you are using applications such as web crawlers, it can sometimes appear as DoS attack and AWS will react accordingly False complaints Other AWS users can report your activity to AWS. The complaint might appear legitimate, and AWS will react accordingly Responding to Abuse Notifications - There is a chance that the investigation of abuse will turn out be a compromised account or resource. If this is the case, the following AWS recommendations can help: Change the root password ant the password for all IAM users Add MFA to all admin users and anyone who access the AWS Console Create a new EC2 Key pair and update instances (deleting the compromised key) Create an AMI and Relaunch Edit the .ssh/authorized_keys file Delete for rotate potentially compromised IAM access keys Delete unrecognized or unauthorized resources Instances IAM Users Spot bids Contact AWs Support Respond to the notification Important: Do not ignore AWS abuse communications and make sure they have the most effective email address on file Be Proactive: Avoid being Compromised Vault root credentials and remove access keys if they exist Require a strong password and MFA on all IAM accounts Use roles whenever possible, do not trust humans Do NOT copy EC2 key pairs to instances and protect them on admin machines Rotate IAM access keys regularly There are People scanning repositories like Github for access keys, EC2 Key pairs, and other sensitive information. AWS has created a tool to hel prevent spillage: Git-secrets: Prevents committing secrets and sensitive information to gir repositories Verify that the Incident Response plan includes relevant AWS services \u00b6 Incident Response Framework Preparation Phase - Doing everything we can to prevent breaches and failures. Eventually some type of security event will happen, it always does. We are building walls and fortifying barricades here Be Proactive - Best Practices Risk Management - Determine where the different levels of risk are Principle of least privilege Architect for failure - High availability and fault tolerance always Train for the real thing - Test and simulate; a real incident is a horrible place to learn lessons Clear ownership and governance - Tag all resources so no time is wasted finding who or what group to contact Data classification - Tagging data stores with classification can quickly identify spilage AWs Services involved - IAM, VPC, Route 53, EC2, EFS, RDS, etc .... Limit the Blast - Carefull planning can reduce the \"blast radius\" of any attack. The ideia here is to segment/section off resources from each other Organizations - We can add accounts under our main account Benefits If there is a breach, it will not affect multiple accounts Service Control, Policies can be set so \"child\" accounts can be limited Using multiple Regions and VPCs can have a similiar affect Services involved: Organizations and VPC Log Everything - Logging is the best way to collect information about our environments. Centralized logging - Collect all the logs from the organization in one place Encrypt and protect (logs contain sensitive data that should not be clear text) It all starts with logs. The following pattern applies Logs => Events => Alerts => Response Services Involved: CloudTrail, VPC Flow Logs, EC2 OS & Apps Logs, S3, Cloudwatch Logs, Config, Lambda Encrypt All - Two Ways Server Side encryption (data-at-rest) Client Side encryption (data-in-transit) Important - Treat your data as if everyone is looking at it all the time because they might be Services Involved: KMS, S3, Certificate Manager, ELB, Route 53 Identification Phase - AKa detection phase, this is where we discover an incident is ocurring. We can do this through behavior-based rules we configure to help detect breaches. We must then determine the following: Intention - Knowing this can help us fin compromised resources quickly Blast Radius - What resources where effected? How deep did the attach go? Data Loss Protection - A combination of encryption and access control. What did they get\u00bb Resources needing clean up - What resources do we need to mitigate or isolate This phase can be very difficult, and we should be heavily dependent on automation to help us with detection. We can then react accordingly or even automate responses. There are also stealth techniques we can use to observe user behaviour without being detected if there is questionable behavior Services involved: Cloudwatch, S3 Events, Third Party tools Containment Phase - Removing the threat. There sould be tools and processes ready to make changes to isolate any compromised resources. The ideal situation would be CLI or SDK scripts we can deploy very quickly when needed. For fast isolation, we need to have following created or have scripts ready: A security group that restricts egress traffic and only allows management ports in A separate subnet with restrictive NACL we can move resources to An S3 Bucket policy that is designed to immediately stop spillage An explicit deny policy created in IAM (Deny *), quick removal of privileges A key policy that denies all decryption In addition there may be additional activities we should perform Snapshot volumes of compromised instances Stopping instances Disabling encryption keys in KMS Change Route 53 record sets Services Involved: VPC, S3, KMS, Route 53 Investigation Phase Investigation involves event correlation and forensics. We need to determine exactly what happened and when. We also need to determine if the threat is still viable. As soon as we start our investigation, forensics can begin. Whether we use live box, or dead box forensics here, proceed with caution and make sure it is in a safe, sandboxed environment. Services Involved: VPC, Flow Logs, EC2, Cloudtrail, Cloudwatch Eradication Phase - Try to remove all the infections and compromises in our resources. In most cases, we can just delete the resources. There are some additional concerns whe dealing with data If encryption was implemented correctly, data that was accessed should not be legible. In this case, we can do the following Delete/disable any KMS Keys For EBS, delete splilled files, create a new encrypted volume, copy all good files For S3 with S3 managed encryption, delete the object For S3 with KMS managed keys or customer keys, delete the object and the CMKs Secure wipe any affected files If our data was not encrypted on EBS, we can attempt to sanitize the volume Not recommended Create new columes or instances with clean files or restore them from \"Last know good\" backups Services Involved: KMS, EBS, S3 Recovery Phase - We need to put everything back to normal. This normally includes verifying eradicated resources and reversing the steps taken in the containment phase Restore resources one at time (or group) Use new encryption keys Restore network access Monitor, monitor, monitor Have the containment phase tools ready This phase can be potentially dangerous as the forensic process may not have revealed everything Follow-up Phase Testing and simulations are vital Must strive for efficiency (tagging, automation) Teams need experience Evaluate the Configuration of Automated Alerting and Execute Possible Remediation of Security-Related Incidents and Emerging Issues \u00b6 Automated Alerting The services we use in the cloud make scalability and reliability easy. These concepts should apply to our logging, monitoring and alerting as well Architecture Logging (CloudTrail, VPC Flow Logs, Route 53 DNS Logs, EC2) => Cloudwatch Logs => cloudWatch Metric Filters => Alarms => Targets (SNS Topic, AutoScaling) Automated Response Once we get alerts generated in Cloudwatch, there are a log of target services we can trigger with those alerts. We can configure these target services to automatically remediate our resources CloudWatch Event Rules => Targets: Lambda - Function Systems Manager - Patch or Run command SNS Topic - Message or application SQS - Application Queue Domain 2 - Logging And Monitoring \u00b6 S3 Events \u00b6 Work at Object Level Events: RRSObjectLost Put Post Copy Complete Multipart Upload Delete Delete marker Created ObjectCreate (All) ObjectDelete(All) Then we send notification to three services SNS Topic SQS Queue Lambda Function S3 Access Logs \u00b6 The default storage for CloudTrail is S3 Cloudwatch Logs can be exported to S3 S3 can help cost savings while still assisting with compliance Lifecycle policies to reduce storage costs Archive older logs to glacier S3 Access Logs Tracks access requests to buckets Each log event contains one access request Log events contain Requester Bucket Name Request Time Request action Response Status Error code Important features of s3 access logging The log delivery group must be granted write permission on the target bucket Not near-real-time logging - Can take one hour to propagate Logs are delivered on a best effort basis Newly enable access logs might not be displayed in the target bucket for up to an hour Changes to the target bucket might take up to an hour propagate Centralized Logging \u00b6 The Multi-Account Strategy Use Organizations and set up accounts by environments or function Production, Development, Staging, etc Security, Administration Will help reduce the blast radius of any incident An additional layer of security Cross-account roles Centralized logging Logs should be contained in one location (the complete picture) Logs should be read-only for most job functions ( including security) Logs should be encrypted (KMS Preferred) Roles Can provide cross account access CloudTrail \u00b6 Enables After-the-fact incident investigation Near-realtime intrusion detection Industry & regulatory compliance Provides Logs API call details (for supported services) Entries can be viewed using Event History (past 90 days) Validating CloudTrail Log File Integrity Was the log file modified, or deleted? CloudTail log file integrity validation: SHA-256 hashing SHA-256 with RSA for digital signing Log files are delivered with a 'digest' file Digest file can be used to validate the integrity of the log file What is Logged Metadata around API calls The identity of the API caller The time of the API call The source IP address of the API caller The request parameters The response elements returned by the service CloudTrail Event Logs Sent to an S3 bucket You manage the retention in S3 Delivered every 5 (active) minutes with up 15 minute delay Can be aggregated across regions Can be aggregated across accounts Notifications Available Setup Enabled by default (For 7 Days) Trail - A configuration allowing for logs to be sent to an S3 bucket Single region or multi-region trails can be configured trails can make multi-account logging possible Configuration Options Management events - Enabling will log control plane events, such as User login events Configuring Security Setting up logging Data Events which include Object Level events in S3 Function level events in Lambda Encryption flexibility Encrypted in S3 server side by default, can be changed to KMS The logs can be sent to an S3 bucket of choice and even prefixed (folders) Security Protect you CloudTrail logs, they contain everything that you are doing in your AWS account and may contain PII Allow your security people admin access to Cloudtrail, auditors read only access to CloudTrail using IAM Use Iam policies to restrict access to unauthorised people Restrict Access to S3 using bucket policies (That contain log files), and use MFA delete on your objects Use SSE-S3 or SSE-KMS to encrypt the logs Use lifecycle rules to move data to Glacier or to delete it Check the integrity of your log files using digest files Json and CSV file formats to export Cloudwatch \u00b6 Enables Resource utilization, operational performance monitoring Log aggregation and basic analysis Provides Real-time monitoring within AWS for resources and applications Hooks to event triggers Key Components CloudWatch CloudWatch logs Cloudwatch Events Notifications CloudWatch Logs Pushed from AWS services (including CloudTrail) Pushed from your applications/systems Stored indefinitely (not user S3) Can stream log data to lambda and Elasticsearch service Components Log Events - Record of activity recorded by the monitored resource Log Streams - Sequence of logs events from the same source/application Log Groups - A collection of logs streams with same access control, monitoring, and retention settings Metric Filters - Assigned to a log groups, it extracts data from the group's log streams and converts that data into Metric data point Retention Settings - Period of time logs are kept. Assigned to log groups, but applies to all the streams in a group (q day to never expire) Use cloudwatch logs to monitor, store, and access your log files from: Cloudtrail VPC flow logs Sent to cloudwatch per default Cloudwatch Agent DNS Logs DNS Query Logs can be enabled on Route53 hosted zones and sent to CloudWatch. Route 53 uses common DNS return codes in the log and includes the edge location (based on airport codes) These logs can be used to determine when there is a DNS problem in an application These logs are only available for hosted zones where Route53 is the endpoint (no outside hosting). Also, the logs are not available for private hosted zones Cloudwatch Metrics Metric Filters: Used to create a custom metric from log data Assigned at the log group level Will filter all the streams in that group Uses a filter and pattern syntax Example: { $.eventName = \"createUser\" } Metric Namespace: The Folder or category the custom metric will appear in Metric Name: The name given to the custom metric Alarms: Assigned to the filter Alarms can trigger: SNS Topics Autoscaling Actions EC2 actions (if the metric chosen is related) Cloudwatch Events Are similiar to alarms, Instead of configuring tresholds and alarming on metrics, CloudWatch Events are matching event Patterns Near real-time stream of system events Common issues: Permissions, Wrong ARN, Typos Three Parts Event Source AWS Resources state change AWS CloudTrail (API Calls) Custom Events (code) Scheduled Rules - match incoming events and route them to one or more targets Targets - Lambda functions, SNS topics, SQS queues, Kinesis streams There can be more than one Examples Alerting on object uploads in S3 (can trigger automatic ACL remediation) Alerting on EC2 instance state changes (can trigger actions on the instances) Alerting on user creation in IAM Cloudwatch Buses Allows different AWS accounts to share Cloudwatch Events Can collect events from all your accounts together in one account Must grant an account permission by adding and then sending the account number to the receiving account bus configuration The sending account send an event to an Event bus target The CloudWatch Event Bus process requires two event rules, one event rule on either end of the event bus. AWS Config \u00b6 Enables Compliance auditing Security analysis Resource tracking Provides Configuration snapshots and logs config changes of AWS resources Automated compliance checking Evaluate resource configurations for desired settings Retrieve configurations of resources in your account Retrieve historical configurations Receive a notification for creations, deletions, and modifications View relationships between resources ( members of a security group) Config Rules - A rule represents the desired value for resources. There are two types: AWS Managed Rules - Pre-built and managed by AWS. You simply choose the rule you want to enable, then supply a few configuration parameters to get started Customer Managed Rules - These are custom rules, defined and built by you. You can create a function in AWS Lambda that can be invoked as part of a custom rule and these functions execute in your account Conformance Packs A conformance pack is a collection of AWs Config Rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS organizations Conformance packs are created by authoring a YAML template that contains the list of AWS Config managed or custom rules and remediation actions An Aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following Multiple accounts and multiple regions Single account and multiple regions An organization in AWS Organizations and all the accounts in that organization Permissions needed AWS Config requires an IAM Role with Read only permissions to the recorded resources Write access to S3 logging bucket Publish access to SNS console will optionally create these for you Restrict Access Users need to be authenticated with AWS and have the appropriate permissions set via IAM policies to gain access Only Admins needing to set up and manage config require full access Provide read only permissions for Config day-to-day use Monitoring Config Use CloudTrail with Config to provide deeper insight into resources Use Cloudtrail to monitor access to config such as someone stopping the config recorder Uses cases Administering resources Notification when a resource violates configuration rules Auditing and compliance Historical records of configurations are sometimes needed in auditing Configuration management and troubleshooting Configuration changes on one resource might affect others Can help find these issues quickly and can restore last know good configurations Security Analysis Allows for historical records of IAM policies For example, what permissions a user had at the time of an issue Allows for historical records of security group configurations AWS Inspector \u00b6 Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS Amazon Inspector automatically assesses applications for vulnerabilities or deviations from best practices. After performing assessment, Inspector produces a detailed list of security findings prioritized by level of severity These findings can be reviewed directly or as part of detailed assessment reports which are available via Amazon inspector console or API How does it Work Create Assessment target Install agents on EC2 Instances Create Assessment Template Perform Assessment Run Review findings against rules Rules Packages Common Vulnerabilities and Exposures CIS Operation System Security Configuration Benchmarks Security Best Practices Runtime Behavior Analysis Severity Levels for Rules High Medium Low Informational It Will Monitor the network, file system, and process activity within the specified target Compare what it 'sees' to security rules Report on security Issues observed within target during run Report findings and advise remediation Analyzing the behaviour of your AWS resources Identifying potential security issues It Will not Relive you of your responsibility under the share responsibility model Perform miracles Components Target: A collection of AWs resources Assesment Template: Made up of security rules and produces a list of findings Assessment Run: Applying the assessment template to a target Features Configuration Scanning and Activity Monitoring Engine Determines what target looks like, its behavior, and any dependencies it may have Identifies security and compliance issues Built-in content library Rules and reports built into inspector Best practice, common compliance standard, and vulnerability evaluations Detailed recommendations for resolving issues Api Automation Allows for security Testing to be included in the development and design stages AWS Trusted Advisor \u00b6 An online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment Advisor will advise you on Cost Optimization, Performance, Security, Fault Tolerance Core checks and Recommendations Full trusted advisor - Business and Enterprise Companies only Cost Optimization Availability Performance As Well as Security AWS Inspector vs AWS Trusted Advisor \u00b6 Questions about Cost Optimization/Performance/Fault Tolerance => Trusted Advisor Questions about Security => If needs instal agent/create reports => Inspector But if question is only about security groups open, IAM => Trusted advisor Logging \u00b6 AWS CloudTrail AWS Config VPC Flow Logs AWS CloudWatch Logs White-paper: Security at Scale: Logging in AWS - Looks at \"Common Logging requirements\" Prevent Unauthorized access IAM users, groups, roles and policies S3 Bucket policies Multi factor authentication Ensure role-based access IAM users, groups, roles, and policies Amazon S3 bucket polices Log changes to system components (AWS Config Rules) Clout Trail Controls exist ro prevent modification to logs IAM and S3 controls and policies CloudTrail log file encryption CloudTrail log file validation Storage of Log Files Logs are stored for at least one year Store logs for an org-defined period of time Store Logs real-time for resiliency S3 S3 Object lifecycle Management 99.999999999 durability and 99.99% availability of objects over a given year Domain 3 - Infrastructure Security \u00b6 CloudFront \u00b6 Cloudfront is a global CDN operating from AWS Edge Locations. Connections to a cloudfront distribution can utilize HTTP or HTTPs. Connections From Cloudfront to your content (origin server) can occur using HTTP or HTTPs. It also removes many invalid HTTP requests at the edge-basic filtering Dedicated IP Vs Shared IP (SNI - Server Name Identifier) Dedicated IP SSL is supported in ALL Browsers, but costs extra. SNI has no extra cost, but browsers need to support it To Support Old browsers, we need choose dedicated IP Viewer protocol policy HTTP and HTTPS -> default selected Redirect HTTP to HTTPS HTTPS only Advanced Security Features Integrates with AWS WAF Supports full access control and signed URLs/cookies Provides basic white/blacklist geo-restriction per distribution Can integrate with 3 party solution using signed URLS/Cookies Field-Level Encryption - help protect sensitive data (Encrypted End-to-End) - (Credit card ,.... ) Supports Lambda at the edge Custom Origin Origin SSL Protocols TLSv1.2 - Default selected TLSv1.1 - Default selected TLSv1 - Default selected SSLv3 - Disabled Origin Protocol Policy HTTP Only - Default selected HTTPS Only Match Viewer Can configure HTTP/HTTPS Ports Key pair is us required for signing URLS or cookeis The application at the custom orgin must send back three Set-Cookie headers in response to viewer Restrict Viewer Access (Use Signed URLs or Signed Cookies) Yes/No options. No is default If select yes, users must use signed urls or signed cookies Lambda Edge Inspect cookies to rewrite URLS to different versions of a site for A/B Testing Send different objects to your users based on the User Agent Header Inspect headers or authorized tokens, inserting a corresponding header and allowing access control before forwarding a request to the oring Add, delete modify headers, and rewrite URL path to direct users to different objects in the cache Generate new HTTP responses to do things like redirect unauthenticated users to login pages, or create and deliver static webpages right from the edge CloudFront SSL Certificates If you are happy for users to access your content using *.cloudfront.net domain name, then select the default CloudFront Certificate If you want to use a domain name that you already own, you will need to use a custom SSL certificate Custom SSL certificates must be stored in either ACM in the us-east-1 (North Virgina) or you can also store them in IAM using the IAM Cli. Signed Certificates Don't work Must match the custom origin name if custom origin is used Can configure alternate domain names Restricting S3 to Use cloudfront By default, when using cloudfront with s3, cloudfront is optional, and S3 can be acessed directly. This can be changed It's necessary check \"Restrict Bucket Access\" option It's necessary create a (or reuse) Origin Access Identity (OAI) (something like a cloudfront user to access to bucket) What is? - Is a virtual identity. A distribution can be configured to use it, so when accessing S3, cloudfront assumes this identity It's necessary update Bucket Policy to give permissions to Origin Access identity access to Bucket Why use? - To use on OAI, public permissions are removed from your S3 bucket policy and permissions for the OAI are added. Only the cloudfront using that OAI can access your S3 bucket Supports Logging Geo Restriction CloudFront can restrict content in one of two ways Using Cloudfront Geo Restriction Simple implement Whitelist or blacklist and it works on country restrictions only Location is based on IP country location - backed by a GeoIP Database ( ~ 99.8% accuracy) No restrictions on ANYTHING ELSE - session/cookie/content/browser etc Using a third-party GeoLocation Service Third-Party Geo Restriction needs a server/serveless application - Signed URLs are used A thrid Party Geolocation service is used... extra accuracy Your application can apply additional restriction - session/browser/account level/os Location can be much more accurate ... city, locale, LAT/Long in some cases The application can apply any logic it wants Needs to be this option with need anything beyond IP Location whitelist/blacklist Pre-Signed URLs and Cookies \u00b6 Signed URLs allow an entity (generally an application) to create a URL which includes the necessary information to provide the holder of that url read/write access to an object, even if they have no permissions to that object Cookies extend this, allowing access to an object type or area/folder and don't need a specifically formatted url You can access objects using pre-signed URL's Typically these are done via the SDK but can also be done using the CLI Features/Limits Signed URLs/Cookies are linked to an existing identity (Role/User), and they have the permissions of that entity They exist for a certain length of time in seconds. Default is 1 Hour. You can change this using \"--expires-in\" followed by the number of seconds They expire either at the end of the period or until the entity on which they are based expires If need the signed URL have long expiration time (several hours , days), don't use IAM Role as entity because IAM Role credentials expire, it's better use IAM User Anyone can create a signed URL, even if they don't have permissions on the object With Cloudfront you defined the accounts which can sign; the key pair TrustedSigners is needed for cloudfront Signed cookies (Cloudfront feature) don't work with RTMP distributions Use Pre Signed URLS when When wants restrict access to individual files USe Pre Signed Cookies when You don't want change URL Provide access to multiple restrict files, for example all files for a video in HLS format or all files in subscribe are in Website area Forcing Encryption using S3 \u00b6 S3 doesn't encrypt bucket, objects are encrypted and the settings are defined at an object level Historically, it wasn't possible to define encryption at bucket level, but you can now set S3 Default Encryption on bucket level If set, then any objects put into a bucket without encryption headers are encrypted using the bucket level default settings aws:SecureTransport # Deny This \"Condition\" : { \"Bool\" : { \"aws:SecureTransport\" : false } } Additionally, bucket policies can be used to deny attempts to put objects into a bucket with individual encryption methods Cross Region Replication \u00b6 Is configured at bucket level Requirements Do not need to use a bucket policy with aws:SecureTransport to replicate objects using SSL. It is done by default Versioning must be enabled (In two buckets) Source and destination buckets must be in different AWS Regions S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf It's possible to use CRR from one AWS account to another. The IAM role must have permissions ro replicate objects in the destination bucket. If the object owner is different than source bucket owner, the object owner must grant the bucket owner the READ and READ_ACP permissions via the object ACL In the replication configuration, can optionally direct Amazon S3 to change the ownership of object replica to the AWs account that owns the destination bucket What is Replicated Any objects created after add a replication configuration In addition to unencrypted objects, S3 replicates objects encrypted using S3 managed keys (SSE-S3) or AWS KMS managed keys (SSE-KMS) Object metadata , ACL Updates, Tags, Ownership, StorageClass Replicates only objects in the source bucket for which the bucket owner has permissions to read objects and read access control lists (ACL) If you just use a delete marker, then that delete marker is replicated What is NOT Replicated Anything created before CRR is turned on Objects created with Server-side encryption using customer-provided (SSE-C) encryption keys Objects created with server-side encryption using AWS KMS - managed encryption (SSE-KMS) unless you explicitly enable this option Objects in the source bucket for which the bucket owner does not have permissions. This can happen when the object owner is different from the bucket owner Delete markers are replicated, deleted versions of files are NOT Lifecycle events are not replicated Standard Replication - Region A to Region B Configure IAM Role with permissions to get Objects from Region A and replicate ro Region B Other Account repplication Need add bucket policy in Account B to account A have permissions to replicate Owner Change Need add Replication Configuration KMS Need Replication Configuration Need KMS config updates AWS WAF Vs AWS Shield \u00b6 AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront or and Application Load Balanacer. AWS WAF also lets you control access to your content You can configure conditions such as what IP addresses are allowed to make this request or what query string parameters need to be passed for the request to be allowed and then the application load balancer or cloudfront will either allow this content to be received or to give a HTTP 403 Status Code At it's most basic level, AWS WAF allows 3 different behaviours: Allow all requests except the ones that you specify Block all requests except the ones that you specify Count the requests that match the properties that you specify What is AWS WAF? Additional protection against web attacks using conditions that you specify. You can define conditions by using characteristics of web requests such as the following: IP addresses that requests originate from Country that requests originate from Values in request headers Strings that appear in requests, either specific strings or string that match regular expression (regex) patterns Length of requests Presence of SQL code that is likely to be malicious (Known as SQL injection) Presence of a script that is likely to be malicious (Knows as cross-site scripting) When multiple conditions exist om a rule, the result must include all conditions Example Rule: Block requests from 2.2.0.0/16 that appear to have SQL Code Both conitions must match for a block Application Load Balancers integrate with WAF at a regional level, Cloudfront at a Global Level You need to associate your rules to AWS resources in order to be able to make it work You can use AWS WAF to protect web sites not hosted in AWS via Cloudfront. Cloudfront supports customs origins outside of AWS IP's can be blocked at a /8, /16, /24 and /32 level IPv4 and IPv6 are supported AWS Shield Turned on by default Standard The basic level of DDoS protection for your web applications Included with WAF with no additional cost Advanced Expands services protected to include Elastic Load balancers, cloudfront Distributions, Route 53 hosted zones, and resources with EIPs Some of the advantages Contact 24x7 DDoS Response Team (DRT) for assistance during an attack You won't pay if you are a victim of an attack *A dvanced gives you an incident response team and in depth reporting Expanded protection against many types of attacks WAF is included in Advanced pricing 3000 a month if you want the advanced option Plus Data Transfer Out usage fees Serverless \u00b6 Lambda Function Policy Controls who or what can invoke it For poll-based services (Kinesis, DynamoDB) or SQS - lambda polls on your behalf, and so permissions are granted via its execution role For anything else, for for external entities or accounts, the PUSH model is used Change to the function policy will be required Execution Role Ensure it has enough permissions to log to cloudwatch logs To access any resources it needs to PULL from or PUSH too For event-driven invocation, the execution role doesn't need permissions to access it For poll based sources such as DynamoDb, SQS, Kinesis it does Egress Only Internet Gateway \u00b6 With IPv4, all AWS resources have a private IP. Some can be provided with a public IP and connectivity using an Internet Gateway. With IPv4 a NAT Instance/gateway can be utilized to provide outgoing only access IPv6 addressing is globally unique and publically routable. Supported resources in AWS are all publically addressable, so a NAT gateway isn't an option. Egress-Only internet gateway provides a feature limited internet gateway, specifically for IPv6, and only allowing outbound connections and return traffic. No incoming IPv6 connections can be initiated to VPC resources using an Egress-Only gateway Systems Manager \u00b6 Insights Two insight features, inventory and Compliacy. Both supported by SSM state manager Inventory periodically scans EC2 intances, or on-premise servers, retrieving details of installed applications, AWS components, network config, windows updates, detailed information on an instance/VM, details on running services, windows roles, and opitonal custom data SSM can collect on your behalf Compliance allows that data to be compared against a baseline, provding a compliant or non-compliant state to a resource. Compliance uses state manager, SSM patching and custom compliant types Actions Are the operational engine part of systems manager. Actions is the part of systems manager which performs collections, runs commands, controls patching and manages the general state of managed instances Automation Run Command Patch Manager State manager is a desired state engine. You define the desired state in the form of a systems manager document. A document can be a command document or a policy doccument. A command document is used by running command and state manager A policy document defines desired states and is only used by State Manager A document is associated with one or more managed instances Shared tooling - Several services Managed Instances Activations The method used to activate non EC2 instances withing Systems manager. Activation generates a code to activate the external machine Document Think of these are scripts or lists of commands that can be run against a managed instance ParameterStore AWS provided services to store configuration data and secrets KMS \u00b6 AWS Key Management Service (KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data, and uses Hardware Security Models (HSMs) to protect the security of your keys. AWS KMS is integrated with other AWS Services including, EBS, S3, Redshift, Amazon Elastic Transcoder, Amazon WorkMail, Amazon RDS and others to make it simple to encrypt your data with encryption keys that you manage KMS uses FIPS 140-2 (Level 2) compliant hardware modules Don't support Java Cryptography Extensions (JCE) CMK - Logical representation of a key. Keys can be generated by KMS or imported Provides alias creation date description key state key material (either customer provided or AWS provided) Can never be exported CMKs never leave KMS and never leave a region CMKS can encrypt or decrypt data up to 4kb in size AWS-managed CMK for each service that is integrated with AWS KMS Or you can have customer managed CMK that you generate by providing AWS with key material Setup Create Alias and description Choose material option Define Key Administrative Permissions IAm Users/roles that can administer (but not use) the key through the KMS API Define Key Usage Permissions IAM users/roles that can use the key to encrypt and decrypt data GeneratedDataKey creates encrypted and plaintext data key. The plaintext version is used to encrypt and then discarded. Its never stored in plaintext. The encrypted version is stored along with the encrypted data; this is envelope encryption * KMS is used to decrypt the encrypted key, returning plaintext, and data is decrypted. Encrypt and Decrypt perform those functions, and are handled by kms Key Material Options Use KMS generated key material Your Own key material You can import a symmetric 256-bit key from your key management infrastructure into KMS and use it like any other customer master key Why import your own key material Prove that randomness meets your requirements (Compliance) Extend your existing processes to AWS To be able to delete key material without a 7-30 days wait. Then be able to import them again to be resilient to AWS failures by storing keys outside AWS How to Import your own key material Create a CMK with no key material Download a public key (wrapping key) and import token Encrypt key material Import the key material Considerations for imported key material Availability and durability is different Secure key generation is up to you No automatic rotation Ciphertexts are not portable between CMKs Key Material import Fail You key material need to be 256-bit symmetric key Import token has 24 hour expiration time AWS KMS never provides option to export key material CAn reimport the key material however the key material must be the same AWS kms update-alias - to update the alias Best Practices Tips Segment Keys based upon business unit, data classification, environment, etc Create Keys within the account where the data exists if possible Rotate the keys Key Polcies IAM Polcies are Not sufficient to allow access to a CMK Edit the default CMK policy to align with your organization's best parctices for least privilege CMK - Least Privilege Separate keys per business unit and data classification Separate CMK admins from users Limit KMS actions within IAM policies (No kms:*) Cross Account Delegation Account Root Principal -> Allows target account to further delegate permissions Explicit management of principals within key policy KMS Key Rotation Options \u00b6 Extensive re-use of encryption keys is not recommended It is best practice to rotate keys on a regular basis The frequency of key rotation is dependent upon local laws, regulations and corporate policies The method of rotation depends on the type of key you are using AWS Managed Customer Managed Customer Managed with imported key material Managed Keys Rotates automatically every 3 years When the CMK is due for rotation, a new backing key is created and marked as the active key for all new requests The old backing key remains available to decrypt any existing ciphertext files that were encrypted using the old key AWS handles everything for you You cannot manage rotation yourself AWS managed keys cannot be deleted Customer Managed Keys Once a year automatically - disabled by default AWS KMS generates new cryptographic material for the CMK every year The CMK's old backing key is saved so it can be used to decrypt data that it previously encrypted On-demand manually Create a new CMK, then change your applications or aliases to use the new CMK You control the rotation frequency Keys can be deleted but be careful! Customer Managed Keys With Imported Key Material Automatic key rotation is NOT available for CMKS with imported key material i.e. the CMK was not generated in AWS The only option is to rotate the keys manually Create a new CMK, then change your applications or aliases to use the nem CMK You control the rotation frequency Keys can be deleted but be careful, if delete the old key, KMS cannot decrypt data that the original CMK encrypted Key Expire To change expiration date of a KMS key, you must reimport the same key material and specify a new expiration date Import the key material to a CMK. Download and use a new wrapping key and import token. Encrypt and reimport the same key material that was originally imported into CMK KMS Key Policy \u00b6 KMS Key policies are resource policies which control access to the Customer Master Keys (CMKs). Unlike most AWS Services, without specifically being allowed, the root user has no access to CMKs. In cases where nobody has access to CMKs, only AWS can restore access If removed Key Policy, nothing can use the key. AWS support is required Key Administrators are permitted (via he key policy) to perform admin actions on the key, but not the ability to use the key. Operations include kms:Create* kms:Describe* kms:Enable kms:Put kms:Update Kms:Delete and more CAn't perform cryptography operations Need give another account permissions to CMK to use it KMS Key Usage \u00b6 KMS has specific operations which are used to utilise CMK. CMKs generally aren't used to encrypt data. They generate data keys which perform the encryption and decryption Operations kms:Encrypt - is suitable for encrypting a file which is less than 4kb. No envelope encryption needed kms:Decrypt - kms:ReEncrypt kms:GenerateDataKey - Not desirable for a distributed system, where each component would use a different key to encrypt the data. To distribute system, if don't need distribute a data key among the system components, choose GenerateDataKeyWithoutPlainText kms:DescribeKey KMS Grants \u00b6 Grants are an alternative access control mechanism to a key policy Delegate a subset of permissions to AWS services/other principals so that they can use the CMK on the customer behalfs Programmatically delegate the use of KMS CMKs to other AWS principals - e.g a user in either your account or another account Temporary, granular permissions (encrypt, decrypt, re-encrypt, describekey etc) Grants allow access, not deny Use Key Policies for relatively static permissions & for explicit deny CLI Commands Grants are configured programatically using the AWS CLI create-grant - adds a grant to the CMK, specifies who can use it and a list of operations the grantee can perform list-grants - lists the grants revoke-grant - to remove a grant A grant token is generated & can be passed as an argument to KMS APi Encryption Context \u00b6 Key value pair of additional data that you want associated with AWS KMS-Protected information Enforce tighter controls for your encrypted resources Insight into the usage of your keys from an audit perspective Is logged in clear text within CloudTrail Data At Rest KMS \u00b6 EBS EBS Vol is encrypted using DataKey generated from a CMK Encrypted data key is stored with volume Used by the hypervisor to decrypt upon detaching IO, Snapshots and persisted data is encrypted DynamoDB For any encrypted table created in a region, DynamoDB uses KMS to create an AWS/DynamoDB service default CMK (in each region) When a table is created and set to be encrypted, this CMK is used to create a data key unique to that tabled, called a table key This key is managed by DynamoDB and stored with the table in encrypted form Every Item encrypted by DynamoDB is encrypted with a data encrypted key, which is encrypted with this table key and stored with the data Table keys are cached for up to 12 hours in plaintext by DynamoDB, but a request is sent to KMS after 5 minutes of table key inactivity to check for permissions changes RDS RDS utilizes EBS for its encryption. RDS Instances are managed versions of EC2 instances, configured to act as managed DB cluster. In Similiar way to ec2, encrypted volumes attached to RDS are handled by the host, with persistent data, snapshots, and IO encrypted and decrypted using KMS S3 Every object in a bucket is encrypted by S3 using a DataKey provided by KMS The DataKey is generated from a CMK ChiperText DataKey is stored with the object as metadata. When decryption is needed, it's passed to KMS, Decrypted, and used by S3 to Decrypt the Object KMS Hierarchy \u00b6 KMS-Managed - Keys on HSAs in a Region All Haderneded Security appliances (HSA) in a Region self-generate keys in memory when provisioned. Private Keys never leave HSA Customer Managed - Customer Master Key 254-bit symmetric Customer Master key generated in HSA or imported by customer Stored in encrypted form in several locations by KMS. Plaintext version used only in memory on HSAs on demand Data Key - Customer-managed or AWS service managed 235-bit symmetric key returned to client by KMS to use for encrypting bulk data KMS Cross Account Access \u00b6 Access to KMS CMKs is controlled using The Key policy IAM Policies If you want to enable another external account to encrypt or decrypt using your CMK, you need to enable cross account access Enable access in the Key Policy for the account which owns the CMK Enable access to KMS in the IAM Policy for the external account Both steps are necessary otherwise it will not work Access to KMS CMKs is controlled using The key Policy - add the root user, not individual IAM users / roles IAM Policies - define the allowed actions and the CMK ARN If you want to enable cross account access Enable access in the Key Policy for the account which owns the CMK Enable access to KMS in the IAM Policy for the external account Both steps are necessary otherwise it will not work KMS in a multi-account configuration The key won't appear in the external account, but if it is configured using a key policy, that account can interact with the key for cryptographic functions Key usage and Key admin are not the same thing Cloud HSM \u00b6 The AWS CloudHSM service helps you meet corporate, contractual and regulatory compliance requirements for data security by using dedicated hardware security module (HSM) appliances within the AWS cloud AWS manages and maintains hardware, but has no access to the cryptographic component Interaction is via industry standard APIs, no normal AWS APIs Keys can be transfered between CloudHSM and other Hardware solutions (on premises) Keys are shared between cluster members. NO HA unless multiple HSM's are provisioned Applications can be outside the VPC - Direct Connect, Peered or VPN On-Premises HSM - for if you really need to control your own physical hardware Enables Control of data Evidence of control Meet tough compliance controls Provides Secure Key Storage Cryptographic operations Tamper-resistant Hardware Security Model Setup Inside a VPC, in the region required A private subnet for the HSM An EC2 instance (control instance) with the cloudhsm_mgmt_util & key_mgmt_util Needs to be accessible by you Create a cluster & HSM Create a VPC Create a Private & Public subnet Create the Cluster Verify HSM identity Initialize the Cluster Launch a client instance Install and configure the client Activate the cluster Setup Users Generate Keys Key Control AWS does not have access to your keys Separation of duties and role-based access control is part of the design of the HSM AWS can only administer the appliance, not the HSM partitions where the keys are stored AWS can (but probably won't) destroy your keys, but otherwise have no access Tampering If the CloudHSM detects physical tampering the keys will be destroyed If the CloudHSM detects five unsuccessful attempts to access an HSM partition as Crypto Officer the HSM appliance erases itself If the CloudHSM detects five unsuccessful attempts to access an HSM with Crypto User (CU) credentials, the user will be locked and must be unlocked by a CO Monitoring Use CloudTrail to log API calls including those made to CloudHSM 4 Main User Types Precrypto Officer (PRECO) Crypto Officer (CO) Performs user management operations For example, a CO can create and delete users and change user password Crypto Users (CU) Key management - Create, delete, share, import and export cryptographic keys Cryptographic operations - Use cryptographic keys for encryption, decryption, signing, verifying and more Appliance User (AU) The appliance User (AU) can perform cloning and synchronization operations. AWS CloudHSM uses the AU to synchronize the HSMs in an AWS CloudHSM cluster The AU exists on all HSMs provided by AWS CloudHSM and has limited permissions To digitally sign firmware sotfware, you must use the sign command of the key_mgmt_util command line of CloudHSM (PKI functionally) FIPS 140-2 Level 3 Supports Java Cryptography Extensions (JCE) API's AWS EC2 Encryption \u00b6 We can use KMS to encrypt EBS volumes, but we cannot use KMS to generate a public key/private to log in EC2 We can import Public Keys into EC2 Key pairs, but we cannot use EC2 Key pairs to encrypt EBS volumes, we must use KMS or third party application/tools We can use KMS to encrypt EBS volumes and it is possible to encrypt root device volumes To encrypt a root device volume create an AMI. The Initial AMI will be unencrypted, but you can then copy it and in doing so encrypt it You can change the encryption keys from amazon managed to customer master keys You can copy from one region to another and make those copies (snapshots) encrypted, but you must use the keys in the destination region to do encryption You cannot copy KMS keys from one region to another SSH Keys You can view the public key by going to /home/ec2-user/.ssh/authorized_keys You can also view the public key using instance metadata curl http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key You can have multiple public keys attached to an EC2 instance You can now add roles to existing EC2 instances Deleting a key Pair in the console will not delete it from the instance or the instances metadata If you lose a KP (Public or Private), simply take a snapshot of the EC2 instance and then deploy it as a new instance this will append a new public key to the /home/ec2-user/.ssh/authorized_keys You can then go in to that file and delete the outdated public keys Because you cannot export keys from KMS and because Amazon in involved the generation of keys, you cannot use KMS with SSH for EC2 With CloudHSM you can however because can export keys from CloudHSM AWS EC2 - Marketplace \u00b6 Can purchase security products from third party vendors on the AWS Market Place Firewalls, Hardened OS's, WAF's, Antivirus, Security Monitoring etc Free, Hourly, Monthly, Annual, BYOL etc CIS OS Hardening EC2 Dedicated Instances vs Dedicated Hosts \u00b6 Dedicated Instances Are EC2 Instances that run in a VPC on hardware that's dedicated to a single customer. Are physically isolated at the host hardware level from instances that belong to other AWS Accounts Dedicated instances may share hardware with other instances from the same AWS Account that are not Dedicated instances Pay for dedicated instances On-Demand, save up to 70% by purchasing Reserved Instances, or save up 90% by purchasing Sport Instances Dedicated Hosts You can use Dedicated Hosts and Dedicated Instances to launch EC2 instances on physical servers that are dedicated for you use An important difference between a Dedicated Host and a Dedicated instance is that a Dedicated Host gives you additional visibility and control over how instances are placed on a physical server, and you can consistently deploy your instances to the same physical server over time As a result, dedicated hosts enable you to use your existing server-bound software licenses and address corporate compliance and regulatory requirements Dedicated Instances Vs Dedicated Hosts Both dedicated instances and dedicated hosts have dedicated hardware Dedicated instances are charged by the instance, dedicated hosts are charge by the host If you have specific regulatory requirements or licensing conditions, choose dedicated hosts Dedicated instances may share the same hardware with other AWS instances from the same account that are not dedicated Dedicated hosts give you much better visibility in to things like sockets, cores and host id AWS Hypervisors \u00b6 A hypervisor or virtual machine monitor (VMM) is a computer software, firmware or hardware that creates and runs virtual machines. A computer on which a hypervisor runs one or more virtual machines is called a host machine, and each virtual machine is called a guest machine EC2 runs on a mixture of Nitro and Xen Hypervisors. Eventually all of EC2 will be based off Nitro hypervisors Both Hypervisors can have guest operation systems running either as Paravirtualization (PV) or using Hardware Virtual Machinhe (HVM) HVM guests are fully virtualized. The VMS on top of the hypervisors are not aware that they are sharing processing time with other VMs PV is a lighter form of virtualization and it used to be quicker However this performance gap has now closed and Amazon now recommend using HVM over PV where possible. It's also worth nothing that Windows EC2 instances can only be HVM where as linux can be both PV and HVM PV Paravirtualized guests rely on the hypervisor to provide support for operations that normally required privileged access, the guest OS has no elevated access to the CPU The CPU provides four separate privelege modes: 0-3 called rings. Ring 0 is the mos privileged and 3 the least The host OS executes in Ring 0. However, rather than executing in Ring 0 as most operating systems do, the guest OS runs in a lesser-privileged Ring 1 and and application in the least privileged Ring 3 Isolation (Layers ordered) Physical Interface Firewall Customer Security Groups (C1 Sg, C2 SG -> one per VM) Virtual Interface Hypervisor VMS (Customer 1, Customer 2, Customer x) Hypervisor Access Administrators with a business need to access the management plane are required to use multifactor authentication to gain access to purpose-built administration hosts These administrative hosts are systems that are specifically desinged, built, configured, and hardened to protect the management plane of the cloud All such access is logged and audited When an employee no longer has a business need to access the management place, the privileges and access to these host and relevant systems can be revoked Guest (EC2) Access Virtual instances are completely controlled by you, the customer You have full root access or administrative control over accounts, services, and applications AWS does not have any access rights to your instances of the guest OS Memory Scrubbing EBS automatically resets every block of storage used by yhe customer, so that one customer's data is never unintentionally exposed to another Also memory allocated to guests is scrubbed (set to zero) by the hypervisor when it is unallocated to a guest The memory is not returned to the poll of free memory available for new allocations until the memory scrubbing is complete Disk EBS volumes are provided to instances in a Zero'd state - this zeroing occurs immediately before reuse If you have specific deletion requirements, you need to do this before terminating the instance/deleting the volume Exam Tips Choose HVM over PV where is possible PV is isolated by layers, Guest OS sits on Layer 1, Applications Layer 3 Only AWS Administrators have access to hypervisors AWS staff do not have access to EC2, that is your responsibility as a customer All storage memory and RAM memory is scrubbed before it's delivered to you Host Proxy Servers \u00b6 Filtering within AWS is perfomed at two points: Security groups attached to network interfaces and NACLs attached to subnets within VPCs. Security Groups and NACLs have viability of protocols, IPs, CIDRs, and ports. The cannot filter on DNS Names, nor can they decide between allowing and denying traffic based on any form of authentication If authentication or additional intelligence beyond IP/CIDR/PORT/PROTOCOL is needed, a proxy server or an enchanced NAT architecture is required Packet capture on EC2 \u00b6 Packet capture or packet sniffing is a process where network traffic can be intercepted, analyzed and logged. Sniffed packets are captured in their entirety and unlike VPC flow logs can be inspected at data level - providing they are not encrypted Common Scenarios Review data flows between components to identify networking problems Support IDS/IPS systems - help detect and remediate intrusion attempts Debug connections between clients and the edge components of an environment Debug communications between tiers of your applications Verify the functionality of other networking components such as firewalls, NATs, and proxies VPC flow logs meet a subset of the above scenarios but don't allow traffic capture - only metadata Important: Traditionally packet sniffing was done in a promiscuous way - a network interface listened for all traffic - even that not destined for the interface. This isn't supported in AWS Recommended architecture - Run command feature of systems manager, install a packet capture agent on ALL EC2 instances, configure the software to store the capture logs in a central location Key Policy Conditions \u00b6 Policy conditions can be used to specify a condition within a key policy or IAM policy. The condition must be tru for the policy statement to take effect You might want a policy statement to take effect only after a specific date has passed Allow or deny an action based the requesting service Predefined condition keys Kms:ViaService Scope down API calls to a CMK based on the AWS service from which it is called Is a condition key which can allow or deny access to your CMK depending on which service originated the request Possibilities Allow access to the CMK only for requests which come from S3 Deny all requests which come from Lambda ViaService can be used in Key Policies and IAM policies which control access to KMS resources The services that you specify must be integrated with KMS e.g S3, EBS, Systems Manager, SQS, Lambda Microservices \u00b6 Microservices Run in Containers Small independent services that communicate over well-defined APIs Does One thing only Easy to Support & Maintain Changes like bug fixes, upgrades, scaling and adding new features are very easy ECS Fargate is the preferred option-Serverless Or managed clusters of EC2 instances Deep integration with AWS services e.g. IAM, VPC, Route53 Used internally e.g amazon.com, Sagemaker, Amazon Lex EKS Fargate is the preferred option-Serverless Or managed clusters of EC2 instances Certified conformant Benefit of open source tooling from the community Container Security Best Practices Don't Store secrets Secrets Manager Use IAM roles instead of user credentials Don't run your containers as root! One Service per Container Minimize the attach surface Avoid unnecessary libraries Trusted images only Avoid using images from public repositories Image Scanning Scan for common vulnerabilities & exposures Protect infrastructure Use ECS Interface endpoints to avoid sending VPC traffic over the internet Encrypt in Transit Using TLS Use ACM Data Protection With VPCs \u00b6 VPC Introduction \u00b6 Think of a VPC as a logical datacenter in AWS Consists of IGWs (Or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, and Security Groups 1 Subnet = 1 AZ Security Groups are Stateful; Network Access Control Lists are Stateless No transitive Peering NAT Instances \u00b6 When creating a NAT instance, Disable Source/Destination check on the Instance NAT instances must be in a public subnet There must be a route out of the private subnet to the NAT instance, in order for this to work The amount of traffic that NAT instances can support dependes on the instance size. If you are bottlenecking, increase the instace size You can create high availability using Autoscaling Groups, multiple subnets in different AZs, and script to automate failover NAT Gateways \u00b6 Preferred by the enterprise Scale automatically up to 100 Gbps No need to patch Not associated with security groups Automatically assigned a public ip address (EIP) Remember to update your route table No need to disable Source/Destination checks More secure than a NAT Instance Cannot have SGs attached Network ACLS \u00b6 Your VPC automatically comes a default network ACL, and by default it allows all outbound and inbound traffic You can create a custom network ACLs. By default, each custom network ACL denies all inbound and outbound traffic until you add rules Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed Network ACLs contain a numbered list of rules that is evaluated in order, starting with the lowest numbered rule Network ACLs have separate inbound and outbound rules, and each rule can either allow or deny traffic Network ACls are stateless; responses to allowed inbound traffic are subject to rules for outbound traffic (and vice versa) Block IP Addresses using network ACLs not Security Groups NACLs are processes only when data enters or leaves subnets, before security groups NACLs work on IP and CIDR only. You can' reference AWS services TLS/SSL Termination Options \u00b6 When using an Elastic Load Balancer, you have the choice to wither terminate TLS/SSL connections either on the Load Balancer or on your EC2 Instances When we terminate TLS/SSL on the Load Balancer, this means the ELB decrypts the encrypted request and sends it on to your application servers as plain text over the local private network inside your VPC Benefits of Terminating TLS/SSL on the ELB Offloads the processing, EC2 has more resources to use for application processing Can be more cost effective less compute power needed to handle your application load Reduces administrative overhead if you have many EC2 instances Security Implication of Terminating at the ELB Traffic between the load balancer and your instance is unencrypted If you have a compliance or regulatory requirement to use encryption end-to-end all the way to your EC2 Instances, you would need to terminate TLS/SSL on the EC2 Instance Which Load Balancer to Use The Application Load Balancer only supports TLS/SSL termination on the Load Balancer and only supports HTTP/S Application Load Balancer supports SNI. Can be installed multiple certificates in one HTTPS Listener If you want to terminate TLS/SSL on your EC2 instances, you'll need to use a Network or Classic Load Balancer and you will also need to use the TCP protocol rather than HTTP/S Classic Load Balancer is a legacy option, but it may still come up in the exam Tips For best use of your EC2 compute resource, terminate TLS/SSL on the Elastic Load Balancer If there is a requirement to ensure traffic is encrypted all the way to the EC2 instance, terminate TLS/SSL on the EC2 instance If you need to terminate traffic at the EC2 instance, then you'll need to use the TCP protocol with Network or Classic Load Balancer Application Load Balancers is HTTP/HTTPS only - for other protocols like TCP, use Network or Classic VPC Flow Logs \u00b6 VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow Log data is stored using Amazon CloudWatch logs. After you've created a flow log, you can view and retrieve its data in Amazon Cloudwatch logs * Can be created at 3 levels * VPC * Subnet * Network Interface Level * You cannot enable flow logs for VPCs that are peered with your VPC unless the peer VPC is in your account * You cannot tag a flow log * After you've created a flow log, you cannot change its configuration; for example, you can't associate a different IAM role with the flow log * Not all IP Traffic is monitored * Traffic generated by instances when they contact the Amazon DNS server. If you use your OWN DNS server, the all traffic to that DNS server is logged * Traffic generated by a Windows instance for Amazon Windows license activation * Traffic to and from 169.254.169.254 for instance metadata * DHCP traffic * Traffic to the reserved IP address for default VPC router * Flow logs only have metadata, not the content NAT vs Bastions \u00b6 A NAT is used to provide internet traffic to EC2 instances in private subnets A Bastion is used to securely administer EC2 instances (using SSH or RDP) in private subnets. Session Manager \u00b6 Secure Remote Login to EC2 Instances Browser Based Interactive session using Powershell or bash. Console, CLI and SDK Single Solution Manage Windows and Linux Instances No RDP or SSH required No SSH keys, no Bastion host to manage AWS recommended approach For interactive sessions on EC2. On-premises physical or virtual hosts. Secured using TLS encryption and auditable The Secure way to administer your Instances Centralized Access Control Using IAM you can control which individual users or groups in your organization can use Session Manager and which instances they can access No ports to open No need to open inbound SSH, RDP, Remote Powershell ports. Increased security Session Logs Connection history recorded in CloudTrail. Session history with keystroke logging can be sent to Cloudwatch or S3 Tips Remote Login Browser, CLI or SDK Powershell or Bash interactive sessions The Most secure option TLS encrypted, no bastion hosts or ports required Everything is logged Encrypted connection & session logging available using Cloudtrail, cloudwatch and s3 Amazon DNS \u00b6 When you create a VPC, your new VPC automatically includes an Amazon provided DNS server which is used to resolve public DNS hostnames Used for DNS hostname resolution for instances in your vpc which are communicating over the internet The Amazon DNS server uses one of the reserved IP address in your VPS CIDR range: In a subnet CIDR block of: 10.0.0.0/16 10.0.0.0 is the network address 10.0.0.1 is your VPC router 10.0.0.2 is the DNS server 10.0.0.3 reserved for future use 10.0.0.255 is usually the network broadcast address but as broadcast is not supported in AWS that is also reserved If you do not want to use the Amazon provided DNS server and instead you want to use a custom DNS server, you can disable this in the settings of your VPC Go TO DNS Resolution and uncheck the box Create a new DHCP options set to use your own custom DNS Transit Gateway \u00b6 Any connected VPC is automatically available to every other connected network. Route tables control which VPCs can commmunicate * Benefits * Highly Scalable * Support 1000s of VPCs with a single transit gateway which scales as you grow * Hub & Spoke * Create and manage a single connection from your data centre to the transit gateway. Centralized connectivity policies * Secure * Traffic between your VPCs and the Transit Gateway is on the AWS network. Inter-region traffic is encrypted * Tips * Centralised Connectivity * Connect VPCs and on-premises networks using a single gateway * When to use It -> if you have 100+ VPCs * Avoid managing a lots of point-to-point connections * Scalable - 1000s of VPCS * A new VPC connected to the Transit Gateway is automatically available to every other connected network * Secure - AWS private network * Traffic between VPCs does not use the public internet. Inter-region traffic encrypted S3 \u00b6 S3 Bucket Policies S3 Bucket policies are attached only to S3 Buckets. S3 Bucket policies specify what actions are allowed or denied on the bucket. They can be broken down to a user level, So Alice can PUT but not DELETE and John can READ but not PUT. Bucket level only, S3 only Explicit Deny always Overrides an Allow Use Cases IAM Policies bump up against the size limit (up to 2kb for users, 5 kb for groups and 10kb for roles). S3 supports bucket policies of up 20kb S3 ACL's Are Legacy access control. AWS recommend IAm Policies and S3 Bucket Policies Access Control only Use them if you need apply policies on the objects themselves. Bucket policies can be applied at bucket level, and S3 ACLS can be applied to individual files (Objects) Only can grant permissions to AWS Accounts. Cannot select IAM Users Cross-Account Access To S3 Buckets and Objects ACL Objects are owned by the identity who puts them If account B put objects in Bucket of Account A, the owner is Account B Bucket Policy Account B users are the owner of any objects created (Simniliar ACL) Permission control is handled within S3. There is no IAM involvement Bucket policies can require Account A (Bucket Owner) be the owner for objects as they are put in the bucket Utilize if want maintain permissions control inside S3 Can utilize anonymous and authenticated access. Utilize bucket polices to this { \"Statement\" :[ { \"Effect\" : \"Allow\" , \"Principal\" :{ \"AWS\" : \"311407276115\" }, # Account B \"Action\" : \"s3:PutObject\" , \"Resource\" :[ \"arn:aws:s3:::la-permissionsdemo/*\" ] }, { \"Effect\" : \"Deny\" , \"Principal\" :{ \"AWS\" : \"311407276115\" }, # Account B \"Action\" : \"s3:PutObject\" , \"Resource\" : \"arn:aws:s3:::la-permissionsdemo/*\" , \"Condition\" : { \"StringNotEquals\" : { \"s3:x-amz-acl\" : \"bucket-owner-full-control\" } } } ] IAM Role Users of account B assume a rule in account A (sts:AssumeRole) Objects are owned by that role so Account A Permissions are managed by IAM, not S3 Prefer option, best pratices Data at Rest: S3 Customer Provided Encryption Keys (SSE-C) SSE-C is a feature of Server side Encryption where S3 still handles the cryptographic operations, but does so with keys that you as the customer manage and supply every object operation x-amz-server-side-encryption-customer-key allows the key to be provided. The is used for encryption and then discarded. The customes is 100% responsible for key management and rotation. Versions can have alternative keys x-amz-server-side-encryption-customer-key-MD5 allows S3 to validate the key (for damage is transit) x-amz-server-sode-encryption-customer-algorithm = AES256 informs S3 that a customer managed key will be supplied as part of the putObject request Identity Federation \u00b6 AWS Supports federation with IdPs (identity Providers) which are OpenID connect (OIDC) Or Saml 2.0 compatible Identity federation is generally grouped into three types Web identity Federation SAML 2.0 identity Federation Custom ID Broker Federation (used when SAML2.0 compatability isn't available) Web Identity Federation Federation allows users to authenticate with a web identity provider (Google, Facebook, Amazon) The User authenticates first with the Web ID Provider and receives an authentication token, which is exchanged for temporary AWS credentials allowing them to assume an IAM role Cognito is an Identity Broker which handles interaction between your applications and the Web ID provider (You don't need to write you own code to do this) Provides sign-up, sign-in, and guest user access Syncs user data for a seamless experience across your devices Cognito is the AWS recommended approach for Web ID federation particularly for mobile apps Use Cases Cognito brokers between the app and Facebook or Google to provide temporary credentials which map to an IAM role allowing access to the required resources No need for the application to embed or store AWS credentials locally on the device and it gives users a seamless experience across all mobile devices SAML 2.0 Federation Enterprise Solution Use your own ID Provider (Not AWs provider) IP Provider, normally AD Authenticate in AD , returns SAML Assertion (similiar token in web identity), connects to AWS SSO enpoint and validate SAML assertion, and STS generates credentials and opens Console AssumeRoleWithSAML operations In ADFS, configure trusted with AWS as the relying party - This configuration is done in Active Directory Federation services System Manager Parameter Store \u00b6 Confidential information such as password, database connection string, and license codes can be stored in SSM Parameter store You can store values as plain text or you can encrypt the data (Using KMS) You can then reference these values by using their names You can use this service with , cloudformation, lambda, EC2 Run Command etc Key Features Configuration and data is separated from code - no chance of leakage via git Data is stored hierarchically - aids management Data is versioned, and access can controlled and audited Parameter store integrates with many AWS services - EC2, ECs, Lambda, CodeBuild/Deploy, and many more Can also be used for automated deployment using cloudformation Serverless, resilient and scalable Incident Response & AWS in The real world \u00b6 DDOS \u00b6 DDOS Attack A distributed denial of service (DDoS) attach is an attack that attempts to make your website or application unavailable to your end users This can be achieved by multiple mechanisms, such as large packet floods, by using combination of reflection and amplification techniques, or by using large buttons Amplification/Reflection Amplification/Reflection attacks can include things such as NTP, SSDP, DNS, Chargen, SNMP attacks, etc. and is where an attacker may send a third party server (such as an NTP server) a request using a spoofed IP address That server will then respond to that request with a greater pauload than initial request (usually within the region of 28 x 54 times larger than the request) to the spoofed IP address This means that if the attacker sends a packet with a spoofed IP address of 64 bytes, the NTP server would respond with up 3.456 bytes of traffic. Attackers can co-ordinate this and use multiple NTP servers a second to send legitimate NTP traffic to target How to mitigate DDos Minimize the Attack surface area Be ready to scale to absorb the attack Safeguard exposed resources Minimize the attack surface area Some production environments have multiple entry points in to them. Perhaps they allow direct SSH or RDP access to their web servers/application and DB servers for management This can be minimized by using a Bastion/JumpBox that only allows access to specific white listed IP address to these bastion servers and move the web, application, and DB servers to private subnets. By minimizing the attack surface area, you are limiting your exposure to just a few hardened entry point Be Ready to Scale to Absorb the Attack The key strategy behind a DDoS attack is to bring your infrastructure to breaking point. This strategy assumes one thing: that you can't scale to meet the attack The easiest way to defeat this strategy is to design your infraestructure to scale as, and when, it is needed You can scale both horinzontally & Vertically Scaling has the following benefits The attack is spread over a larger are Attackers then have to counter attack, taking up more of their resources Scaling buys you time to analyze the attack and to respond with the appropriate countermeasures Scaling has the added benefit of providing you with additional levels of redundancy Safeguard Exposed Resources In situations where you cannot eliminate internet entry points to your applications, you will need to take additional measures to restrict access and protect those entry points without interrupting legitimate end user traffic Three resources that can provide this control and flexibility are Amazon cloudFront, Amazon Route 53 and Web Apllication Firewalls (WAFs) CloudFront Geo Restriction/Blocking - Restrict access to users in specific countries (Using whitelists or blacklists) Origin Access identity - Restrict access to your S3 bucket so that people can only access S3 using Cloudfront URLS Route53 Alias Record Sets - You can use these to immediately redirect your traffic to an CloudFront distribution, or to a different ELB load balancer with higher capacity EC2 instances running WAFs or your own security tools. No DNS change, and no need to worry about propagation Private DNS - Allows you to manage internal DNS names for your application resources ( web servers, application servers, databases) without exposing this information to the public internet WAFs DDoS attacks that happen at the application layer commonly target web applications with lower volumes of traffic compared to infrastructure attacks. To mitigate these type of attacks, you'll want to include a WAF as part of your infraestructure New WAF Service - You can use the new AWS WAF service AWs MarketPlace - You can buy other WAF's on the AWS Marketplace Learn Normal Behavior Be aware of normal and unusual behavior Know the different types of traffic and what normal levels of this traffic should be Understand expected and unexpected resource spikes What are the benefits Allows you to spot abnormalities fast *`You can create alarms to alert you of abnormal behaviour Helps you to collect forensic data to understand the attack Create a plan for attacks - Having a plan in place before an attack ensures that You've validated the design of your architecture You understand the costs for your increased resiliency and already know what techniques to employ when you come under attack You know who to contact when an attack happens AWS Shield Free service that protects all AWs customers on ELB, CloudFront, and Route 53 Protects against SYN/UDP floods, reflection attacks, and other layer3/layer4 attacks Advanced provides enhanced protections for your applications running on ELB, Cloudfront and Route 53 against larger and more sophisticated attack. 3000 per month AWs Shield Advanced Provides Always-on, flow-based monitoring of network traffic and active application monitoring to provide near real-time notifications of DDoS attacks DDos Response Team (DRT) 24x7 to manage and mitigate application layer DDoS attacks Protects your AWS bill against higher fees due to ELB, cloudfront and Route53 usage spikes during DDoS attack Web Application Layer Attacks Mitigate using a WAF Block using a WAF Rate-based blacklisting Invest ime in limiting query string and header fowarding - Eliminates many common attacks deploy HTTP->HTTPs redirect at the edge - shields the origin from redirect floods Implement an SNI-based infrastructure - Many DDoS toolkits fail TLS handshake Tips Read white paper https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Papper.pdf technologies can use to mitigate DDoS attack CloudFront Route53 ELB's WAFs Autoscaling (use for both WAFs and Web servers) Cloudwatch WAF Integration \u00b6 WAF (Web application Firewall) integrates with both Application Load balancers and CloudFront. It does not integrate with EC2 directly, nor Route53 or any other services EC2 Has been Hacker \u00b6 Some Scenarios around EC2 being compromised. What steps should you take Stop the instance immediately Take a snapshot of the EBS Volume Deploy the instance in to a totally isolated environment. isolated VPC, no internet access - ideally private subnet Access the instance using a foresinc workstation Read through the logs to figure out how (Windows Event Logs) Pen Tests \u00b6 Allowed without approve in: EC2 Instances, Nat Gateways and ELB RDS Cloudfront Aurora Api Gateway Lambda and Lambda Edge Lightsail Beanstalk Prohibited Activies DNS zone walking via Route 53 hosted zones DoS, DDoS, Simulated DoS and DDoS Port flooding Protocol flooding Request flooding (login request flooding, API request flooding) To run Penetration testing on AWs go to AWS Marketplace and search for a penetration testing tool like kali linux AWS Certificate Manager - Data in Transit \u00b6 Managed service providing X509 v3 SSL/TLS Certificates. The certificates are Asymmetric. One half is private and stored on resources (Servers, LoadBalancers etc) and the other half is public SSL Certificates renew automatically (Valid for 13 months) provided you purchased the domain name from Route 53 and it's not for an Route 53 private hosted zone Integrates with Route53 to perform DNS checks as part of certificate issuing process You can use Amazon SSL certificates with both load balancers and cloudfront, beanstalk and API Gateway You cannot export the certificates No cost Regional KMS is used - certificates are never stored unencrypted AWS handles the painful parts of PKI Key pair and certificates signing request feneration Encryption and secure storage of private keys Managed renewal and deployment Domain validation (DV) through DNS validation/email Api Gateway \u00b6 To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API When request submissions exceed the steady-state request rate and burst limites, API Gateway fails the limit-exceeding requets and returns 429 Too Many Requests error responses to the client By default, API Gateway limits the steady-state request rate to 10000 requets per second (rps) It limits the burst to 5000 requets across all APIs within AWS The account-level rate limit and burst limit can be increased upon request Caching You can enable API caching in Amazon APi Gateway to cache your endpoints response. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of the requests to your API When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. APi Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled System Manager EC2 Run Command \u00b6 You work as System administrator managing a large number of EC2 instances and on premise systems You would like to automate common admin tasks and ad hoc configuration changes e.g. installing applications, applying the latest patches, joining new intances to an Windows domain without having to login to each instance Commands can be applied to a group os systems based on AWS instance tags or be selecting manually SSM agent needs to be installed on all your managed instances The commands and parameters are defined in a Systems Manager Document Commands can be issued using AWs Console, AWS CLI, AWS Tools for Windows PowerShell, Systems Manager API or Amazon SDKs You can use this service with your on-premise systems as well as EC2 instances Compliance \u00b6 ISO 27001:2005/10/13 Specifies the requirements for establishing, implementing , operating, monitoring, reviewing, maintaining and improving a documented information security management system within the context ot the organiztion's overall business risks HIPPA HIPAA is the federal health insurance portability and accountablility act of 1996. The primary goal of the law is to make it easier for people to keep health insurance, protect the confidentiality and security of healthcare information and help the healtcare industry control administritve costs PCI DSS V3.2 Build and Maintain a Secure Network and Systems Requirement 1: install and maintain a firewall configuration to protect cardholder data Requirement 2: Do not use vendor-supplied defaults for system passwords and other security parameters Protect cardholder data Requirement 3: Protect stored cardholder data Requirement 4: Encrypt transmission of cardholder data across open, public networks Maintain a vulnerability Requirement 5: Protect all systems against malware and regularly update anti-virus software or programs Requirement 6: develop and maintain secure systems and applications Implement Strong Access control Measures Requirement 7: Restrict access to cardholder data by business need to know Requirement 8: identify and authenticate access to system components Requirement 9: Restrict physical access to cardholder data Regularly Monitor and Test Networks Requirement 10: Track and monitor all access to network resources and cardholder data Requirement 11: Regularly test security systems and processes SAS70 SOC1 FISMA FIPS 140-2 - Cloud HSM meets the level 3 standard . KMS don't meet this Chapter 8 \u00b6 Athena \u00b6 Interactive query service which enables you to analyse and query data located in S3 using Standard SQL * Serverless, nothing ro provision, pay per query / per TB scanned * No need to set up complex Extract/Transform/Load (ETL) processes * Works directly with data stored in S3 What can Athena be used for * Can be used to query log files stored in S3, e.g. ELB logs, S3 access logs * Generate business reports on data stored in S3 * Analyse AWs cost and usage reports * Run queries on click-stream data Exam Tips * Commonly used to analyse log data stored in S3 Macie \u00b6 What is PII (Personally identifiable information) Personal data used to establish an individuals identity This data could be exploited by criminals, used in identity theft and financial fraud Home address, email address, SSN Passport number, Drivers license number D.O.B, phone number, bank account, credit card number What is -> Security service which uses Machine Learning and NLP (Natural languague Processing) to discover, classify and protect sensitive data stored in S3 Uses AI to recognise if your S3 objects contain sensitive data such as PII Dashboards, reporting and alerts Works directly with data stored in S3 Can also analyze Cloudtrail logs Great for PCI-DSS and preventing ID theft Macie Classifies your data By content Type JSON, PDF, Excel, TAR or Zip file, source code, XML By File extension .bin, .c, .bat, .exe, .html, .sql By theme AmEx, Visa, MasterCard credit card keywords, banking or financial keywords, hacker and web exploitation keywords By Regular Expression aws_secret_key, RSA Private key, SWIFT codes, cisco router config How can Macie Protect your data Analyze and classify the data Dashboards, Alerts and Reports on the presence of PII Gives visibility on how the data is being accessed Analyze Cloudtrails logs and report on suspicious API activity Tips Macie uses AI to analyze data in S3 and helps identify PII Can also be used to analyse Cloudtrail logs for suspicious API activity Includes dasboards, Reports and alerting Great for PCS-DSS compliance and preventing ID theft GuardDuty \u00b6 Guardduty is a threat detection service which uses ML to continuously monitor malicious behaviour Unusual API calls, calls from a know malicious IP Attempts to disable CloudTrail logging Unauthorized deployments Compromised instances Reconnaissance by would be attackers Port scanning, failed logins Features Alerts appear in the Guardduty console & Cloudwatch events Receives feed from third parties like Proofpoint, Crowdstrike and AWS Security - know malicious domains / IP addresses Monitors cloudtrail logs, VPC flow logs, DNS logs Centralize threat detection across multiple AWS accounts Automated response using cloudwatch events and lambda Machine learning and anomaly detection Setting up 7-14 days to set a baseline - What is normal behaviour on your account Once active you will see findings on the GuardDuty console and in cloudwatch events only if GuardDuty detects behaviour it considers a threat 30 days free. Charges based on Quantity of cloudtrail events volume of DNS and VPC flow log data Tips Uses AI to learn what normal behaviour looks like in your account and to alert you of any abnormal or malicious behaviour Updates a database of know malicious domains using external feeds from third parties Monitor cloudtrail logs, VPC flow logs, DNS logs Findings appear in the Guarduty dashboard, cloudwatch events can be used to trigger a lambda function to address a threat Secrets Manager \u00b6 Secrets Manager is a service which securely stores, encrypts and rotates your DB credentials and other secrets Encryption in-transit and at rest using KMS Automatically rotates credentials Apply fine grained access control using IAM Policies Your application makes an API call to secrets Manager to retrieve the secret programmatically Reduces the risk of credentials being compromised Rotation Options Multi-User Rotation Separate master user credentials are used for secret rotation. The old version of secret continues to operate and handle service requests, while the new version is prepared and tested. The old version isn't deleted until after the clients switch to the new version. There is no downtime while changing between versions Single-user Rotation Secrets master uses a single user to rotate its own credentials. Sign-in failures can occur between the moment when the old password is removed by rotation and the moment when the updated password is made accessible as a new version of the secret. This time window should be very short, but it can happen To avoid this, use Multi-User Rotation or exponential back-off What Can Store RDS credentials Credentials for non-RDS databases Any other type of secret provided you can store it as a key value pair (SSH keys, API keys) Enabling Secrets manager Automatic Rotation Important: if you enable rotation, secrets manager immediately rotates the secret once to test the configuration Ensure that all of your applications that use these credentials are updated to retrieve the credentials from this secret using secrets manager Disable automatic rotation if your applications are still using embedded credentials do not enable rotation because the embedded credentials will no longer work and this will break your application Enable Rotation - recommended setting if your applications are not already using embedded credentials i.e. they are not going to try to connect to the database using old credentials Tips Secrets Manager can be used to securely store your application secrets: DB credentials, API keys, SSH keys, passwords etc Applications use the secrets manager API Rotating credentials is super easy - but be careful When enabled, Secrets manager will rotate credentials immediately Make sure all your application instances are configured to use Secrets manager before enabling credential rotation SES \u00b6 SES is a cloud based email service, which supports both sending and receiving email Can be used to send marketing emails, transaction emails and email notifications from your applications It uses a standard SMTP interface and can also be accessed using an API to allow you to integrate with existing applications All connections to the SMTP endpoint must be encrypted in transit using TLS Configuring Access to SES for EC2 instances * Configure the security group associated with your ec2 instances to allow connections to the SES smtp endpoint * Port 25 is the default but EC2 throttles email traffic over port 25 * To avoid timeouts use either port 587 or 2587 Security Hub \u00b6 Central hub for security alerts one place to manage and aggregate the findings and alerts from key AWS security services Automated checks PCS-DSS (Payment card industry) CIS (center for internet security) Ongoing Security Audit automated ongoing security audit for your AWS accounts Integration with Guardduty Macie Inspector IAM Access Analyzer Firewall manager 3 Party tools CloudWatch From cloudwatch cand send the events to lambda/chat/SIEM/notification/... Network Packet Inspection \u00b6 Inspects packet headers and data content of the packet * Also know as Deep Packet Inspection (DPI) * Filters non-compliant protocols viruses, spam, intrusions * Takes action blocking, re-routing or logging * IDS / IPS combined with a traditional firewall What AWS provide VPC Flow Logs Capture network flow for a VPC, subnet or network interface, storing the data in cloudwatch logs, can be used for troubleshooting and profiling network flow AWS WAF protects web applications against know exploits like SQL injection and cross site scripting host based firewalls like Iptables / windows Firewall IDS /IPS Use a third party solution AWS does not provide a solution for Network packet inspection, IDS/IPS You will need to run host based solution in EC2 - Alert Logic, Trend Micro, Mcafee A host based IDS solution compliments the features available within AWS WAF - Provides edge security before a threat arrives at your environment edge IDS Appliance - Monitor and analyses data as it moves in your plafform AWs Config - Ensures a stable and compliant configuration of account level aspects SSM - Ensures compute resources are compliant with patch levels Inspector - Reviews resources for know exploits and questionable OS/Software configurations Host Based IDS - Handles everything else Active Directory Federation with AWS \u00b6 AWS allows federated sign-in to AWS using AD credentials Minimises the admin overhead by leveraging existing user accounts, passwords, password policies and groups Provides SSO for users ADFS - Active Directory Federation Services, SSO and ID broker solution SAML 2.0 Security Assertion Markup Language, Open standard for exchanging identity and security information between identity providers and applications. Enables SSO for AWS accounts AD federation with AWS 2-way trust: In AWs, ADFS is trusted as the ID provider In ADFS, configure a Relying Party trust with AWS as the Relying party Corporate user accesses the Corporate AFS portal sign-in and provides their AD username and password ADFS authenticates the user against AD AD returns user's information including group membership ADFS sends a SAML token to the user's browser which sends the token the AWS sign-in endpoint The AWS sign-in endpoint makes an STS AssumeRoleWithSAML request and STS returns temporary credentials User is authenticated and allowed to access the AWS management console ADFS baiscally acts as an identity broker between AWS and your Active Directory AD users can assume roles in AWS based on group membership in AD Tips Remember how it works at a high level User authenticates with ADFS/AD first, they receive a SAMl token which is exchanged with the AWs sign-in endpoint for temporary credentials to the AWS console (STS API AssumeRoleWithSAML) User is automatically redirected to the AWS console No need to create duplicate accounts in AWS ADFS it the trusted ID provider, AWS is the Trusted Relying Party Artifact \u00b6 Central resource for compliance & security related information Download AWS security and compliance documents - Widely used industry standards ISO certifications PCI (Payment card industry) SOC Reports (Service Organizational control) AWS Artifact enables you * Demonstrate compliance to regulators * Evaluate your own cloud architecture * Assess the effectiveness of your company's internal controls AWS Encryption SDK \u00b6 Is an encryption library that helps make it easier for you implement encryption best practices in your application Data Key Caching Troubleshooting & Monitoring & Alerting \u00b6 Troubleshooting Cloudwatch \u00b6 Common Issues Does the IAm user have the correct permissions to allow them to read the coudwatch dashboard? Check that cloudwatch events has permission to invoke the event target Check the lambda has permissions to terminate EC2 CloudWatch Events - Common issues General configuration issues Wrong resource name when connection resources Typos - This should be checked during configuration, but we all make mistakes Huge with API calls, filter patterns, and lambda functions Not waiting long enough after making changes or new configurations Roles do not have sufficient permissions (AWS does not always tell you with an error) This includes targets and subscriptions to encrypted resources - must include KMS policies CloudEvents With Cloudwatch, we can create an alarm on Events Metrics. We can use FailedInvocations to notify us when our Cloudwatch Events rules are broken Lambda functions Lambda delivers logs to cloudwatch logs. It will log errors with invocations. We can then alarm on this using a metric filter and notify via SNS Cloudtrail Trail and logging enabled Cloudwatch logs configured Role for Cloudwatch logs Log group name VPC Flow Flogs Flows enabled on VPC or Subnet Check Filter Role for cloudwatch logs Log group name Route 53 DNS Logs Query loggin configured New or existing log group Log group name EC2 Agent installation and config Agent must be started Role For cloudwatch logs Trouleshooting CloudTrail \u00b6 S3 Bucket Problems Cloudtrail logs not appearing in S3? Check the Cloudtrail is enabled Check you have provided the correct S3 bucket name Important to remember that S3 and lambda data events are high volume so they are not enabled by default Touble accessing the cloudtrail logs? Check your user has read access to cloudtrail: AWSCloutRailReadOnlyAccess Policy Check All Regions Enabled Troubleshooting Secure Network Infra \u00b6 Check routing tables, Security Groups, NACLs Remember NACLs are stateless - Need configure both inbound and oubount rules Security Groups deny by default, use NACL to explicitly deny If you are peering 2 VPCs, remember to configure routing tables in both VPCs Internet access - NAT Gateway, Internet Gateway check VPC flow logs to view allow / deny messages Troubleshooting Identity Federation \u00b6 Use the Correct API for the JOB Authenticated by a Web identity Provider - Facebook ect STS:AssumeroleWithWebIdentity Autenthicated by a SAML Compliant ID provider - AD STS:AssumeRoleWithSAML Authenticated by AWS STS:AssumeRole Troubleshooting Cross Account Roles \u00b6 Cross-Account Roles are IAM Roles in an account A which are accessed by identities in account B Like all roles, they consist of two parts: a trust policy, and a permissions policy Using STS:AssumeRole - Common Issues Check the external account has permission to call STS:AssumeRole - Dev Account IAM Policy Check the external account is trusted AND has permission to perform the action you are attempting - Prod Account, Role Using STS:AssumeRole - Access to KMS The key policy needs to trust the external account The external account needs an IAM policy allowing users to run specific API calls for the resource (in this case the CMK) Tips For cross account access to S3: check that the account is trusted, the iam policy the external account needs to allow the user to call STS:AssumeRole, The iam policy in the trusting account needs allow the action Cross account access to kms, check you have configured the key policy to allow access to the external account as well as the iam policy in the local account Troubleshooting Lambda Access \u00b6 Lambda cannot perform some action e.g. write to S3, Log to Cloudwatch, terminate instances, use a CMK, etc - Check the Lambda execution role has the correct permissions Remember that some services have their own resource based policies which will also impact who or what can access them - S3 bucket policies, key policies If cloudwatch events or some other event source cannot invoke lambda function, double check that the function policy allows it Troubleshooting Access to KMS CMKs \u00b6 Access to use KMS CMKs is defined by: Key Policy - Resource based policy attached to the CMK, defines key users and key administrators and trusted external accounts IAM Policy - Assigned to User, Group Or Role, defines the allowed actions e.g. kms:ListKeys, kms:Encrypt, kms:Decrypt Important : There is no hard truste between a CMK and an account. The permission can be removed, resulting in unusable key and a requirement to involve AWS Support Key admins can admin keys, not use them. Permissions granted via IAM Policy, or Key Policy or both Troubleshooting KMS \u00b6 KMS Permissions Permissions within KMS are centered around Customer Master Key (CMKs) A default policy to a CMK trusts the account the key is created within, and this trust can be provided to IAM users via IDENTITY policies, or, on the KEY policy itself Permissions within KMS are either ADMIN permissions or Usage Permissions You can Lock out a CMK making it unusable to everyone KMS Limits Simple Limits 1000 (customer managed) CMKS per region - in Any state 1100 Aliases per account 2500 Grants per CMK - e.g max of 2500 EBS Volumes using CMK Breaching the shared, or per operation limits result in KMS throttling the requests Rate Limit There is a 5500 Shared API limit shared across a number of operations relating KMS - the high volume operations Actions With rate Limit: Decrypt, Encrypt, GenerateDataKey, GenerateDataKeyWithoutPlainText, GenerateRandom, ReEncrypt Troubleshooting S3 Access Logs \u00b6 Confusion over source and destination buckets Log delivery group permissions Troubleshoot Multi-Account logging \u00b6 CloudTrail logging across multiple accounts S3 bucket policy for accounts sending logs Bucket names should be double-checked for accuracy CloudWatch Logs across multiple accounts Cloudwatch does not send logs directly to another account S3 access issues blocking exports (scheduled or manual) Kinesis stream is not setup properly (only target for real-time logs) Common Issues with Multi-Account Logging Issues will mostly be around permissions (roles and resource policies) Make sure all permissions only grant read-only access","title":"Security-Specialty"},{"location":"certifications/aws/security-specialty/#aws-certified-security-specialty","text":"","title":"AWS Certified Security Specialty"},{"location":"certifications/aws/security-specialty/#resources","text":"AWS Security White Paper: Introduction to AWS Security Processes Linux academy Guide WhitePapers KMS best Practices KMS Cryptographic Details DDOS Best Practices Logging in AWS Well-Architected framework - Security Pillar Re:invent Videos (2017) - NET 3XX - Medium level , NET 4XX - Advanced KMS best Practices AWS Encryption Deepdive Become an IAM Policy Master DDOS Best Practices VPC fundamentals & Connectivity options Logging in AWS Advanced security best practices masterclass Advanced VPC Design and New captabilities for Amazon VPC FAQs https://aws.amazon.com/faqs/ Security, identity & Compliance","title":"Resources"},{"location":"certifications/aws/security-specialty/#exams","text":"Exam Guide AWS Practice Exam AWS Practice Exam 2 Quiz 1 Quiz 2 Quiz 3 Quiz 4 Exam Sample Questions ExamTopics Cloud Guru Linux Academy","title":"Exams"},{"location":"certifications/aws/security-specialty/#introduction","text":"","title":"Introduction"},{"location":"certifications/aws/security-specialty/#security-101","text":"","title":"Security 101"},{"location":"certifications/aws/security-specialty/#security-basics-models","text":"CIA Confidentiality - IAM, MFA Integrity - Certificate Manager, IAM, Bucket Policies Availability - Auto-Scaling, Multi-AZ AAA Authentication - IAM Authorization - Policies Accounting - Cloudtrail Non-repudiation (Can't deny you did something)","title":"Security Basics - Models"},{"location":"certifications/aws/security-specialty/#shared-responsibility-model","text":"Infrastructure (EC2, EBS, VPC) Container (RDS, EMR, Elastic Beanstalk) Abstracted (S3, Glacier, DynamoDB, SQS, SES)","title":"Shared Responsibility Model"},{"location":"certifications/aws/security-specialty/#security-in-aws","text":"Visibility AWS Config Auditability AWS CloudTrail Controllability AWS KMS - Multi-Tenant AWS CloudHSM - Dedicated - FIPS 140-2 Compliance Agility AWS Cloudformation AWS Elastic Beanstalk Automation AWS OpsWorks AWS CodeDeploy","title":"Security IN AWS"},{"location":"certifications/aws/security-specialty/#iam-s3-security-policies","text":"","title":"IAM, S3 &amp; Security Policies"},{"location":"certifications/aws/security-specialty/#resetting-root-users","text":"Create a new root user password and strong password policy Delete previous 2 factor authentication and re-create Check if the root user has an Access Key Id and Secret Access Key. If so delete these immediately Check other user accounts. Verify they are legitimate and if not, delete these","title":"Resetting Root Users"},{"location":"certifications/aws/security-specialty/#iam-policies","text":"IAM is global. Applies to all areas of AWS, not just S3 Three different types os IAM Policies AWS Managed Policies Customer Managed Policies Inline Policies Components Version: current lastest is 2012-10-17 - check this is correct Statement: Enclosed {}, and deliminated by commas . make sure the formatting is good Effect: Needs to be Allow or Deny Principal: The entity which the statement applies to, invalid for IAM user policies since it's implied - be careful to make sure it exists and is correct for resource policies Action: The api Actions which the statement refers to. Make sure the actions and resources match. Resource: One or more ARNs or wildcards which refert to AWS objects Condition (optional): Conditions for the statement. Check for validity. sourceIP won't work with endpoints","title":"IAM Policies"},{"location":"certifications/aws/security-specialty/#permissions-boundaries","text":"A permissions policy allows or denies, actions on resources. Policies are applied to identities (Users, Groups, Or Roles) in the case of identity policies, or resources in the case of resources policies Permissions boundary is a set of access which an entity (user, role, organisation) can never exceed It can act as a safety net to ensure adherence to organizational policies, or it can act as delegation tools A permissions boundary on its own grants no permissions, it only restricts","title":"Permissions Boundaries"},{"location":"certifications/aws/security-specialty/#policy-evaluation","text":"With least-privilege, decisions ALWAYS default to DENY ALSO an explicit DENY ALWAYS trumps an ALLOW If no method specifies an ALLOW, then the request will be denied by default Only if no method specifies as DENY and one or more methods specify an ALLOW will the request be allowed Evaluation Order Boundaries are always processed first, starting with organizational and then identity (User or Role) Then AWS checks if you have chosen a subset of permissions for sts:AssumeRole Final effective permissions are a merge of identity, resource, and ACL","title":"Policy Evaluation"},{"location":"certifications/aws/security-specialty/#security-token-service-sts","text":"Grants users limited and temporary access to AWS resources. Users can come from three sources: Federation (typically Active Directory) Uses security assertion markup language (SAML) Grants temporary access based off the users Active Directory credentials. Does not need to be a user in IAM Single sign on allows users to log in to AWS console without assigning IAM credentials Federation with Mobile Apps Use Facebook/Amazon/Google or other OpenID providers to log in Cross Account Access Let's users from one AWS account access resources in another Key Terms Federation: combining or joining a list of users in one domain (such as IAM) with a list of users in another domain (such as Active Directory, Facebook etc) Identity Broker: a service that allows you to take an identity from point A and join it (federate it) to point B Identity store: services like Active Directory, Facebook, Google etc Identities: a user of a service like Facebook etc Use Case 1. Employee enters their username and password 2. The application calls an identity broker. The broker captures the username and password 3. The identity broker uses the organization's LDAP directory to validate the employee's identity 4. The identity broker calls the new GetFederationToken function using IAM credentials. The call must include an IAM policy and a duration (1 to 36 hours), along with a policy tha specifies the permissions to be granted to the temporary security credentials 5. The STS confirms that the policy of the IAM user making the call to GetFederationToken gives the permission to create new tokens and then returns four values to the application: An Access key, a secret access key, a token, and a duration (the token's lifetime) 6. The identity broker returns the temporary security credentials to the reporting application 7. The data storage application uses the temporary security credentials (including the token) to make requests to S3 8. S3 uses IAM to verify that the credentials allow the requested operation on the given S3 bucket and key 9. IAM provides S3 with the go-ahead to perform the requested operation","title":"Security Token Service (STS)"},{"location":"certifications/aws/security-specialty/#cognito","text":"User Pools User pools are user directories used to manage sign-up and sign-in functionallity for mobile and web applications Users can sign-in directly to the user Pool, or indirectly via an identity provider like Facebook, Amazon, Or Google. Cognito acts as an Identity broker, handling all interaction with Web Identity Providers Successfull authentication generates a number of JSON Web Tokens (JWTs) Groups Collection of users in a user Pool, which is often done to set the permissions for those users Identity Pools Enable you to create unique identities for your users and authenticate them with identity providers. With an identity, you can obtain temporary, limited-privilege AWS credentials to access other AWS Services","title":"Cognito"},{"location":"certifications/aws/security-specialty/#glacier-vault","text":"Glacier is low-cost storage for archiving and long-term backup Files are stored as Archives (Single of Multiple files a .tar or .zip file), Archives are stored in Vaults A Vault Lock Policy allows you to configure and enforce compliance controls for Glacier Vaults, using Vault Lock Policy Similiar to an IAM Policy Configure WORM (write once read many) archives Create data retentions policys, ex: 5 year retention 2 Steps to configuring a Vault Policy Initiate the lock by attaching a vault policy to your vault, this sets the lock to an in-progress state You have 24 hours to validate the lock policy, once validated, Lock policies are immutable If the policy doesn't work as expected you can abort Enforce regulatory and compliance controls","title":"Glacier Vault"},{"location":"certifications/aws/security-specialty/#aws-organizations","text":"Allows you to organize your accounts into groups / OUs for access control and centralized billing Attach policy based controls - Service Control Policies Centrally manage permissions for OUs - groups of accounts or individual accounts Access to accounts created in Organizations is initially via a role OrganizationAccountAccessRole, which is created automatically SCP is used to centrally control the use of AWS Serices across multiple accounts Like a filter which restricts access to AWS services The SCP applies to all OUs and accounts below the OU to which it is attached Can be used to create a Permissions Boundary Restricting the actions the users, groups, roles in those accounts can do - including root SCPs can deny access only, then cannot allow","title":"AWS Organizations"},{"location":"certifications/aws/security-specialty/#iam-credential-report","text":"Credential Report List all users in your account and the status of their various credentials, including password, access keys, and MFA devices Console Head to IAM section, find credential report on the left and download CSV containing the report CLI aws iam generate-credential-report aws iam get-credential-report","title":"IAM Credential Report"},{"location":"certifications/aws/security-specialty/#domain-1-incident-response","text":"","title":"Domain 1 - Incident Response"},{"location":"certifications/aws/security-specialty/#given-an-aws-abuse-notice-evaluate-the-suspected-compromised-instance-or-exposed-access-keys","text":"AWS Acceptable Use Policy Compromised Resources and Abuse Abuse Activities: Externally observed behavior of AWS customer instances or resources that are malicious, offensive, illegal, or cloud harm other internet sites AWS will shut down malicious abusers, but many of the abuse complaints are about customers conducting legitimate business on AWS Example causes of abuse that are not intentional Compromised Resource: EC2 instance becoming part of a botnet then attacking other hosts on the internet. This traffic could be going to other AWS Accounts as well Secondary abuse One of your end-users posts an infected file on your resources. When that file calls \"home\", it is going to appear to be traffic generated in your account Application Function If you are using applications such as web crawlers, it can sometimes appear as DoS attack and AWS will react accordingly False complaints Other AWS users can report your activity to AWS. The complaint might appear legitimate, and AWS will react accordingly Responding to Abuse Notifications - There is a chance that the investigation of abuse will turn out be a compromised account or resource. If this is the case, the following AWS recommendations can help: Change the root password ant the password for all IAM users Add MFA to all admin users and anyone who access the AWS Console Create a new EC2 Key pair and update instances (deleting the compromised key) Create an AMI and Relaunch Edit the .ssh/authorized_keys file Delete for rotate potentially compromised IAM access keys Delete unrecognized or unauthorized resources Instances IAM Users Spot bids Contact AWs Support Respond to the notification Important: Do not ignore AWS abuse communications and make sure they have the most effective email address on file Be Proactive: Avoid being Compromised Vault root credentials and remove access keys if they exist Require a strong password and MFA on all IAM accounts Use roles whenever possible, do not trust humans Do NOT copy EC2 key pairs to instances and protect them on admin machines Rotate IAM access keys regularly There are People scanning repositories like Github for access keys, EC2 Key pairs, and other sensitive information. AWS has created a tool to hel prevent spillage: Git-secrets: Prevents committing secrets and sensitive information to gir repositories","title":"Given an AWS abuse notice, evaluate the suspected compromised instance or exposed access keys"},{"location":"certifications/aws/security-specialty/#verify-that-the-incident-response-plan-includes-relevant-aws-services","text":"Incident Response Framework Preparation Phase - Doing everything we can to prevent breaches and failures. Eventually some type of security event will happen, it always does. We are building walls and fortifying barricades here Be Proactive - Best Practices Risk Management - Determine where the different levels of risk are Principle of least privilege Architect for failure - High availability and fault tolerance always Train for the real thing - Test and simulate; a real incident is a horrible place to learn lessons Clear ownership and governance - Tag all resources so no time is wasted finding who or what group to contact Data classification - Tagging data stores with classification can quickly identify spilage AWs Services involved - IAM, VPC, Route 53, EC2, EFS, RDS, etc .... Limit the Blast - Carefull planning can reduce the \"blast radius\" of any attack. The ideia here is to segment/section off resources from each other Organizations - We can add accounts under our main account Benefits If there is a breach, it will not affect multiple accounts Service Control, Policies can be set so \"child\" accounts can be limited Using multiple Regions and VPCs can have a similiar affect Services involved: Organizations and VPC Log Everything - Logging is the best way to collect information about our environments. Centralized logging - Collect all the logs from the organization in one place Encrypt and protect (logs contain sensitive data that should not be clear text) It all starts with logs. The following pattern applies Logs => Events => Alerts => Response Services Involved: CloudTrail, VPC Flow Logs, EC2 OS & Apps Logs, S3, Cloudwatch Logs, Config, Lambda Encrypt All - Two Ways Server Side encryption (data-at-rest) Client Side encryption (data-in-transit) Important - Treat your data as if everyone is looking at it all the time because they might be Services Involved: KMS, S3, Certificate Manager, ELB, Route 53 Identification Phase - AKa detection phase, this is where we discover an incident is ocurring. We can do this through behavior-based rules we configure to help detect breaches. We must then determine the following: Intention - Knowing this can help us fin compromised resources quickly Blast Radius - What resources where effected? How deep did the attach go? Data Loss Protection - A combination of encryption and access control. What did they get\u00bb Resources needing clean up - What resources do we need to mitigate or isolate This phase can be very difficult, and we should be heavily dependent on automation to help us with detection. We can then react accordingly or even automate responses. There are also stealth techniques we can use to observe user behaviour without being detected if there is questionable behavior Services involved: Cloudwatch, S3 Events, Third Party tools Containment Phase - Removing the threat. There sould be tools and processes ready to make changes to isolate any compromised resources. The ideal situation would be CLI or SDK scripts we can deploy very quickly when needed. For fast isolation, we need to have following created or have scripts ready: A security group that restricts egress traffic and only allows management ports in A separate subnet with restrictive NACL we can move resources to An S3 Bucket policy that is designed to immediately stop spillage An explicit deny policy created in IAM (Deny *), quick removal of privileges A key policy that denies all decryption In addition there may be additional activities we should perform Snapshot volumes of compromised instances Stopping instances Disabling encryption keys in KMS Change Route 53 record sets Services Involved: VPC, S3, KMS, Route 53 Investigation Phase Investigation involves event correlation and forensics. We need to determine exactly what happened and when. We also need to determine if the threat is still viable. As soon as we start our investigation, forensics can begin. Whether we use live box, or dead box forensics here, proceed with caution and make sure it is in a safe, sandboxed environment. Services Involved: VPC, Flow Logs, EC2, Cloudtrail, Cloudwatch Eradication Phase - Try to remove all the infections and compromises in our resources. In most cases, we can just delete the resources. There are some additional concerns whe dealing with data If encryption was implemented correctly, data that was accessed should not be legible. In this case, we can do the following Delete/disable any KMS Keys For EBS, delete splilled files, create a new encrypted volume, copy all good files For S3 with S3 managed encryption, delete the object For S3 with KMS managed keys or customer keys, delete the object and the CMKs Secure wipe any affected files If our data was not encrypted on EBS, we can attempt to sanitize the volume Not recommended Create new columes or instances with clean files or restore them from \"Last know good\" backups Services Involved: KMS, EBS, S3 Recovery Phase - We need to put everything back to normal. This normally includes verifying eradicated resources and reversing the steps taken in the containment phase Restore resources one at time (or group) Use new encryption keys Restore network access Monitor, monitor, monitor Have the containment phase tools ready This phase can be potentially dangerous as the forensic process may not have revealed everything Follow-up Phase Testing and simulations are vital Must strive for efficiency (tagging, automation) Teams need experience","title":"Verify that the Incident Response plan includes relevant AWS services"},{"location":"certifications/aws/security-specialty/#evaluate-the-configuration-of-automated-alerting-and-execute-possible-remediation-of-security-related-incidents-and-emerging-issues","text":"Automated Alerting The services we use in the cloud make scalability and reliability easy. These concepts should apply to our logging, monitoring and alerting as well Architecture Logging (CloudTrail, VPC Flow Logs, Route 53 DNS Logs, EC2) => Cloudwatch Logs => cloudWatch Metric Filters => Alarms => Targets (SNS Topic, AutoScaling) Automated Response Once we get alerts generated in Cloudwatch, there are a log of target services we can trigger with those alerts. We can configure these target services to automatically remediate our resources CloudWatch Event Rules => Targets: Lambda - Function Systems Manager - Patch or Run command SNS Topic - Message or application SQS - Application Queue","title":"Evaluate the Configuration of Automated Alerting and Execute Possible Remediation of Security-Related Incidents and Emerging Issues"},{"location":"certifications/aws/security-specialty/#domain-2-logging-and-monitoring","text":"","title":"Domain 2 - Logging And Monitoring"},{"location":"certifications/aws/security-specialty/#s3-events","text":"Work at Object Level Events: RRSObjectLost Put Post Copy Complete Multipart Upload Delete Delete marker Created ObjectCreate (All) ObjectDelete(All) Then we send notification to three services SNS Topic SQS Queue Lambda Function","title":"S3 Events"},{"location":"certifications/aws/security-specialty/#s3-access-logs","text":"The default storage for CloudTrail is S3 Cloudwatch Logs can be exported to S3 S3 can help cost savings while still assisting with compliance Lifecycle policies to reduce storage costs Archive older logs to glacier S3 Access Logs Tracks access requests to buckets Each log event contains one access request Log events contain Requester Bucket Name Request Time Request action Response Status Error code Important features of s3 access logging The log delivery group must be granted write permission on the target bucket Not near-real-time logging - Can take one hour to propagate Logs are delivered on a best effort basis Newly enable access logs might not be displayed in the target bucket for up to an hour Changes to the target bucket might take up to an hour propagate","title":"S3 Access Logs"},{"location":"certifications/aws/security-specialty/#centralized-logging","text":"The Multi-Account Strategy Use Organizations and set up accounts by environments or function Production, Development, Staging, etc Security, Administration Will help reduce the blast radius of any incident An additional layer of security Cross-account roles Centralized logging Logs should be contained in one location (the complete picture) Logs should be read-only for most job functions ( including security) Logs should be encrypted (KMS Preferred) Roles Can provide cross account access","title":"Centralized Logging"},{"location":"certifications/aws/security-specialty/#cloudtrail","text":"Enables After-the-fact incident investigation Near-realtime intrusion detection Industry & regulatory compliance Provides Logs API call details (for supported services) Entries can be viewed using Event History (past 90 days) Validating CloudTrail Log File Integrity Was the log file modified, or deleted? CloudTail log file integrity validation: SHA-256 hashing SHA-256 with RSA for digital signing Log files are delivered with a 'digest' file Digest file can be used to validate the integrity of the log file What is Logged Metadata around API calls The identity of the API caller The time of the API call The source IP address of the API caller The request parameters The response elements returned by the service CloudTrail Event Logs Sent to an S3 bucket You manage the retention in S3 Delivered every 5 (active) minutes with up 15 minute delay Can be aggregated across regions Can be aggregated across accounts Notifications Available Setup Enabled by default (For 7 Days) Trail - A configuration allowing for logs to be sent to an S3 bucket Single region or multi-region trails can be configured trails can make multi-account logging possible Configuration Options Management events - Enabling will log control plane events, such as User login events Configuring Security Setting up logging Data Events which include Object Level events in S3 Function level events in Lambda Encryption flexibility Encrypted in S3 server side by default, can be changed to KMS The logs can be sent to an S3 bucket of choice and even prefixed (folders) Security Protect you CloudTrail logs, they contain everything that you are doing in your AWS account and may contain PII Allow your security people admin access to Cloudtrail, auditors read only access to CloudTrail using IAM Use Iam policies to restrict access to unauthorised people Restrict Access to S3 using bucket policies (That contain log files), and use MFA delete on your objects Use SSE-S3 or SSE-KMS to encrypt the logs Use lifecycle rules to move data to Glacier or to delete it Check the integrity of your log files using digest files Json and CSV file formats to export","title":"CloudTrail"},{"location":"certifications/aws/security-specialty/#cloudwatch","text":"Enables Resource utilization, operational performance monitoring Log aggregation and basic analysis Provides Real-time monitoring within AWS for resources and applications Hooks to event triggers Key Components CloudWatch CloudWatch logs Cloudwatch Events Notifications CloudWatch Logs Pushed from AWS services (including CloudTrail) Pushed from your applications/systems Stored indefinitely (not user S3) Can stream log data to lambda and Elasticsearch service Components Log Events - Record of activity recorded by the monitored resource Log Streams - Sequence of logs events from the same source/application Log Groups - A collection of logs streams with same access control, monitoring, and retention settings Metric Filters - Assigned to a log groups, it extracts data from the group's log streams and converts that data into Metric data point Retention Settings - Period of time logs are kept. Assigned to log groups, but applies to all the streams in a group (q day to never expire) Use cloudwatch logs to monitor, store, and access your log files from: Cloudtrail VPC flow logs Sent to cloudwatch per default Cloudwatch Agent DNS Logs DNS Query Logs can be enabled on Route53 hosted zones and sent to CloudWatch. Route 53 uses common DNS return codes in the log and includes the edge location (based on airport codes) These logs can be used to determine when there is a DNS problem in an application These logs are only available for hosted zones where Route53 is the endpoint (no outside hosting). Also, the logs are not available for private hosted zones Cloudwatch Metrics Metric Filters: Used to create a custom metric from log data Assigned at the log group level Will filter all the streams in that group Uses a filter and pattern syntax Example: { $.eventName = \"createUser\" } Metric Namespace: The Folder or category the custom metric will appear in Metric Name: The name given to the custom metric Alarms: Assigned to the filter Alarms can trigger: SNS Topics Autoscaling Actions EC2 actions (if the metric chosen is related) Cloudwatch Events Are similiar to alarms, Instead of configuring tresholds and alarming on metrics, CloudWatch Events are matching event Patterns Near real-time stream of system events Common issues: Permissions, Wrong ARN, Typos Three Parts Event Source AWS Resources state change AWS CloudTrail (API Calls) Custom Events (code) Scheduled Rules - match incoming events and route them to one or more targets Targets - Lambda functions, SNS topics, SQS queues, Kinesis streams There can be more than one Examples Alerting on object uploads in S3 (can trigger automatic ACL remediation) Alerting on EC2 instance state changes (can trigger actions on the instances) Alerting on user creation in IAM Cloudwatch Buses Allows different AWS accounts to share Cloudwatch Events Can collect events from all your accounts together in one account Must grant an account permission by adding and then sending the account number to the receiving account bus configuration The sending account send an event to an Event bus target The CloudWatch Event Bus process requires two event rules, one event rule on either end of the event bus.","title":"Cloudwatch"},{"location":"certifications/aws/security-specialty/#aws-config","text":"Enables Compliance auditing Security analysis Resource tracking Provides Configuration snapshots and logs config changes of AWS resources Automated compliance checking Evaluate resource configurations for desired settings Retrieve configurations of resources in your account Retrieve historical configurations Receive a notification for creations, deletions, and modifications View relationships between resources ( members of a security group) Config Rules - A rule represents the desired value for resources. There are two types: AWS Managed Rules - Pre-built and managed by AWS. You simply choose the rule you want to enable, then supply a few configuration parameters to get started Customer Managed Rules - These are custom rules, defined and built by you. You can create a function in AWS Lambda that can be invoked as part of a custom rule and these functions execute in your account Conformance Packs A conformance pack is a collection of AWs Config Rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS organizations Conformance packs are created by authoring a YAML template that contains the list of AWS Config managed or custom rules and remediation actions An Aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following Multiple accounts and multiple regions Single account and multiple regions An organization in AWS Organizations and all the accounts in that organization Permissions needed AWS Config requires an IAM Role with Read only permissions to the recorded resources Write access to S3 logging bucket Publish access to SNS console will optionally create these for you Restrict Access Users need to be authenticated with AWS and have the appropriate permissions set via IAM policies to gain access Only Admins needing to set up and manage config require full access Provide read only permissions for Config day-to-day use Monitoring Config Use CloudTrail with Config to provide deeper insight into resources Use Cloudtrail to monitor access to config such as someone stopping the config recorder Uses cases Administering resources Notification when a resource violates configuration rules Auditing and compliance Historical records of configurations are sometimes needed in auditing Configuration management and troubleshooting Configuration changes on one resource might affect others Can help find these issues quickly and can restore last know good configurations Security Analysis Allows for historical records of IAM policies For example, what permissions a user had at the time of an issue Allows for historical records of security group configurations","title":"AWS Config"},{"location":"certifications/aws/security-specialty/#aws-inspector","text":"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS Amazon Inspector automatically assesses applications for vulnerabilities or deviations from best practices. After performing assessment, Inspector produces a detailed list of security findings prioritized by level of severity These findings can be reviewed directly or as part of detailed assessment reports which are available via Amazon inspector console or API How does it Work Create Assessment target Install agents on EC2 Instances Create Assessment Template Perform Assessment Run Review findings against rules Rules Packages Common Vulnerabilities and Exposures CIS Operation System Security Configuration Benchmarks Security Best Practices Runtime Behavior Analysis Severity Levels for Rules High Medium Low Informational It Will Monitor the network, file system, and process activity within the specified target Compare what it 'sees' to security rules Report on security Issues observed within target during run Report findings and advise remediation Analyzing the behaviour of your AWS resources Identifying potential security issues It Will not Relive you of your responsibility under the share responsibility model Perform miracles Components Target: A collection of AWs resources Assesment Template: Made up of security rules and produces a list of findings Assessment Run: Applying the assessment template to a target Features Configuration Scanning and Activity Monitoring Engine Determines what target looks like, its behavior, and any dependencies it may have Identifies security and compliance issues Built-in content library Rules and reports built into inspector Best practice, common compliance standard, and vulnerability evaluations Detailed recommendations for resolving issues Api Automation Allows for security Testing to be included in the development and design stages","title":"AWS Inspector"},{"location":"certifications/aws/security-specialty/#aws-trusted-advisor","text":"An online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment Advisor will advise you on Cost Optimization, Performance, Security, Fault Tolerance Core checks and Recommendations Full trusted advisor - Business and Enterprise Companies only Cost Optimization Availability Performance As Well as Security","title":"AWS Trusted Advisor"},{"location":"certifications/aws/security-specialty/#aws-inspector-vs-aws-trusted-advisor","text":"Questions about Cost Optimization/Performance/Fault Tolerance => Trusted Advisor Questions about Security => If needs instal agent/create reports => Inspector But if question is only about security groups open, IAM => Trusted advisor","title":"AWS Inspector vs AWS Trusted Advisor"},{"location":"certifications/aws/security-specialty/#logging","text":"AWS CloudTrail AWS Config VPC Flow Logs AWS CloudWatch Logs White-paper: Security at Scale: Logging in AWS - Looks at \"Common Logging requirements\" Prevent Unauthorized access IAM users, groups, roles and policies S3 Bucket policies Multi factor authentication Ensure role-based access IAM users, groups, roles, and policies Amazon S3 bucket polices Log changes to system components (AWS Config Rules) Clout Trail Controls exist ro prevent modification to logs IAM and S3 controls and policies CloudTrail log file encryption CloudTrail log file validation Storage of Log Files Logs are stored for at least one year Store logs for an org-defined period of time Store Logs real-time for resiliency S3 S3 Object lifecycle Management 99.999999999 durability and 99.99% availability of objects over a given year","title":"Logging"},{"location":"certifications/aws/security-specialty/#domain-3-infrastructure-security","text":"","title":"Domain 3 - Infrastructure Security"},{"location":"certifications/aws/security-specialty/#cloudfront","text":"Cloudfront is a global CDN operating from AWS Edge Locations. Connections to a cloudfront distribution can utilize HTTP or HTTPs. Connections From Cloudfront to your content (origin server) can occur using HTTP or HTTPs. It also removes many invalid HTTP requests at the edge-basic filtering Dedicated IP Vs Shared IP (SNI - Server Name Identifier) Dedicated IP SSL is supported in ALL Browsers, but costs extra. SNI has no extra cost, but browsers need to support it To Support Old browsers, we need choose dedicated IP Viewer protocol policy HTTP and HTTPS -> default selected Redirect HTTP to HTTPS HTTPS only Advanced Security Features Integrates with AWS WAF Supports full access control and signed URLs/cookies Provides basic white/blacklist geo-restriction per distribution Can integrate with 3 party solution using signed URLS/Cookies Field-Level Encryption - help protect sensitive data (Encrypted End-to-End) - (Credit card ,.... ) Supports Lambda at the edge Custom Origin Origin SSL Protocols TLSv1.2 - Default selected TLSv1.1 - Default selected TLSv1 - Default selected SSLv3 - Disabled Origin Protocol Policy HTTP Only - Default selected HTTPS Only Match Viewer Can configure HTTP/HTTPS Ports Key pair is us required for signing URLS or cookeis The application at the custom orgin must send back three Set-Cookie headers in response to viewer Restrict Viewer Access (Use Signed URLs or Signed Cookies) Yes/No options. No is default If select yes, users must use signed urls or signed cookies Lambda Edge Inspect cookies to rewrite URLS to different versions of a site for A/B Testing Send different objects to your users based on the User Agent Header Inspect headers or authorized tokens, inserting a corresponding header and allowing access control before forwarding a request to the oring Add, delete modify headers, and rewrite URL path to direct users to different objects in the cache Generate new HTTP responses to do things like redirect unauthenticated users to login pages, or create and deliver static webpages right from the edge CloudFront SSL Certificates If you are happy for users to access your content using *.cloudfront.net domain name, then select the default CloudFront Certificate If you want to use a domain name that you already own, you will need to use a custom SSL certificate Custom SSL certificates must be stored in either ACM in the us-east-1 (North Virgina) or you can also store them in IAM using the IAM Cli. Signed Certificates Don't work Must match the custom origin name if custom origin is used Can configure alternate domain names Restricting S3 to Use cloudfront By default, when using cloudfront with s3, cloudfront is optional, and S3 can be acessed directly. This can be changed It's necessary check \"Restrict Bucket Access\" option It's necessary create a (or reuse) Origin Access Identity (OAI) (something like a cloudfront user to access to bucket) What is? - Is a virtual identity. A distribution can be configured to use it, so when accessing S3, cloudfront assumes this identity It's necessary update Bucket Policy to give permissions to Origin Access identity access to Bucket Why use? - To use on OAI, public permissions are removed from your S3 bucket policy and permissions for the OAI are added. Only the cloudfront using that OAI can access your S3 bucket Supports Logging Geo Restriction CloudFront can restrict content in one of two ways Using Cloudfront Geo Restriction Simple implement Whitelist or blacklist and it works on country restrictions only Location is based on IP country location - backed by a GeoIP Database ( ~ 99.8% accuracy) No restrictions on ANYTHING ELSE - session/cookie/content/browser etc Using a third-party GeoLocation Service Third-Party Geo Restriction needs a server/serveless application - Signed URLs are used A thrid Party Geolocation service is used... extra accuracy Your application can apply additional restriction - session/browser/account level/os Location can be much more accurate ... city, locale, LAT/Long in some cases The application can apply any logic it wants Needs to be this option with need anything beyond IP Location whitelist/blacklist","title":"CloudFront"},{"location":"certifications/aws/security-specialty/#pre-signed-urls-and-cookies","text":"Signed URLs allow an entity (generally an application) to create a URL which includes the necessary information to provide the holder of that url read/write access to an object, even if they have no permissions to that object Cookies extend this, allowing access to an object type or area/folder and don't need a specifically formatted url You can access objects using pre-signed URL's Typically these are done via the SDK but can also be done using the CLI Features/Limits Signed URLs/Cookies are linked to an existing identity (Role/User), and they have the permissions of that entity They exist for a certain length of time in seconds. Default is 1 Hour. You can change this using \"--expires-in\" followed by the number of seconds They expire either at the end of the period or until the entity on which they are based expires If need the signed URL have long expiration time (several hours , days), don't use IAM Role as entity because IAM Role credentials expire, it's better use IAM User Anyone can create a signed URL, even if they don't have permissions on the object With Cloudfront you defined the accounts which can sign; the key pair TrustedSigners is needed for cloudfront Signed cookies (Cloudfront feature) don't work with RTMP distributions Use Pre Signed URLS when When wants restrict access to individual files USe Pre Signed Cookies when You don't want change URL Provide access to multiple restrict files, for example all files for a video in HLS format or all files in subscribe are in Website area","title":"Pre-Signed URLs and Cookies"},{"location":"certifications/aws/security-specialty/#forcing-encryption-using-s3","text":"S3 doesn't encrypt bucket, objects are encrypted and the settings are defined at an object level Historically, it wasn't possible to define encryption at bucket level, but you can now set S3 Default Encryption on bucket level If set, then any objects put into a bucket without encryption headers are encrypted using the bucket level default settings aws:SecureTransport # Deny This \"Condition\" : { \"Bool\" : { \"aws:SecureTransport\" : false } } Additionally, bucket policies can be used to deny attempts to put objects into a bucket with individual encryption methods","title":"Forcing Encryption using S3"},{"location":"certifications/aws/security-specialty/#cross-region-replication","text":"Is configured at bucket level Requirements Do not need to use a bucket policy with aws:SecureTransport to replicate objects using SSL. It is done by default Versioning must be enabled (In two buckets) Source and destination buckets must be in different AWS Regions S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf It's possible to use CRR from one AWS account to another. The IAM role must have permissions ro replicate objects in the destination bucket. If the object owner is different than source bucket owner, the object owner must grant the bucket owner the READ and READ_ACP permissions via the object ACL In the replication configuration, can optionally direct Amazon S3 to change the ownership of object replica to the AWs account that owns the destination bucket What is Replicated Any objects created after add a replication configuration In addition to unencrypted objects, S3 replicates objects encrypted using S3 managed keys (SSE-S3) or AWS KMS managed keys (SSE-KMS) Object metadata , ACL Updates, Tags, Ownership, StorageClass Replicates only objects in the source bucket for which the bucket owner has permissions to read objects and read access control lists (ACL) If you just use a delete marker, then that delete marker is replicated What is NOT Replicated Anything created before CRR is turned on Objects created with Server-side encryption using customer-provided (SSE-C) encryption keys Objects created with server-side encryption using AWS KMS - managed encryption (SSE-KMS) unless you explicitly enable this option Objects in the source bucket for which the bucket owner does not have permissions. This can happen when the object owner is different from the bucket owner Delete markers are replicated, deleted versions of files are NOT Lifecycle events are not replicated Standard Replication - Region A to Region B Configure IAM Role with permissions to get Objects from Region A and replicate ro Region B Other Account repplication Need add bucket policy in Account B to account A have permissions to replicate Owner Change Need add Replication Configuration KMS Need Replication Configuration Need KMS config updates","title":"Cross Region Replication"},{"location":"certifications/aws/security-specialty/#aws-waf-vs-aws-shield","text":"AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront or and Application Load Balanacer. AWS WAF also lets you control access to your content You can configure conditions such as what IP addresses are allowed to make this request or what query string parameters need to be passed for the request to be allowed and then the application load balancer or cloudfront will either allow this content to be received or to give a HTTP 403 Status Code At it's most basic level, AWS WAF allows 3 different behaviours: Allow all requests except the ones that you specify Block all requests except the ones that you specify Count the requests that match the properties that you specify What is AWS WAF? Additional protection against web attacks using conditions that you specify. You can define conditions by using characteristics of web requests such as the following: IP addresses that requests originate from Country that requests originate from Values in request headers Strings that appear in requests, either specific strings or string that match regular expression (regex) patterns Length of requests Presence of SQL code that is likely to be malicious (Known as SQL injection) Presence of a script that is likely to be malicious (Knows as cross-site scripting) When multiple conditions exist om a rule, the result must include all conditions Example Rule: Block requests from 2.2.0.0/16 that appear to have SQL Code Both conitions must match for a block Application Load Balancers integrate with WAF at a regional level, Cloudfront at a Global Level You need to associate your rules to AWS resources in order to be able to make it work You can use AWS WAF to protect web sites not hosted in AWS via Cloudfront. Cloudfront supports customs origins outside of AWS IP's can be blocked at a /8, /16, /24 and /32 level IPv4 and IPv6 are supported AWS Shield Turned on by default Standard The basic level of DDoS protection for your web applications Included with WAF with no additional cost Advanced Expands services protected to include Elastic Load balancers, cloudfront Distributions, Route 53 hosted zones, and resources with EIPs Some of the advantages Contact 24x7 DDoS Response Team (DRT) for assistance during an attack You won't pay if you are a victim of an attack *A dvanced gives you an incident response team and in depth reporting Expanded protection against many types of attacks WAF is included in Advanced pricing 3000 a month if you want the advanced option Plus Data Transfer Out usage fees","title":"AWS WAF Vs AWS Shield"},{"location":"certifications/aws/security-specialty/#serverless","text":"Lambda Function Policy Controls who or what can invoke it For poll-based services (Kinesis, DynamoDB) or SQS - lambda polls on your behalf, and so permissions are granted via its execution role For anything else, for for external entities or accounts, the PUSH model is used Change to the function policy will be required Execution Role Ensure it has enough permissions to log to cloudwatch logs To access any resources it needs to PULL from or PUSH too For event-driven invocation, the execution role doesn't need permissions to access it For poll based sources such as DynamoDb, SQS, Kinesis it does","title":"Serverless"},{"location":"certifications/aws/security-specialty/#egress-only-internet-gateway","text":"With IPv4, all AWS resources have a private IP. Some can be provided with a public IP and connectivity using an Internet Gateway. With IPv4 a NAT Instance/gateway can be utilized to provide outgoing only access IPv6 addressing is globally unique and publically routable. Supported resources in AWS are all publically addressable, so a NAT gateway isn't an option. Egress-Only internet gateway provides a feature limited internet gateway, specifically for IPv6, and only allowing outbound connections and return traffic. No incoming IPv6 connections can be initiated to VPC resources using an Egress-Only gateway","title":"Egress Only Internet Gateway"},{"location":"certifications/aws/security-specialty/#systems-manager","text":"Insights Two insight features, inventory and Compliacy. Both supported by SSM state manager Inventory periodically scans EC2 intances, or on-premise servers, retrieving details of installed applications, AWS components, network config, windows updates, detailed information on an instance/VM, details on running services, windows roles, and opitonal custom data SSM can collect on your behalf Compliance allows that data to be compared against a baseline, provding a compliant or non-compliant state to a resource. Compliance uses state manager, SSM patching and custom compliant types Actions Are the operational engine part of systems manager. Actions is the part of systems manager which performs collections, runs commands, controls patching and manages the general state of managed instances Automation Run Command Patch Manager State manager is a desired state engine. You define the desired state in the form of a systems manager document. A document can be a command document or a policy doccument. A command document is used by running command and state manager A policy document defines desired states and is only used by State Manager A document is associated with one or more managed instances Shared tooling - Several services Managed Instances Activations The method used to activate non EC2 instances withing Systems manager. Activation generates a code to activate the external machine Document Think of these are scripts or lists of commands that can be run against a managed instance ParameterStore AWS provided services to store configuration data and secrets","title":"Systems Manager"},{"location":"certifications/aws/security-specialty/#kms","text":"AWS Key Management Service (KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data, and uses Hardware Security Models (HSMs) to protect the security of your keys. AWS KMS is integrated with other AWS Services including, EBS, S3, Redshift, Amazon Elastic Transcoder, Amazon WorkMail, Amazon RDS and others to make it simple to encrypt your data with encryption keys that you manage KMS uses FIPS 140-2 (Level 2) compliant hardware modules Don't support Java Cryptography Extensions (JCE) CMK - Logical representation of a key. Keys can be generated by KMS or imported Provides alias creation date description key state key material (either customer provided or AWS provided) Can never be exported CMKs never leave KMS and never leave a region CMKS can encrypt or decrypt data up to 4kb in size AWS-managed CMK for each service that is integrated with AWS KMS Or you can have customer managed CMK that you generate by providing AWS with key material Setup Create Alias and description Choose material option Define Key Administrative Permissions IAm Users/roles that can administer (but not use) the key through the KMS API Define Key Usage Permissions IAM users/roles that can use the key to encrypt and decrypt data GeneratedDataKey creates encrypted and plaintext data key. The plaintext version is used to encrypt and then discarded. Its never stored in plaintext. The encrypted version is stored along with the encrypted data; this is envelope encryption * KMS is used to decrypt the encrypted key, returning plaintext, and data is decrypted. Encrypt and Decrypt perform those functions, and are handled by kms Key Material Options Use KMS generated key material Your Own key material You can import a symmetric 256-bit key from your key management infrastructure into KMS and use it like any other customer master key Why import your own key material Prove that randomness meets your requirements (Compliance) Extend your existing processes to AWS To be able to delete key material without a 7-30 days wait. Then be able to import them again to be resilient to AWS failures by storing keys outside AWS How to Import your own key material Create a CMK with no key material Download a public key (wrapping key) and import token Encrypt key material Import the key material Considerations for imported key material Availability and durability is different Secure key generation is up to you No automatic rotation Ciphertexts are not portable between CMKs Key Material import Fail You key material need to be 256-bit symmetric key Import token has 24 hour expiration time AWS KMS never provides option to export key material CAn reimport the key material however the key material must be the same AWS kms update-alias - to update the alias Best Practices Tips Segment Keys based upon business unit, data classification, environment, etc Create Keys within the account where the data exists if possible Rotate the keys Key Polcies IAM Polcies are Not sufficient to allow access to a CMK Edit the default CMK policy to align with your organization's best parctices for least privilege CMK - Least Privilege Separate keys per business unit and data classification Separate CMK admins from users Limit KMS actions within IAM policies (No kms:*) Cross Account Delegation Account Root Principal -> Allows target account to further delegate permissions Explicit management of principals within key policy","title":"KMS"},{"location":"certifications/aws/security-specialty/#kms-key-rotation-options","text":"Extensive re-use of encryption keys is not recommended It is best practice to rotate keys on a regular basis The frequency of key rotation is dependent upon local laws, regulations and corporate policies The method of rotation depends on the type of key you are using AWS Managed Customer Managed Customer Managed with imported key material Managed Keys Rotates automatically every 3 years When the CMK is due for rotation, a new backing key is created and marked as the active key for all new requests The old backing key remains available to decrypt any existing ciphertext files that were encrypted using the old key AWS handles everything for you You cannot manage rotation yourself AWS managed keys cannot be deleted Customer Managed Keys Once a year automatically - disabled by default AWS KMS generates new cryptographic material for the CMK every year The CMK's old backing key is saved so it can be used to decrypt data that it previously encrypted On-demand manually Create a new CMK, then change your applications or aliases to use the new CMK You control the rotation frequency Keys can be deleted but be careful! Customer Managed Keys With Imported Key Material Automatic key rotation is NOT available for CMKS with imported key material i.e. the CMK was not generated in AWS The only option is to rotate the keys manually Create a new CMK, then change your applications or aliases to use the nem CMK You control the rotation frequency Keys can be deleted but be careful, if delete the old key, KMS cannot decrypt data that the original CMK encrypted Key Expire To change expiration date of a KMS key, you must reimport the same key material and specify a new expiration date Import the key material to a CMK. Download and use a new wrapping key and import token. Encrypt and reimport the same key material that was originally imported into CMK","title":"KMS Key Rotation Options"},{"location":"certifications/aws/security-specialty/#kms-key-policy","text":"KMS Key policies are resource policies which control access to the Customer Master Keys (CMKs). Unlike most AWS Services, without specifically being allowed, the root user has no access to CMKs. In cases where nobody has access to CMKs, only AWS can restore access If removed Key Policy, nothing can use the key. AWS support is required Key Administrators are permitted (via he key policy) to perform admin actions on the key, but not the ability to use the key. Operations include kms:Create* kms:Describe* kms:Enable kms:Put kms:Update Kms:Delete and more CAn't perform cryptography operations Need give another account permissions to CMK to use it","title":"KMS Key Policy"},{"location":"certifications/aws/security-specialty/#kms-key-usage","text":"KMS has specific operations which are used to utilise CMK. CMKs generally aren't used to encrypt data. They generate data keys which perform the encryption and decryption Operations kms:Encrypt - is suitable for encrypting a file which is less than 4kb. No envelope encryption needed kms:Decrypt - kms:ReEncrypt kms:GenerateDataKey - Not desirable for a distributed system, where each component would use a different key to encrypt the data. To distribute system, if don't need distribute a data key among the system components, choose GenerateDataKeyWithoutPlainText kms:DescribeKey","title":"KMS Key Usage"},{"location":"certifications/aws/security-specialty/#kms-grants","text":"Grants are an alternative access control mechanism to a key policy Delegate a subset of permissions to AWS services/other principals so that they can use the CMK on the customer behalfs Programmatically delegate the use of KMS CMKs to other AWS principals - e.g a user in either your account or another account Temporary, granular permissions (encrypt, decrypt, re-encrypt, describekey etc) Grants allow access, not deny Use Key Policies for relatively static permissions & for explicit deny CLI Commands Grants are configured programatically using the AWS CLI create-grant - adds a grant to the CMK, specifies who can use it and a list of operations the grantee can perform list-grants - lists the grants revoke-grant - to remove a grant A grant token is generated & can be passed as an argument to KMS APi","title":"KMS Grants"},{"location":"certifications/aws/security-specialty/#encryption-context","text":"Key value pair of additional data that you want associated with AWS KMS-Protected information Enforce tighter controls for your encrypted resources Insight into the usage of your keys from an audit perspective Is logged in clear text within CloudTrail","title":"Encryption Context"},{"location":"certifications/aws/security-specialty/#data-at-rest-kms","text":"EBS EBS Vol is encrypted using DataKey generated from a CMK Encrypted data key is stored with volume Used by the hypervisor to decrypt upon detaching IO, Snapshots and persisted data is encrypted DynamoDB For any encrypted table created in a region, DynamoDB uses KMS to create an AWS/DynamoDB service default CMK (in each region) When a table is created and set to be encrypted, this CMK is used to create a data key unique to that tabled, called a table key This key is managed by DynamoDB and stored with the table in encrypted form Every Item encrypted by DynamoDB is encrypted with a data encrypted key, which is encrypted with this table key and stored with the data Table keys are cached for up to 12 hours in plaintext by DynamoDB, but a request is sent to KMS after 5 minutes of table key inactivity to check for permissions changes RDS RDS utilizes EBS for its encryption. RDS Instances are managed versions of EC2 instances, configured to act as managed DB cluster. In Similiar way to ec2, encrypted volumes attached to RDS are handled by the host, with persistent data, snapshots, and IO encrypted and decrypted using KMS S3 Every object in a bucket is encrypted by S3 using a DataKey provided by KMS The DataKey is generated from a CMK ChiperText DataKey is stored with the object as metadata. When decryption is needed, it's passed to KMS, Decrypted, and used by S3 to Decrypt the Object","title":"Data At Rest KMS"},{"location":"certifications/aws/security-specialty/#kms-hierarchy","text":"KMS-Managed - Keys on HSAs in a Region All Haderneded Security appliances (HSA) in a Region self-generate keys in memory when provisioned. Private Keys never leave HSA Customer Managed - Customer Master Key 254-bit symmetric Customer Master key generated in HSA or imported by customer Stored in encrypted form in several locations by KMS. Plaintext version used only in memory on HSAs on demand Data Key - Customer-managed or AWS service managed 235-bit symmetric key returned to client by KMS to use for encrypting bulk data","title":"KMS Hierarchy"},{"location":"certifications/aws/security-specialty/#kms-cross-account-access","text":"Access to KMS CMKs is controlled using The Key policy IAM Policies If you want to enable another external account to encrypt or decrypt using your CMK, you need to enable cross account access Enable access in the Key Policy for the account which owns the CMK Enable access to KMS in the IAM Policy for the external account Both steps are necessary otherwise it will not work Access to KMS CMKs is controlled using The key Policy - add the root user, not individual IAM users / roles IAM Policies - define the allowed actions and the CMK ARN If you want to enable cross account access Enable access in the Key Policy for the account which owns the CMK Enable access to KMS in the IAM Policy for the external account Both steps are necessary otherwise it will not work KMS in a multi-account configuration The key won't appear in the external account, but if it is configured using a key policy, that account can interact with the key for cryptographic functions Key usage and Key admin are not the same thing","title":"KMS Cross Account Access"},{"location":"certifications/aws/security-specialty/#cloud-hsm","text":"The AWS CloudHSM service helps you meet corporate, contractual and regulatory compliance requirements for data security by using dedicated hardware security module (HSM) appliances within the AWS cloud AWS manages and maintains hardware, but has no access to the cryptographic component Interaction is via industry standard APIs, no normal AWS APIs Keys can be transfered between CloudHSM and other Hardware solutions (on premises) Keys are shared between cluster members. NO HA unless multiple HSM's are provisioned Applications can be outside the VPC - Direct Connect, Peered or VPN On-Premises HSM - for if you really need to control your own physical hardware Enables Control of data Evidence of control Meet tough compliance controls Provides Secure Key Storage Cryptographic operations Tamper-resistant Hardware Security Model Setup Inside a VPC, in the region required A private subnet for the HSM An EC2 instance (control instance) with the cloudhsm_mgmt_util & key_mgmt_util Needs to be accessible by you Create a cluster & HSM Create a VPC Create a Private & Public subnet Create the Cluster Verify HSM identity Initialize the Cluster Launch a client instance Install and configure the client Activate the cluster Setup Users Generate Keys Key Control AWS does not have access to your keys Separation of duties and role-based access control is part of the design of the HSM AWS can only administer the appliance, not the HSM partitions where the keys are stored AWS can (but probably won't) destroy your keys, but otherwise have no access Tampering If the CloudHSM detects physical tampering the keys will be destroyed If the CloudHSM detects five unsuccessful attempts to access an HSM partition as Crypto Officer the HSM appliance erases itself If the CloudHSM detects five unsuccessful attempts to access an HSM with Crypto User (CU) credentials, the user will be locked and must be unlocked by a CO Monitoring Use CloudTrail to log API calls including those made to CloudHSM 4 Main User Types Precrypto Officer (PRECO) Crypto Officer (CO) Performs user management operations For example, a CO can create and delete users and change user password Crypto Users (CU) Key management - Create, delete, share, import and export cryptographic keys Cryptographic operations - Use cryptographic keys for encryption, decryption, signing, verifying and more Appliance User (AU) The appliance User (AU) can perform cloning and synchronization operations. AWS CloudHSM uses the AU to synchronize the HSMs in an AWS CloudHSM cluster The AU exists on all HSMs provided by AWS CloudHSM and has limited permissions To digitally sign firmware sotfware, you must use the sign command of the key_mgmt_util command line of CloudHSM (PKI functionally) FIPS 140-2 Level 3 Supports Java Cryptography Extensions (JCE) API's","title":"Cloud HSM"},{"location":"certifications/aws/security-specialty/#aws-ec2-encryption","text":"We can use KMS to encrypt EBS volumes, but we cannot use KMS to generate a public key/private to log in EC2 We can import Public Keys into EC2 Key pairs, but we cannot use EC2 Key pairs to encrypt EBS volumes, we must use KMS or third party application/tools We can use KMS to encrypt EBS volumes and it is possible to encrypt root device volumes To encrypt a root device volume create an AMI. The Initial AMI will be unencrypted, but you can then copy it and in doing so encrypt it You can change the encryption keys from amazon managed to customer master keys You can copy from one region to another and make those copies (snapshots) encrypted, but you must use the keys in the destination region to do encryption You cannot copy KMS keys from one region to another SSH Keys You can view the public key by going to /home/ec2-user/.ssh/authorized_keys You can also view the public key using instance metadata curl http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key You can have multiple public keys attached to an EC2 instance You can now add roles to existing EC2 instances Deleting a key Pair in the console will not delete it from the instance or the instances metadata If you lose a KP (Public or Private), simply take a snapshot of the EC2 instance and then deploy it as a new instance this will append a new public key to the /home/ec2-user/.ssh/authorized_keys You can then go in to that file and delete the outdated public keys Because you cannot export keys from KMS and because Amazon in involved the generation of keys, you cannot use KMS with SSH for EC2 With CloudHSM you can however because can export keys from CloudHSM","title":"AWS EC2 Encryption"},{"location":"certifications/aws/security-specialty/#aws-ec2-marketplace","text":"Can purchase security products from third party vendors on the AWS Market Place Firewalls, Hardened OS's, WAF's, Antivirus, Security Monitoring etc Free, Hourly, Monthly, Annual, BYOL etc CIS OS Hardening","title":"AWS EC2 - Marketplace"},{"location":"certifications/aws/security-specialty/#ec2-dedicated-instances-vs-dedicated-hosts","text":"Dedicated Instances Are EC2 Instances that run in a VPC on hardware that's dedicated to a single customer. Are physically isolated at the host hardware level from instances that belong to other AWS Accounts Dedicated instances may share hardware with other instances from the same AWS Account that are not Dedicated instances Pay for dedicated instances On-Demand, save up to 70% by purchasing Reserved Instances, or save up 90% by purchasing Sport Instances Dedicated Hosts You can use Dedicated Hosts and Dedicated Instances to launch EC2 instances on physical servers that are dedicated for you use An important difference between a Dedicated Host and a Dedicated instance is that a Dedicated Host gives you additional visibility and control over how instances are placed on a physical server, and you can consistently deploy your instances to the same physical server over time As a result, dedicated hosts enable you to use your existing server-bound software licenses and address corporate compliance and regulatory requirements Dedicated Instances Vs Dedicated Hosts Both dedicated instances and dedicated hosts have dedicated hardware Dedicated instances are charged by the instance, dedicated hosts are charge by the host If you have specific regulatory requirements or licensing conditions, choose dedicated hosts Dedicated instances may share the same hardware with other AWS instances from the same account that are not dedicated Dedicated hosts give you much better visibility in to things like sockets, cores and host id","title":"EC2 Dedicated Instances vs Dedicated Hosts"},{"location":"certifications/aws/security-specialty/#aws-hypervisors","text":"A hypervisor or virtual machine monitor (VMM) is a computer software, firmware or hardware that creates and runs virtual machines. A computer on which a hypervisor runs one or more virtual machines is called a host machine, and each virtual machine is called a guest machine EC2 runs on a mixture of Nitro and Xen Hypervisors. Eventually all of EC2 will be based off Nitro hypervisors Both Hypervisors can have guest operation systems running either as Paravirtualization (PV) or using Hardware Virtual Machinhe (HVM) HVM guests are fully virtualized. The VMS on top of the hypervisors are not aware that they are sharing processing time with other VMs PV is a lighter form of virtualization and it used to be quicker However this performance gap has now closed and Amazon now recommend using HVM over PV where possible. It's also worth nothing that Windows EC2 instances can only be HVM where as linux can be both PV and HVM PV Paravirtualized guests rely on the hypervisor to provide support for operations that normally required privileged access, the guest OS has no elevated access to the CPU The CPU provides four separate privelege modes: 0-3 called rings. Ring 0 is the mos privileged and 3 the least The host OS executes in Ring 0. However, rather than executing in Ring 0 as most operating systems do, the guest OS runs in a lesser-privileged Ring 1 and and application in the least privileged Ring 3 Isolation (Layers ordered) Physical Interface Firewall Customer Security Groups (C1 Sg, C2 SG -> one per VM) Virtual Interface Hypervisor VMS (Customer 1, Customer 2, Customer x) Hypervisor Access Administrators with a business need to access the management plane are required to use multifactor authentication to gain access to purpose-built administration hosts These administrative hosts are systems that are specifically desinged, built, configured, and hardened to protect the management plane of the cloud All such access is logged and audited When an employee no longer has a business need to access the management place, the privileges and access to these host and relevant systems can be revoked Guest (EC2) Access Virtual instances are completely controlled by you, the customer You have full root access or administrative control over accounts, services, and applications AWS does not have any access rights to your instances of the guest OS Memory Scrubbing EBS automatically resets every block of storage used by yhe customer, so that one customer's data is never unintentionally exposed to another Also memory allocated to guests is scrubbed (set to zero) by the hypervisor when it is unallocated to a guest The memory is not returned to the poll of free memory available for new allocations until the memory scrubbing is complete Disk EBS volumes are provided to instances in a Zero'd state - this zeroing occurs immediately before reuse If you have specific deletion requirements, you need to do this before terminating the instance/deleting the volume Exam Tips Choose HVM over PV where is possible PV is isolated by layers, Guest OS sits on Layer 1, Applications Layer 3 Only AWS Administrators have access to hypervisors AWS staff do not have access to EC2, that is your responsibility as a customer All storage memory and RAM memory is scrubbed before it's delivered to you","title":"AWS Hypervisors"},{"location":"certifications/aws/security-specialty/#host-proxy-servers","text":"Filtering within AWS is perfomed at two points: Security groups attached to network interfaces and NACLs attached to subnets within VPCs. Security Groups and NACLs have viability of protocols, IPs, CIDRs, and ports. The cannot filter on DNS Names, nor can they decide between allowing and denying traffic based on any form of authentication If authentication or additional intelligence beyond IP/CIDR/PORT/PROTOCOL is needed, a proxy server or an enchanced NAT architecture is required","title":"Host Proxy Servers"},{"location":"certifications/aws/security-specialty/#packet-capture-on-ec2","text":"Packet capture or packet sniffing is a process where network traffic can be intercepted, analyzed and logged. Sniffed packets are captured in their entirety and unlike VPC flow logs can be inspected at data level - providing they are not encrypted Common Scenarios Review data flows between components to identify networking problems Support IDS/IPS systems - help detect and remediate intrusion attempts Debug connections between clients and the edge components of an environment Debug communications between tiers of your applications Verify the functionality of other networking components such as firewalls, NATs, and proxies VPC flow logs meet a subset of the above scenarios but don't allow traffic capture - only metadata Important: Traditionally packet sniffing was done in a promiscuous way - a network interface listened for all traffic - even that not destined for the interface. This isn't supported in AWS Recommended architecture - Run command feature of systems manager, install a packet capture agent on ALL EC2 instances, configure the software to store the capture logs in a central location","title":"Packet capture on EC2"},{"location":"certifications/aws/security-specialty/#key-policy-conditions","text":"Policy conditions can be used to specify a condition within a key policy or IAM policy. The condition must be tru for the policy statement to take effect You might want a policy statement to take effect only after a specific date has passed Allow or deny an action based the requesting service Predefined condition keys Kms:ViaService Scope down API calls to a CMK based on the AWS service from which it is called Is a condition key which can allow or deny access to your CMK depending on which service originated the request Possibilities Allow access to the CMK only for requests which come from S3 Deny all requests which come from Lambda ViaService can be used in Key Policies and IAM policies which control access to KMS resources The services that you specify must be integrated with KMS e.g S3, EBS, Systems Manager, SQS, Lambda","title":"Key Policy Conditions"},{"location":"certifications/aws/security-specialty/#microservices","text":"Microservices Run in Containers Small independent services that communicate over well-defined APIs Does One thing only Easy to Support & Maintain Changes like bug fixes, upgrades, scaling and adding new features are very easy ECS Fargate is the preferred option-Serverless Or managed clusters of EC2 instances Deep integration with AWS services e.g. IAM, VPC, Route53 Used internally e.g amazon.com, Sagemaker, Amazon Lex EKS Fargate is the preferred option-Serverless Or managed clusters of EC2 instances Certified conformant Benefit of open source tooling from the community Container Security Best Practices Don't Store secrets Secrets Manager Use IAM roles instead of user credentials Don't run your containers as root! One Service per Container Minimize the attach surface Avoid unnecessary libraries Trusted images only Avoid using images from public repositories Image Scanning Scan for common vulnerabilities & exposures Protect infrastructure Use ECS Interface endpoints to avoid sending VPC traffic over the internet Encrypt in Transit Using TLS Use ACM","title":"Microservices"},{"location":"certifications/aws/security-specialty/#data-protection-with-vpcs","text":"","title":"Data Protection With VPCs"},{"location":"certifications/aws/security-specialty/#vpc-introduction","text":"Think of a VPC as a logical datacenter in AWS Consists of IGWs (Or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, and Security Groups 1 Subnet = 1 AZ Security Groups are Stateful; Network Access Control Lists are Stateless No transitive Peering","title":"VPC Introduction"},{"location":"certifications/aws/security-specialty/#nat-instances","text":"When creating a NAT instance, Disable Source/Destination check on the Instance NAT instances must be in a public subnet There must be a route out of the private subnet to the NAT instance, in order for this to work The amount of traffic that NAT instances can support dependes on the instance size. If you are bottlenecking, increase the instace size You can create high availability using Autoscaling Groups, multiple subnets in different AZs, and script to automate failover","title":"NAT Instances"},{"location":"certifications/aws/security-specialty/#nat-gateways","text":"Preferred by the enterprise Scale automatically up to 100 Gbps No need to patch Not associated with security groups Automatically assigned a public ip address (EIP) Remember to update your route table No need to disable Source/Destination checks More secure than a NAT Instance Cannot have SGs attached","title":"NAT Gateways"},{"location":"certifications/aws/security-specialty/#network-acls","text":"Your VPC automatically comes a default network ACL, and by default it allows all outbound and inbound traffic You can create a custom network ACLs. By default, each custom network ACL denies all inbound and outbound traffic until you add rules Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed Network ACLs contain a numbered list of rules that is evaluated in order, starting with the lowest numbered rule Network ACLs have separate inbound and outbound rules, and each rule can either allow or deny traffic Network ACls are stateless; responses to allowed inbound traffic are subject to rules for outbound traffic (and vice versa) Block IP Addresses using network ACLs not Security Groups NACLs are processes only when data enters or leaves subnets, before security groups NACLs work on IP and CIDR only. You can' reference AWS services","title":"Network ACLS"},{"location":"certifications/aws/security-specialty/#tlsssl-termination-options","text":"When using an Elastic Load Balancer, you have the choice to wither terminate TLS/SSL connections either on the Load Balancer or on your EC2 Instances When we terminate TLS/SSL on the Load Balancer, this means the ELB decrypts the encrypted request and sends it on to your application servers as plain text over the local private network inside your VPC Benefits of Terminating TLS/SSL on the ELB Offloads the processing, EC2 has more resources to use for application processing Can be more cost effective less compute power needed to handle your application load Reduces administrative overhead if you have many EC2 instances Security Implication of Terminating at the ELB Traffic between the load balancer and your instance is unencrypted If you have a compliance or regulatory requirement to use encryption end-to-end all the way to your EC2 Instances, you would need to terminate TLS/SSL on the EC2 Instance Which Load Balancer to Use The Application Load Balancer only supports TLS/SSL termination on the Load Balancer and only supports HTTP/S Application Load Balancer supports SNI. Can be installed multiple certificates in one HTTPS Listener If you want to terminate TLS/SSL on your EC2 instances, you'll need to use a Network or Classic Load Balancer and you will also need to use the TCP protocol rather than HTTP/S Classic Load Balancer is a legacy option, but it may still come up in the exam Tips For best use of your EC2 compute resource, terminate TLS/SSL on the Elastic Load Balancer If there is a requirement to ensure traffic is encrypted all the way to the EC2 instance, terminate TLS/SSL on the EC2 instance If you need to terminate traffic at the EC2 instance, then you'll need to use the TCP protocol with Network or Classic Load Balancer Application Load Balancers is HTTP/HTTPS only - for other protocols like TCP, use Network or Classic","title":"TLS/SSL Termination Options"},{"location":"certifications/aws/security-specialty/#vpc-flow-logs","text":"VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow Log data is stored using Amazon CloudWatch logs. After you've created a flow log, you can view and retrieve its data in Amazon Cloudwatch logs * Can be created at 3 levels * VPC * Subnet * Network Interface Level * You cannot enable flow logs for VPCs that are peered with your VPC unless the peer VPC is in your account * You cannot tag a flow log * After you've created a flow log, you cannot change its configuration; for example, you can't associate a different IAM role with the flow log * Not all IP Traffic is monitored * Traffic generated by instances when they contact the Amazon DNS server. If you use your OWN DNS server, the all traffic to that DNS server is logged * Traffic generated by a Windows instance for Amazon Windows license activation * Traffic to and from 169.254.169.254 for instance metadata * DHCP traffic * Traffic to the reserved IP address for default VPC router * Flow logs only have metadata, not the content","title":"VPC Flow Logs"},{"location":"certifications/aws/security-specialty/#nat-vs-bastions","text":"A NAT is used to provide internet traffic to EC2 instances in private subnets A Bastion is used to securely administer EC2 instances (using SSH or RDP) in private subnets.","title":"NAT vs Bastions"},{"location":"certifications/aws/security-specialty/#session-manager","text":"Secure Remote Login to EC2 Instances Browser Based Interactive session using Powershell or bash. Console, CLI and SDK Single Solution Manage Windows and Linux Instances No RDP or SSH required No SSH keys, no Bastion host to manage AWS recommended approach For interactive sessions on EC2. On-premises physical or virtual hosts. Secured using TLS encryption and auditable The Secure way to administer your Instances Centralized Access Control Using IAM you can control which individual users or groups in your organization can use Session Manager and which instances they can access No ports to open No need to open inbound SSH, RDP, Remote Powershell ports. Increased security Session Logs Connection history recorded in CloudTrail. Session history with keystroke logging can be sent to Cloudwatch or S3 Tips Remote Login Browser, CLI or SDK Powershell or Bash interactive sessions The Most secure option TLS encrypted, no bastion hosts or ports required Everything is logged Encrypted connection & session logging available using Cloudtrail, cloudwatch and s3","title":"Session Manager"},{"location":"certifications/aws/security-specialty/#amazon-dns","text":"When you create a VPC, your new VPC automatically includes an Amazon provided DNS server which is used to resolve public DNS hostnames Used for DNS hostname resolution for instances in your vpc which are communicating over the internet The Amazon DNS server uses one of the reserved IP address in your VPS CIDR range: In a subnet CIDR block of: 10.0.0.0/16 10.0.0.0 is the network address 10.0.0.1 is your VPC router 10.0.0.2 is the DNS server 10.0.0.3 reserved for future use 10.0.0.255 is usually the network broadcast address but as broadcast is not supported in AWS that is also reserved If you do not want to use the Amazon provided DNS server and instead you want to use a custom DNS server, you can disable this in the settings of your VPC Go TO DNS Resolution and uncheck the box Create a new DHCP options set to use your own custom DNS","title":"Amazon DNS"},{"location":"certifications/aws/security-specialty/#transit-gateway","text":"Any connected VPC is automatically available to every other connected network. Route tables control which VPCs can commmunicate * Benefits * Highly Scalable * Support 1000s of VPCs with a single transit gateway which scales as you grow * Hub & Spoke * Create and manage a single connection from your data centre to the transit gateway. Centralized connectivity policies * Secure * Traffic between your VPCs and the Transit Gateway is on the AWS network. Inter-region traffic is encrypted * Tips * Centralised Connectivity * Connect VPCs and on-premises networks using a single gateway * When to use It -> if you have 100+ VPCs * Avoid managing a lots of point-to-point connections * Scalable - 1000s of VPCS * A new VPC connected to the Transit Gateway is automatically available to every other connected network * Secure - AWS private network * Traffic between VPCs does not use the public internet. Inter-region traffic encrypted","title":"Transit Gateway"},{"location":"certifications/aws/security-specialty/#s3","text":"S3 Bucket Policies S3 Bucket policies are attached only to S3 Buckets. S3 Bucket policies specify what actions are allowed or denied on the bucket. They can be broken down to a user level, So Alice can PUT but not DELETE and John can READ but not PUT. Bucket level only, S3 only Explicit Deny always Overrides an Allow Use Cases IAM Policies bump up against the size limit (up to 2kb for users, 5 kb for groups and 10kb for roles). S3 supports bucket policies of up 20kb S3 ACL's Are Legacy access control. AWS recommend IAm Policies and S3 Bucket Policies Access Control only Use them if you need apply policies on the objects themselves. Bucket policies can be applied at bucket level, and S3 ACLS can be applied to individual files (Objects) Only can grant permissions to AWS Accounts. Cannot select IAM Users Cross-Account Access To S3 Buckets and Objects ACL Objects are owned by the identity who puts them If account B put objects in Bucket of Account A, the owner is Account B Bucket Policy Account B users are the owner of any objects created (Simniliar ACL) Permission control is handled within S3. There is no IAM involvement Bucket policies can require Account A (Bucket Owner) be the owner for objects as they are put in the bucket Utilize if want maintain permissions control inside S3 Can utilize anonymous and authenticated access. Utilize bucket polices to this { \"Statement\" :[ { \"Effect\" : \"Allow\" , \"Principal\" :{ \"AWS\" : \"311407276115\" }, # Account B \"Action\" : \"s3:PutObject\" , \"Resource\" :[ \"arn:aws:s3:::la-permissionsdemo/*\" ] }, { \"Effect\" : \"Deny\" , \"Principal\" :{ \"AWS\" : \"311407276115\" }, # Account B \"Action\" : \"s3:PutObject\" , \"Resource\" : \"arn:aws:s3:::la-permissionsdemo/*\" , \"Condition\" : { \"StringNotEquals\" : { \"s3:x-amz-acl\" : \"bucket-owner-full-control\" } } } ] IAM Role Users of account B assume a rule in account A (sts:AssumeRole) Objects are owned by that role so Account A Permissions are managed by IAM, not S3 Prefer option, best pratices Data at Rest: S3 Customer Provided Encryption Keys (SSE-C) SSE-C is a feature of Server side Encryption where S3 still handles the cryptographic operations, but does so with keys that you as the customer manage and supply every object operation x-amz-server-side-encryption-customer-key allows the key to be provided. The is used for encryption and then discarded. The customes is 100% responsible for key management and rotation. Versions can have alternative keys x-amz-server-side-encryption-customer-key-MD5 allows S3 to validate the key (for damage is transit) x-amz-server-sode-encryption-customer-algorithm = AES256 informs S3 that a customer managed key will be supplied as part of the putObject request","title":"S3"},{"location":"certifications/aws/security-specialty/#identity-federation","text":"AWS Supports federation with IdPs (identity Providers) which are OpenID connect (OIDC) Or Saml 2.0 compatible Identity federation is generally grouped into three types Web identity Federation SAML 2.0 identity Federation Custom ID Broker Federation (used when SAML2.0 compatability isn't available) Web Identity Federation Federation allows users to authenticate with a web identity provider (Google, Facebook, Amazon) The User authenticates first with the Web ID Provider and receives an authentication token, which is exchanged for temporary AWS credentials allowing them to assume an IAM role Cognito is an Identity Broker which handles interaction between your applications and the Web ID provider (You don't need to write you own code to do this) Provides sign-up, sign-in, and guest user access Syncs user data for a seamless experience across your devices Cognito is the AWS recommended approach for Web ID federation particularly for mobile apps Use Cases Cognito brokers between the app and Facebook or Google to provide temporary credentials which map to an IAM role allowing access to the required resources No need for the application to embed or store AWS credentials locally on the device and it gives users a seamless experience across all mobile devices SAML 2.0 Federation Enterprise Solution Use your own ID Provider (Not AWs provider) IP Provider, normally AD Authenticate in AD , returns SAML Assertion (similiar token in web identity), connects to AWS SSO enpoint and validate SAML assertion, and STS generates credentials and opens Console AssumeRoleWithSAML operations In ADFS, configure trusted with AWS as the relying party - This configuration is done in Active Directory Federation services","title":"Identity Federation"},{"location":"certifications/aws/security-specialty/#system-manager-parameter-store","text":"Confidential information such as password, database connection string, and license codes can be stored in SSM Parameter store You can store values as plain text or you can encrypt the data (Using KMS) You can then reference these values by using their names You can use this service with , cloudformation, lambda, EC2 Run Command etc Key Features Configuration and data is separated from code - no chance of leakage via git Data is stored hierarchically - aids management Data is versioned, and access can controlled and audited Parameter store integrates with many AWS services - EC2, ECs, Lambda, CodeBuild/Deploy, and many more Can also be used for automated deployment using cloudformation Serverless, resilient and scalable","title":"System Manager Parameter Store"},{"location":"certifications/aws/security-specialty/#incident-response-aws-in-the-real-world","text":"","title":"Incident Response &amp; AWS in The real world"},{"location":"certifications/aws/security-specialty/#ddos","text":"DDOS Attack A distributed denial of service (DDoS) attach is an attack that attempts to make your website or application unavailable to your end users This can be achieved by multiple mechanisms, such as large packet floods, by using combination of reflection and amplification techniques, or by using large buttons Amplification/Reflection Amplification/Reflection attacks can include things such as NTP, SSDP, DNS, Chargen, SNMP attacks, etc. and is where an attacker may send a third party server (such as an NTP server) a request using a spoofed IP address That server will then respond to that request with a greater pauload than initial request (usually within the region of 28 x 54 times larger than the request) to the spoofed IP address This means that if the attacker sends a packet with a spoofed IP address of 64 bytes, the NTP server would respond with up 3.456 bytes of traffic. Attackers can co-ordinate this and use multiple NTP servers a second to send legitimate NTP traffic to target How to mitigate DDos Minimize the Attack surface area Be ready to scale to absorb the attack Safeguard exposed resources Minimize the attack surface area Some production environments have multiple entry points in to them. Perhaps they allow direct SSH or RDP access to their web servers/application and DB servers for management This can be minimized by using a Bastion/JumpBox that only allows access to specific white listed IP address to these bastion servers and move the web, application, and DB servers to private subnets. By minimizing the attack surface area, you are limiting your exposure to just a few hardened entry point Be Ready to Scale to Absorb the Attack The key strategy behind a DDoS attack is to bring your infrastructure to breaking point. This strategy assumes one thing: that you can't scale to meet the attack The easiest way to defeat this strategy is to design your infraestructure to scale as, and when, it is needed You can scale both horinzontally & Vertically Scaling has the following benefits The attack is spread over a larger are Attackers then have to counter attack, taking up more of their resources Scaling buys you time to analyze the attack and to respond with the appropriate countermeasures Scaling has the added benefit of providing you with additional levels of redundancy Safeguard Exposed Resources In situations where you cannot eliminate internet entry points to your applications, you will need to take additional measures to restrict access and protect those entry points without interrupting legitimate end user traffic Three resources that can provide this control and flexibility are Amazon cloudFront, Amazon Route 53 and Web Apllication Firewalls (WAFs) CloudFront Geo Restriction/Blocking - Restrict access to users in specific countries (Using whitelists or blacklists) Origin Access identity - Restrict access to your S3 bucket so that people can only access S3 using Cloudfront URLS Route53 Alias Record Sets - You can use these to immediately redirect your traffic to an CloudFront distribution, or to a different ELB load balancer with higher capacity EC2 instances running WAFs or your own security tools. No DNS change, and no need to worry about propagation Private DNS - Allows you to manage internal DNS names for your application resources ( web servers, application servers, databases) without exposing this information to the public internet WAFs DDoS attacks that happen at the application layer commonly target web applications with lower volumes of traffic compared to infrastructure attacks. To mitigate these type of attacks, you'll want to include a WAF as part of your infraestructure New WAF Service - You can use the new AWS WAF service AWs MarketPlace - You can buy other WAF's on the AWS Marketplace Learn Normal Behavior Be aware of normal and unusual behavior Know the different types of traffic and what normal levels of this traffic should be Understand expected and unexpected resource spikes What are the benefits Allows you to spot abnormalities fast *`You can create alarms to alert you of abnormal behaviour Helps you to collect forensic data to understand the attack Create a plan for attacks - Having a plan in place before an attack ensures that You've validated the design of your architecture You understand the costs for your increased resiliency and already know what techniques to employ when you come under attack You know who to contact when an attack happens AWS Shield Free service that protects all AWs customers on ELB, CloudFront, and Route 53 Protects against SYN/UDP floods, reflection attacks, and other layer3/layer4 attacks Advanced provides enhanced protections for your applications running on ELB, Cloudfront and Route 53 against larger and more sophisticated attack. 3000 per month AWs Shield Advanced Provides Always-on, flow-based monitoring of network traffic and active application monitoring to provide near real-time notifications of DDoS attacks DDos Response Team (DRT) 24x7 to manage and mitigate application layer DDoS attacks Protects your AWS bill against higher fees due to ELB, cloudfront and Route53 usage spikes during DDoS attack Web Application Layer Attacks Mitigate using a WAF Block using a WAF Rate-based blacklisting Invest ime in limiting query string and header fowarding - Eliminates many common attacks deploy HTTP->HTTPs redirect at the edge - shields the origin from redirect floods Implement an SNI-based infrastructure - Many DDoS toolkits fail TLS handshake Tips Read white paper https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Papper.pdf technologies can use to mitigate DDoS attack CloudFront Route53 ELB's WAFs Autoscaling (use for both WAFs and Web servers) Cloudwatch","title":"DDOS"},{"location":"certifications/aws/security-specialty/#waf-integration","text":"WAF (Web application Firewall) integrates with both Application Load balancers and CloudFront. It does not integrate with EC2 directly, nor Route53 or any other services","title":"WAF Integration"},{"location":"certifications/aws/security-specialty/#ec2-has-been-hacker","text":"Some Scenarios around EC2 being compromised. What steps should you take Stop the instance immediately Take a snapshot of the EBS Volume Deploy the instance in to a totally isolated environment. isolated VPC, no internet access - ideally private subnet Access the instance using a foresinc workstation Read through the logs to figure out how (Windows Event Logs)","title":"EC2 Has been Hacker"},{"location":"certifications/aws/security-specialty/#pen-tests","text":"Allowed without approve in: EC2 Instances, Nat Gateways and ELB RDS Cloudfront Aurora Api Gateway Lambda and Lambda Edge Lightsail Beanstalk Prohibited Activies DNS zone walking via Route 53 hosted zones DoS, DDoS, Simulated DoS and DDoS Port flooding Protocol flooding Request flooding (login request flooding, API request flooding) To run Penetration testing on AWs go to AWS Marketplace and search for a penetration testing tool like kali linux","title":"Pen Tests"},{"location":"certifications/aws/security-specialty/#aws-certificate-manager-data-in-transit","text":"Managed service providing X509 v3 SSL/TLS Certificates. The certificates are Asymmetric. One half is private and stored on resources (Servers, LoadBalancers etc) and the other half is public SSL Certificates renew automatically (Valid for 13 months) provided you purchased the domain name from Route 53 and it's not for an Route 53 private hosted zone Integrates with Route53 to perform DNS checks as part of certificate issuing process You can use Amazon SSL certificates with both load balancers and cloudfront, beanstalk and API Gateway You cannot export the certificates No cost Regional KMS is used - certificates are never stored unencrypted AWS handles the painful parts of PKI Key pair and certificates signing request feneration Encryption and secure storage of private keys Managed renewal and deployment Domain validation (DV) through DNS validation/email","title":"AWS Certificate Manager - Data in Transit"},{"location":"certifications/aws/security-specialty/#api-gateway","text":"To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API When request submissions exceed the steady-state request rate and burst limites, API Gateway fails the limit-exceeding requets and returns 429 Too Many Requests error responses to the client By default, API Gateway limits the steady-state request rate to 10000 requets per second (rps) It limits the burst to 5000 requets across all APIs within AWS The account-level rate limit and burst limit can be increased upon request Caching You can enable API caching in Amazon APi Gateway to cache your endpoints response. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of the requests to your API When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. APi Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled","title":"Api Gateway"},{"location":"certifications/aws/security-specialty/#system-manager-ec2-run-command","text":"You work as System administrator managing a large number of EC2 instances and on premise systems You would like to automate common admin tasks and ad hoc configuration changes e.g. installing applications, applying the latest patches, joining new intances to an Windows domain without having to login to each instance Commands can be applied to a group os systems based on AWS instance tags or be selecting manually SSM agent needs to be installed on all your managed instances The commands and parameters are defined in a Systems Manager Document Commands can be issued using AWs Console, AWS CLI, AWS Tools for Windows PowerShell, Systems Manager API or Amazon SDKs You can use this service with your on-premise systems as well as EC2 instances","title":"System Manager EC2 Run Command"},{"location":"certifications/aws/security-specialty/#compliance","text":"ISO 27001:2005/10/13 Specifies the requirements for establishing, implementing , operating, monitoring, reviewing, maintaining and improving a documented information security management system within the context ot the organiztion's overall business risks HIPPA HIPAA is the federal health insurance portability and accountablility act of 1996. The primary goal of the law is to make it easier for people to keep health insurance, protect the confidentiality and security of healthcare information and help the healtcare industry control administritve costs PCI DSS V3.2 Build and Maintain a Secure Network and Systems Requirement 1: install and maintain a firewall configuration to protect cardholder data Requirement 2: Do not use vendor-supplied defaults for system passwords and other security parameters Protect cardholder data Requirement 3: Protect stored cardholder data Requirement 4: Encrypt transmission of cardholder data across open, public networks Maintain a vulnerability Requirement 5: Protect all systems against malware and regularly update anti-virus software or programs Requirement 6: develop and maintain secure systems and applications Implement Strong Access control Measures Requirement 7: Restrict access to cardholder data by business need to know Requirement 8: identify and authenticate access to system components Requirement 9: Restrict physical access to cardholder data Regularly Monitor and Test Networks Requirement 10: Track and monitor all access to network resources and cardholder data Requirement 11: Regularly test security systems and processes SAS70 SOC1 FISMA FIPS 140-2 - Cloud HSM meets the level 3 standard . KMS don't meet this","title":"Compliance"},{"location":"certifications/aws/security-specialty/#chapter-8","text":"","title":"Chapter 8"},{"location":"certifications/aws/security-specialty/#athena","text":"Interactive query service which enables you to analyse and query data located in S3 using Standard SQL * Serverless, nothing ro provision, pay per query / per TB scanned * No need to set up complex Extract/Transform/Load (ETL) processes * Works directly with data stored in S3 What can Athena be used for * Can be used to query log files stored in S3, e.g. ELB logs, S3 access logs * Generate business reports on data stored in S3 * Analyse AWs cost and usage reports * Run queries on click-stream data Exam Tips * Commonly used to analyse log data stored in S3","title":"Athena"},{"location":"certifications/aws/security-specialty/#macie","text":"What is PII (Personally identifiable information) Personal data used to establish an individuals identity This data could be exploited by criminals, used in identity theft and financial fraud Home address, email address, SSN Passport number, Drivers license number D.O.B, phone number, bank account, credit card number What is -> Security service which uses Machine Learning and NLP (Natural languague Processing) to discover, classify and protect sensitive data stored in S3 Uses AI to recognise if your S3 objects contain sensitive data such as PII Dashboards, reporting and alerts Works directly with data stored in S3 Can also analyze Cloudtrail logs Great for PCI-DSS and preventing ID theft Macie Classifies your data By content Type JSON, PDF, Excel, TAR or Zip file, source code, XML By File extension .bin, .c, .bat, .exe, .html, .sql By theme AmEx, Visa, MasterCard credit card keywords, banking or financial keywords, hacker and web exploitation keywords By Regular Expression aws_secret_key, RSA Private key, SWIFT codes, cisco router config How can Macie Protect your data Analyze and classify the data Dashboards, Alerts and Reports on the presence of PII Gives visibility on how the data is being accessed Analyze Cloudtrails logs and report on suspicious API activity Tips Macie uses AI to analyze data in S3 and helps identify PII Can also be used to analyse Cloudtrail logs for suspicious API activity Includes dasboards, Reports and alerting Great for PCS-DSS compliance and preventing ID theft","title":"Macie"},{"location":"certifications/aws/security-specialty/#guardduty","text":"Guardduty is a threat detection service which uses ML to continuously monitor malicious behaviour Unusual API calls, calls from a know malicious IP Attempts to disable CloudTrail logging Unauthorized deployments Compromised instances Reconnaissance by would be attackers Port scanning, failed logins Features Alerts appear in the Guardduty console & Cloudwatch events Receives feed from third parties like Proofpoint, Crowdstrike and AWS Security - know malicious domains / IP addresses Monitors cloudtrail logs, VPC flow logs, DNS logs Centralize threat detection across multiple AWS accounts Automated response using cloudwatch events and lambda Machine learning and anomaly detection Setting up 7-14 days to set a baseline - What is normal behaviour on your account Once active you will see findings on the GuardDuty console and in cloudwatch events only if GuardDuty detects behaviour it considers a threat 30 days free. Charges based on Quantity of cloudtrail events volume of DNS and VPC flow log data Tips Uses AI to learn what normal behaviour looks like in your account and to alert you of any abnormal or malicious behaviour Updates a database of know malicious domains using external feeds from third parties Monitor cloudtrail logs, VPC flow logs, DNS logs Findings appear in the Guarduty dashboard, cloudwatch events can be used to trigger a lambda function to address a threat","title":"GuardDuty"},{"location":"certifications/aws/security-specialty/#secrets-manager","text":"Secrets Manager is a service which securely stores, encrypts and rotates your DB credentials and other secrets Encryption in-transit and at rest using KMS Automatically rotates credentials Apply fine grained access control using IAM Policies Your application makes an API call to secrets Manager to retrieve the secret programmatically Reduces the risk of credentials being compromised Rotation Options Multi-User Rotation Separate master user credentials are used for secret rotation. The old version of secret continues to operate and handle service requests, while the new version is prepared and tested. The old version isn't deleted until after the clients switch to the new version. There is no downtime while changing between versions Single-user Rotation Secrets master uses a single user to rotate its own credentials. Sign-in failures can occur between the moment when the old password is removed by rotation and the moment when the updated password is made accessible as a new version of the secret. This time window should be very short, but it can happen To avoid this, use Multi-User Rotation or exponential back-off What Can Store RDS credentials Credentials for non-RDS databases Any other type of secret provided you can store it as a key value pair (SSH keys, API keys) Enabling Secrets manager Automatic Rotation Important: if you enable rotation, secrets manager immediately rotates the secret once to test the configuration Ensure that all of your applications that use these credentials are updated to retrieve the credentials from this secret using secrets manager Disable automatic rotation if your applications are still using embedded credentials do not enable rotation because the embedded credentials will no longer work and this will break your application Enable Rotation - recommended setting if your applications are not already using embedded credentials i.e. they are not going to try to connect to the database using old credentials Tips Secrets Manager can be used to securely store your application secrets: DB credentials, API keys, SSH keys, passwords etc Applications use the secrets manager API Rotating credentials is super easy - but be careful When enabled, Secrets manager will rotate credentials immediately Make sure all your application instances are configured to use Secrets manager before enabling credential rotation","title":"Secrets Manager"},{"location":"certifications/aws/security-specialty/#ses","text":"SES is a cloud based email service, which supports both sending and receiving email Can be used to send marketing emails, transaction emails and email notifications from your applications It uses a standard SMTP interface and can also be accessed using an API to allow you to integrate with existing applications All connections to the SMTP endpoint must be encrypted in transit using TLS Configuring Access to SES for EC2 instances * Configure the security group associated with your ec2 instances to allow connections to the SES smtp endpoint * Port 25 is the default but EC2 throttles email traffic over port 25 * To avoid timeouts use either port 587 or 2587","title":"SES"},{"location":"certifications/aws/security-specialty/#security-hub","text":"Central hub for security alerts one place to manage and aggregate the findings and alerts from key AWS security services Automated checks PCS-DSS (Payment card industry) CIS (center for internet security) Ongoing Security Audit automated ongoing security audit for your AWS accounts Integration with Guardduty Macie Inspector IAM Access Analyzer Firewall manager 3 Party tools CloudWatch From cloudwatch cand send the events to lambda/chat/SIEM/notification/...","title":"Security Hub"},{"location":"certifications/aws/security-specialty/#network-packet-inspection","text":"Inspects packet headers and data content of the packet * Also know as Deep Packet Inspection (DPI) * Filters non-compliant protocols viruses, spam, intrusions * Takes action blocking, re-routing or logging * IDS / IPS combined with a traditional firewall What AWS provide VPC Flow Logs Capture network flow for a VPC, subnet or network interface, storing the data in cloudwatch logs, can be used for troubleshooting and profiling network flow AWS WAF protects web applications against know exploits like SQL injection and cross site scripting host based firewalls like Iptables / windows Firewall IDS /IPS Use a third party solution AWS does not provide a solution for Network packet inspection, IDS/IPS You will need to run host based solution in EC2 - Alert Logic, Trend Micro, Mcafee A host based IDS solution compliments the features available within AWS WAF - Provides edge security before a threat arrives at your environment edge IDS Appliance - Monitor and analyses data as it moves in your plafform AWs Config - Ensures a stable and compliant configuration of account level aspects SSM - Ensures compute resources are compliant with patch levels Inspector - Reviews resources for know exploits and questionable OS/Software configurations Host Based IDS - Handles everything else","title":"Network Packet Inspection"},{"location":"certifications/aws/security-specialty/#active-directory-federation-with-aws","text":"AWS allows federated sign-in to AWS using AD credentials Minimises the admin overhead by leveraging existing user accounts, passwords, password policies and groups Provides SSO for users ADFS - Active Directory Federation Services, SSO and ID broker solution SAML 2.0 Security Assertion Markup Language, Open standard for exchanging identity and security information between identity providers and applications. Enables SSO for AWS accounts AD federation with AWS 2-way trust: In AWs, ADFS is trusted as the ID provider In ADFS, configure a Relying Party trust with AWS as the Relying party Corporate user accesses the Corporate AFS portal sign-in and provides their AD username and password ADFS authenticates the user against AD AD returns user's information including group membership ADFS sends a SAML token to the user's browser which sends the token the AWS sign-in endpoint The AWS sign-in endpoint makes an STS AssumeRoleWithSAML request and STS returns temporary credentials User is authenticated and allowed to access the AWS management console ADFS baiscally acts as an identity broker between AWS and your Active Directory AD users can assume roles in AWS based on group membership in AD Tips Remember how it works at a high level User authenticates with ADFS/AD first, they receive a SAMl token which is exchanged with the AWs sign-in endpoint for temporary credentials to the AWS console (STS API AssumeRoleWithSAML) User is automatically redirected to the AWS console No need to create duplicate accounts in AWS ADFS it the trusted ID provider, AWS is the Trusted Relying Party","title":"Active Directory Federation with AWS"},{"location":"certifications/aws/security-specialty/#artifact","text":"Central resource for compliance & security related information Download AWS security and compliance documents - Widely used industry standards ISO certifications PCI (Payment card industry) SOC Reports (Service Organizational control) AWS Artifact enables you * Demonstrate compliance to regulators * Evaluate your own cloud architecture * Assess the effectiveness of your company's internal controls","title":"Artifact"},{"location":"certifications/aws/security-specialty/#aws-encryption-sdk","text":"Is an encryption library that helps make it easier for you implement encryption best practices in your application Data Key Caching","title":"AWS Encryption SDK"},{"location":"certifications/aws/security-specialty/#troubleshooting-monitoring-alerting","text":"","title":"Troubleshooting &amp; Monitoring &amp; Alerting"},{"location":"certifications/aws/security-specialty/#troubleshooting-cloudwatch","text":"Common Issues Does the IAm user have the correct permissions to allow them to read the coudwatch dashboard? Check that cloudwatch events has permission to invoke the event target Check the lambda has permissions to terminate EC2 CloudWatch Events - Common issues General configuration issues Wrong resource name when connection resources Typos - This should be checked during configuration, but we all make mistakes Huge with API calls, filter patterns, and lambda functions Not waiting long enough after making changes or new configurations Roles do not have sufficient permissions (AWS does not always tell you with an error) This includes targets and subscriptions to encrypted resources - must include KMS policies CloudEvents With Cloudwatch, we can create an alarm on Events Metrics. We can use FailedInvocations to notify us when our Cloudwatch Events rules are broken Lambda functions Lambda delivers logs to cloudwatch logs. It will log errors with invocations. We can then alarm on this using a metric filter and notify via SNS Cloudtrail Trail and logging enabled Cloudwatch logs configured Role for Cloudwatch logs Log group name VPC Flow Flogs Flows enabled on VPC or Subnet Check Filter Role for cloudwatch logs Log group name Route 53 DNS Logs Query loggin configured New or existing log group Log group name EC2 Agent installation and config Agent must be started Role For cloudwatch logs","title":"Troubleshooting Cloudwatch"},{"location":"certifications/aws/security-specialty/#trouleshooting-cloudtrail","text":"S3 Bucket Problems Cloudtrail logs not appearing in S3? Check the Cloudtrail is enabled Check you have provided the correct S3 bucket name Important to remember that S3 and lambda data events are high volume so they are not enabled by default Touble accessing the cloudtrail logs? Check your user has read access to cloudtrail: AWSCloutRailReadOnlyAccess Policy Check All Regions Enabled","title":"Trouleshooting CloudTrail"},{"location":"certifications/aws/security-specialty/#troubleshooting-secure-network-infra","text":"Check routing tables, Security Groups, NACLs Remember NACLs are stateless - Need configure both inbound and oubount rules Security Groups deny by default, use NACL to explicitly deny If you are peering 2 VPCs, remember to configure routing tables in both VPCs Internet access - NAT Gateway, Internet Gateway check VPC flow logs to view allow / deny messages","title":"Troubleshooting Secure Network Infra"},{"location":"certifications/aws/security-specialty/#troubleshooting-identity-federation","text":"Use the Correct API for the JOB Authenticated by a Web identity Provider - Facebook ect STS:AssumeroleWithWebIdentity Autenthicated by a SAML Compliant ID provider - AD STS:AssumeRoleWithSAML Authenticated by AWS STS:AssumeRole","title":"Troubleshooting Identity Federation"},{"location":"certifications/aws/security-specialty/#troubleshooting-cross-account-roles","text":"Cross-Account Roles are IAM Roles in an account A which are accessed by identities in account B Like all roles, they consist of two parts: a trust policy, and a permissions policy Using STS:AssumeRole - Common Issues Check the external account has permission to call STS:AssumeRole - Dev Account IAM Policy Check the external account is trusted AND has permission to perform the action you are attempting - Prod Account, Role Using STS:AssumeRole - Access to KMS The key policy needs to trust the external account The external account needs an IAM policy allowing users to run specific API calls for the resource (in this case the CMK) Tips For cross account access to S3: check that the account is trusted, the iam policy the external account needs to allow the user to call STS:AssumeRole, The iam policy in the trusting account needs allow the action Cross account access to kms, check you have configured the key policy to allow access to the external account as well as the iam policy in the local account","title":"Troubleshooting Cross Account Roles"},{"location":"certifications/aws/security-specialty/#troubleshooting-lambda-access","text":"Lambda cannot perform some action e.g. write to S3, Log to Cloudwatch, terminate instances, use a CMK, etc - Check the Lambda execution role has the correct permissions Remember that some services have their own resource based policies which will also impact who or what can access them - S3 bucket policies, key policies If cloudwatch events or some other event source cannot invoke lambda function, double check that the function policy allows it","title":"Troubleshooting Lambda Access"},{"location":"certifications/aws/security-specialty/#troubleshooting-access-to-kms-cmks","text":"Access to use KMS CMKs is defined by: Key Policy - Resource based policy attached to the CMK, defines key users and key administrators and trusted external accounts IAM Policy - Assigned to User, Group Or Role, defines the allowed actions e.g. kms:ListKeys, kms:Encrypt, kms:Decrypt Important : There is no hard truste between a CMK and an account. The permission can be removed, resulting in unusable key and a requirement to involve AWS Support Key admins can admin keys, not use them. Permissions granted via IAM Policy, or Key Policy or both","title":"Troubleshooting Access to KMS CMKs"},{"location":"certifications/aws/security-specialty/#troubleshooting-kms","text":"KMS Permissions Permissions within KMS are centered around Customer Master Key (CMKs) A default policy to a CMK trusts the account the key is created within, and this trust can be provided to IAM users via IDENTITY policies, or, on the KEY policy itself Permissions within KMS are either ADMIN permissions or Usage Permissions You can Lock out a CMK making it unusable to everyone KMS Limits Simple Limits 1000 (customer managed) CMKS per region - in Any state 1100 Aliases per account 2500 Grants per CMK - e.g max of 2500 EBS Volumes using CMK Breaching the shared, or per operation limits result in KMS throttling the requests Rate Limit There is a 5500 Shared API limit shared across a number of operations relating KMS - the high volume operations Actions With rate Limit: Decrypt, Encrypt, GenerateDataKey, GenerateDataKeyWithoutPlainText, GenerateRandom, ReEncrypt","title":"Troubleshooting KMS"},{"location":"certifications/aws/security-specialty/#troubleshooting-s3-access-logs","text":"Confusion over source and destination buckets Log delivery group permissions","title":"Troubleshooting S3 Access Logs"},{"location":"certifications/aws/security-specialty/#troubleshoot-multi-account-logging","text":"CloudTrail logging across multiple accounts S3 bucket policy for accounts sending logs Bucket names should be double-checked for accuracy CloudWatch Logs across multiple accounts Cloudwatch does not send logs directly to another account S3 access issues blocking exports (scheduled or manual) Kinesis stream is not setup properly (only target for real-time logs) Common Issues with Multi-Account Logging Issues will mostly be around permissions (roles and resource policies) Make sure all permissions only grant read-only access","title":"Troubleshoot Multi-Account logging"},{"location":"certifications/azure/fundamentals/","text":"Azure Fundamentals \u00b6 Cloud Concepts \u00b6 Language of Cloud Computing \u00b6 High availability Maintaining Acceptable continuous performance despite temporary load fluctuations or failures in services, hardware, or data centers Data Center Redundancies Power Cooling Networking ETC Availability Zone Redundancies 1 or more data centers Region Redundancies Multiple Availability Zones Fault Tolerance A system's ability to continue operation properly when one or more of its component fails Proactive Regularly back up data/apps/resources Deploy to multiple azs or regions Load Balance across multiple availability zones or regions Monitor health or data/apps/resources Reactive Restore data/apps/resources to different availability zones or regions Deploy to different availability zones or regions Disaster Recovery A system's ability to back up and restore data/apps/resources when needed Can use Azure to Restore On-premises to on-premises On-premises to Azure Other cloud to Azure Azure to Azure Scalability The ability to increase the instance count or size of existing resources Scaling Out Increase instance count of existing resources Non-disruptive Scaling Up Increase instance size of existing resources Disruptive Elasticity is the ability to increase or decrease the instance count or size of existing resources based on fluctuations in traffic, load, or resource workload Ability to scale in both directions ( in+out, up + down ) Can be manual or automatic Based on changes in load or workload Pay only for what you use Agility means do the ability to rapidly develop, test and launch software applications Focus on Time to Value Innovation Low Latency Economic Effectiveness Rapid Adaptation Flexibility Language of Cloud Economics \u00b6 Economies of scale Supply-Side Savings Lower costs of land, servers, power, labor, etc Demand-Side Savings Serving more customers means using more server resources ( 80% CPU) Higher server utilization rates mean lower costs Multi-Tenancy Savings More tenants (customers or users) lowers the cost of servers and management per tenant Capital expenditure (CapEx) is buying hardware outright, paid upfront as a one time purchase Up-front investment Creates benefits over long period of time Usually non-recurring Usally related to purchase of permanent assets Operational Expenditure (OpEx) is ongoing costs needed to run your business No upfront investment Creates immediate benefits Recurring cost of doing business Usually not related to purchase of permanent assets Consumption-based pricing let's you pay only for what you use Compute - Pay by CPU/hour Storage - Pay by GB Data Transfer - Pay by GB for data out, not in Cloud Service Models \u00b6 IaaS provides servers, storage and networking as a service You Manage O/S Middleware Runtime Data Applications Microsoft Manages Virtualization Servers Storage Networking PaaS is a superset of IaaS and also includes middleware, such as database management tools You Manage Applications Data SaaS is when a service is built on top of PaaS, like Office 365 You Manage Nothing Serverless means that you don't have any servers. Let's a single function be hosted, deployed, run and managed on its own Cloud Architect Models \u00b6 Private Cloud is azure on your own hardware in a location of your choice. All the benefits of public cloud, but you can lock it down. A Lot of staff required Public Cloud Is Azure, AWS, GCP. No upfront costs, but monthly usage. Little control over services and infrastructure Hybrid Cloud model is the best of private and public, but could be complex Core Azure Products & Services \u00b6 Azure Architecture \u00b6 Regions Groups of multiple Azure data centers withing a latency-defined perimeter and connected through a dedicated regional low-latency network Deploying your applications/systems to multiple regions allows for resiliency to region-wide outages Multi-Region Failover (Active/Passive) Failover across regions instantly Resiliency to region-wide outages Best when all users are in the same region Multi-Region Deployments (Active/Active) Distribute traffic/load across regions Resiliency to region-wide outages Best when all users are in different regions Availability Zones Unique physical locations within An azure region made up of one or more data centers Allow for high availability by protecting your applications and data from data center failures Some Azure services can be deployed to two or more AZs within an Azure region Resource Groups - Containers of related Azure services grouped together All the resources in the group should share the same lifecycle - They should be deployed, updated, and deleted together A resource can only exist in one resource group A resource group can contain resources that are located in different regions A resource group can be used to scope access control for administrative A resource can interact with resources in other resource groups (When two resources are related but don't share the same lifecycle; for example web apps connection to a database) Azure Resource Manager The Azure deployment and management service Provides a consistent management layer that enables you to create, update, and delete resources in your Azure subscription, including wihth templates You can use its access control, auditing, and tagging features to secure and organize your resources after deployment Azure Products and Services \u00b6 Compute \u00b6 Virtual Machines Pricing - Calculated Hourly A Virtual Machine is your machine exclusively You don't buy, own or control any hardware. Azure does this VMs are an IaaS offering, where you are responsible for the entire machine Azure virtual machines take advantage of azure tools Pricing goes up as resources go up, and you pay by the hour Scale Sets Auto-scaling pool of virtual machines Centrally manage, configure and update a large number of vms Scale sets are identical VMs. They can be activated or deactivated as needed A baseline VM for the scale set ensures application stability. A baseline VM is what you copy to make up the scale set VMS As resources usage increases, more VMs are activated to take the load You only pay for the VM, storage and networking resources you use. Nothing additional for scale sets App Services On-Demand, auto-scaling web/mobile/api app hosting Linux & Windows platforms App services are a PaaS offering on Azure Web Apps are used to ost web sites and web applications Web Apps for containers can host your existing container images API Apps can host your data backend services Serverless functions Event-driven serverless compute resources Auto-scale on demand Pay only for what you consume Kubernetes Kubernetes = AKS EMR = ACR Networking \u00b6 VNet An address space is a range of IP addresses you can use for your resources A subnet is smaller network, which is part of your VNET. Use these for security and logical division of resources A VNet is in a single region and single subscription VNets in the cloud can scale, have high availability and isolation VPN Gateway A VPN Gateway is a specific VNet Gateway, It consists of two or more dedicated VMNets VNET Gateway + \"vpn\" becomes a VPN Connection Send encrypted data between Azure and on premises network Azure Gateway Subnet, secure tunnel and on-premises gateway makes up a VPN Gateway Scenario Azure Load Balancer AWS ELB Similiar Azure APP Gateway AWS ALB Similiar It works on the HTTP request of the traffic, instead of the IP address and port Traffic from a specific web address can go to a specific machine Is a fit for most other Azure Services Supports auto-scaling, end-to-end encryption, zone redundancy ans multi-site hosting Azure content Delivery Network Efficently dlivers web content from locations that are close to end users to minimize latency Storage \u00b6 BLOB Highly scalable, REST based object store Unstructure data Types Block Store text and binary data up to 4.7TB. Made up of individually managed blocks of data Append Block blobs that are optimized for append operations Works well for logging where data is constantly appended Page Store files up to 8TB Any part of the file could be accessed at any time, for example a virtual hard drive Pricing Hot Frequently accessed files. Lower access times and higher access costs Cool Lower storage costs and higher access times. Data remains here for at least 30 days Archive Lowest costs and highest access times Tables Massive auto-scaling NoSQl Store Dynamic scaling based on load Scale to PBs of table Data Fast Key/value lookups Queues Reliable queues at scale for cloud services Decouple and scale components Disk - Types HDD Spinning hard drive. Low cost and suitable for backups Standard SSD Standard for production. Higher reliability, scalability and lower latency over HDD Premium SSD Super fast and high performance. Very low latency. Use for critical workloads Ultra Disk For the most demanding, data-intensive workloads. Disks up to 64 TB Files AWS EFS Fully managed File Shares in the cloud SMB and Rest Access Lift and shift legacy apps Databases \u00b6 Cosmos DB Globally distributed database. It is super fast and easy to manage Scale to infinite performance and size Can be Costly Supports multiple APIs, including SQl, MongoDB, Cassandra, Tables & Gremlin Supports analytics through Apache Spark Azure SQL IaaS SQL Server on Azure Virtual Machines Pay-as you-fo for SQL server license or use an existing license PaaS Fully managed SQL database engine Available as single database, elastic pool, and managed instances Azure Database for MySQL Azure Database for PostGreSQL Horizontal scaling Azure SQL Data Warehouse Cloud based Enterprise Data Warehouse (EDW) Leverages massively parallel processing (MPP) to quicly run complex queries across petabytes of data Key component of a Big Data solution Database Migration Services Enables seamless migrations from multiple database sources to azure data platforms with minimal downtime Offline migration: Application downtime starts when the migration starts Online migration: Downtime is limited to the time to cut over the end of the migration Azure Marketplace \u00b6 An online applications and services marketplace Offers technical solutions and services from Microsoft and parteners that are designed to extend azure products and services Azure Solutions \u00b6 IoT \u00b6 An IoT solution is made up of one or more IoT devices and one or more back-end services running in the cloud that communicate with each other Devices usually have sensors and connect to the internet IoT Hub PaaS Solution that provides more control over the IoT data collection and processing Enables reliable and secure bidirectional communications between milions of IoT devices and cloud solution Managed and Secure Ease of Deployment Platform-as-a-service Scaling and Authentication IoT Central SaaS solution that provides pre-made IoT connections and dashboards to get set up quickly Software-as-a-service Simplify and speed up the implementation of your IoT solution No Coding Needed You don't have to know how to write code to deploy your IoT project Receive feeds from devices and focus on metrics and business value Pre-made Connectors Use any of the hundreds of connectors that are ready to use in IoT Central Big Data \u00b6 Data Lake Analytics Large Amounts of Data A data lake is a very large body of data Parallel Processing Two or more processes or computers processing the same data at the same time Ready to Go Servers, processes and any other needed services are ready to go from the start Jump straight into the data analytics HD Insights Similar to Azure Data Lake Analytics Open Source, which is free and community supported Includes Apache Hadoop, spark and kafka Azure Databricks Based on Apache Spark, a distributed cluster-computing framework Run and process a dataset on many computers simultaneously Databricks provides all the computing power Integrates with other Azure Storage Services Azure SQL Data Warehouse Cloud-based enterprise warehouse (EDW) Leverages massively parallel processing (MPP) to quickly run complext queries across petabytes of data Artificial Intelligence \u00b6 Machine Learning/AI Models The definition of what your machine learning application is learning A model is a set of rules of how to use the data provided The model finds patterns based on the rules Knowledge Mining Use Azure search to finding existing insights in your data File relationships, geography connections and more Built-in Apps Azure has a number of built-in apps that you can use for machine learning and AI straight away. These include cognitive services and bot services Azure Cognitive Services Vision You can use the vision service to recognize, identify and caption your videos and images. Automatically Decision Apps can make decisions based on content. Detect potential offensive language, detect IoT anomalies and leverage data analytics Speech Automatic speech-to-text transcription. Speaker identification and verification Azure Machine Learning Studio Supports all Azure Machine Learning tools Pre-made modules for your project Use for real world scenarios such as twitter sentiment analysis, photo grouping and movie recommendations Use this if you want to experiment with machine learning models quickly and easily, and the built-in machine learning algorithms are sufficient for your solutions Uses prebuilt and preconfigured machine learning algorithms and data handling modules Collaborative, drag-and-drop visual workspace where you can build, test, and deploy machine learning solutions without the need to write code Machine Learning Service End-to-End Service The service to use AI and machine learning almost anywhere on Azure Tooling The Machine Learning service is a collection of tools to help you build AI applications Automation Azure automatically recognizes trends in your applications and creates models for you Use this if you work in a python environment, you want more control over your machine learning algorithms, or you want use open-source machine learning libraries Fully supports open-source technologies: tens of thousands of open-source machine learning components are available Provides a cloud-based environment tou can use to prep data and train, test, deploy, manage, and track machine learning models Serverless \u00b6 Azure Functions Configure your code to load and run only when triggered Triggers can be HTTP requests, data from other Azure Services, etc Can Return data or call other azure services Logic Apps Are a quick and simple way to integrate systems and applications inside and outside of azure Visual designer for automating and orchestration tasks processes and workflows Great for integrating apps/data/systems and for business-to-business(B2B) integrations Connect Systems Connect systems bot inside and outside of the Azure platform Integrate not only apps, but also data flows, services and entire systems Automation A lot of ways to schedule, automate and orchestrate tasks and processes Quick Start No coding required to get started straight away. You just need access to your apps to integrate Event Grid Fully managed event routing service Greatly simplifies the development of event-based applications and serverless workflows Handles all event routing from any source, to any destination for any application Azure Management Tools \u00b6 Azure cli, PowerShell, Azure Portal \u00b6 Azure Cli Command line experience for managing Azure resources You can use it in your browser with Azure cloud shell (interactive browser-accessible shell that runs basb or powershell) You can install it on macOs, Linux, and Windows and run it from the command line Power Shell Set of PowerShell cmdlets for managing Azure resources You can use it in your cloud shell Uses .NET standard making it available for windows, macOS, and Linux Azure Portal Build, manage and monitor every Azure resource in a single unified console Browse active resources modify settings, provision new resources, and view basic monitoring data from all your azure products and services Azure Advisor \u00b6 Personalized cloud consultant that helps you follow best practices to optimize your azure deployments Analyzes your resource configuration and usage telemetry Recommends solutions to help improve the cost effectiveness, performance, security, and availability your resources Security, Privacy, Compliance, and Trust \u00b6 Network Security \u00b6 Azure Firewall \u00b6 Managed, cloud-based network security service that protects your azure virtual Network resources Centrally create, enforce, and log application and network connectivity policies across subscriptions and virtual networks Fully integrated with azure monitor for logging and analytics Network Security Groups (NSGs) \u00b6 Enforce and control network traffic rules at the networking level Security rules allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources For each rule, you can specify a source, destination, port, and protocol Azure DDoS Protection \u00b6 Choosing an Azure Security Solution \u00b6 Isolation Use different virtual networks, or virtual network subnets, to segment your network or isolate resources when required Principle of least privilege Limit each user's access rights to the bare minimum permissions they need to perform their work Use NSGs for Bare Minimum Network Connectivity Use network security groups (NSGs) to enforce rules allowing only the minimum network access needed Use Azure Firewall and Azure DDoS Protection Use Azure Firewall and Azure DDoS protection to protect your resources and prevent large unanticipated costs Azure Identity Services \u00b6 Authentication and Authorization Azure Multi-Factor Authentication (MFA) Something you know ( Typicall password) Something you have ( a trusted device that is not easily duplicated, like a phone) Something you are ( biometrics ) Azure Active Directory (AAD) Comprehensive, highly available identity and access management (IAM) cloud solution Fully managed identity service - requires no hardware, virtual machines, etc Main objects are users (pay per user) and groups(free) Users are assigned a license based on which edition of Azure AD they should have Groups can be assigned multiple licenses, which are inherited by all users in the group Active Directory (AD) is not the same as Azure Active Directory Different skillset from AD to Azure AD Every Azure account will have and azure AD service Organization A tenant represents the organization Dedicated AAD A tenant is a dedicated of AAD that an organization receives when signing up for Azure Separate Each tenant is distinct and completely separate from other AAD tenants One User - One Tenant Each user in Azure only belong to a single tenant Users can be guests of other tenants Azure AD can help manage users in a hybrid cloud setup Azure Security tools and features \u00b6 Security is built into every layer of azure Secure Foundation State-of-the-art security in Azure data centers around the world, integrated security controls in Azure hardware and firmware components, and protection against threats such as DDoS Simplify security with Built-In Services Easily manage identify and control access, secure your network, safeguard data, manage secrets and certificates, prevent attacks, and gain centralized visibility Detect threats early with unique intelligence identify new threats and respond quickly with services that are informed by real-time global cybersecurity intelligence Azure Security Center \u00b6 Protects Azure and non-Azure servers and virtual machines - including windows and linux servers - when the microsoft monitoring agent is installed Events collected from azure and the agents are sent to the security analytics engine, which provides threat detection alerts and tailored recommendations for securing workloads Azure PaaS services - including SQL databases and storage accounts - are automatically monitored and protected by Azure Security Center Azure Key Vault \u00b6 Encrypt and safeguard authentication keys, storage account keys, data encryption keys, certificates, and passwords Data is protected by hardware security modules (HSMs) Control access to keys using Azure Active Directory Grant access to individual users or to applications and systems Azure information Protection (AIP) \u00b6 Cloud-based solution that helps organizations classify and protect documents and emails Manually or automatically classify sensitive data using 80+ built-in data types (credit card numbers, ID/SSN numbers) Enforce policies on classified daa - require credit card numbers to be encrypted Can be retroactively applied to existing documents and emails Azure Advanced Threat Protection (ATP) \u00b6 Cloud-based security solution that identifies, detects, and helps you investigate advanced threats, compromised identifies, and malicious insider actions Built-in advanced threat detection using data from Azure Active Directory (Azure AD), Azure Monitor logs, and Azure security Center Azure Governance \u00b6 Azure Policies \u00b6 Service in Azure that you use to create, assign, and manage policies Policies enforce different rules for your resources Policies can be assigned to Management Groups, subscriptions, or Resource Groups and are inherited downward Evaluate your resources for non-compliance with assigned policies Policies can be audited or applied to resources Azure Initiatives \u00b6 A collection of policy definitions that are tailored toward achieving a single overarching goal Simplify managing and assigning policy definitions Initiatives can be assigned to Management Groups, subscriptions, or Resource Groups and are inherited downward Role-Based Access Control (RBAC) \u00b6 An authorization system built on Azure Resource Manager that provides fine-grained access management for azure resources Roles are defined and assigned to a security principal: User, group or service principal (i.e., application account) Role can be scoped to management groups, subscriptions, resources groups, or resources and are inherited downward Resource Locking \u00b6 Prevent other users in your organization from accidentally deleting or modifying critical resources Locks can be set to prevent deletion only, or to prevent modifications and deletion Locks can be assigned to Management Groups, subscriptions, resource groups, or resources and are inherited downward Hierarchy : Management groups -> subscriptions -> resource groups -> resources Security Assistance n Azure Advisor \u00b6 Azure Advisor integrates with Azure security Center to bring you security recommendations in addition to the high availability, performance, and other recommendations already included in Azure advisor Periodically analyzes the security state of your azure resources and makes recommendations for any potential security vulnerabilities Azure Monitor \u00b6 Subscription \u00b6 Billing Entity All Resources within a subscription are billed together Cost Separation You can have multiple subscriptions within a tenant to separate costs Payment If a subscription ins't paid, all the resources and services associated with the subscription stop DevOps \u00b6 Azure Boards Keep track of work tasks, timelines, issues, planning and much more Azure pipelines Produce and test your software automatically and continuously Azure Repos Azure Test Plans Design tests of applications to implement automatically Azure Artifacts Share applications and code libraries with other teams inside and outside your organization Azure DevTest Labs Environment Management Allow developers and engineers to create environment for test and development Cost Management You won't incur unexpected costs and will even minimize wast of resources on your account Templates Tailor your development and test environments and reuse them with template deployments Security \u00b6 Azure Security Center Threat Alerts Ready for Hybrid architectures Each VM has an agent installed that sends data Azure analyzes the data and alerts if necessary Policy and compliance metrics A \"secure score\" to entice great security hygiene Integrate with other cloud providers Alerts for resources that aren't secure Define policies Protect Resources Response Azure Advisor for security Assistance Azure Key Vault Azure Information Protection Advanced Threat Protection Monitor users Baseline Behavior Suggest Changes Privacy, Compliance and Trust \u00b6 Azure Policy \u00b6 Azure Policy ensures that policies applied to resources are compliant A policy is a set of rules to ensure compliant resources RBAC Security Principal An object representing an entity such as user or group, which can access the resource Role Definition A collection of permissions such as read, write and delete Scope The resources the access applies to. Specify which role can access a resource or resource group Locks Assigning Assign a lock to a subscription, resource group or resource Types A lock can be of two types. Delete, where you can't delete the locked object. Read-only, where you can't make any changes to the object Azure Blueprints \u00b6 Templates for creating standard Azure environments Use Blueprints Resource templaes Role Based Access Control (RBAC) Policies Samples for common regulations Azure Monitor \u00b6 Constant Feed Most Azure services feed telemetry data into Azure Monitor Even on-premises services can send telemetry data to Azure Monitor Fully Managed Azure Monitor is centralized and fully managed. You can analyze all the data from one place Query Language Full access to an interactive query language to learn about the telemetry data Machine Learning Predict and recognize problems faster with built-in machine learning Azure Service Health \u00b6 Dashboard Custom Alerts Real-Time tracking Free Service Compliance \u00b6 Industry Regulations General Data Protection Regulation ISO Standard NIST Azure compliance manager Azure knows about compliance and resources, and can give you recommendations through the compliance manager Recommendations Tasks Compliance Score Secure Storage Reports Exam Tips GDPR, ISO and NIST are regulations and standards to ensure compliance with applicable legislation Azure compliance manager provides recommendations, tasks to assign team members, a compliance score, secure document storage and reports Azure government cloud provids dedicated datacenters to US Government bodies. Compliant with US federal, state and local requirements Azure china region contains all data and datacenters within china. Complies with all applicable chinese regulations Privacy \u00b6 Azure Information Protection Classify, label and protect data based on data sensitivity Azure Policy Define and enforce rules to ensure privacy and external regulations Guides Use Guides on azure to respond and comply with GDPR privacy requests Compliance Manager Make sure you are following privacy guidelines Trust \u00b6 TrustCenter Learn about Microsoft's effort on security, privacy, GDPR, data location, compliance about trust in each product and service Service Trust Portal Review all the independent reports and audits performed on Microsoft products and services Azures complies with more standards than any cloud provider Pricing \u00b6 Subscriptions \u00b6 Multiple Subscriptions Any Azure account can have multiple subscriptions Useful for organizing who pays for what Billing Admin One or more users can be a \"billing admin\", which manages anything to do with billing and invoicing on azure Ensures separation of responsibility Billing Cycle A billing Cycle on Azure is either 30 or 60 days Management Groups Group subscriptions Take action across subscriptions in bulk Very useful in large organizations with many subscriptions Organize Manage access, policies and compliance in bulk E.g. have a management group per country or department Billing logic You maintain the billing associated with the right budgets Nest management groups to indicated hierarchy and relationship Cost Management \u00b6 Pricing Calculator \u00b6 Limit Default Limit Some Azure accounts with monthly credits to use will have default spending limits When the credits are used, the limit kicks in No increase When the credits are gone, either remove the limit entirely or leave it in effect No spending limit Pay-as-you-go accounts have no spending limit functionality Quotas Property limit A quota is a limit on a certain property of an azure service For example, a maximum of 100 namespaces for event hub Ensure service level The quotas are necessary to ensure azure can maintain high service level Quota Change If you need to increase the quota for a particular service, you can ask Microsoft to increase them Tags Identity Roles Protect sensitive data by defining which roles can access a resource Filter Filter resources per project, customer or for reporting purposes Related Resources To make bulk processing and updating easier, define which resources are related Unambiguous Create a list for tags used that includes: description, tag name, and potential values Support \u00b6 Plans \u00b6 Basic 24/7 Access Online Self-Help Forums Azure Advisor Service Health Developer Bus. Hours Email Standard 24/7 Email/phone Professional Direct Operations Support - Onboarding Reviews Webinarrs Premier Tech reviews, Reporting, Tech Account Man Training On-demand Support Channels \u00b6 Azure Documentation Forums Social Media","title":"Fundamentals"},{"location":"certifications/azure/fundamentals/#azure-fundamentals","text":"","title":"Azure Fundamentals"},{"location":"certifications/azure/fundamentals/#cloud-concepts","text":"","title":"Cloud Concepts"},{"location":"certifications/azure/fundamentals/#language-of-cloud-computing","text":"High availability Maintaining Acceptable continuous performance despite temporary load fluctuations or failures in services, hardware, or data centers Data Center Redundancies Power Cooling Networking ETC Availability Zone Redundancies 1 or more data centers Region Redundancies Multiple Availability Zones Fault Tolerance A system's ability to continue operation properly when one or more of its component fails Proactive Regularly back up data/apps/resources Deploy to multiple azs or regions Load Balance across multiple availability zones or regions Monitor health or data/apps/resources Reactive Restore data/apps/resources to different availability zones or regions Deploy to different availability zones or regions Disaster Recovery A system's ability to back up and restore data/apps/resources when needed Can use Azure to Restore On-premises to on-premises On-premises to Azure Other cloud to Azure Azure to Azure Scalability The ability to increase the instance count or size of existing resources Scaling Out Increase instance count of existing resources Non-disruptive Scaling Up Increase instance size of existing resources Disruptive Elasticity is the ability to increase or decrease the instance count or size of existing resources based on fluctuations in traffic, load, or resource workload Ability to scale in both directions ( in+out, up + down ) Can be manual or automatic Based on changes in load or workload Pay only for what you use Agility means do the ability to rapidly develop, test and launch software applications Focus on Time to Value Innovation Low Latency Economic Effectiveness Rapid Adaptation Flexibility","title":"Language of Cloud Computing"},{"location":"certifications/azure/fundamentals/#language-of-cloud-economics","text":"Economies of scale Supply-Side Savings Lower costs of land, servers, power, labor, etc Demand-Side Savings Serving more customers means using more server resources ( 80% CPU) Higher server utilization rates mean lower costs Multi-Tenancy Savings More tenants (customers or users) lowers the cost of servers and management per tenant Capital expenditure (CapEx) is buying hardware outright, paid upfront as a one time purchase Up-front investment Creates benefits over long period of time Usually non-recurring Usally related to purchase of permanent assets Operational Expenditure (OpEx) is ongoing costs needed to run your business No upfront investment Creates immediate benefits Recurring cost of doing business Usually not related to purchase of permanent assets Consumption-based pricing let's you pay only for what you use Compute - Pay by CPU/hour Storage - Pay by GB Data Transfer - Pay by GB for data out, not in","title":"Language of Cloud Economics"},{"location":"certifications/azure/fundamentals/#cloud-service-models","text":"IaaS provides servers, storage and networking as a service You Manage O/S Middleware Runtime Data Applications Microsoft Manages Virtualization Servers Storage Networking PaaS is a superset of IaaS and also includes middleware, such as database management tools You Manage Applications Data SaaS is when a service is built on top of PaaS, like Office 365 You Manage Nothing Serverless means that you don't have any servers. Let's a single function be hosted, deployed, run and managed on its own","title":"Cloud Service Models"},{"location":"certifications/azure/fundamentals/#cloud-architect-models","text":"Private Cloud is azure on your own hardware in a location of your choice. All the benefits of public cloud, but you can lock it down. A Lot of staff required Public Cloud Is Azure, AWS, GCP. No upfront costs, but monthly usage. Little control over services and infrastructure Hybrid Cloud model is the best of private and public, but could be complex","title":"Cloud Architect Models"},{"location":"certifications/azure/fundamentals/#core-azure-products-services","text":"","title":"Core Azure Products &amp; Services"},{"location":"certifications/azure/fundamentals/#azure-architecture","text":"Regions Groups of multiple Azure data centers withing a latency-defined perimeter and connected through a dedicated regional low-latency network Deploying your applications/systems to multiple regions allows for resiliency to region-wide outages Multi-Region Failover (Active/Passive) Failover across regions instantly Resiliency to region-wide outages Best when all users are in the same region Multi-Region Deployments (Active/Active) Distribute traffic/load across regions Resiliency to region-wide outages Best when all users are in different regions Availability Zones Unique physical locations within An azure region made up of one or more data centers Allow for high availability by protecting your applications and data from data center failures Some Azure services can be deployed to two or more AZs within an Azure region Resource Groups - Containers of related Azure services grouped together All the resources in the group should share the same lifecycle - They should be deployed, updated, and deleted together A resource can only exist in one resource group A resource group can contain resources that are located in different regions A resource group can be used to scope access control for administrative A resource can interact with resources in other resource groups (When two resources are related but don't share the same lifecycle; for example web apps connection to a database) Azure Resource Manager The Azure deployment and management service Provides a consistent management layer that enables you to create, update, and delete resources in your Azure subscription, including wihth templates You can use its access control, auditing, and tagging features to secure and organize your resources after deployment","title":"Azure Architecture"},{"location":"certifications/azure/fundamentals/#azure-products-and-services","text":"","title":"Azure Products and Services"},{"location":"certifications/azure/fundamentals/#compute","text":"Virtual Machines Pricing - Calculated Hourly A Virtual Machine is your machine exclusively You don't buy, own or control any hardware. Azure does this VMs are an IaaS offering, where you are responsible for the entire machine Azure virtual machines take advantage of azure tools Pricing goes up as resources go up, and you pay by the hour Scale Sets Auto-scaling pool of virtual machines Centrally manage, configure and update a large number of vms Scale sets are identical VMs. They can be activated or deactivated as needed A baseline VM for the scale set ensures application stability. A baseline VM is what you copy to make up the scale set VMS As resources usage increases, more VMs are activated to take the load You only pay for the VM, storage and networking resources you use. Nothing additional for scale sets App Services On-Demand, auto-scaling web/mobile/api app hosting Linux & Windows platforms App services are a PaaS offering on Azure Web Apps are used to ost web sites and web applications Web Apps for containers can host your existing container images API Apps can host your data backend services Serverless functions Event-driven serverless compute resources Auto-scale on demand Pay only for what you consume Kubernetes Kubernetes = AKS EMR = ACR","title":"Compute"},{"location":"certifications/azure/fundamentals/#networking","text":"VNet An address space is a range of IP addresses you can use for your resources A subnet is smaller network, which is part of your VNET. Use these for security and logical division of resources A VNet is in a single region and single subscription VNets in the cloud can scale, have high availability and isolation VPN Gateway A VPN Gateway is a specific VNet Gateway, It consists of two or more dedicated VMNets VNET Gateway + \"vpn\" becomes a VPN Connection Send encrypted data between Azure and on premises network Azure Gateway Subnet, secure tunnel and on-premises gateway makes up a VPN Gateway Scenario Azure Load Balancer AWS ELB Similiar Azure APP Gateway AWS ALB Similiar It works on the HTTP request of the traffic, instead of the IP address and port Traffic from a specific web address can go to a specific machine Is a fit for most other Azure Services Supports auto-scaling, end-to-end encryption, zone redundancy ans multi-site hosting Azure content Delivery Network Efficently dlivers web content from locations that are close to end users to minimize latency","title":"Networking"},{"location":"certifications/azure/fundamentals/#storage","text":"BLOB Highly scalable, REST based object store Unstructure data Types Block Store text and binary data up to 4.7TB. Made up of individually managed blocks of data Append Block blobs that are optimized for append operations Works well for logging where data is constantly appended Page Store files up to 8TB Any part of the file could be accessed at any time, for example a virtual hard drive Pricing Hot Frequently accessed files. Lower access times and higher access costs Cool Lower storage costs and higher access times. Data remains here for at least 30 days Archive Lowest costs and highest access times Tables Massive auto-scaling NoSQl Store Dynamic scaling based on load Scale to PBs of table Data Fast Key/value lookups Queues Reliable queues at scale for cloud services Decouple and scale components Disk - Types HDD Spinning hard drive. Low cost and suitable for backups Standard SSD Standard for production. Higher reliability, scalability and lower latency over HDD Premium SSD Super fast and high performance. Very low latency. Use for critical workloads Ultra Disk For the most demanding, data-intensive workloads. Disks up to 64 TB Files AWS EFS Fully managed File Shares in the cloud SMB and Rest Access Lift and shift legacy apps","title":"Storage"},{"location":"certifications/azure/fundamentals/#databases","text":"Cosmos DB Globally distributed database. It is super fast and easy to manage Scale to infinite performance and size Can be Costly Supports multiple APIs, including SQl, MongoDB, Cassandra, Tables & Gremlin Supports analytics through Apache Spark Azure SQL IaaS SQL Server on Azure Virtual Machines Pay-as you-fo for SQL server license or use an existing license PaaS Fully managed SQL database engine Available as single database, elastic pool, and managed instances Azure Database for MySQL Azure Database for PostGreSQL Horizontal scaling Azure SQL Data Warehouse Cloud based Enterprise Data Warehouse (EDW) Leverages massively parallel processing (MPP) to quicly run complex queries across petabytes of data Key component of a Big Data solution Database Migration Services Enables seamless migrations from multiple database sources to azure data platforms with minimal downtime Offline migration: Application downtime starts when the migration starts Online migration: Downtime is limited to the time to cut over the end of the migration","title":"Databases"},{"location":"certifications/azure/fundamentals/#azure-marketplace","text":"An online applications and services marketplace Offers technical solutions and services from Microsoft and parteners that are designed to extend azure products and services","title":"Azure Marketplace"},{"location":"certifications/azure/fundamentals/#azure-solutions","text":"","title":"Azure Solutions"},{"location":"certifications/azure/fundamentals/#iot","text":"An IoT solution is made up of one or more IoT devices and one or more back-end services running in the cloud that communicate with each other Devices usually have sensors and connect to the internet IoT Hub PaaS Solution that provides more control over the IoT data collection and processing Enables reliable and secure bidirectional communications between milions of IoT devices and cloud solution Managed and Secure Ease of Deployment Platform-as-a-service Scaling and Authentication IoT Central SaaS solution that provides pre-made IoT connections and dashboards to get set up quickly Software-as-a-service Simplify and speed up the implementation of your IoT solution No Coding Needed You don't have to know how to write code to deploy your IoT project Receive feeds from devices and focus on metrics and business value Pre-made Connectors Use any of the hundreds of connectors that are ready to use in IoT Central","title":"IoT"},{"location":"certifications/azure/fundamentals/#big-data","text":"Data Lake Analytics Large Amounts of Data A data lake is a very large body of data Parallel Processing Two or more processes or computers processing the same data at the same time Ready to Go Servers, processes and any other needed services are ready to go from the start Jump straight into the data analytics HD Insights Similar to Azure Data Lake Analytics Open Source, which is free and community supported Includes Apache Hadoop, spark and kafka Azure Databricks Based on Apache Spark, a distributed cluster-computing framework Run and process a dataset on many computers simultaneously Databricks provides all the computing power Integrates with other Azure Storage Services Azure SQL Data Warehouse Cloud-based enterprise warehouse (EDW) Leverages massively parallel processing (MPP) to quickly run complext queries across petabytes of data","title":"Big Data"},{"location":"certifications/azure/fundamentals/#artificial-intelligence","text":"Machine Learning/AI Models The definition of what your machine learning application is learning A model is a set of rules of how to use the data provided The model finds patterns based on the rules Knowledge Mining Use Azure search to finding existing insights in your data File relationships, geography connections and more Built-in Apps Azure has a number of built-in apps that you can use for machine learning and AI straight away. These include cognitive services and bot services Azure Cognitive Services Vision You can use the vision service to recognize, identify and caption your videos and images. Automatically Decision Apps can make decisions based on content. Detect potential offensive language, detect IoT anomalies and leverage data analytics Speech Automatic speech-to-text transcription. Speaker identification and verification Azure Machine Learning Studio Supports all Azure Machine Learning tools Pre-made modules for your project Use for real world scenarios such as twitter sentiment analysis, photo grouping and movie recommendations Use this if you want to experiment with machine learning models quickly and easily, and the built-in machine learning algorithms are sufficient for your solutions Uses prebuilt and preconfigured machine learning algorithms and data handling modules Collaborative, drag-and-drop visual workspace where you can build, test, and deploy machine learning solutions without the need to write code Machine Learning Service End-to-End Service The service to use AI and machine learning almost anywhere on Azure Tooling The Machine Learning service is a collection of tools to help you build AI applications Automation Azure automatically recognizes trends in your applications and creates models for you Use this if you work in a python environment, you want more control over your machine learning algorithms, or you want use open-source machine learning libraries Fully supports open-source technologies: tens of thousands of open-source machine learning components are available Provides a cloud-based environment tou can use to prep data and train, test, deploy, manage, and track machine learning models","title":"Artificial Intelligence"},{"location":"certifications/azure/fundamentals/#serverless","text":"Azure Functions Configure your code to load and run only when triggered Triggers can be HTTP requests, data from other Azure Services, etc Can Return data or call other azure services Logic Apps Are a quick and simple way to integrate systems and applications inside and outside of azure Visual designer for automating and orchestration tasks processes and workflows Great for integrating apps/data/systems and for business-to-business(B2B) integrations Connect Systems Connect systems bot inside and outside of the Azure platform Integrate not only apps, but also data flows, services and entire systems Automation A lot of ways to schedule, automate and orchestrate tasks and processes Quick Start No coding required to get started straight away. You just need access to your apps to integrate Event Grid Fully managed event routing service Greatly simplifies the development of event-based applications and serverless workflows Handles all event routing from any source, to any destination for any application","title":"Serverless"},{"location":"certifications/azure/fundamentals/#azure-management-tools","text":"","title":"Azure Management Tools"},{"location":"certifications/azure/fundamentals/#azure-cli-powershell-azure-portal","text":"Azure Cli Command line experience for managing Azure resources You can use it in your browser with Azure cloud shell (interactive browser-accessible shell that runs basb or powershell) You can install it on macOs, Linux, and Windows and run it from the command line Power Shell Set of PowerShell cmdlets for managing Azure resources You can use it in your cloud shell Uses .NET standard making it available for windows, macOS, and Linux Azure Portal Build, manage and monitor every Azure resource in a single unified console Browse active resources modify settings, provision new resources, and view basic monitoring data from all your azure products and services","title":"Azure cli, PowerShell, Azure Portal"},{"location":"certifications/azure/fundamentals/#azure-advisor","text":"Personalized cloud consultant that helps you follow best practices to optimize your azure deployments Analyzes your resource configuration and usage telemetry Recommends solutions to help improve the cost effectiveness, performance, security, and availability your resources","title":"Azure Advisor"},{"location":"certifications/azure/fundamentals/#security-privacy-compliance-and-trust","text":"","title":"Security, Privacy, Compliance, and Trust"},{"location":"certifications/azure/fundamentals/#network-security","text":"","title":"Network Security"},{"location":"certifications/azure/fundamentals/#azure-firewall","text":"Managed, cloud-based network security service that protects your azure virtual Network resources Centrally create, enforce, and log application and network connectivity policies across subscriptions and virtual networks Fully integrated with azure monitor for logging and analytics","title":"Azure Firewall"},{"location":"certifications/azure/fundamentals/#network-security-groups-nsgs","text":"Enforce and control network traffic rules at the networking level Security rules allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources For each rule, you can specify a source, destination, port, and protocol","title":"Network Security Groups (NSGs)"},{"location":"certifications/azure/fundamentals/#azure-ddos-protection","text":"","title":"Azure DDoS Protection"},{"location":"certifications/azure/fundamentals/#choosing-an-azure-security-solution","text":"Isolation Use different virtual networks, or virtual network subnets, to segment your network or isolate resources when required Principle of least privilege Limit each user's access rights to the bare minimum permissions they need to perform their work Use NSGs for Bare Minimum Network Connectivity Use network security groups (NSGs) to enforce rules allowing only the minimum network access needed Use Azure Firewall and Azure DDoS Protection Use Azure Firewall and Azure DDoS protection to protect your resources and prevent large unanticipated costs","title":"Choosing an Azure Security Solution"},{"location":"certifications/azure/fundamentals/#azure-identity-services","text":"Authentication and Authorization Azure Multi-Factor Authentication (MFA) Something you know ( Typicall password) Something you have ( a trusted device that is not easily duplicated, like a phone) Something you are ( biometrics ) Azure Active Directory (AAD) Comprehensive, highly available identity and access management (IAM) cloud solution Fully managed identity service - requires no hardware, virtual machines, etc Main objects are users (pay per user) and groups(free) Users are assigned a license based on which edition of Azure AD they should have Groups can be assigned multiple licenses, which are inherited by all users in the group Active Directory (AD) is not the same as Azure Active Directory Different skillset from AD to Azure AD Every Azure account will have and azure AD service Organization A tenant represents the organization Dedicated AAD A tenant is a dedicated of AAD that an organization receives when signing up for Azure Separate Each tenant is distinct and completely separate from other AAD tenants One User - One Tenant Each user in Azure only belong to a single tenant Users can be guests of other tenants Azure AD can help manage users in a hybrid cloud setup","title":"Azure Identity Services"},{"location":"certifications/azure/fundamentals/#azure-security-tools-and-features","text":"Security is built into every layer of azure Secure Foundation State-of-the-art security in Azure data centers around the world, integrated security controls in Azure hardware and firmware components, and protection against threats such as DDoS Simplify security with Built-In Services Easily manage identify and control access, secure your network, safeguard data, manage secrets and certificates, prevent attacks, and gain centralized visibility Detect threats early with unique intelligence identify new threats and respond quickly with services that are informed by real-time global cybersecurity intelligence","title":"Azure Security tools and features"},{"location":"certifications/azure/fundamentals/#azure-security-center","text":"Protects Azure and non-Azure servers and virtual machines - including windows and linux servers - when the microsoft monitoring agent is installed Events collected from azure and the agents are sent to the security analytics engine, which provides threat detection alerts and tailored recommendations for securing workloads Azure PaaS services - including SQL databases and storage accounts - are automatically monitored and protected by Azure Security Center","title":"Azure Security Center"},{"location":"certifications/azure/fundamentals/#azure-key-vault","text":"Encrypt and safeguard authentication keys, storage account keys, data encryption keys, certificates, and passwords Data is protected by hardware security modules (HSMs) Control access to keys using Azure Active Directory Grant access to individual users or to applications and systems","title":"Azure Key Vault"},{"location":"certifications/azure/fundamentals/#azure-information-protection-aip","text":"Cloud-based solution that helps organizations classify and protect documents and emails Manually or automatically classify sensitive data using 80+ built-in data types (credit card numbers, ID/SSN numbers) Enforce policies on classified daa - require credit card numbers to be encrypted Can be retroactively applied to existing documents and emails","title":"Azure information Protection (AIP)"},{"location":"certifications/azure/fundamentals/#azure-advanced-threat-protection-atp","text":"Cloud-based security solution that identifies, detects, and helps you investigate advanced threats, compromised identifies, and malicious insider actions Built-in advanced threat detection using data from Azure Active Directory (Azure AD), Azure Monitor logs, and Azure security Center","title":"Azure Advanced Threat Protection (ATP)"},{"location":"certifications/azure/fundamentals/#azure-governance","text":"","title":"Azure Governance"},{"location":"certifications/azure/fundamentals/#azure-policies","text":"Service in Azure that you use to create, assign, and manage policies Policies enforce different rules for your resources Policies can be assigned to Management Groups, subscriptions, or Resource Groups and are inherited downward Evaluate your resources for non-compliance with assigned policies Policies can be audited or applied to resources","title":"Azure Policies"},{"location":"certifications/azure/fundamentals/#azure-initiatives","text":"A collection of policy definitions that are tailored toward achieving a single overarching goal Simplify managing and assigning policy definitions Initiatives can be assigned to Management Groups, subscriptions, or Resource Groups and are inherited downward","title":"Azure Initiatives"},{"location":"certifications/azure/fundamentals/#role-based-access-control-rbac","text":"An authorization system built on Azure Resource Manager that provides fine-grained access management for azure resources Roles are defined and assigned to a security principal: User, group or service principal (i.e., application account) Role can be scoped to management groups, subscriptions, resources groups, or resources and are inherited downward","title":"Role-Based Access Control (RBAC)"},{"location":"certifications/azure/fundamentals/#resource-locking","text":"Prevent other users in your organization from accidentally deleting or modifying critical resources Locks can be set to prevent deletion only, or to prevent modifications and deletion Locks can be assigned to Management Groups, subscriptions, resource groups, or resources and are inherited downward Hierarchy : Management groups -> subscriptions -> resource groups -> resources","title":"Resource Locking"},{"location":"certifications/azure/fundamentals/#security-assistance-n-azure-advisor","text":"Azure Advisor integrates with Azure security Center to bring you security recommendations in addition to the high availability, performance, and other recommendations already included in Azure advisor Periodically analyzes the security state of your azure resources and makes recommendations for any potential security vulnerabilities","title":"Security Assistance n Azure Advisor"},{"location":"certifications/azure/fundamentals/#azure-monitor","text":"","title":"Azure Monitor"},{"location":"certifications/azure/fundamentals/#subscription","text":"Billing Entity All Resources within a subscription are billed together Cost Separation You can have multiple subscriptions within a tenant to separate costs Payment If a subscription ins't paid, all the resources and services associated with the subscription stop","title":"Subscription"},{"location":"certifications/azure/fundamentals/#devops","text":"Azure Boards Keep track of work tasks, timelines, issues, planning and much more Azure pipelines Produce and test your software automatically and continuously Azure Repos Azure Test Plans Design tests of applications to implement automatically Azure Artifacts Share applications and code libraries with other teams inside and outside your organization Azure DevTest Labs Environment Management Allow developers and engineers to create environment for test and development Cost Management You won't incur unexpected costs and will even minimize wast of resources on your account Templates Tailor your development and test environments and reuse them with template deployments","title":"DevOps"},{"location":"certifications/azure/fundamentals/#security","text":"Azure Security Center Threat Alerts Ready for Hybrid architectures Each VM has an agent installed that sends data Azure analyzes the data and alerts if necessary Policy and compliance metrics A \"secure score\" to entice great security hygiene Integrate with other cloud providers Alerts for resources that aren't secure Define policies Protect Resources Response Azure Advisor for security Assistance Azure Key Vault Azure Information Protection Advanced Threat Protection Monitor users Baseline Behavior Suggest Changes","title":"Security"},{"location":"certifications/azure/fundamentals/#privacy-compliance-and-trust","text":"","title":"Privacy, Compliance and Trust"},{"location":"certifications/azure/fundamentals/#azure-policy","text":"Azure Policy ensures that policies applied to resources are compliant A policy is a set of rules to ensure compliant resources RBAC Security Principal An object representing an entity such as user or group, which can access the resource Role Definition A collection of permissions such as read, write and delete Scope The resources the access applies to. Specify which role can access a resource or resource group Locks Assigning Assign a lock to a subscription, resource group or resource Types A lock can be of two types. Delete, where you can't delete the locked object. Read-only, where you can't make any changes to the object","title":"Azure Policy"},{"location":"certifications/azure/fundamentals/#azure-blueprints","text":"Templates for creating standard Azure environments Use Blueprints Resource templaes Role Based Access Control (RBAC) Policies Samples for common regulations","title":"Azure Blueprints"},{"location":"certifications/azure/fundamentals/#azure-monitor_1","text":"Constant Feed Most Azure services feed telemetry data into Azure Monitor Even on-premises services can send telemetry data to Azure Monitor Fully Managed Azure Monitor is centralized and fully managed. You can analyze all the data from one place Query Language Full access to an interactive query language to learn about the telemetry data Machine Learning Predict and recognize problems faster with built-in machine learning","title":"Azure Monitor"},{"location":"certifications/azure/fundamentals/#azure-service-health","text":"Dashboard Custom Alerts Real-Time tracking Free Service","title":"Azure Service Health"},{"location":"certifications/azure/fundamentals/#compliance","text":"Industry Regulations General Data Protection Regulation ISO Standard NIST Azure compliance manager Azure knows about compliance and resources, and can give you recommendations through the compliance manager Recommendations Tasks Compliance Score Secure Storage Reports Exam Tips GDPR, ISO and NIST are regulations and standards to ensure compliance with applicable legislation Azure compliance manager provides recommendations, tasks to assign team members, a compliance score, secure document storage and reports Azure government cloud provids dedicated datacenters to US Government bodies. Compliant with US federal, state and local requirements Azure china region contains all data and datacenters within china. Complies with all applicable chinese regulations","title":"Compliance"},{"location":"certifications/azure/fundamentals/#privacy","text":"Azure Information Protection Classify, label and protect data based on data sensitivity Azure Policy Define and enforce rules to ensure privacy and external regulations Guides Use Guides on azure to respond and comply with GDPR privacy requests Compliance Manager Make sure you are following privacy guidelines","title":"Privacy"},{"location":"certifications/azure/fundamentals/#trust","text":"TrustCenter Learn about Microsoft's effort on security, privacy, GDPR, data location, compliance about trust in each product and service Service Trust Portal Review all the independent reports and audits performed on Microsoft products and services Azures complies with more standards than any cloud provider","title":"Trust"},{"location":"certifications/azure/fundamentals/#pricing","text":"","title":"Pricing"},{"location":"certifications/azure/fundamentals/#subscriptions","text":"Multiple Subscriptions Any Azure account can have multiple subscriptions Useful for organizing who pays for what Billing Admin One or more users can be a \"billing admin\", which manages anything to do with billing and invoicing on azure Ensures separation of responsibility Billing Cycle A billing Cycle on Azure is either 30 or 60 days Management Groups Group subscriptions Take action across subscriptions in bulk Very useful in large organizations with many subscriptions Organize Manage access, policies and compliance in bulk E.g. have a management group per country or department Billing logic You maintain the billing associated with the right budgets Nest management groups to indicated hierarchy and relationship","title":"Subscriptions"},{"location":"certifications/azure/fundamentals/#cost-management","text":"","title":"Cost Management"},{"location":"certifications/azure/fundamentals/#pricing-calculator","text":"Limit Default Limit Some Azure accounts with monthly credits to use will have default spending limits When the credits are used, the limit kicks in No increase When the credits are gone, either remove the limit entirely or leave it in effect No spending limit Pay-as-you-go accounts have no spending limit functionality Quotas Property limit A quota is a limit on a certain property of an azure service For example, a maximum of 100 namespaces for event hub Ensure service level The quotas are necessary to ensure azure can maintain high service level Quota Change If you need to increase the quota for a particular service, you can ask Microsoft to increase them Tags Identity Roles Protect sensitive data by defining which roles can access a resource Filter Filter resources per project, customer or for reporting purposes Related Resources To make bulk processing and updating easier, define which resources are related Unambiguous Create a list for tags used that includes: description, tag name, and potential values","title":"Pricing Calculator"},{"location":"certifications/azure/fundamentals/#support","text":"","title":"Support"},{"location":"certifications/azure/fundamentals/#plans","text":"Basic 24/7 Access Online Self-Help Forums Azure Advisor Service Health Developer Bus. Hours Email Standard 24/7 Email/phone Professional Direct Operations Support - Onboarding Reviews Webinarrs Premier Tech reviews, Reporting, Tech Account Man Training On-demand","title":"Plans"},{"location":"certifications/azure/fundamentals/#support-channels","text":"Azure Documentation Forums Social Media","title":"Support Channels"},{"location":"certifications/google/associate-cloud-engineer/","text":"Associate Cloud Engineer \u00b6 Billing \u00b6 Export must be set up per billing account Resources should be placed into appropriate projects Resources should be tagged with labels Billing export is not real-time Delay is Hours Cloud Storage \u00b6 Names are globally unique Location Type: Region - Lowest latency within a single region Multi-Region - Highest availability across largest area Dual-Region - High availability and low latency across 2 regions Storage Class Standard - Best for short-ter storage and frequently accessed data Nearline - Best for backups and data accessed less than once a month Coldline - Best for disaster recovery and data accessed les once a quarter Archive - Best for long-term digital preservation of data accessed less than once a year Access Fine-Grained - Access per object (ACLS) in addition to bucket level permissions Uniform - Permissions only using Bucket-Level permissions (IAM) Encryption Google managed Key Customer managed key Retention Policy Minimum duration that this buckets objects must be protected from deletion or modification after they're uploaded.","title":"Associate-Cloud-Engineer"},{"location":"certifications/google/associate-cloud-engineer/#associate-cloud-engineer","text":"","title":"Associate Cloud Engineer"},{"location":"certifications/google/associate-cloud-engineer/#billing","text":"Export must be set up per billing account Resources should be placed into appropriate projects Resources should be tagged with labels Billing export is not real-time Delay is Hours","title":"Billing"},{"location":"certifications/google/associate-cloud-engineer/#cloud-storage","text":"Names are globally unique Location Type: Region - Lowest latency within a single region Multi-Region - Highest availability across largest area Dual-Region - High availability and low latency across 2 regions Storage Class Standard - Best for short-ter storage and frequently accessed data Nearline - Best for backups and data accessed less than once a month Coldline - Best for disaster recovery and data accessed les once a quarter Archive - Best for long-term digital preservation of data accessed less than once a year Access Fine-Grained - Access per object (ACLS) in addition to bucket level permissions Uniform - Permissions only using Bucket-Level permissions (IAM) Encryption Google managed Key Customer managed key Retention Policy Minimum duration that this buckets objects must be protected from deletion or modification after they're uploaded.","title":"Cloud Storage"},{"location":"certifications/kubernetes/ckad/","text":"Exam tips \u00b6 source < ( kubectl completion bash ) echo \"source <(kubectl completion bash)\" >> ~/.bashrc export ns = default alias k = 'kubectl -n $ns' # This helps when namespace in question doesn't have a friendly name # n\u00e3o funciona alias kdr = 'kubectl -n $ns -o yaml --dry-run' . # run commands in dry run mode and generate yaml. export KUBE_EDITOR = \"nano\" $ k run nginx --image = nginx --restart = Never --dry-run -o yaml > mypod.yaml $ k run nginx --image = nginx ( deployment ) $ k run nginx --image = nginx --restart = Never ( pod ) $ k run busybox --image = busybox --restart = OnFailure ( job ) $ k run busybox --image = busybox --schedule = \"* * * * *\" --restart = OnFailure ( cronJob ) $ k explain pod.spec.containers $ k explain deployment.spec.template.metadata Useful Links \u00b6 Exercises CKAD-Home Kubernetes Docs","title":"CKAD"},{"location":"certifications/kubernetes/ckad/#exam-tips","text":"source < ( kubectl completion bash ) echo \"source <(kubectl completion bash)\" >> ~/.bashrc export ns = default alias k = 'kubectl -n $ns' # This helps when namespace in question doesn't have a friendly name # n\u00e3o funciona alias kdr = 'kubectl -n $ns -o yaml --dry-run' . # run commands in dry run mode and generate yaml. export KUBE_EDITOR = \"nano\" $ k run nginx --image = nginx --restart = Never --dry-run -o yaml > mypod.yaml $ k run nginx --image = nginx ( deployment ) $ k run nginx --image = nginx --restart = Never ( pod ) $ k run busybox --image = busybox --restart = OnFailure ( job ) $ k run busybox --image = busybox --schedule = \"* * * * *\" --restart = OnFailure ( cronJob ) $ k explain pod.spec.containers $ k explain deployment.spec.template.metadata","title":"Exam tips"},{"location":"certifications/kubernetes/ckad/#useful-links","text":"Exercises CKAD-Home Kubernetes Docs","title":"Useful Links"},{"location":"databases/elasticsearch/beat/","text":"ES Beats \u00b6 Create \u00b6 Creating New Beat Examples Know Errors \u00b6 bash: mage: command not found Use vendoring \u00b6 We recommend to use vendoring for your beat. This means the dependencies are put into your beat folder. The beats team currently uses govendor for vendoring. govendor init govendor update +e This will create a directory vendor inside your repository. To make sure all dependencies for the Makefile commands are loaded from the vendor directory, find the following line in your Makefile: ES_BEATS = ${ GOPATH } /src/github.com/elastic/beats Replace it with: ES_BEATS = ./vendor/github.com/elastic/beats To Fetch: govendor fetch github.com/vmware/govmomi/^ +out","title":"Beat"},{"location":"databases/elasticsearch/beat/#es-beats","text":"","title":"ES Beats"},{"location":"databases/elasticsearch/beat/#create","text":"Creating New Beat Examples","title":"Create"},{"location":"databases/elasticsearch/beat/#know-errors","text":"bash: mage: command not found","title":"Know Errors"},{"location":"databases/elasticsearch/beat/#use-vendoring","text":"We recommend to use vendoring for your beat. This means the dependencies are put into your beat folder. The beats team currently uses govendor for vendoring. govendor init govendor update +e This will create a directory vendor inside your repository. To make sure all dependencies for the Makefile commands are loaded from the vendor directory, find the following line in your Makefile: ES_BEATS = ${ GOPATH } /src/github.com/elastic/beats Replace it with: ES_BEATS = ./vendor/github.com/elastic/beats To Fetch: govendor fetch github.com/vmware/govmomi/^ +out","title":"Use vendoring"},{"location":"databases/elasticsearch/cat/","text":"Cat \u00b6 Indices \u00b6 List GET /_cat/indices?v With Selected columns GET /_cat/indices?h=creation.date.string Help (Get Indices Columns) GET /_cat/indices?help","title":"Cat"},{"location":"databases/elasticsearch/cat/#cat","text":"","title":"Cat"},{"location":"databases/elasticsearch/cat/#indices","text":"List GET /_cat/indices?v With Selected columns GET /_cat/indices?h=creation.date.string Help (Get Indices Columns) GET /_cat/indices?help","title":"Indices"},{"location":"databases/elasticsearch/cluster/","text":"Cluster \u00b6 Health \u00b6 GET /_cluster/health Allocation Errors Explain \u00b6 GET /_cluster/allocation/explain Know Errors \u00b6 Max virtual memory (vm.max_map_count) \u00b6 max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] sudo sysctl -w vm.max_map_count = 262144","title":"Cluster"},{"location":"databases/elasticsearch/cluster/#cluster","text":"","title":"Cluster"},{"location":"databases/elasticsearch/cluster/#health","text":"GET /_cluster/health","title":"Health"},{"location":"databases/elasticsearch/cluster/#allocation-errors-explain","text":"GET /_cluster/allocation/explain","title":"Allocation Errors Explain"},{"location":"databases/elasticsearch/cluster/#know-errors","text":"","title":"Know Errors"},{"location":"databases/elasticsearch/cluster/#max-virtual-memory-vmmax_map_count","text":"max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] sudo sysctl -w vm.max_map_count = 262144","title":"Max virtual memory (vm.max_map_count)"},{"location":"databases/elasticsearch/indices/","text":"Indices \u00b6 Open \u00b6 POST / { index } /_open Close \u00b6 POST /_all/_close Know Errors \u00b6 Forbidden Index x-read-only-allow-delete-api \u00b6 Forbidden Issue PUT .kibana/_settings { \"index\" : { \"blocks\" : { \"read_only_allow_delete\" : \"false\" } } }","title":"Indices"},{"location":"databases/elasticsearch/indices/#indices","text":"","title":"Indices"},{"location":"databases/elasticsearch/indices/#open","text":"POST / { index } /_open","title":"Open"},{"location":"databases/elasticsearch/indices/#close","text":"POST /_all/_close","title":"Close"},{"location":"databases/elasticsearch/indices/#know-errors","text":"","title":"Know Errors"},{"location":"databases/elasticsearch/indices/#forbidden-index-x-read-only-allow-delete-api","text":"Forbidden Issue PUT .kibana/_settings { \"index\" : { \"blocks\" : { \"read_only_allow_delete\" : \"false\" } } }","title":"Forbidden Index x-read-only-allow-delete-api"},{"location":"databases/elasticsearch/kibana/","text":"kibana \u00b6 Time Series \u00b6 Filters performancemanager.virtualmachines.metric.info.metric: \"cpu.usagemhz.average\" AND NOT performancemanager.virtualmachines.metric.sample.instance: \"*\" AND performancemanager.hosts.metric.sample.instance: \"*\" Timelion \u00b6 Expression .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metric.sample.value,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000 ) .label ( \"Disk Provisioned [TB]\" ) .color ( black ) .lines ( fill = 1 ,width = 2 ) .title ( \"Capacity Assessment\" ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000000 ) .label ( \"Total Capacity [TB]\" ) .color ( yellow ) .lines ( fill = 2 ,width = 2 ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metric.sample.value,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.used.latest\" ) .divide ( 1000000000 ) .label ( \"Disk Used [TB]\" ) .color ( green ) .lines ( fill = 3 ,width = 1 ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000000 ) .multiply ( 1 .1 ) .label ( \"Provisioning Threshold [TB]\" ) .color ( red ) .lines ( fill = 0 ,width = 3 ) ,","title":"Kibana"},{"location":"databases/elasticsearch/kibana/#kibana","text":"","title":"kibana"},{"location":"databases/elasticsearch/kibana/#time-series","text":"Filters performancemanager.virtualmachines.metric.info.metric: \"cpu.usagemhz.average\" AND NOT performancemanager.virtualmachines.metric.sample.instance: \"*\" AND performancemanager.hosts.metric.sample.instance: \"*\"","title":"Time Series"},{"location":"databases/elasticsearch/kibana/#timelion","text":"Expression .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metric.sample.value,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000 ) .label ( \"Disk Provisioned [TB]\" ) .color ( black ) .lines ( fill = 1 ,width = 2 ) .title ( \"Capacity Assessment\" ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000000 ) .label ( \"Total Capacity [TB]\" ) .color ( yellow ) .lines ( fill = 2 ,width = 2 ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metric.sample.value,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.used.latest\" ) .divide ( 1000000000 ) .label ( \"Disk Used [TB]\" ) .color ( green ) .lines ( fill = 3 ,width = 1 ) , .es ( index = vspherebeat-emp-imopolis-*, timefield = performancemanager.datastoresclusters.metric.sample.timestamp, metric = sum:performancemanager.datastoresclusters.metaData.Storage.Capacity,q = \"performancemanager.datastoresclusters.metric.info.metric: disk.provisioned.latest\" ) .divide ( 1000000000000 ) .multiply ( 1 .1 ) .label ( \"Provisioning Threshold [TB]\" ) .color ( red ) .lines ( fill = 0 ,width = 3 ) ,","title":"Timelion"},{"location":"databases/elasticsearch/nodes/","text":"Nodes \u00b6 Stats \u00b6 GET /_nodes/stats","title":"Nodes"},{"location":"databases/elasticsearch/nodes/#nodes","text":"","title":"Nodes"},{"location":"databases/elasticsearch/nodes/#stats","text":"GET /_nodes/stats","title":"Stats"},{"location":"databases/elasticsearch/search/","text":"Search \u00b6 All \u00b6 GET /vspherebeat/_search { \"query\" : { \"match_all\" : {} } } Field Match string \u00b6 GET /vspherebeat/_search { \"query\" : { \"match\" : { \"performancemanager.hosts.metaData.name\" : \"nsvwsdv001.mngt.local\" } } } Field doesn't match string \u00b6 GET /vspherebeat/_search { \"query\" : { \"bool\" : { \"must_not\" : [ { \"match\" : { \"performancemanager.virtualmachines.metaData.name\" : \"WSTPMNGT007\" } } ] } } } Bool Query (Match and Not Match) \u00b6 GET /vspherebeat/_search { \"query\" : { \"bool\" : { \"must\" : [ { \"match\" : { \"performancemanager.virtualmachines.metaData.name\" : \"APM-Server\" } }, { \"match\" : { \"performancemanager.virtualmachines.metric.info.metric\" : \"cpu.usage.average\" } } ], \"must_not\" : [ { \"match\" : { \"performancemanager.virtualmachines.metric.sample.instance\" : \"*\" } } ] } } } Exists Specified Field \u00b6 GET /vspherebeat/_search { \"query\" : { \"exists\" : { \"field\" : \"performancemanager.hosts\" } } } Unique Values from a field \u00b6 GET vspherebeat/_search { \"size\" : \"0\" , \"aggs\" : { \"uniq_hotsr\" : { \"terms\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } } Total of unique values from a field \u00b6 GET /vspherebeat/_search { \"size\" : 0 , \"aggs\" : { \"distinct_hots\" : { \"cardinality\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Search"},{"location":"databases/elasticsearch/search/#search","text":"","title":"Search"},{"location":"databases/elasticsearch/search/#all","text":"GET /vspherebeat/_search { \"query\" : { \"match_all\" : {} } }","title":"All"},{"location":"databases/elasticsearch/search/#field-match-string","text":"GET /vspherebeat/_search { \"query\" : { \"match\" : { \"performancemanager.hosts.metaData.name\" : \"nsvwsdv001.mngt.local\" } } }","title":"Field Match string"},{"location":"databases/elasticsearch/search/#field-doesnt-match-string","text":"GET /vspherebeat/_search { \"query\" : { \"bool\" : { \"must_not\" : [ { \"match\" : { \"performancemanager.virtualmachines.metaData.name\" : \"WSTPMNGT007\" } } ] } } }","title":"Field doesn't match string"},{"location":"databases/elasticsearch/search/#bool-query-match-and-not-match","text":"GET /vspherebeat/_search { \"query\" : { \"bool\" : { \"must\" : [ { \"match\" : { \"performancemanager.virtualmachines.metaData.name\" : \"APM-Server\" } }, { \"match\" : { \"performancemanager.virtualmachines.metric.info.metric\" : \"cpu.usage.average\" } } ], \"must_not\" : [ { \"match\" : { \"performancemanager.virtualmachines.metric.sample.instance\" : \"*\" } } ] } } }","title":"Bool Query (Match and Not Match)"},{"location":"databases/elasticsearch/search/#exists-specified-field","text":"GET /vspherebeat/_search { \"query\" : { \"exists\" : { \"field\" : \"performancemanager.hosts\" } } }","title":"Exists Specified Field"},{"location":"databases/elasticsearch/search/#unique-values-from-a-field","text":"GET vspherebeat/_search { \"size\" : \"0\" , \"aggs\" : { \"uniq_hotsr\" : { \"terms\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Unique Values from a field"},{"location":"databases/elasticsearch/search/#total-of-unique-values-from-a-field","text":"GET /vspherebeat/_search { \"size\" : 0 , \"aggs\" : { \"distinct_hots\" : { \"cardinality\" : { \"field\" : \"performancemanager.hosts.metaData.name\" } } } }","title":"Total of unique values from a field"},{"location":"databases/elasticsearch/snapshots/","text":"Snapshots \u00b6 Repositories \u00b6 Create \u00b6 PUT /_snapshot/my_backup { \"type\" : \"fs\" , \"settings\" : { \"location\" : \"/usr/share/elasticsearch/snapshots/backup\" } } List \u00b6 GET /_cat/repositories?v GET /_snapshot/_all Delete \u00b6 DELETE /_snapshot/ { repository } Snapshots \u00b6 Elasticsearch config Change parameter in elastic search config file for all nodes The path.repo needs to be a shared folder beetween the cluster path.repo : [ \"/usr/share/elasticsearch/snapshots\" ] Create \u00b6 PUT /_snapshot/ { repository } / { snapshot } ?wait_for_completion= true List all Snapshots from a repo \u00b6 GET /_cat/snapshots/ { repository } ?v&s=id Delete \u00b6 DELETE /_snapshot/ { repository } / { snapshot } Restore \u00b6 Restore - Official Doc Restore From one Snapshot \u00b6 POST /_snapshot/ { repository } / { snapshot } /_restore","title":"Snapshots"},{"location":"databases/elasticsearch/snapshots/#snapshots","text":"","title":"Snapshots"},{"location":"databases/elasticsearch/snapshots/#repositories","text":"","title":"Repositories"},{"location":"databases/elasticsearch/snapshots/#create","text":"PUT /_snapshot/my_backup { \"type\" : \"fs\" , \"settings\" : { \"location\" : \"/usr/share/elasticsearch/snapshots/backup\" } }","title":"Create"},{"location":"databases/elasticsearch/snapshots/#list","text":"GET /_cat/repositories?v GET /_snapshot/_all","title":"List"},{"location":"databases/elasticsearch/snapshots/#delete","text":"DELETE /_snapshot/ { repository }","title":"Delete"},{"location":"databases/elasticsearch/snapshots/#snapshots_1","text":"Elasticsearch config Change parameter in elastic search config file for all nodes The path.repo needs to be a shared folder beetween the cluster path.repo : [ \"/usr/share/elasticsearch/snapshots\" ]","title":"Snapshots"},{"location":"databases/elasticsearch/snapshots/#create_1","text":"PUT /_snapshot/ { repository } / { snapshot } ?wait_for_completion= true","title":"Create"},{"location":"databases/elasticsearch/snapshots/#list-all-snapshots-from-a-repo","text":"GET /_cat/snapshots/ { repository } ?v&s=id","title":"List all Snapshots from a repo"},{"location":"databases/elasticsearch/snapshots/#delete_1","text":"DELETE /_snapshot/ { repository } / { snapshot }","title":"Delete"},{"location":"databases/elasticsearch/snapshots/#restore","text":"Restore - Official Doc","title":"Restore"},{"location":"databases/elasticsearch/snapshots/#restore-from-one-snapshot","text":"POST /_snapshot/ { repository } / { snapshot } /_restore","title":"Restore From one Snapshot"},{"location":"databases/mysql/galera-cluster/","text":"Docker \u00b6 Know Errors \u00b6 WSREP: failed to open gcomm backend connection \u00b6 [ERROR] WSREP: failed to open gcomm backend connection: 131: invalid UUID: 00000000 (FATAL) at gcomm/src/pc.cpp:PC():271 Related Links itheadaches Github Issues","title":"Galera"},{"location":"databases/mysql/galera-cluster/#docker","text":"","title":"Docker"},{"location":"databases/mysql/galera-cluster/#know-errors","text":"","title":"Know Errors"},{"location":"databases/mysql/galera-cluster/#wsrep-failed-to-open-gcomm-backend-connection","text":"[ERROR] WSREP: failed to open gcomm backend connection: 131: invalid UUID: 00000000 (FATAL) at gcomm/src/pc.cpp:PC():271 Related Links itheadaches Github Issues","title":"WSREP: failed to open gcomm backend connection"},{"location":"developer/ciscoAci/cobra/","text":"Cisco ACI Cobra \u00b6 Create Session \u00b6 from credentials import * import cobra.mit.session import cobra.mit.access auth = cobra . mit . session . LoginSession ( URL , LOGIN , PASSWORD ) session = cobra . mit . access . MoDirectory ( auth ) session . login () DN Query \u00b6 import cobra.mit.request tenant_query = cobra . mit . request . DnQuery ( \"uni/tn-Heroes\" ) heroes_tenant = session . query ( tenant_query ) heroes = heroes_tenant [ 0 ] dir ( heroes ) Class Query \u00b6 propFilter (query-target-filter) filters the results based on class attributes. queryTarget (query-target) specifies what part of the MIT to query: self children subtree classFilter (target-subtree-class) can be used when queryTarget is set to either \"children\" or \"subtree\" to filter the returned children to only objects of a specific class or classes. For example, to filter for Application Profiles use fvAp. *subtree (rsp-subtree) specifies how much of the subtree to retrieve: no children full subtreeClassFilter (rsp-subtree-class) filters what subtree classes to include in the response. subtreeInclude (rsp-subtree-include) specifies what type of information to include in the subtree audit-logs event-logs faults faults,no-scoped health Make a Class Query for All App Profiles \u00b6 ```python app_query = cobra.mit.request.ClassQuery('fvAp') apps = session.query(app_query) ### Scope the Query to Applications Named \"Save_The_Planet\" ```python # set the property filter to only return the app named \"Save_The_Planet\" app_query.propFilter = 'eq(fvAp.name, \"Save_The_Planet\")' save_the_planet_app = session.query(app_query) Child Objects \u00b6 Have the Query Return Child Objects \u00b6 # set the scope to subtree full app_query . subtree = \"full\" # demonstrate a typo; cobra provides the acceptable options app_query . queryTarget = \"subtre\" Traceback ( most recent call last ): ... ( value , str ( allowedValues ))) ValueError : \"subtre\" is invalid , valid values are \"set(['self', 'subtree', 'children'])\" # scope the query to subtree app_query . queryTarget = \"subtree\" # look at the applied query scopes using .options app_query . options 'rsp-subtree=full&query-target-filter=eq(fvAp.name, \"Save_The_Planet\")&query-target=subtree' save_the_planet_app_subtree = session . query ( app_query ) View the Child Objects for the Query \u00b6 save_the_planet_app_subtree [ < cobra . modelimpl . fv . ap . Ap object at 0x7f3c3c735150 > ] save_the_planet_app_subtree [ 0 ] . numChildren 3 for epg in save_the_planet_app_subtree [ 0 ] . children : print epg . dn uni / tn - Heroes / ap - Save_The_Planet / epg - web uni / tn - Heroes / ap - Save_The_Planet / epg - app uni / tn - Heroes / ap - Save_The_Planet / epg - db Submitting Configurations \u00b6 # submit staged configuration to APIC config_request = cobra . mit . request . ConfigRequest () config_request . addMo ( epg ) session . commit ( config_request ) # You will see these two output lines # SSL Warning # <Response [200]> Lookups \u00b6 LookupbyDn \u00b6 # set top level universe directory uniMo = moDir . lookupByDn ( 'uni' ) lookupByClass \u00b6 # use the \"lookupByClass\" method to find all endpoints (fvCEp) endpoints = md . lookupByClass ( 'fvCEp' ) print ([ str ( ep . dn ) for ep in endpoints ])","title":"Cisco ACI"},{"location":"developer/ciscoAci/cobra/#cisco-aci-cobra","text":"","title":"Cisco ACI Cobra"},{"location":"developer/ciscoAci/cobra/#create-session","text":"from credentials import * import cobra.mit.session import cobra.mit.access auth = cobra . mit . session . LoginSession ( URL , LOGIN , PASSWORD ) session = cobra . mit . access . MoDirectory ( auth ) session . login ()","title":"Create Session"},{"location":"developer/ciscoAci/cobra/#dn-query","text":"import cobra.mit.request tenant_query = cobra . mit . request . DnQuery ( \"uni/tn-Heroes\" ) heroes_tenant = session . query ( tenant_query ) heroes = heroes_tenant [ 0 ] dir ( heroes )","title":"DN Query"},{"location":"developer/ciscoAci/cobra/#class-query","text":"propFilter (query-target-filter) filters the results based on class attributes. queryTarget (query-target) specifies what part of the MIT to query: self children subtree classFilter (target-subtree-class) can be used when queryTarget is set to either \"children\" or \"subtree\" to filter the returned children to only objects of a specific class or classes. For example, to filter for Application Profiles use fvAp. *subtree (rsp-subtree) specifies how much of the subtree to retrieve: no children full subtreeClassFilter (rsp-subtree-class) filters what subtree classes to include in the response. subtreeInclude (rsp-subtree-include) specifies what type of information to include in the subtree audit-logs event-logs faults faults,no-scoped health","title":"Class Query"},{"location":"developer/ciscoAci/cobra/#make-a-class-query-for-all-app-profiles","text":"```python app_query = cobra.mit.request.ClassQuery('fvAp') apps = session.query(app_query) ### Scope the Query to Applications Named \"Save_The_Planet\" ```python # set the property filter to only return the app named \"Save_The_Planet\" app_query.propFilter = 'eq(fvAp.name, \"Save_The_Planet\")' save_the_planet_app = session.query(app_query)","title":"Make a Class Query for All App Profiles"},{"location":"developer/ciscoAci/cobra/#child-objects","text":"","title":"Child Objects"},{"location":"developer/ciscoAci/cobra/#have-the-query-return-child-objects","text":"# set the scope to subtree full app_query . subtree = \"full\" # demonstrate a typo; cobra provides the acceptable options app_query . queryTarget = \"subtre\" Traceback ( most recent call last ): ... ( value , str ( allowedValues ))) ValueError : \"subtre\" is invalid , valid values are \"set(['self', 'subtree', 'children'])\" # scope the query to subtree app_query . queryTarget = \"subtree\" # look at the applied query scopes using .options app_query . options 'rsp-subtree=full&query-target-filter=eq(fvAp.name, \"Save_The_Planet\")&query-target=subtree' save_the_planet_app_subtree = session . query ( app_query )","title":"Have the Query Return Child Objects"},{"location":"developer/ciscoAci/cobra/#view-the-child-objects-for-the-query","text":"save_the_planet_app_subtree [ < cobra . modelimpl . fv . ap . Ap object at 0x7f3c3c735150 > ] save_the_planet_app_subtree [ 0 ] . numChildren 3 for epg in save_the_planet_app_subtree [ 0 ] . children : print epg . dn uni / tn - Heroes / ap - Save_The_Planet / epg - web uni / tn - Heroes / ap - Save_The_Planet / epg - app uni / tn - Heroes / ap - Save_The_Planet / epg - db","title":"View the Child Objects for the Query"},{"location":"developer/ciscoAci/cobra/#submitting-configurations","text":"# submit staged configuration to APIC config_request = cobra . mit . request . ConfigRequest () config_request . addMo ( epg ) session . commit ( config_request ) # You will see these two output lines # SSL Warning # <Response [200]>","title":"Submitting Configurations"},{"location":"developer/ciscoAci/cobra/#lookups","text":"","title":"Lookups"},{"location":"developer/ciscoAci/cobra/#lookupbydn","text":"# set top level universe directory uniMo = moDir . lookupByDn ( 'uni' )","title":"LookupbyDn"},{"location":"developer/ciscoAci/cobra/#lookupbyclass","text":"# use the \"lookupByClass\" method to find all endpoints (fvCEp) endpoints = md . lookupByClass ( 'fvCEp' ) print ([ str ( ep . dn ) for ep in endpoints ])","title":"lookupByClass"},{"location":"developer/css/cssgrid/","text":"CSS Grid \u00b6","title":"Grid"},{"location":"developer/css/cssgrid/#css-grid","text":"","title":"CSS Grid"},{"location":"developer/css/flexbox/","text":"Flexbox \u00b6 Properties \u00b6 Flex Container \u00b6 Justify-content \u00b6 Controls how the items should be positioning along the main axis center - Don't care with space space-between - Space between items is evenly distribute space-around - Puts same amount space left side and the right side space-evenly - The space is always the same flex-end flex-center Align-items \u00b6 Defines how the items should be positioning in cross axis Flex Items \u00b6 align-self \u00b6 Replaces align-items flex-grow \u00b6 Allows element grows up. Is relative to the other flex-grow flex-basis \u00b6 Is the width. Put the container percentagem or pixels","title":"Flexbox"},{"location":"developer/css/flexbox/#flexbox","text":"","title":"Flexbox"},{"location":"developer/css/flexbox/#properties","text":"","title":"Properties"},{"location":"developer/css/flexbox/#flex-container","text":"","title":"Flex Container"},{"location":"developer/css/flexbox/#justify-content","text":"Controls how the items should be positioning along the main axis center - Don't care with space space-between - Space between items is evenly distribute space-around - Puts same amount space left side and the right side space-evenly - The space is always the same flex-end flex-center","title":"Justify-content"},{"location":"developer/css/flexbox/#align-items","text":"Defines how the items should be positioning in cross axis","title":"Align-items"},{"location":"developer/css/flexbox/#flex-items","text":"","title":"Flex Items"},{"location":"developer/css/flexbox/#align-self","text":"Replaces align-items","title":"align-self"},{"location":"developer/css/flexbox/#flex-grow","text":"Allows element grows up. Is relative to the other flex-grow","title":"flex-grow"},{"location":"developer/css/flexbox/#flex-basis","text":"Is the width. Put the container percentagem or pixels","title":"flex-basis"},{"location":"developer/css/vom/","text":"VOM \u00b6 Box Model \u00b6 Box Types \u00b6 Position \u00b6","title":"VOM"},{"location":"developer/css/vom/#vom","text":"","title":"VOM"},{"location":"developer/css/vom/#box-model","text":"","title":"Box Model"},{"location":"developer/css/vom/#box-types","text":"","title":"Box Types"},{"location":"developer/css/vom/#position","text":"","title":"Position"},{"location":"developer/python/packages/","text":"Python Packages \u00b6 No Internet Access and No Root \u00b6 No PIP \u00b6 export PYTHONPATH = ~/lib/python # Download and upload package tar.gz tar -xvz package-name.tar.gz cd package-name python3.6 setup.py install --home ~ PIP \u00b6 This process install all dependencies Stackoverflow # Machine with Internet Access # Use Docker to have environment similiar to target machine mkdir keystone-deps pip download python-keystoneclient -d \"/home/aviuser/keystone-deps\" tar cvfz keystone-deps.tgz keystone-deps # Machine to install Package tar xvfz keystone-deps.tgz cd keystone-deps pip install --target = ~/lib/python keystone-deps.whl -f ./ --no-index PIP but not installed \u00b6 Stackoverflow","title":"Packages"},{"location":"developer/python/packages/#python-packages","text":"","title":"Python Packages"},{"location":"developer/python/packages/#no-internet-access-and-no-root","text":"","title":"No Internet Access and No Root"},{"location":"developer/python/packages/#no-pip","text":"export PYTHONPATH = ~/lib/python # Download and upload package tar.gz tar -xvz package-name.tar.gz cd package-name python3.6 setup.py install --home ~","title":"No PIP"},{"location":"developer/python/packages/#pip","text":"This process install all dependencies Stackoverflow # Machine with Internet Access # Use Docker to have environment similiar to target machine mkdir keystone-deps pip download python-keystoneclient -d \"/home/aviuser/keystone-deps\" tar cvfz keystone-deps.tgz keystone-deps # Machine to install Package tar xvfz keystone-deps.tgz cd keystone-deps pip install --target = ~/lib/python keystone-deps.whl -f ./ --no-index","title":"PIP"},{"location":"developer/python/packages/#pip-but-not-installed","text":"Stackoverflow","title":"PIP but not installed"},{"location":"developer/robotframework/ddd/","text":"Data Driver \u00b6 Install \u00b6 pip install robotframework-datadriver Resources \u00b6 *** Settings *** Documentation Suite description Library SeleniumLibrary *** Variables *** ${ browser } chrome ${ url } https://admin-demo.nopcommerce.com *** Keywords *** Open my Browser Open Browser ${ url } ${ browser } maximize browser window Close Browsers close all browsers Open Login Page go to ${ url } Input username [Arguments] ${ username } input text id:Email ${ username } Input pwd [Arguments] ${ password } input text id:Password ${ password } click login button click element xpath://input[@class='button-1 login-button'] Error message should be visible page should contain Login was unsuccessful Robot File with Data \u00b6 *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/login_resources.robot Suite Setup Open my Browser Suite Teardown Close Browsers Test Template Invalid Login *** Test Cases *** username password Right user empty pwd admin@yourstore.com ${ EMPTY } Right user wrong pass admin@yourstore.com xyz Wrong user right pass adm@yourstore.com admin Wrong user empty pass adm@yourstore.com ${ EMPTY } Wrong user wrong pass adm@yourstore.com xyz *** Keywords *** Invalid Login [Arguments] ${ username } ${ password } Input username ${ username } Input pwd ${ password } click login button Error message should be visible From Excel and CSV \u00b6 *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/login_resources.robot #Library DataDriver ../TestData/LoginData.xlsx sheet_name=Sheet1 Library DataDriver ../TestData/LoginData.csv Suite Setup Open my Browser Suite Teardown Close Browsers Test Template Invalid Login *** Test Cases *** LoginTestWithExcel using ${ username } and ${ password } *** Keywords *** Invalid Login [Arguments] ${ username } ${ password } Input username ${ username } Input pwd ${ password } click login button Error message should be visible","title":"Data-Driver"},{"location":"developer/robotframework/ddd/#data-driver","text":"","title":"Data Driver"},{"location":"developer/robotframework/ddd/#install","text":"pip install robotframework-datadriver","title":"Install"},{"location":"developer/robotframework/ddd/#resources","text":"*** Settings *** Documentation Suite description Library SeleniumLibrary *** Variables *** ${ browser } chrome ${ url } https://admin-demo.nopcommerce.com *** Keywords *** Open my Browser Open Browser ${ url } ${ browser } maximize browser window Close Browsers close all browsers Open Login Page go to ${ url } Input username [Arguments] ${ username } input text id:Email ${ username } Input pwd [Arguments] ${ password } input text id:Password ${ password } click login button click element xpath://input[@class='button-1 login-button'] Error message should be visible page should contain Login was unsuccessful","title":"Resources"},{"location":"developer/robotframework/ddd/#robot-file-with-data","text":"*** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/login_resources.robot Suite Setup Open my Browser Suite Teardown Close Browsers Test Template Invalid Login *** Test Cases *** username password Right user empty pwd admin@yourstore.com ${ EMPTY } Right user wrong pass admin@yourstore.com xyz Wrong user right pass adm@yourstore.com admin Wrong user empty pass adm@yourstore.com ${ EMPTY } Wrong user wrong pass adm@yourstore.com xyz *** Keywords *** Invalid Login [Arguments] ${ username } ${ password } Input username ${ username } Input pwd ${ password } click login button Error message should be visible","title":"Robot File with Data"},{"location":"developer/robotframework/ddd/#from-excel-and-csv","text":"*** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/login_resources.robot #Library DataDriver ../TestData/LoginData.xlsx sheet_name=Sheet1 Library DataDriver ../TestData/LoginData.csv Suite Setup Open my Browser Suite Teardown Close Browsers Test Template Invalid Login *** Test Cases *** LoginTestWithExcel using ${ username } and ${ password } *** Keywords *** Invalid Login [Arguments] ${ username } ${ password } Input username ${ username } Input pwd ${ password } click login button Error message should be visible","title":"From Excel and CSV"},{"location":"developer/robotframework/gettingstarted/","text":"Getting Started \u00b6 Install \u00b6 pip install robotframework Structure \u00b6 SCALAR ${VARIABLE NAME} value LIST @{VARIABLE NAME} value1 value2 DICTIONARY &{VARIABLE NAME} k1=value1 k2=value2 ENVIRONMENT %{USERNAME} BUILT-IN Robot Commands \u00b6 # Run One Test robot FirstTestSuite.robot # Run All Tests in folder robot TestCases/ # Run Tests based in regular expressions robot TestsCases \\R eg*.robot # With Tags robot --include = sanity Setup_TearDown.robot robot -i sanity -i regression Setup_TearDown.robot robot -e regression Setup_TearDown.robot robot -i sanity -e regression Setup_TearDown.robot Pabot Commands \u00b6 pip install -U robotframework-pabot pabot --processes 2 TestCases/*.robot pabot --processes 2 --outputdir Results TestCases/*.robot","title":"Getting Started"},{"location":"developer/robotframework/gettingstarted/#getting-started","text":"","title":"Getting Started"},{"location":"developer/robotframework/gettingstarted/#install","text":"pip install robotframework","title":"Install"},{"location":"developer/robotframework/gettingstarted/#structure","text":"SCALAR ${VARIABLE NAME} value LIST @{VARIABLE NAME} value1 value2 DICTIONARY &{VARIABLE NAME} k1=value1 k2=value2 ENVIRONMENT %{USERNAME} BUILT-IN","title":"Structure"},{"location":"developer/robotframework/gettingstarted/#robot-commands","text":"# Run One Test robot FirstTestSuite.robot # Run All Tests in folder robot TestCases/ # Run Tests based in regular expressions robot TestsCases \\R eg*.robot # With Tags robot --include = sanity Setup_TearDown.robot robot -i sanity -i regression Setup_TearDown.robot robot -e regression Setup_TearDown.robot robot -i sanity -e regression Setup_TearDown.robot","title":"Robot Commands"},{"location":"developer/robotframework/gettingstarted/#pabot-commands","text":"pip install -U robotframework-pabot pabot --processes 2 TestCases/*.robot pabot --processes 2 --outputdir Results TestCases/*.robot","title":"Pabot Commands"},{"location":"developer/robotframework/pom/","text":"Page Object Model \u00b6 Locators \u00b6 File: PageObjects/Locators.py # Login Page Elements txt_loginUserName = \"name:userName\" txt_loginPassword = \"name:password\" btn_signIn = \"xpath://input[@name='login']\" # Registration Page Elements link_Reg = \"link:REGISTER\" txt_firstName = \"name:firstName\" txt_lastName = \"name:lastName\" txt_phone = \"name:phone\" txt_email = \"name:userName\" txt_add1 = \"name:address1\" txt_add2 = \"name:address2\" txt_city = \"name:city\" txt_state = \"name:state\" txt_postCode = \"name:postalCode\" drp_country = \"name:country\" txt_userName = \"name:email\" txt_Password = \"name:password\" txt_conformedPassword = \"name:confirmPassword\" btn_submit = \"xpath://input[@name='register']\" Resources \u00b6 File: Resources/LoginKeywords.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Variables ../PageObjects/Locators.py *** Keywords *** Open my Browser [Arguments] ${ SiteUrl } ${ Browser } Open Browser ${ SiteUrl } ${ Browser } Maximize browser window Enter UserName [Arguments] ${ username } Input Text ${ txt_loginUserName } ${ username } Enter Password [Arguments] ${ password } Input Text ${ txt_loginPassword } ${ password } Click SignIn Click Button ${ btn_signIn } Verify Succesfull Login title should be Find a Flight: Mercury Tours: Close my browsers close all browsers File: Resources/Registration.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Variables ../PageObjects/Locators.py *** Keywords *** Open my Browser [Arguments] ${ SiteUrl } ${ Browser } Open Browser ${ SiteUrl } ${ Browser } Maximize browser window Click Register Link click link ${ link_Reg } Enter FirstName [Arguments] ${ firstName } Input Text ${ txt_firstName } ${ firstName } Enter LastName [Arguments] ${ lastName } Input Text ${ txt_lastName } ${ lastName } Enter Phone [Arguments] ${ phone } Input Text ${ txt_phone } ${ phone } Enter Email [Arguments] ${ email } Input Text ${ txt_email } ${ email } Enter Address1 [Arguments] ${ add1 } Input Text ${ txt_add1 } ${ add1 } Enter Address2 [Arguments] ${ add2 } Input Text ${ txt_add2 } ${ add2 } Enter City [Arguments] ${ city } Input Text ${ txt_city } ${ city } Enter State [Arguments] ${ state } Input Text ${ txt_state } ${ state } Enter Postal Code [Arguments] ${ postalcode } Input Text ${ txt_postCode } ${ postalcode } Select Country [Arguments] ${ country } Select from list by label ${ drp_country } ${ country } Enter User Name [Arguments] ${ username } Input Text ${ txt_userName } ${ username } Enter Password [Arguments] ${ password } Input Text ${ txt_Password } ${ password } Enter Confirmed Password [Arguments] ${ password } Input Text ${ txt_conformedPassword } ${ password } Click Submit Click button ${ btn_submit } Verify Succesfull Registration page should contain Thank you for registeringdsadsa Close my browsers close all browsers Tests \u00b6 File: TestCases/LoginTest.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/LoginKeywords.robot *** Variables *** ${ Browser } chrome ${ SiteUrl } http://newtours.demoaut.com/ ${ user } tutorial ${ pwd } tutorial *** Test Cases *** LoginTest Open my Browser ${ SiteUrl } ${ Browser } Enter UserName ${ user } Enter Password ${ pwd } Click SignIn Sleep 3 seconds Verify Succesfull Login Close my browsers File: TestCases/RegistrationTest.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../ Resources / RegistrationKeywords . robot *** Variables *** ${ Browser } headlesschrome ${ SiteUrl } http: // newtours . demoaut . com / *** Test Cases *** RegistrationTest Open my Browser ${ SiteUrl } ${ Browser } Click Register Link Enter FirstName David Enter LastName John Enter Phone 1234567890 Enter Email davidjohn @gmail . com Enter Address1 Toronto Enter Address2 Canada Enter City Toronto Enter State Brampton Enter Postal Code L3S 1E7 Select Country CANADA Enter User Name johnxyz Enter Password johnxyz Enter Confirmed Password johnxyz Click Submit Verify Succesfull Registration Close my browsers","title":"Page-Object Model"},{"location":"developer/robotframework/pom/#page-object-model","text":"","title":"Page Object Model"},{"location":"developer/robotframework/pom/#locators","text":"File: PageObjects/Locators.py # Login Page Elements txt_loginUserName = \"name:userName\" txt_loginPassword = \"name:password\" btn_signIn = \"xpath://input[@name='login']\" # Registration Page Elements link_Reg = \"link:REGISTER\" txt_firstName = \"name:firstName\" txt_lastName = \"name:lastName\" txt_phone = \"name:phone\" txt_email = \"name:userName\" txt_add1 = \"name:address1\" txt_add2 = \"name:address2\" txt_city = \"name:city\" txt_state = \"name:state\" txt_postCode = \"name:postalCode\" drp_country = \"name:country\" txt_userName = \"name:email\" txt_Password = \"name:password\" txt_conformedPassword = \"name:confirmPassword\" btn_submit = \"xpath://input[@name='register']\"","title":"Locators"},{"location":"developer/robotframework/pom/#resources","text":"File: Resources/LoginKeywords.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Variables ../PageObjects/Locators.py *** Keywords *** Open my Browser [Arguments] ${ SiteUrl } ${ Browser } Open Browser ${ SiteUrl } ${ Browser } Maximize browser window Enter UserName [Arguments] ${ username } Input Text ${ txt_loginUserName } ${ username } Enter Password [Arguments] ${ password } Input Text ${ txt_loginPassword } ${ password } Click SignIn Click Button ${ btn_signIn } Verify Succesfull Login title should be Find a Flight: Mercury Tours: Close my browsers close all browsers File: Resources/Registration.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Variables ../PageObjects/Locators.py *** Keywords *** Open my Browser [Arguments] ${ SiteUrl } ${ Browser } Open Browser ${ SiteUrl } ${ Browser } Maximize browser window Click Register Link click link ${ link_Reg } Enter FirstName [Arguments] ${ firstName } Input Text ${ txt_firstName } ${ firstName } Enter LastName [Arguments] ${ lastName } Input Text ${ txt_lastName } ${ lastName } Enter Phone [Arguments] ${ phone } Input Text ${ txt_phone } ${ phone } Enter Email [Arguments] ${ email } Input Text ${ txt_email } ${ email } Enter Address1 [Arguments] ${ add1 } Input Text ${ txt_add1 } ${ add1 } Enter Address2 [Arguments] ${ add2 } Input Text ${ txt_add2 } ${ add2 } Enter City [Arguments] ${ city } Input Text ${ txt_city } ${ city } Enter State [Arguments] ${ state } Input Text ${ txt_state } ${ state } Enter Postal Code [Arguments] ${ postalcode } Input Text ${ txt_postCode } ${ postalcode } Select Country [Arguments] ${ country } Select from list by label ${ drp_country } ${ country } Enter User Name [Arguments] ${ username } Input Text ${ txt_userName } ${ username } Enter Password [Arguments] ${ password } Input Text ${ txt_Password } ${ password } Enter Confirmed Password [Arguments] ${ password } Input Text ${ txt_conformedPassword } ${ password } Click Submit Click button ${ btn_submit } Verify Succesfull Registration page should contain Thank you for registeringdsadsa Close my browsers close all browsers","title":"Resources"},{"location":"developer/robotframework/pom/#tests","text":"File: TestCases/LoginTest.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/LoginKeywords.robot *** Variables *** ${ Browser } chrome ${ SiteUrl } http://newtours.demoaut.com/ ${ user } tutorial ${ pwd } tutorial *** Test Cases *** LoginTest Open my Browser ${ SiteUrl } ${ Browser } Enter UserName ${ user } Enter Password ${ pwd } Click SignIn Sleep 3 seconds Verify Succesfull Login Close my browsers File: TestCases/RegistrationTest.robot *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../ Resources / RegistrationKeywords . robot *** Variables *** ${ Browser } headlesschrome ${ SiteUrl } http: // newtours . demoaut . com / *** Test Cases *** RegistrationTest Open my Browser ${ SiteUrl } ${ Browser } Click Register Link Enter FirstName David Enter LastName John Enter Phone 1234567890 Enter Email davidjohn @gmail . com Enter Address1 Toronto Enter Address2 Canada Enter City Toronto Enter State Brampton Enter Postal Code L3S 1E7 Select Country CANADA Enter User Name johnxyz Enter Password johnxyz Enter Confirmed Password johnxyz Click Submit Verify Succesfull Registration Close my browsers","title":"Tests"},{"location":"developer/robotframework/robotfile/","text":"Robot File \u00b6 *** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/RegistrationKeywords.robot Suite Setup Log This is open browser Test Suite Teardown Log This is close browser Test Test Setup Log This is Login Test Test Teardown Log This is Logout Test *** Variables *** ${ Browser } Chrome ${ URL } https://opensource-demo.orangehrmlive.com/ @{Credentials} Admin admin123 & {LoginData} username=Admin password=admin123 *** Test Cases *** TC_001 [Tags] sanity ${ PageTitle } = launchBrowser ${ url } ${ browser } log to console ${ PageTitle } log ${ PageTitle } Input Text name:userName mercury Input Text name:password mercury TC_002 [Tags] sanity Log This is postpaid recharge test TC_003 [Tags] regression Log This is search test TC_004 [Tags] regression Log This is Advanced search test *** Keywords *** Do login [Arguments] ${ test } Input Text id:txtUsername @{Credentials}[0] Input Text id:txtPassword & {LoginData}[password] Click Element name:Submit","title":"RobotFile"},{"location":"developer/robotframework/robotfile/#robot-file","text":"*** Settings *** Documentation Suite description Library SeleniumLibrary Resource ../Resources/RegistrationKeywords.robot Suite Setup Log This is open browser Test Suite Teardown Log This is close browser Test Test Setup Log This is Login Test Test Teardown Log This is Logout Test *** Variables *** ${ Browser } Chrome ${ URL } https://opensource-demo.orangehrmlive.com/ @{Credentials} Admin admin123 & {LoginData} username=Admin password=admin123 *** Test Cases *** TC_001 [Tags] sanity ${ PageTitle } = launchBrowser ${ url } ${ browser } log to console ${ PageTitle } log ${ PageTitle } Input Text name:userName mercury Input Text name:password mercury TC_002 [Tags] sanity Log This is postpaid recharge test TC_003 [Tags] regression Log This is search test TC_004 [Tags] regression Log This is Advanced search test *** Keywords *** Do login [Arguments] ${ test } Input Text id:txtUsername @{Credentials}[0] Input Text id:txtPassword & {LoginData}[password] Click Element name:Submit","title":"Robot File"},{"location":"developer/robotframework/selenium/","text":"Selenium \u00b6 Install \u00b6 pip install -U selenium pip install robotframework-seleniumlibrary # Download Browser Drivers Input Text and Click \u00b6 *** Test Cases *** Login Test Open Browser $ { url } $ { browser } loginToApplication Close Browser *** Keywords *** loginToApplication Click Link xpath : //a[@class='ico-login'] Input Text id : Email pavanoltraining @ gmail . com Input Text id : Password Test @123 Click Element xpath : //input[@class='button-1 login-button'] Radio Buttons and Check Boxes \u00b6 Testing Radio Buttons and Check Boxes Open Browser ${ url } ${ browser } Maximize Browser Window Select Radio Button sex Female Select Radio Button exp 5 Select Checkbox BlackTea Select Checkbox RedTea Unselect Checkbox BlackTea Close Browser Dropdown and Lists \u00b6 Handling DropDownds and Lists Open Browser ${ url } ${ browser } Maximize Browser Window Select From List By Label continents Asia Sleep 5 Select From List By Index continents 5 Select From List By Label selenium_commands Switch Commands Select From List By Label selenium_commands WebElement Commands Sleep 5 Unselect From List By Label selenium_commands Switch Commands HTML Table \u00b6 TableValidations open browser https://www.testautomationpractice.blogspot.com/ chrome maximize browser window ${ rows } = get element count xpath://table[@name='BookTable']/tbody/tr ${ cols } = get element count xpath://table[@name='BookTable']/tbody/tr[1]/th log to console ${ rows } log to console ${ cols } ${ data } = get text xpath://table[@name='BookTable']/tbody/tr[5]/td[1] log to console ${ data } table column should contain xpath://table[@name='BookTable'] 2 Author table row should contain xpath://table[@name='BookTable'] 4 Learn JS table cell should contain xpath://table[@name='BookTable'] 5 2 Mukesh table header should contain xpath://table[@name='BookTable'] BookName close browser Speed \u00b6 RegTest ${ speed } = Get Selenium Speed Log To Console ${ speed } Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Speed 3 seconds Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn ${ speed } = Get Selenium Speed Log To Console ${ speed } Close Browser Timeouts \u00b6 Timeouts Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Timeout 10 seconds Wait Until Page Contains Registeration Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn Close Browser Implict Wait \u00b6 Implict Wait Test Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Implicit wait 10 seconds Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn Close Browser Multiple Browsers \u00b6 MultipleBrowsers Open Browser https://www.google.com/ chrome Maximize Browser Window Sleep 3 Open Browser https://www.bing.com/ chrome Maximize Browser Window Switch Browser 1 ${ title1 } = Get Title log to console ${ title1 } Switch Browser 2 ${ title2 } = Get Title log to console ${ title2 } Sleep 3 Close All Browsers Tabbed Windows \u00b6 TabbedWindowsTest Open Browser http://demo.automationtesting.in/Windows.html chrome Click Element xpath://*[@id=\"Tabbed\"]/a/button Select Window title=Sakinalium | Home Click Element xpath://*[@id=\"container\"]/header/div/div/div[2]/ul/Li[4]/a Sleep 3 Close All Browsers Navigation Keywords \u00b6 Navigation Test Open Browser https://www.google.com chrome ${ loc } = Get Location log to console ${ loc } sleep 3 go to https://www.bing.com/ ${ loc } = Get Location log to console ${ loc } sleep 3 go back ${ loc } = Get Location log to console ${ loc } sleep 3 close browser Handling Alerts \u00b6 Handling Alerts Open Browser https://testautomationpractice.blogspot.com/ chrome Click Element xpath://*[@id=\"HTML9\"]/div[1]/button # Opens Alert Sleep 1 Alert Should be Present Press a button! Handle Alert accept Handling Frames \u00b6 Testing Frames Open Browser https://seleniumhq.github.io/selenium/docs/api/java/index chrome Select Frame packageListFrame Click Link org.openqa.selenium Unselect Frame Select Frame packageFrame Click Link WebDriver Unselect Frame Select Frame classFrame Click Link Help Close Browser Scroll And Executing Javascript \u00b6 ScrollingTest Open Browser https://www.countries-ofthe-world.com/flags-of-the-world.html chrome execute javascript window.scrollTo(0,2500) scroll element into view xpath://table[1]//tbody[1]//tr[105]//td[1]//img[1] execute javascript window.scrollTo(0,document.body.scrollHeight) # End of the Page sleep 5 execute javascript window.scrollTo(0,-document.body.scrollHeight) # Starting Point","title":"Selenium"},{"location":"developer/robotframework/selenium/#selenium","text":"","title":"Selenium"},{"location":"developer/robotframework/selenium/#install","text":"pip install -U selenium pip install robotframework-seleniumlibrary # Download Browser Drivers","title":"Install"},{"location":"developer/robotframework/selenium/#input-text-and-click","text":"*** Test Cases *** Login Test Open Browser $ { url } $ { browser } loginToApplication Close Browser *** Keywords *** loginToApplication Click Link xpath : //a[@class='ico-login'] Input Text id : Email pavanoltraining @ gmail . com Input Text id : Password Test @123 Click Element xpath : //input[@class='button-1 login-button']","title":"Input Text and Click"},{"location":"developer/robotframework/selenium/#radio-buttons-and-check-boxes","text":"Testing Radio Buttons and Check Boxes Open Browser ${ url } ${ browser } Maximize Browser Window Select Radio Button sex Female Select Radio Button exp 5 Select Checkbox BlackTea Select Checkbox RedTea Unselect Checkbox BlackTea Close Browser","title":"Radio Buttons and Check Boxes"},{"location":"developer/robotframework/selenium/#dropdown-and-lists","text":"Handling DropDownds and Lists Open Browser ${ url } ${ browser } Maximize Browser Window Select From List By Label continents Asia Sleep 5 Select From List By Index continents 5 Select From List By Label selenium_commands Switch Commands Select From List By Label selenium_commands WebElement Commands Sleep 5 Unselect From List By Label selenium_commands Switch Commands","title":"Dropdown and Lists"},{"location":"developer/robotframework/selenium/#html-table","text":"TableValidations open browser https://www.testautomationpractice.blogspot.com/ chrome maximize browser window ${ rows } = get element count xpath://table[@name='BookTable']/tbody/tr ${ cols } = get element count xpath://table[@name='BookTable']/tbody/tr[1]/th log to console ${ rows } log to console ${ cols } ${ data } = get text xpath://table[@name='BookTable']/tbody/tr[5]/td[1] log to console ${ data } table column should contain xpath://table[@name='BookTable'] 2 Author table row should contain xpath://table[@name='BookTable'] 4 Learn JS table cell should contain xpath://table[@name='BookTable'] 5 2 Mukesh table header should contain xpath://table[@name='BookTable'] BookName close browser","title":"HTML Table"},{"location":"developer/robotframework/selenium/#speed","text":"RegTest ${ speed } = Get Selenium Speed Log To Console ${ speed } Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Speed 3 seconds Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn ${ speed } = Get Selenium Speed Log To Console ${ speed } Close Browser","title":"Speed"},{"location":"developer/robotframework/selenium/#timeouts","text":"Timeouts Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Timeout 10 seconds Wait Until Page Contains Registeration Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn Close Browser","title":"Timeouts"},{"location":"developer/robotframework/selenium/#implict-wait","text":"Implict Wait Test Open Browser ${ url } ${ browser } Maximize Browser Window Set Selenium Implicit wait 10 seconds Select Radio Button Gender M Input Text name:FirstName David Input Text name:LastName John Input Text name:Email anhc@gmail.com Input Text name:Password davidjohn Input Text name:ConfirmPassword davidjohn Close Browser","title":"Implict Wait"},{"location":"developer/robotframework/selenium/#multiple-browsers","text":"MultipleBrowsers Open Browser https://www.google.com/ chrome Maximize Browser Window Sleep 3 Open Browser https://www.bing.com/ chrome Maximize Browser Window Switch Browser 1 ${ title1 } = Get Title log to console ${ title1 } Switch Browser 2 ${ title2 } = Get Title log to console ${ title2 } Sleep 3 Close All Browsers","title":"Multiple Browsers"},{"location":"developer/robotframework/selenium/#tabbed-windows","text":"TabbedWindowsTest Open Browser http://demo.automationtesting.in/Windows.html chrome Click Element xpath://*[@id=\"Tabbed\"]/a/button Select Window title=Sakinalium | Home Click Element xpath://*[@id=\"container\"]/header/div/div/div[2]/ul/Li[4]/a Sleep 3 Close All Browsers","title":"Tabbed Windows"},{"location":"developer/robotframework/selenium/#navigation-keywords","text":"Navigation Test Open Browser https://www.google.com chrome ${ loc } = Get Location log to console ${ loc } sleep 3 go to https://www.bing.com/ ${ loc } = Get Location log to console ${ loc } sleep 3 go back ${ loc } = Get Location log to console ${ loc } sleep 3 close browser","title":"Navigation Keywords"},{"location":"developer/robotframework/selenium/#handling-alerts","text":"Handling Alerts Open Browser https://testautomationpractice.blogspot.com/ chrome Click Element xpath://*[@id=\"HTML9\"]/div[1]/button # Opens Alert Sleep 1 Alert Should be Present Press a button! Handle Alert accept","title":"Handling Alerts"},{"location":"developer/robotframework/selenium/#handling-frames","text":"Testing Frames Open Browser https://seleniumhq.github.io/selenium/docs/api/java/index chrome Select Frame packageListFrame Click Link org.openqa.selenium Unselect Frame Select Frame packageFrame Click Link WebDriver Unselect Frame Select Frame classFrame Click Link Help Close Browser","title":"Handling Frames"},{"location":"developer/robotframework/selenium/#scroll-and-executing-javascript","text":"ScrollingTest Open Browser https://www.countries-ofthe-world.com/flags-of-the-world.html chrome execute javascript window.scrollTo(0,2500) scroll element into view xpath://table[1]//tbody[1]//tr[105]//td[1]//img[1] execute javascript window.scrollTo(0,document.body.scrollHeight) # End of the Page sleep 5 execute javascript window.scrollTo(0,-document.body.scrollHeight) # Starting Point","title":"Scroll And Executing Javascript"},{"location":"devops/git/","text":"Remove file from History \u00b6 git filter-branch --force --index-filter \"git rm --cached --ignore-unmatch private.key\" --prune-empty --tag-name-filter cat -- --all git push origin --force --all ( master branch needs to be unprotected ) git push origin --force --tags","title":"Git"},{"location":"devops/git/#remove-file-from-history","text":"git filter-branch --force --index-filter \"git rm --cached --ignore-unmatch private.key\" --prune-empty --tag-name-filter cat -- --all git push origin --force --all ( master branch needs to be unprotected ) git push origin --force --tags","title":"Remove file from History"},{"location":"devops/jenkins/","text":"Jenkins \u00b6 Recommended Plugins \u00b6 Ant Plugin Credentials Binding Plugin LDAP Plugin Pipeline: Stage View Plugin Pipeline OWASP Markup Formatter Plugin Email Extension Plugin Mailer Plugin SSH Slaves Plugin Github Organization Folder Plugin Build timeout Plugin Git Plugin Matrix Authorization Strategy Plugin Subversion Plug-in Workspace Cleanup Plugin CloudBees Folders Plugin Gradle Plugin PAM Authentication Plugin Timestamper Declarative Pipelines \u00b6 pipeline { agent any stages { agent { label \"master\" } options { # Runs before agent provisioning timeout ( time: 1 , unit: 'HOURS' ) } steps { sh \"./might-hang.sh\" } } post { always { echo 'This will always run' } success { echo 'This will run only if successful' } failure { echo 'This will run only if failed' } unstable { echo 'This will run only if the run was marked as unstable' } changed { echo 'This will run only if the state of the pipeline has changed' echo 'For example, if the pipeline was previously failing but is now successful' } } } Post Conditions \u00b6 cleanup Always run regardless of build or stage result, but runs after all other post conditions fixed Runs if the current build's status is SUCCESS and the previous build's status was either FAILURE or UNSTABLE regression Runs if the current build's status ir woese than the previous build's status Tag \u00b6 When building a tag you might for example want to run more tests and publish the binaries etc Will run the stage if the tab being built is starting with \"release-\" when { tag \"release-*\" } To match on any tag there is an alias that doesn't take an argument when { buildingTag () } Change Request \u00b6 Runs the stage if a change Request, a.k.a Pull Request on Github, is currently building when { changeRequest () } You might for example only care about running static analysis on change requests when { not { changeRequest () } } You can also check the various attributes of the pull request for more fine control id, target, branch, fork, url, title, author, authorDisplayName, authorEmail when { changeRequest authorEmail: 'rsandel@cloudbees.com' } Comparator \u00b6 Both changeRequest and tag has gotten on optional parameter; comparator where you can set how to compare the values EQUALS - simple string comparison GLOB - ANT style glob REGEXP - Regular Expression PreserveStashes \u00b6 options { preserveStashes ( buildCount: 5 ) } Github \u00b6 Setting Up GitHub Webhooks \u00b6 Create an access token in GitHub that has permission to read and create webhooks Add a GitHub server in Jenkins for Github.com Create a jenkins credential with the token and configure the GitHub server configuration to use it Check \"Manage Hooks\" for the GitHub server configuration In the project configuration, under \"Build Triggers\", select \"Github hook trigger for GitScm polling\" Tips \u00b6 Try Avoid script Try avoid variables across stages When { equals expected: ..., actual: ... } Covers a lot of common usages of when { expression {...} } } No more if (currentBuild.result == \"SUCCESS\") - just use equals expected: \"SUCCEESS\", actual: currentBuild.result knowledge Center \u00b6 Tomcat \u00b6 The username you provided is not allowed to use the text-based Tomcat Manager (error 403) when deploying on remote Tomcat8 using Jenkins Cause : By default Tomcat does not allow access to the manager from external machines Solve: Link Common Errors \u00b6 Send Email With Gmail \u00b6 StackOverfow Look at this response","title":"Jenkins"},{"location":"devops/jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"devops/jenkins/#recommended-plugins","text":"Ant Plugin Credentials Binding Plugin LDAP Plugin Pipeline: Stage View Plugin Pipeline OWASP Markup Formatter Plugin Email Extension Plugin Mailer Plugin SSH Slaves Plugin Github Organization Folder Plugin Build timeout Plugin Git Plugin Matrix Authorization Strategy Plugin Subversion Plug-in Workspace Cleanup Plugin CloudBees Folders Plugin Gradle Plugin PAM Authentication Plugin Timestamper","title":"Recommended Plugins"},{"location":"devops/jenkins/#declarative-pipelines","text":"pipeline { agent any stages { agent { label \"master\" } options { # Runs before agent provisioning timeout ( time: 1 , unit: 'HOURS' ) } steps { sh \"./might-hang.sh\" } } post { always { echo 'This will always run' } success { echo 'This will run only if successful' } failure { echo 'This will run only if failed' } unstable { echo 'This will run only if the run was marked as unstable' } changed { echo 'This will run only if the state of the pipeline has changed' echo 'For example, if the pipeline was previously failing but is now successful' } } }","title":"Declarative Pipelines"},{"location":"devops/jenkins/#post-conditions","text":"cleanup Always run regardless of build or stage result, but runs after all other post conditions fixed Runs if the current build's status is SUCCESS and the previous build's status was either FAILURE or UNSTABLE regression Runs if the current build's status ir woese than the previous build's status","title":"Post Conditions"},{"location":"devops/jenkins/#tag","text":"When building a tag you might for example want to run more tests and publish the binaries etc Will run the stage if the tab being built is starting with \"release-\" when { tag \"release-*\" } To match on any tag there is an alias that doesn't take an argument when { buildingTag () }","title":"Tag"},{"location":"devops/jenkins/#change-request","text":"Runs the stage if a change Request, a.k.a Pull Request on Github, is currently building when { changeRequest () } You might for example only care about running static analysis on change requests when { not { changeRequest () } } You can also check the various attributes of the pull request for more fine control id, target, branch, fork, url, title, author, authorDisplayName, authorEmail when { changeRequest authorEmail: 'rsandel@cloudbees.com' }","title":"Change Request"},{"location":"devops/jenkins/#comparator","text":"Both changeRequest and tag has gotten on optional parameter; comparator where you can set how to compare the values EQUALS - simple string comparison GLOB - ANT style glob REGEXP - Regular Expression","title":"Comparator"},{"location":"devops/jenkins/#preservestashes","text":"options { preserveStashes ( buildCount: 5 ) }","title":"PreserveStashes"},{"location":"devops/jenkins/#github","text":"","title":"Github"},{"location":"devops/jenkins/#setting-up-github-webhooks","text":"Create an access token in GitHub that has permission to read and create webhooks Add a GitHub server in Jenkins for Github.com Create a jenkins credential with the token and configure the GitHub server configuration to use it Check \"Manage Hooks\" for the GitHub server configuration In the project configuration, under \"Build Triggers\", select \"Github hook trigger for GitScm polling\"","title":"Setting Up GitHub Webhooks"},{"location":"devops/jenkins/#tips","text":"Try Avoid script Try avoid variables across stages When { equals expected: ..., actual: ... } Covers a lot of common usages of when { expression {...} } } No more if (currentBuild.result == \"SUCCESS\") - just use equals expected: \"SUCCEESS\", actual: currentBuild.result","title":"Tips"},{"location":"devops/jenkins/#knowledge-center","text":"","title":"knowledge Center"},{"location":"devops/jenkins/#tomcat","text":"The username you provided is not allowed to use the text-based Tomcat Manager (error 403) when deploying on remote Tomcat8 using Jenkins Cause : By default Tomcat does not allow access to the manager from external machines Solve: Link","title":"Tomcat"},{"location":"devops/jenkins/#common-errors","text":"","title":"Common Errors"},{"location":"devops/jenkins/#send-email-with-gmail","text":"StackOverfow Look at this response","title":"Send Email With Gmail"},{"location":"devops/packer/","text":"Install \u00b6 wget https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip unzip packer_1.4.2_linux_amd64.zip rm packer_1.4.2_linux_amd64.zip sudo mv packer /usr/bin/ packer -v Commands \u00b6 packer validate packer.json packer build -var 'tag=0.0.1' -var 'another=212' packer.json # Not Tested it packer fix packer validate Templates \u00b6 packer.json AMI \u00b6 { \"variables\" : { \"subnet_id\" : \"\" , \"instance_size\" : \"t2.micro\" , \"ami_name\" : \"bastion\" , \"ssh_username\" : \"ec2-user\" }, \"builders\" : [ { \"type\" : \"amazon-ebs\" , \"instance_type\" : \"{{user `instance_size`}}\" , \"ssh_username\" : \"{{user `ssh_username`}}\" , \"ssh_timeout\" : \"20m\" , \"ssh_pty\" : \"true\" , \"ami_name\" : \"{{user `ami_name`}}\" , \"subnet_id\" : \"{{user `subnet_id`}}\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"amzn2-ami-hvm-2.0.*-x86_64-gp2*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"amazon\" ], \"most_recent\" : true }, \"tags\" : { \"Name\" : \"{{user `ami_name`}}\" , \"BuiltBy\" : \"Packer\" } } ], \"description\" : \"AWS Bastion AMI\" , \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"sudo yum update -y\" , \"sudo hostnamectl set-hostname bastion\" ] } ] } Docker \u00b6 { \"variables\" : { \"repository\" : \"la/express\" , \"tag\" : \"0.1.0\" }, \"builders\" : [ { \"type\" : \"docker\" , \"author\" : \"Fabio Santos\" , \"image\" : \"node\" , \"commit\" : \"true\" , \"changes\" : [ \"EXPOSE 3000\" ] } ], \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"apt-get update -y && apt-get install curl -y\" , \"mkdir -p /var/code\" , \"cd /root\" , \"curl -L https://github.com/linuxacademy/content-nodejs-hello-world/archive/v1.0.tar.gz -o code.tar.gz\" , \"tar zxvf code.tar.gz -C /var/code --strip-components=1\" , \"cd /var/code\" , \"npm install\" ] } ], \"post-processors\" : [ { \"type\" : \"docker-tag\" , \"repository\" : \"{{user `repository`}}\" , \"tag\" : \"{{user `tag`}}\" } ] } Vsphere \u00b6 { \"variables\" : { \"vcenter\" : \"\" , \"username\" : \"\" , \"password\" : \"\" , \"insecure\" : \"true\" , \"datacenter\" : \"\" , \"cluster\" : \"\" , \"host\" : \"\" , \"vm_name\" : \"\" , \"datastore\" : \"\" , \"cpus\" : \"\" , \"ram\" : \"\" , \"network\" : \"\" , \"guest_os\" : \"\" , \"disk_size\" : \"\" , \"iso\" : \"\" , \"ip\" : \"\" , \"netmask\" : \"\" , \"gateway\" : \"\" , \"localIp\" : \"\" }, \"sensitive-variables\" : [ \"password\" ], \"builders\" : [ { \"type\" : \"vsphere-iso\" , \"vcenter_server\" : \"{{user `vcenter`}}\" , \"username\" : \"{{user `username`}}\" , \"password\" : \"{{user `password`}}\" , \"insecure_connection\" : \"{{user `insecure`}}\" , \"datacenter\" : \"{{user `datacenter`}}\" , \"cluster\" : \"{{user `cluster`}}\" , \"host\" : \"{{user `host`}}\" , \"datastore\" : \"{{user `datastore`}}\" , \"vm_name\" : \"{{user `vm_name`}}\" , \"CPUs\" : \"{{user `cpus`}}\" , \"ram\" : \"{{user `ram`}}\" , \"network\" : \"{{user `network`}}\" , \"network_card\" : \"vmxnet3\" , \"guest_os_type\" : \"{{user `guest_os`}}\" , \"disk_size\" : \"{{user `disk_size`}}\" , \"iso_paths\" : [ \"{{user `iso`}}\" ], \"cdrom_type\" : \"sata\" , \"ssh_username\" : \"jenkins\" , \"ssh_password\" : \"jenkins\" , \"http_directory\" : \"http\" , \"boot_wait\" : \"5s\" , \"boot_command\" : [ \"<up><wait><tab> ip={{user `ip`}} netmask={{user `netmask`}} gateway={{user `gateway`}} text ks=http://{{user `localIp`}}:{{ .HTTPPort}}/ks.cfg <enter>\" ] } ], \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"dnf -y update\" , \"dnf install -y java-1.8.0-openjdk-devel\" , \"sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo\" , \"sudo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key\" , \"sudo yum install jenkins -y\" ] } ] }","title":"Packer"},{"location":"devops/packer/#install","text":"wget https://releases.hashicorp.com/packer/1.4.2/packer_1.4.2_linux_amd64.zip unzip packer_1.4.2_linux_amd64.zip rm packer_1.4.2_linux_amd64.zip sudo mv packer /usr/bin/ packer -v","title":"Install"},{"location":"devops/packer/#commands","text":"packer validate packer.json packer build -var 'tag=0.0.1' -var 'another=212' packer.json # Not Tested it packer fix packer validate","title":"Commands"},{"location":"devops/packer/#templates","text":"packer.json","title":"Templates"},{"location":"devops/packer/#ami","text":"{ \"variables\" : { \"subnet_id\" : \"\" , \"instance_size\" : \"t2.micro\" , \"ami_name\" : \"bastion\" , \"ssh_username\" : \"ec2-user\" }, \"builders\" : [ { \"type\" : \"amazon-ebs\" , \"instance_type\" : \"{{user `instance_size`}}\" , \"ssh_username\" : \"{{user `ssh_username`}}\" , \"ssh_timeout\" : \"20m\" , \"ssh_pty\" : \"true\" , \"ami_name\" : \"{{user `ami_name`}}\" , \"subnet_id\" : \"{{user `subnet_id`}}\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"amzn2-ami-hvm-2.0.*-x86_64-gp2*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"amazon\" ], \"most_recent\" : true }, \"tags\" : { \"Name\" : \"{{user `ami_name`}}\" , \"BuiltBy\" : \"Packer\" } } ], \"description\" : \"AWS Bastion AMI\" , \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"sudo yum update -y\" , \"sudo hostnamectl set-hostname bastion\" ] } ] }","title":"AMI"},{"location":"devops/packer/#docker","text":"{ \"variables\" : { \"repository\" : \"la/express\" , \"tag\" : \"0.1.0\" }, \"builders\" : [ { \"type\" : \"docker\" , \"author\" : \"Fabio Santos\" , \"image\" : \"node\" , \"commit\" : \"true\" , \"changes\" : [ \"EXPOSE 3000\" ] } ], \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"apt-get update -y && apt-get install curl -y\" , \"mkdir -p /var/code\" , \"cd /root\" , \"curl -L https://github.com/linuxacademy/content-nodejs-hello-world/archive/v1.0.tar.gz -o code.tar.gz\" , \"tar zxvf code.tar.gz -C /var/code --strip-components=1\" , \"cd /var/code\" , \"npm install\" ] } ], \"post-processors\" : [ { \"type\" : \"docker-tag\" , \"repository\" : \"{{user `repository`}}\" , \"tag\" : \"{{user `tag`}}\" } ] }","title":"Docker"},{"location":"devops/packer/#vsphere","text":"{ \"variables\" : { \"vcenter\" : \"\" , \"username\" : \"\" , \"password\" : \"\" , \"insecure\" : \"true\" , \"datacenter\" : \"\" , \"cluster\" : \"\" , \"host\" : \"\" , \"vm_name\" : \"\" , \"datastore\" : \"\" , \"cpus\" : \"\" , \"ram\" : \"\" , \"network\" : \"\" , \"guest_os\" : \"\" , \"disk_size\" : \"\" , \"iso\" : \"\" , \"ip\" : \"\" , \"netmask\" : \"\" , \"gateway\" : \"\" , \"localIp\" : \"\" }, \"sensitive-variables\" : [ \"password\" ], \"builders\" : [ { \"type\" : \"vsphere-iso\" , \"vcenter_server\" : \"{{user `vcenter`}}\" , \"username\" : \"{{user `username`}}\" , \"password\" : \"{{user `password`}}\" , \"insecure_connection\" : \"{{user `insecure`}}\" , \"datacenter\" : \"{{user `datacenter`}}\" , \"cluster\" : \"{{user `cluster`}}\" , \"host\" : \"{{user `host`}}\" , \"datastore\" : \"{{user `datastore`}}\" , \"vm_name\" : \"{{user `vm_name`}}\" , \"CPUs\" : \"{{user `cpus`}}\" , \"ram\" : \"{{user `ram`}}\" , \"network\" : \"{{user `network`}}\" , \"network_card\" : \"vmxnet3\" , \"guest_os_type\" : \"{{user `guest_os`}}\" , \"disk_size\" : \"{{user `disk_size`}}\" , \"iso_paths\" : [ \"{{user `iso`}}\" ], \"cdrom_type\" : \"sata\" , \"ssh_username\" : \"jenkins\" , \"ssh_password\" : \"jenkins\" , \"http_directory\" : \"http\" , \"boot_wait\" : \"5s\" , \"boot_command\" : [ \"<up><wait><tab> ip={{user `ip`}} netmask={{user `netmask`}} gateway={{user `gateway`}} text ks=http://{{user `localIp`}}:{{ .HTTPPort}}/ks.cfg <enter>\" ] } ], \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"dnf -y update\" , \"dnf install -y java-1.8.0-openjdk-devel\" , \"sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo\" , \"sudo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key\" , \"sudo yum install jenkins -y\" ] } ] }","title":"Vsphere"},{"location":"devops/terraform/","text":"Terraform \u00b6 Commands \u00b6 Workspaces \u00b6 terraform workspace list Shell Script Problem Link # Use like this. With quotes echo \" $( terraform workspace list ) \"","title":"Terraform"},{"location":"devops/terraform/#terraform","text":"","title":"Terraform"},{"location":"devops/terraform/#commands","text":"","title":"Commands"},{"location":"devops/terraform/#workspaces","text":"terraform workspace list Shell Script Problem Link # Use like this. With quotes echo \" $( terraform workspace list ) \"","title":"Workspaces"},{"location":"platforms/aws/compute/ebs/","text":"Resize Root Volumes \u00b6 Manual method Create a snapshot of the current root volume Create a new volume from the snapshot with new storage specifications Select larger \"size\", thus increasing IOPS Must select the same availability zone as the source Stop the Instance Detach the original root volume Attach the new volume to the instance at same mount point (xvda) \"Automated\" method We can replace the launch configuration of an Auto Scaling Group In a true 3-tier application that is decoupled, we should then be able to terminate instances one by one to recreate them using new configuration Note: If the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity Monitoring \u00b6 VolumeReadOps Total number of IOPS in a specific period of time. To 1000 in 1 minute = 1000/60 IOPS - Change IOPS based off of usage. VolumeWriteOps Total number of IOPS in a specific period of time . TO 1000 in 1 minute = 1000/60 iops - Change IOPS based off of usage VolumeQueueLength The number of read/write operation requests waiting to process. We want close at 0. If higher than 0, increase the number of available IOPS VolumeReadBytes VolumeWriteBytes VolumeTotalReadTime VolumeTotalWriteTime VolumeIdleTime VolumeThroughputPercentage Provisioned IOPS SSD only *Determine the percentage of I/O operations delivered of the total IOPS provisioned for an EBS volume. During a write, it there are no other pending I/O requests in a minute, the metric value will be 100 percent. VolumeConsumedReadWriteOps Provisioned IOPS only BurstBalance (gp2, st1, and sc1 volumes only) Reach IOPS limit of your volume will result in: Will start to get your IO requests queuing Depending on your applications sensitivity to IOPS and latency, you may see your applications becaming slow","title":"EBS"},{"location":"platforms/aws/compute/ebs/#resize-root-volumes","text":"Manual method Create a snapshot of the current root volume Create a new volume from the snapshot with new storage specifications Select larger \"size\", thus increasing IOPS Must select the same availability zone as the source Stop the Instance Detach the original root volume Attach the new volume to the instance at same mount point (xvda) \"Automated\" method We can replace the launch configuration of an Auto Scaling Group In a true 3-tier application that is decoupled, we should then be able to terminate instances one by one to recreate them using new configuration Note: If the size of the volume was modified, extend the volume's file system to take advantage of the increased storage capacity","title":"Resize Root Volumes"},{"location":"platforms/aws/compute/ebs/#monitoring","text":"VolumeReadOps Total number of IOPS in a specific period of time. To 1000 in 1 minute = 1000/60 IOPS - Change IOPS based off of usage. VolumeWriteOps Total number of IOPS in a specific period of time . TO 1000 in 1 minute = 1000/60 iops - Change IOPS based off of usage VolumeQueueLength The number of read/write operation requests waiting to process. We want close at 0. If higher than 0, increase the number of available IOPS VolumeReadBytes VolumeWriteBytes VolumeTotalReadTime VolumeTotalWriteTime VolumeIdleTime VolumeThroughputPercentage Provisioned IOPS SSD only *Determine the percentage of I/O operations delivered of the total IOPS provisioned for an EBS volume. During a write, it there are no other pending I/O requests in a minute, the metric value will be 100 percent. VolumeConsumedReadWriteOps Provisioned IOPS only BurstBalance (gp2, st1, and sc1 volumes only) Reach IOPS limit of your volume will result in: Will start to get your IO requests queuing Depending on your applications sensitivity to IOPS and latency, you may see your applications becaming slow","title":"Monitoring"},{"location":"platforms/aws/compute/ec2/","text":"Restore Snapshots \u00b6 Read all blocks to eliminate penalty in production sudo dd if = /dev/xvdf of = /dev/null bs = 1M # Or sudo fio \u2013filename = /dev/xvdf \u2013rw = read \u2013bs = 128k \u2013iodepth = 32 \u2013ioengine = libaio \u2013direct = 1 \u2013name = volume-initialize Configure SSH Config \u00b6 Proxy SSH Recover Instance \u00b6 Recover Instance without console access How To Repair An AWS EC2 Instance Without Console Duplicate XFS UUIDs","title":"EC2"},{"location":"platforms/aws/compute/ec2/#restore-snapshots","text":"Read all blocks to eliminate penalty in production sudo dd if = /dev/xvdf of = /dev/null bs = 1M # Or sudo fio \u2013filename = /dev/xvdf \u2013rw = read \u2013bs = 128k \u2013iodepth = 32 \u2013ioengine = libaio \u2013direct = 1 \u2013name = volume-initialize","title":"Restore Snapshots"},{"location":"platforms/aws/compute/ec2/#configure-ssh-config","text":"Proxy SSH","title":"Configure SSH Config"},{"location":"platforms/aws/compute/ec2/#recover-instance","text":"Recover Instance without console access How To Repair An AWS EC2 Instance Without Console Duplicate XFS UUIDs","title":"Recover Instance"},{"location":"platforms/aws/compute/eks/","text":"Eks \u00b6 KubeConfig \u00b6 Create KubeConfig Users/Roles \u00b6 New User/Role Expose Services \u00b6 Expose","title":"EKS"},{"location":"platforms/aws/compute/eks/#eks","text":"","title":"Eks"},{"location":"platforms/aws/compute/eks/#kubeconfig","text":"Create KubeConfig","title":"KubeConfig"},{"location":"platforms/aws/compute/eks/#usersroles","text":"New User/Role","title":"Users/Roles"},{"location":"platforms/aws/compute/eks/#expose-services","text":"Expose","title":"Expose Services"},{"location":"platforms/aws/compute/elb/","text":"Monitoring \u00b6 ActiveConnectionCount HealthyHostCount, UnHealthyHostCount, HostCount BackendConnectionErrors The count of the number of connections that were NOT successfully established between the Load Balancer and the registered instance Average stat is most useful Will report errors for all AZ's Could indicate an issue with the web server HTTPCode_Backend_2XX,3XX,4XX,5XX Generated by registered instances HTTPCode_ELB_4XX,5XX Generated by Load Balancer ActiveFlowCount (Network Load Balancer) Metrics for performance Latency Measures the time elapsed (in seconds) after the request leaves the Load Balancer until the response is received Average stat is most useful Will report latency for all AZs Page Load Time RequestCount Number of requests completed/connections made during specified interval (1 or 5 mins) SurgeQueueLenght (Classic only) A count of the total number of requests that are pending submissions to a registered instance Max queue size is 1024 Additional requests will be rejected Scaling up instances to ensure that never increases beyond the maximum queue capacity SpilloverCount (Classic only) Number of requests rejected because the surge queue is full Direct consequence of a surge queue length increase Should monitor SpilloverCount and SurgeQueueLenght High numbers in these metrics can indicate a performance issue, need to scale infrastructure, etc","title":"ELB"},{"location":"platforms/aws/compute/elb/#monitoring","text":"ActiveConnectionCount HealthyHostCount, UnHealthyHostCount, HostCount BackendConnectionErrors The count of the number of connections that were NOT successfully established between the Load Balancer and the registered instance Average stat is most useful Will report errors for all AZ's Could indicate an issue with the web server HTTPCode_Backend_2XX,3XX,4XX,5XX Generated by registered instances HTTPCode_ELB_4XX,5XX Generated by Load Balancer ActiveFlowCount (Network Load Balancer) Metrics for performance Latency Measures the time elapsed (in seconds) after the request leaves the Load Balancer until the response is received Average stat is most useful Will report latency for all AZs Page Load Time RequestCount Number of requests completed/connections made during specified interval (1 or 5 mins) SurgeQueueLenght (Classic only) A count of the total number of requests that are pending submissions to a registered instance Max queue size is 1024 Additional requests will be rejected Scaling up instances to ensure that never increases beyond the maximum queue capacity SpilloverCount (Classic only) Number of requests rejected because the surge queue is full Direct consequence of a surge queue length increase Should monitor SpilloverCount and SurgeQueueLenght High numbers in these metrics can indicate a performance issue, need to scale infrastructure, etc","title":"Monitoring"},{"location":"platforms/aws/databases/dynamodb/","text":"Monitoring \u00b6 If an application\u2019s read or write requests exceed the provisioned throughput for a table, Amazon DynamoDB might throttle that request. When this happens, the request fails with an HTTP 400 code (Bad Request), accompanied by a ProvisionedThroughputExceededException High Availability \u00b6 Scalability Unlimited amount of storage Elasticity Increase additional IOPS for additional spikes in traffic. Drecrease that IOPS after spike We can increase or decrease read and write throughput capacity on demand As read requests increase, we can increase read throughput capacity As read requests slow down, we can decrease capacity","title":"DynamoDB"},{"location":"platforms/aws/databases/dynamodb/#monitoring","text":"If an application\u2019s read or write requests exceed the provisioned throughput for a table, Amazon DynamoDB might throttle that request. When this happens, the request fails with an HTTP 400 code (Bad Request), accompanied by a ProvisionedThroughputExceededException","title":"Monitoring"},{"location":"platforms/aws/databases/dynamodb/#high-availability","text":"Scalability Unlimited amount of storage Elasticity Increase additional IOPS for additional spikes in traffic. Drecrease that IOPS after spike We can increase or decrease read and write throughput capacity on demand As read requests increase, we can increase read throughput capacity As read requests slow down, we can decrease capacity","title":"High Availability"},{"location":"platforms/aws/databases/elasticache/","text":"Monitoring \u00b6 CPU Utilization \u00b6 MemCached Multi-threaded Can handle loads of up to 90%. if it exceeds 90% Increase the node family or add more nodes to cluster or read replicas Redis Not multi-threaded. To determine the point in which to scale, take 90 and divide by the number of cores Example: if you have 4 cpus, the threshold would be ( 90 / 4 = 22.5%) if this threshold is exceeded and the main workload is from read requests, scale the cache cluster out by adding read replicas. If the main workload is from write requests, AWS recommends scaling up by using a larger cache instance type Evictions \u00b6 An Eviction occurs when a new item is added and an old item must be removed due to lack of free space in the system Older items are removed to free up memory for new items Frequent evictions will decrease performance - Increase the node size (memory) No recommended setting. Choose the threshold based off of application requirements Memcached Scale UP ( increase the memory of existing nodes) OR Scale OUT ( add more nodes) Redis Scale Out ( add more read replicas) Swap Usage \u00b6 Swap usage is simply the amount of the Swap file that is used. Swap file (or paging file) is the amount of disk storage space reserved on disk if your computer runs out of ram. Typically the size of swap file = the size of the RAM. So if you have 4GB of RAM, you will have 4GB SWAP file Memcached Affects performance if increased Should be close to 0 (should not exceed 50mb) If this exceeds 50 Mb you should increase the memcached_connections_overhead parameter Defines the amount of memory to be reserved for connections and other misc/overhead. Redis AWS suggest not setting a cloudwatch alarm as there is no suggested \"web service\" fix No SwapUsage metric, instead use reserved-memory CurrConnections \u00b6 MemCached & Redis No recommended setting, Chooses threshold based off of application requirements Increased CurrConnections could indicate a larger issue with your application OR the ELB may not be releasing connections (tied down) or is a large and sustained spike in the number of concurrent connections Remember to set an alarm on the number of concurrent connections for elasticcache","title":"Elasticache"},{"location":"platforms/aws/databases/elasticache/#monitoring","text":"","title":"Monitoring"},{"location":"platforms/aws/databases/elasticache/#cpu-utilization","text":"MemCached Multi-threaded Can handle loads of up to 90%. if it exceeds 90% Increase the node family or add more nodes to cluster or read replicas Redis Not multi-threaded. To determine the point in which to scale, take 90 and divide by the number of cores Example: if you have 4 cpus, the threshold would be ( 90 / 4 = 22.5%) if this threshold is exceeded and the main workload is from read requests, scale the cache cluster out by adding read replicas. If the main workload is from write requests, AWS recommends scaling up by using a larger cache instance type","title":"CPU Utilization"},{"location":"platforms/aws/databases/elasticache/#evictions","text":"An Eviction occurs when a new item is added and an old item must be removed due to lack of free space in the system Older items are removed to free up memory for new items Frequent evictions will decrease performance - Increase the node size (memory) No recommended setting. Choose the threshold based off of application requirements Memcached Scale UP ( increase the memory of existing nodes) OR Scale OUT ( add more nodes) Redis Scale Out ( add more read replicas)","title":"Evictions"},{"location":"platforms/aws/databases/elasticache/#swap-usage","text":"Swap usage is simply the amount of the Swap file that is used. Swap file (or paging file) is the amount of disk storage space reserved on disk if your computer runs out of ram. Typically the size of swap file = the size of the RAM. So if you have 4GB of RAM, you will have 4GB SWAP file Memcached Affects performance if increased Should be close to 0 (should not exceed 50mb) If this exceeds 50 Mb you should increase the memcached_connections_overhead parameter Defines the amount of memory to be reserved for connections and other misc/overhead. Redis AWS suggest not setting a cloudwatch alarm as there is no suggested \"web service\" fix No SwapUsage metric, instead use reserved-memory","title":"Swap Usage"},{"location":"platforms/aws/databases/elasticache/#currconnections","text":"MemCached & Redis No recommended setting, Chooses threshold based off of application requirements Increased CurrConnections could indicate a larger issue with your application OR the ELB may not be releasing connections (tied down) or is a large and sustained spike in the number of concurrent connections Remember to set an alarm on the number of concurrent connections for elasticcache","title":"CurrConnections"},{"location":"platforms/aws/databases/rds/","text":"Monitoring \u00b6 BinLogDiskUsage Amount of disk space occupied by binary logs on the primary database. Applies to MySQL Read Replicas and is measured in bytes DatabaseConnections Number of database connections in use DiskQueueDepth The number of outstanding read and write requests waiting to access the disk FreeStorageSpace Amount of available storage space FreeableMemory Amount of available RAM NetworkReceiveThroughput Inbound network traffic on the DB instance that includes both customer database traffic and Amazon RDS traffic used for monitoring and replication NetworkTransmitThroughput Outbound network traffic on the DB instance that includes both customer database traffic and Amazon RDS traffic used for monitoring and replication ReadIOPS/WriteIOPS Average number of disk I/O operations per second Use this to determine storage type changes ReadLatency /WriteLatency Average amount of time taken per disk I/O operation More IOPS needed ReadThroughput/WriteThroughput Average number of bytes read from disk per second ReplicaLag Amount of time a Read Replica DB instance lags behind the source DB instance. Applies to MySQL, MariaDB, and PostgreSQL Read Replicas SwapUsage Amount of swap space used on the DB instance If increase - low or no available ram High CPU or RAM consumption High values for CPU or RAM consumption might be appropriate, provided that they are within an expected range. Disk space consumption Investigate disk space consumption if there is consistently less than 15 percent of available free space Network traffic Investigate network traffic if throughput is consistently lower than expected Database connections If an increase of usage causes performance issues, consider limiting the number of available database connections. The best number of user connections for a DB instance will depend on the instance class and the complexity of the operations performed. IOPS metrics The expected values for IOPS metrics depend on disk specification and server configuration. Establish a baseline to know what utilization is typical. For best IOPS performance, make sure that the typical working set will fit into memory to minimize read and write operations. Aurora 100% CPU utilization Is it writes causing the issue? if so scale up (increase instance size) Is it reads causing the issue? If so scale out (increase the number of read replicas)","title":"RDS"},{"location":"platforms/aws/databases/rds/#monitoring","text":"BinLogDiskUsage Amount of disk space occupied by binary logs on the primary database. Applies to MySQL Read Replicas and is measured in bytes DatabaseConnections Number of database connections in use DiskQueueDepth The number of outstanding read and write requests waiting to access the disk FreeStorageSpace Amount of available storage space FreeableMemory Amount of available RAM NetworkReceiveThroughput Inbound network traffic on the DB instance that includes both customer database traffic and Amazon RDS traffic used for monitoring and replication NetworkTransmitThroughput Outbound network traffic on the DB instance that includes both customer database traffic and Amazon RDS traffic used for monitoring and replication ReadIOPS/WriteIOPS Average number of disk I/O operations per second Use this to determine storage type changes ReadLatency /WriteLatency Average amount of time taken per disk I/O operation More IOPS needed ReadThroughput/WriteThroughput Average number of bytes read from disk per second ReplicaLag Amount of time a Read Replica DB instance lags behind the source DB instance. Applies to MySQL, MariaDB, and PostgreSQL Read Replicas SwapUsage Amount of swap space used on the DB instance If increase - low or no available ram High CPU or RAM consumption High values for CPU or RAM consumption might be appropriate, provided that they are within an expected range. Disk space consumption Investigate disk space consumption if there is consistently less than 15 percent of available free space Network traffic Investigate network traffic if throughput is consistently lower than expected Database connections If an increase of usage causes performance issues, consider limiting the number of available database connections. The best number of user connections for a DB instance will depend on the instance class and the complexity of the operations performed. IOPS metrics The expected values for IOPS metrics depend on disk specification and server configuration. Establish a baseline to know what utilization is typical. For best IOPS performance, make sure that the typical working set will fit into memory to minimize read and write operations. Aurora 100% CPU utilization Is it writes causing the issue? if so scale up (increase instance size) Is it reads causing the issue? If so scale out (increase the number of read replicas)","title":"Monitoring"},{"location":"platforms/aws/databases/redshift/","text":"Monitoring \u00b6 Database Audit logging Events and Notifications Performance CPU utilization Latency Throughput Redshift also provides query and load performance data to help monitor database activity in a cluster","title":"Redshift"},{"location":"platforms/aws/databases/redshift/#monitoring","text":"Database Audit logging Events and Notifications Performance CPU utilization Latency Throughput Redshift also provides query and load performance data to help monitor database activity in a cluster","title":"Monitoring"},{"location":"platforms/aws/devops/cloudformation/","text":"CDK \u00b6 Workshop app.py \u00b6 app = core . App () SecurityGroupsStack ( app , \"security-groups\" , env = { 'account' : os . environ [ 'CDK_DEFAULT_ACCOUNT' ], 'region' : os . environ [ 'CDK_DEFAULT_REGION' ] }) app . synth () Stack.py \u00b6 # Reference a outside VPC vpc = ec2 . Vpc . from_lookup ( self , id = \"VPC\" , vpc_id = config [ 'VPC_ID' ]) CloudFormation \u00b6 Deploy \u00b6 aws cloudformation package --template-file template.yaml --s3-bucket ${ bucket } --output-template-file output-template.yaml aws cloudformation deploy --template-file output-template.yaml --capabilities CAPABILITY_IAM --stack-name ${ stack_name } Delete Stack \u00b6 aws cloudformation delete-stack --stack-name ${ stack_name } Init \u00b6 Samples Logs are stored in: /var/log/cfn-init* /var/log/cloud-init* AWSTemplateFormatVersion : \"2010-09-09\" Description : Cloud Formation Template to create Jenkins Cluster Parameters : VPC : Type : AWS::EC2::VPC::Id MasterSubnet : Type : AWS::EC2::Subnet::Id MasterInstanceType : Type : String Default : t2.micro KeyPair : Type : AWS::EC2::KeyPair::KeyName Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Network Configuration\" Parameters : - VPC - Label : default : \"Master Jenkins Configuration\" Parameters : - MasterSubnet - MasterInstanceType - Label : default : \"Common Configuration\" Parameters : - KeyPair ParameterLabels : VPC : default : \"Which VPC should this be deployed to?\" MasterSubnet : default : \"Which Subnet should this be deployed to?\" MasterInstanceType : default : \"Which Instance type should this use?\" KeyPair : default : \"Which KeyPair should this use?\" Mappings : AMI : eu-west-1 : \"HVM64\" : \"ami-04d5cc9b88f9d1d39\" Resources : Master : Type : AWS::EC2::Instance Metadata : AWS::CloudFormation::Init : configSets : full_install : - prepare - install prepare : files : /etc/yum.repos.d/jenkins.repo : source : http://pkg.jenkins-ci.org/redhat/jenkins.repo commands : jenkins-import-key : command : rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key test : rpm -q gpg-pubkey-d50582e6-4a3feef6 | grep \"not installed\" install : packages : yum : java-1.8.0-openjdk-devel : [] jenkins : [] services : sysvinit : jenkins : enabled : \"true\" ensureRunning : \"true\" Properties : ImageId : !FindInMap [ AMI , !Ref \"AWS::Region\" , HVM64 ] InstanceType : !Ref MasterInstanceType KeyName : !Ref KeyPair SubnetId : !Ref MasterSubnet UserData : 'Fn::Base64' : !Sub | #!/bin/bash -x yum update -y /opt/aws/bin/cfn-init -v -c full_install --stack ${AWS::StackName} --resource Master --region ${AWS::Region} /opt/aws/bin/cfn-signal -e 0 --stack ${AWS::StackName} --resource Master --region ${AWS::Region} Tags : - Key : Name Value : Jenkins-Master","title":"Cloudformation"},{"location":"platforms/aws/devops/cloudformation/#cdk","text":"Workshop","title":"CDK"},{"location":"platforms/aws/devops/cloudformation/#apppy","text":"app = core . App () SecurityGroupsStack ( app , \"security-groups\" , env = { 'account' : os . environ [ 'CDK_DEFAULT_ACCOUNT' ], 'region' : os . environ [ 'CDK_DEFAULT_REGION' ] }) app . synth ()","title":"app.py"},{"location":"platforms/aws/devops/cloudformation/#stackpy","text":"# Reference a outside VPC vpc = ec2 . Vpc . from_lookup ( self , id = \"VPC\" , vpc_id = config [ 'VPC_ID' ])","title":"Stack.py"},{"location":"platforms/aws/devops/cloudformation/#cloudformation","text":"","title":"CloudFormation"},{"location":"platforms/aws/devops/cloudformation/#deploy","text":"aws cloudformation package --template-file template.yaml --s3-bucket ${ bucket } --output-template-file output-template.yaml aws cloudformation deploy --template-file output-template.yaml --capabilities CAPABILITY_IAM --stack-name ${ stack_name }","title":"Deploy"},{"location":"platforms/aws/devops/cloudformation/#delete-stack","text":"aws cloudformation delete-stack --stack-name ${ stack_name }","title":"Delete Stack"},{"location":"platforms/aws/devops/cloudformation/#init","text":"Samples Logs are stored in: /var/log/cfn-init* /var/log/cloud-init* AWSTemplateFormatVersion : \"2010-09-09\" Description : Cloud Formation Template to create Jenkins Cluster Parameters : VPC : Type : AWS::EC2::VPC::Id MasterSubnet : Type : AWS::EC2::Subnet::Id MasterInstanceType : Type : String Default : t2.micro KeyPair : Type : AWS::EC2::KeyPair::KeyName Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Network Configuration\" Parameters : - VPC - Label : default : \"Master Jenkins Configuration\" Parameters : - MasterSubnet - MasterInstanceType - Label : default : \"Common Configuration\" Parameters : - KeyPair ParameterLabels : VPC : default : \"Which VPC should this be deployed to?\" MasterSubnet : default : \"Which Subnet should this be deployed to?\" MasterInstanceType : default : \"Which Instance type should this use?\" KeyPair : default : \"Which KeyPair should this use?\" Mappings : AMI : eu-west-1 : \"HVM64\" : \"ami-04d5cc9b88f9d1d39\" Resources : Master : Type : AWS::EC2::Instance Metadata : AWS::CloudFormation::Init : configSets : full_install : - prepare - install prepare : files : /etc/yum.repos.d/jenkins.repo : source : http://pkg.jenkins-ci.org/redhat/jenkins.repo commands : jenkins-import-key : command : rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key test : rpm -q gpg-pubkey-d50582e6-4a3feef6 | grep \"not installed\" install : packages : yum : java-1.8.0-openjdk-devel : [] jenkins : [] services : sysvinit : jenkins : enabled : \"true\" ensureRunning : \"true\" Properties : ImageId : !FindInMap [ AMI , !Ref \"AWS::Region\" , HVM64 ] InstanceType : !Ref MasterInstanceType KeyName : !Ref KeyPair SubnetId : !Ref MasterSubnet UserData : 'Fn::Base64' : !Sub | #!/bin/bash -x yum update -y /opt/aws/bin/cfn-init -v -c full_install --stack ${AWS::StackName} --resource Master --region ${AWS::Region} /opt/aws/bin/cfn-signal -e 0 --stack ${AWS::StackName} --resource Master --region ${AWS::Region} Tags : - Key : Name Value : Jenkins-Master","title":"Init"},{"location":"platforms/aws/devops/cloudwatch/","text":"Filter Events \u00b6 { ( ( $.eventSource = \"iam.amazonaws.com\" ) && ( $.requestParameters.userName = \"sceptre\" ) ) } { ( $.eventSource = \"iam.amazonaws.com\" ) } { ( ( $.eventSource = \"iam.amazonaws.com\" ) && (( $.eventName = \"Add*\" ) || ( $.eventName = \"Attach*\" ) || ( $.eventName = \"Change*\" ) || ( $.eventName = \"Create*\" ) || ( $.eventName = \"Deactivate*\" ) || ( $.eventName = \"Delete*\" ) || ( $.eventName = \"Detach*\" ) || ( $.eventName = \"Enable*\" ) || ( $.eventName = \"Put*\" ) || ( $.eventName = \"Remove*\" ) || ( $.eventName = \"Set*\" ) || ( $.eventName = \"Update*\" ) || ( $.eventName = \"Upload*\" )) ) } { ( ( $.eventSource = \"iam.amazonaws.com\" ) && (( $.eventName = \"Put*Policy\" ) || ( $.eventName = \"Attach*\" ) || ( $.eventName = \"Detach*\" ) || ( $.eventName = \"Create*\" ) || ( $.eventName = \"Update*\" ) || ( $.eventName = \"Upload*\" ) || ( $.eventName = \"Delete*\" ) || ( $.eventName = \"Remove*\" ) || ( $.eventName = \"Set*\" )) ) }","title":"CloudWatch"},{"location":"platforms/aws/devops/cloudwatch/#filter-events","text":"{ ( ( $.eventSource = \"iam.amazonaws.com\" ) && ( $.requestParameters.userName = \"sceptre\" ) ) } { ( $.eventSource = \"iam.amazonaws.com\" ) } { ( ( $.eventSource = \"iam.amazonaws.com\" ) && (( $.eventName = \"Add*\" ) || ( $.eventName = \"Attach*\" ) || ( $.eventName = \"Change*\" ) || ( $.eventName = \"Create*\" ) || ( $.eventName = \"Deactivate*\" ) || ( $.eventName = \"Delete*\" ) || ( $.eventName = \"Detach*\" ) || ( $.eventName = \"Enable*\" ) || ( $.eventName = \"Put*\" ) || ( $.eventName = \"Remove*\" ) || ( $.eventName = \"Set*\" ) || ( $.eventName = \"Update*\" ) || ( $.eventName = \"Upload*\" )) ) } { ( ( $.eventSource = \"iam.amazonaws.com\" ) && (( $.eventName = \"Put*Policy\" ) || ( $.eventName = \"Attach*\" ) || ( $.eventName = \"Detach*\" ) || ( $.eventName = \"Create*\" ) || ( $.eventName = \"Update*\" ) || ( $.eventName = \"Upload*\" ) || ( $.eventName = \"Delete*\" ) || ( $.eventName = \"Remove*\" ) || ( $.eventName = \"Set*\" )) ) }","title":"Filter Events"},{"location":"platforms/aws/migrate/aspnet/","text":"Lab Link \u00b6 Link","title":"Aspnet"},{"location":"platforms/aws/migrate/aspnet/#lab-link","text":"Link","title":"Lab Link"},{"location":"platforms/aws/migrate/sqlserver/","text":"Lab Link \u00b6 Link","title":"SqlServer"},{"location":"platforms/aws/migrate/sqlserver/#lab-link","text":"Link","title":"Lab Link"},{"location":"platforms/aws/migrate/vmware/","text":"VMWare \u00b6 VM Import \u00b6 AWS Link Create Ova File \u00b6 Install ovf tool ovftool Build the image with the new version bundle file downloaded from VMWare site docker run --rm -it -v $( pwd ) :/tmp ovftool --noSSLVerify vi://<user>@<ip>/LAB/vm/aws /tmp/aws.ova AWS PreRequisites \u00b6 Create vmimport role Documentation The documentation has a error in trust policy \"sts:Externalid\": \"vmimport\" should be \"sts:ExternalId\": \"vmimport\" Error explain Upload to S3 Use Multipart Upload MultiPart Upload Import Image \u00b6 aws ec2 import-image --description \"My server VM\" --disk-containers \"file://containers.json\" --region eu-west-1 # Monitor aws ec2 describe-import-image-tasks --import-task-ids ${ TASK_ID } --region eu-west-1 Containers.json file [ { \"Description\" : \"My Server OVA\" , \"Format\" : \"ova\" , \"UserBucket\" : { \"S3Bucket\" : \"fsantos-vmware-ova-templates\" , \"S3Key\" : \"aws.ova\" } }]","title":"VmWare"},{"location":"platforms/aws/migrate/vmware/#vmware","text":"","title":"VMWare"},{"location":"platforms/aws/migrate/vmware/#vm-import","text":"AWS Link","title":"VM Import"},{"location":"platforms/aws/migrate/vmware/#create-ova-file","text":"Install ovf tool ovftool Build the image with the new version bundle file downloaded from VMWare site docker run --rm -it -v $( pwd ) :/tmp ovftool --noSSLVerify vi://<user>@<ip>/LAB/vm/aws /tmp/aws.ova","title":"Create Ova File"},{"location":"platforms/aws/migrate/vmware/#aws-prerequisites","text":"Create vmimport role Documentation The documentation has a error in trust policy \"sts:Externalid\": \"vmimport\" should be \"sts:ExternalId\": \"vmimport\" Error explain Upload to S3 Use Multipart Upload MultiPart Upload","title":"AWS PreRequisites"},{"location":"platforms/aws/migrate/vmware/#import-image","text":"aws ec2 import-image --description \"My server VM\" --disk-containers \"file://containers.json\" --region eu-west-1 # Monitor aws ec2 describe-import-image-tasks --import-task-ids ${ TASK_ID } --region eu-west-1 Containers.json file [ { \"Description\" : \"My Server OVA\" , \"Format\" : \"ova\" , \"UserBucket\" : { \"S3Bucket\" : \"fsantos-vmware-ova-templates\" , \"S3Key\" : \"aws.ova\" } }]","title":"Import Image"},{"location":"platforms/aws/migrate/windows/","text":"Migrate Windows Workloads \u00b6 Applications \u00b6 CloudEndure AWS Server Migration Server -> Create AMIS Lab - Trust Relationship between AWS Managed AD and On-Premise AD with FSX \u00b6 Lab Guide AWS Support Contact: Hans Moser - hansmose@amazon.ch Hans Moser - Info About D: Drive Regarding D: Drive: In our \u2018source Fileserver\u2019 we have the files on C:, hence the first export also include the directory paths to C: But on FSX, our target, the data drive is D: So to make the import work, we\u2019ve to change the paths, so that we target the correct folder paths on the destination (our FSX). Cloudformation \u00b6 Parameters \u00b6 Key Value ADServer1InstanceType m5.large ADServer1NetBIOSName DC1 ADServer1PrivateIP 10.0.0.10 ASGInstanceType m5.large ASGSize 2 AvailabilityZones us-west-2a,us-west-2b DemoInstances No DeployManagedAD Yes DomainAdminPassword **** DomainAdminUser Admin DomainDNSName example.com DomainNetBIOSName example ManagedADEdition Standard ManagedDomainDNSName managedexample.com ManagedDomainNetBIOSName managedexample NumberOfAZs 2 PrivateSubnet1CIDR 10.0.0.0/19 PrivateSubnet2CIDR 10.0.32.0/19 PrivateSubnet3CIDR - PublicSubnet1CIDR 10.0.128.0/20 PublicSubnet2CIDR 10.0.144.0/20 PublicSubnet3CIDR - QSS3BucketName alpublic QSS3KeyPrefix quickstart-microsoft-activedirectory/ SrvInstanceType t3.large VPCCIDR 10.0.0.0/16 VPCStack \u00b6 { \"AWSTemplateFormatVersion\" : \"2010-09-09\" , \"Description\" : \"This template creates a Multi-AZ, multi-subnet VPC infrastructure with managed NAT gateways in the public subnet for each Availability Zone. You can also create additional private subnets with dedicated custom network access control lists (ACLs). If you deploy the Quick Start in a region that doesn't support NAT gateways, NAT instances are deployed instead. **WARNING** This template creates AWS resources. You will be billed for the AWS resources used if you create a stack from this template. QS(0027)\" , \"Metadata\" : { \"AWS::CloudFormation::Interface\" : { \"ParameterGroups\" : [ { \"Label\" : { \"default\" : \"Availability Zone Configuration\" }, \"Parameters\" : [ \"AvailabilityZones\" , \"NumberOfAZs\" ] }, { \"Label\" : { \"default\" : \"Network Configuration\" }, \"Parameters\" : [ \"VPCCIDR\" , \"PublicSubnet1CIDR\" , \"PublicSubnet2CIDR\" , \"PublicSubnet3CIDR\" , \"PublicSubnet4CIDR\" , \"PublicSubnetTag1\" , \"PublicSubnetTag2\" , \"PublicSubnetTag3\" , \"CreatePrivateSubnets\" , \"PrivateSubnet1ACIDR\" , \"PrivateSubnet2ACIDR\" , \"PrivateSubnet3ACIDR\" , \"PrivateSubnet4ACIDR\" , \"PrivateSubnetATag1\" , \"PrivateSubnetATag2\" , \"PrivateSubnetATag3\" , \"CreateAdditionalPrivateSubnets\" , \"PrivateSubnet1BCIDR\" , \"PrivateSubnet2BCIDR\" , \"PrivateSubnet3BCIDR\" , \"PrivateSubnet4BCIDR\" , \"PrivateSubnetBTag1\" , \"PrivateSubnetBTag2\" , \"PrivateSubnetBTag3\" , \"VPCTenancy\" ] }, { \"Label\" : { \"default\" : \"Deprecated: NAT Instance Configuration\" }, \"Parameters\" : [ \"KeyPairName\" , \"NATInstanceType\" ] } ], \"ParameterLabels\" : { \"AvailabilityZones\" : { \"default\" : \"Availability Zones\" }, \"CreateAdditionalPrivateSubnets\" : { \"default\" : \"Create additional private subnets with dedicated network ACLs\" }, \"CreatePrivateSubnets\" : { \"default\" : \"Create private subnets\" }, \"KeyPairName\" : { \"default\" : \"Deprecated: Key pair name\" }, \"NATInstanceType\" : { \"default\" : \"Deprecated: NAT instance type\" }, \"NumberOfAZs\" : { \"default\" : \"Number of Availability Zones\" }, \"PrivateSubnet1ACIDR\" : { \"default\" : \"Private subnet 1A CIDR\" }, \"PrivateSubnet1BCIDR\" : { \"default\" : \"Private subnet 1B with dedicated network ACL CIDR\" }, \"PrivateSubnet2ACIDR\" : { \"default\" : \"Private subnet 2A CIDR\" }, \"PrivateSubnet2BCIDR\" : { \"default\" : \"Private subnet 2B with dedicated network ACL CIDR\" }, \"PrivateSubnet3ACIDR\" : { \"default\" : \"Private subnet 3A CIDR\" }, \"PrivateSubnet3BCIDR\" : { \"default\" : \"Private subnet 3B with dedicated network ACL CIDR\" }, \"PrivateSubnet4ACIDR\" : { \"default\" : \"Private subnet 4A CIDR\" }, \"PrivateSubnet4BCIDR\" : { \"default\" : \"Private subnet 4B with dedicated network ACL CIDR\" }, \"PrivateSubnetATag1\" : { \"default\" : \"Tag for Private A Subnets\" }, \"PrivateSubnetATag2\" : { \"default\" : \"Tag for Private A Subnets\" }, \"PrivateSubnetATag3\" : { \"default\" : \"Tag for Private A Subnets\" }, \"PrivateSubnetBTag1\" : { \"default\" : \"Tag for Private B Subnets\" }, \"PrivateSubnetBTag2\" : { \"default\" : \"Tag for Private B Subnets\" }, \"PrivateSubnetBTag3\" : { \"default\" : \"Tag for Private B Subnets\" }, \"PublicSubnet1CIDR\" : { \"default\" : \"Public subnet 1 CIDR\" }, \"PublicSubnet2CIDR\" : { \"default\" : \"Public subnet 2 CIDR\" }, \"PublicSubnet3CIDR\" : { \"default\" : \"Public subnet 3 CIDR\" }, \"PublicSubnet4CIDR\" : { \"default\" : \"Public subnet 4 CIDR\" }, \"PublicSubnetTag1\" : { \"default\" : \"Tag for Public Subnets\" }, \"PublicSubnetTag2\" : { \"default\" : \"Tag for Public Subnets\" }, \"PublicSubnetTag3\" : { \"default\" : \"Tag for Public Subnets\" }, \"VPCCIDR\" : { \"default\" : \"VPC CIDR\" }, \"VPCTenancy\" : { \"default\" : \"VPC Tenancy\" } } } }, \"Parameters\" : { \"AvailabilityZones\" : { \"Description\" : \"List of Availability Zones to use for the subnets in the VPC. Note: The logical order is preserved.\" , \"Type\" : \"List<AWS::EC2::AvailabilityZone::Name>\" }, \"CreateAdditionalPrivateSubnets\" : { \"AllowedValues\" : [ \"true\" , \"false\" ], \"Default\" : \"false\" , \"Description\" : \"Set to true to create a network ACL protected subnet in each Availability Zone. If false, the CIDR parameters for those subnets will be ignored. If true, it also requires that the 'Create private subnets' parameter is also true to have any effect.\" , \"Type\" : \"String\" }, \"CreatePrivateSubnets\" : { \"AllowedValues\" : [ \"true\" , \"false\" ], \"Default\" : \"true\" , \"Description\" : \"Set to false to create only public subnets. If false, the CIDR parameters for ALL private subnets will be ignored.\" , \"Type\" : \"String\" }, \"KeyPairName\" : { \"Description\" : \"Deprecated. NAT gateways are now supported in all regions.\" , \"Type\" : \"String\" , \"Default\" : \"deprecated\" }, \"NATInstanceType\" : { \"Default\" : \"deprecated\" , \"Description\" : \"Deprecated. NAT gateways are now supported in all regions.\" , \"Type\" : \"String\" }, \"NumberOfAZs\" : { \"AllowedValues\" : [ \"2\" , \"3\" , \"4\" ], \"Default\" : \"2\" , \"Description\" : \"Number of Availability Zones to use in the VPC. This must match your selections in the list of Availability Zones parameter.\" , \"Type\" : \"String\" }, \"PrivateSubnet1ACIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.0.0/19\" , \"Description\" : \"CIDR block for private subnet 1A located in Availability Zone 1\" , \"Type\" : \"String\" }, \"PrivateSubnet1BCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.192.0/21\" , \"Description\" : \"CIDR block for private subnet 1B with dedicated network ACL located in Availability Zone 1\" , \"Type\" : \"String\" }, \"PrivateSubnet2ACIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.32.0/19\" , \"Description\" : \"CIDR block for private subnet 2A located in Availability Zone 2\" , \"Type\" : \"String\" }, \"PrivateSubnet2BCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.200.0/21\" , \"Description\" : \"CIDR block for private subnet 2B with dedicated network ACL located in Availability Zone 2\" , \"Type\" : \"String\" }, \"PrivateSubnet3ACIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.64.0/19\" , \"Description\" : \"CIDR block for private subnet 3A located in Availability Zone 3\" , \"Type\" : \"String\" }, \"PrivateSubnet3BCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.208.0/21\" , \"Description\" : \"CIDR block for private subnet 3B with dedicated network ACL located in Availability Zone 3\" , \"Type\" : \"String\" }, \"PrivateSubnet4ACIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.96.0/19\" , \"Description\" : \"CIDR block for private subnet 4A located in Availability Zone 4\" , \"Type\" : \"String\" }, \"PrivateSubnet4BCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.216.0/21\" , \"Description\" : \"CIDR block for private subnet 4B with dedicated network ACL located in Availability Zone 4\" , \"Type\" : \"String\" }, \"PrivateSubnetATag1\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"Network=Private\" , \"Description\" : \"tag to add to private subnets A, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetATag2\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to private subnets A, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetATag3\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to private subnets A, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetBTag1\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"Network=Private\" , \"Description\" : \"tag to add to private subnets B, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetBTag2\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to private subnets B, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetBTag3\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to private subnets B, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PublicSubnet1CIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.128.0/20\" , \"Description\" : \"CIDR block for the public DMZ subnet 1 located in Availability Zone 1\" , \"Type\" : \"String\" }, \"PublicSubnet2CIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.144.0/20\" , \"Description\" : \"CIDR block for the public DMZ subnet 2 located in Availability Zone 2\" , \"Type\" : \"String\" }, \"PublicSubnet3CIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.160.0/20\" , \"Description\" : \"CIDR block for the public DMZ subnet 3 located in Availability Zone 3\" , \"Type\" : \"String\" }, \"PublicSubnet4CIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.176.0/20\" , \"Description\" : \"CIDR block for the public DMZ subnet 4 located in Availability Zone 4\" , \"Type\" : \"String\" }, \"PublicSubnetTag1\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"Network=Public\" , \"Description\" : \"tag to add to public subnets, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PublicSubnetTag2\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to public subnets, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PublicSubnetTag3\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to public subnets, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"VPCCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.0.0/16\" , \"Description\" : \"CIDR block for the VPC\" , \"Type\" : \"String\" }, \"VPCTenancy\" : { \"AllowedValues\" : [ \"default\" , \"dedicated\" ], \"Default\" : \"default\" , \"Description\" : \"The allowed tenancy of instances launched into the VPC\" , \"Type\" : \"String\" } }, \"Conditions\" : { \"3AZCondition\" : { \"Fn::Or\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"NumberOfAZs\" }, \"3\" ] }, { \"Condition\" : \"4AZCondition\" } ] }, \"4AZCondition\" : { \"Fn::Equals\" : [ { \"Ref\" : \"NumberOfAZs\" }, \"4\" ] }, \"AdditionalPrivateSubnetsCondition\" : { \"Fn::And\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"CreatePrivateSubnets\" }, \"true\" ] }, { \"Fn::Equals\" : [ { \"Ref\" : \"CreateAdditionalPrivateSubnets\" }, \"true\" ] } ] }, \"AdditionalPrivateSubnets&3AZCondition\" : { \"Fn::And\" : [ { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" }, { \"Condition\" : \"3AZCondition\" } ] }, \"AdditionalPrivateSubnets&4AZCondition\" : { \"Fn::And\" : [ { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" }, { \"Condition\" : \"4AZCondition\" } ] }, \"GovCloudCondition\" : { \"Fn::Equals\" : [ { \"Ref\" : \"AWS::Region\" }, \"us-gov-west-1\" ] }, \"NVirginiaRegionCondition\" : { \"Fn::Equals\" : [ { \"Ref\" : \"AWS::Region\" }, \"us-east-1\" ] }, \"PrivateSubnetsCondition\" : { \"Fn::Equals\" : [ { \"Ref\" : \"CreatePrivateSubnets\" }, \"true\" ] }, \"PrivateSubnets&3AZCondition\" : { \"Fn::And\" : [ { \"Condition\" : \"PrivateSubnetsCondition\" }, { \"Condition\" : \"3AZCondition\" } ] }, \"PrivateSubnets&4AZCondition\" : { \"Fn::And\" : [ { \"Condition\" : \"PrivateSubnetsCondition\" }, { \"Condition\" : \"4AZCondition\" } ] }, \"PrivateSubnetATag1Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetATag1\" }, \"\" ] } ] }, \"PrivateSubnetATag2Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetATag2\" }, \"\" ] } ] }, \"PrivateSubnetATag3Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetATag3\" }, \"\" ] } ] }, \"PrivateSubnetBTag1Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetBTag1\" }, \"\" ] } ] }, \"PrivateSubnetBTag2Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetBTag2\" }, \"\" ] } ] }, \"PrivateSubnetBTag3Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetBTag3\" }, \"\" ] } ] }, \"PublicSubnetTag1Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PublicSubnetTag1\" }, \"\" ] } ] }, \"PublicSubnetTag2Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PublicSubnetTag2\" }, \"\" ] } ] }, \"PublicSubnetTag3Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PublicSubnetTag3\" }, \"\" ] } ] } }, \"Resources\" : { \"DHCPOptions\" : { \"Type\" : \"AWS::EC2::DHCPOptions\" , \"Properties\" : { \"DomainName\" : { \"Fn::If\" : [ \"NVirginiaRegionCondition\" , \"ec2.internal\" , { \"Fn::Sub\" : \"${AWS::Region}.compute.internal\" } ] }, \"DomainNameServers\" : [ \"AmazonProvidedDNS\" ] } }, \"VPC\" : { \"Type\" : \"AWS::EC2::VPC\" , \"Properties\" : { \"CidrBlock\" : { \"Ref\" : \"VPCCIDR\" }, \"InstanceTenancy\" : { \"Ref\" : \"VPCTenancy\" }, \"EnableDnsSupport\" : true , \"EnableDnsHostnames\" : true , \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : { \"Ref\" : \"AWS::StackName\" } } ] } }, \"VPCDHCPOptionsAssociation\" : { \"Type\" : \"AWS::EC2::VPCDHCPOptionsAssociation\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"DhcpOptionsId\" : { \"Ref\" : \"DHCPOptions\" } } }, \"InternetGateway\" : { \"Type\" : \"AWS::EC2::InternetGateway\" , \"Properties\" : { \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : { \"Ref\" : \"AWS::StackName\" } } ] } }, \"VPCGatewayAttachment\" : { \"Type\" : \"AWS::EC2::VPCGatewayAttachment\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"InternetGatewayId\" : { \"Ref\" : \"InternetGateway\" } } }, \"PrivateSubnet1A\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet1ACIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"0\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 1A\" }, { \"Fn::If\" : [ \"PrivateSubnetATag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet1B\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet1BCIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"0\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 1B\" }, { \"Fn::If\" : [ \"PrivateSubnetBTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet2A\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet2ACIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"1\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 2A\" }, { \"Fn::If\" : [ \"PrivateSubnetATag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet2B\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet2BCIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"1\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 2B\" }, { \"Fn::If\" : [ \"PrivateSubnetBTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet3A\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet3ACIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"2\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 3A\" }, { \"Fn::If\" : [ \"PrivateSubnetATag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet3B\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet3BCIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"2\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 3B\" }, { \"Fn::If\" : [ \"PrivateSubnetBTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet4A\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet4ACIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"3\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 4A\" }, { \"Fn::If\" : [ \"PrivateSubnetATag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet4B\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet4BCIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"3\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 4B\" }, { \"Fn::If\" : [ \"PrivateSubnetBTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PublicSubnet1\" : { \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PublicSubnet1CIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"0\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public subnet 1\" }, { \"Fn::If\" : [ \"PublicSubnetTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"MapPublicIpOnLaunch\" : true } }, \"PublicSubnet2\" : { \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PublicSubnet2CIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"1\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public subnet 2\" }, { \"Fn::If\" : [ \"PublicSubnetTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"MapPublicIpOnLaunch\" : true } }, \"PublicSubnet3\" : { \"Condition\" : \"3AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PublicSubnet3CIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"2\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public subnet 3\" }, { \"Fn::If\" : [ \"PublicSubnetTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"MapPublicIpOnLaunch\" : true } }, \"PublicSubnet4\" : { \"Condition\" : \"4AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PublicSubnet4CIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"3\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public subnet 4\" }, { \"Fn::If\" : [ \"PublicSubnetTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"MapPublicIpOnLaunch\" : true } }, \"PrivateSubnet1ARouteTable\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 1A\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet1ARoute\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet1ARouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway1\" } } }, \"PrivateSubnet1ARouteTableAssociation\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet1A\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet1ARouteTable\" } } }, \"PrivateSubnet2ARouteTable\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 2A\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet2ARoute\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet2ARouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway2\" } } }, \"PrivateSubnet2ARouteTableAssociation\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet2A\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet2ARouteTable\" } } }, \"PrivateSubnet3ARouteTable\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 3A\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet3ARoute\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet3ARouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway3\" } } }, \"PrivateSubnet3ARouteTableAssociation\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet3A\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet3ARouteTable\" } } }, \"PrivateSubnet4ARouteTable\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 4A\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet4ARoute\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet4ARouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway4\" } } }, \"PrivateSubnet4ARouteTableAssociation\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet4A\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet4ARouteTable\" } } }, \"PrivateSubnet1BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 1B\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet1BRoute\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet1BRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway1\" } } }, \"PrivateSubnet1BRouteTableAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet1B\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet1BRouteTable\" } } }, \"PrivateSubnet1BNetworkAcl\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAcl\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"NACL Protected subnet 1\" }, { \"Key\" : \"Network\" , \"Value\" : \"NACL Protected\" } ] } }, \"PrivateSubnet1BNetworkAclEntryInbound\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : false , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet1BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet1BNetworkAclEntryOutbound\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : true , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet1BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet1BNetworkAclAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetNetworkAclAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet1B\" }, \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet1BNetworkAcl\" } } }, \"PrivateSubnet2BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 2B\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet2BRoute\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet2BRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway2\" } } }, \"PrivateSubnet2BRouteTableAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet2B\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet2BRouteTable\" } } }, \"PrivateSubnet2BNetworkAcl\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAcl\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"NACL Protected subnet 2\" }, { \"Key\" : \"Network\" , \"Value\" : \"NACL Protected\" } ] } }, \"PrivateSubnet2BNetworkAclEntryInbound\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : false , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet2BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet2BNetworkAclEntryOutbound\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : true , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet2BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet2BNetworkAclAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetNetworkAclAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet2B\" }, \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet2BNetworkAcl\" } } }, \"PrivateSubnet3BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 3B\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet3BRoute\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet3BRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway3\" } } }, \"PrivateSubnet3BRouteTableAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet3B\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet3BRouteTable\" } } }, \"PrivateSubnet3BNetworkAcl\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAcl\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"NACL Protected subnet 3\" }, { \"Key\" : \"Network\" , \"Value\" : \"NACL Protected\" } ] } }, \"PrivateSubnet3BNetworkAclEntryInbound\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : false , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet3BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet3BNetworkAclEntryOutbound\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : true , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet3BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet3BNetworkAclAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::SubnetNetworkAclAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet3B\" }, \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet3BNetworkAcl\" } } }, \"PrivateSubnet4BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 4B\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet4BRoute\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet4BRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway4\" } } }, \"PrivateSubnet4BRouteTableAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet4B\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet4BRouteTable\" } } }, \"PrivateSubnet4BNetworkAcl\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAcl\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"NACL Protected subnet 4\" }, { \"Key\" : \"Network\" , \"Value\" : \"NACL Protected\" } ] } }, \"PrivateSubnet4BNetworkAclEntryInbound\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : false , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet4BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet4BNetworkAclEntryOutbound\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : true , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet4BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet4BNetworkAclAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::SubnetNetworkAclAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet4B\" }, \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet4BNetworkAcl\" } } }, \"PublicSubnetRouteTable\" : { \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public Subnets\" }, { \"Key\" : \"Network\" , \"Value\" : \"Public\" } ] } }, \"PublicSubnetRoute\" : { \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"GatewayId\" : { \"Ref\" : \"InternetGateway\" } } }, \"PublicSubnet1RouteTableAssociation\" : { \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" }, \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" } } }, \"PublicSubnet2RouteTableAssociation\" : { \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PublicSubnet2\" }, \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" } } }, \"PublicSubnet3RouteTableAssociation\" : { \"Condition\" : \"3AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PublicSubnet3\" }, \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" } } }, \"PublicSubnet4RouteTableAssociation\" : { \"Condition\" : \"4AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PublicSubnet4\" }, \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" } } }, \"NAT1EIP\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"Domain\" : \"vpc\" } }, \"NAT2EIP\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"Domain\" : \"vpc\" } }, \"NAT3EIP\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"Domain\" : \"vpc\" } }, \"NAT4EIP\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"Domain\" : \"vpc\" } }, \"NATGateway1\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::NatGateway\" , \"Properties\" : { \"AllocationId\" : { \"Fn::GetAtt\" : [ \"NAT1EIP\" , \"AllocationId\" ] }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" } } }, \"NATGateway2\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::NatGateway\" , \"Properties\" : { \"AllocationId\" : { \"Fn::GetAtt\" : [ \"NAT2EIP\" , \"AllocationId\" ] }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet2\" } } }, \"NATGateway3\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::NatGateway\" , \"Properties\" : { \"AllocationId\" : { \"Fn::GetAtt\" : [ \"NAT3EIP\" , \"AllocationId\" ] }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet3\" } } }, \"NATGateway4\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::NatGateway\" , \"Properties\" : { \"AllocationId\" : { \"Fn::GetAtt\" : [ \"NAT4EIP\" , \"AllocationId\" ] }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet4\" } } }, \"S3VPCEndpoint\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::VPCEndpoint\" , \"Properties\" : { \"PolicyDocument\" : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"*\" , \"Effect\" : \"Allow\" , \"Resource\" : \"*\" , \"Principal\" : \"*\" } ] }, \"RouteTableIds\" : [ { \"Ref\" : \"PrivateSubnet1ARouteTable\" }, { \"Ref\" : \"PrivateSubnet2ARouteTable\" }, { \"Fn::If\" : [ \"PrivateSubnets&3AZCondition\" , { \"Ref\" : \"PrivateSubnet3ARouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnets&4AZCondition\" , { \"Ref\" : \"PrivateSubnet4ARouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"AdditionalPrivateSubnetsCondition\" , { \"Ref\" : \"PrivateSubnet1BRouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"AdditionalPrivateSubnetsCondition\" , { \"Ref\" : \"PrivateSubnet2BRouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"AdditionalPrivateSubnets&3AZCondition\" , { \"Ref\" : \"PrivateSubnet3BRouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"AdditionalPrivateSubnets&4AZCondition\" , { \"Ref\" : \"PrivateSubnet4BRouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"ServiceName\" : { \"Fn::Sub\" : \"com.amazonaws.${AWS::Region}.s3\" }, \"VpcId\" : { \"Ref\" : \"VPC\" } } } }, \"Outputs\" : { \"NAT1EIP\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"NAT 1 IP address\" , \"Value\" : { \"Ref\" : \"NAT1EIP\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-NAT1EIP\" } } }, \"NAT2EIP\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"NAT 2 IP address\" , \"Value\" : { \"Ref\" : \"NAT2EIP\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-NAT2EIP\" } } }, \"NAT3EIP\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Description\" : \"NAT 3 IP address\" , \"Value\" : { \"Ref\" : \"NAT3EIP\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-NAT3EIP\" } } }, \"NAT4EIP\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Description\" : \"NAT 4 IP address\" , \"Value\" : { \"Ref\" : \"NAT4EIP\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-NAT4EIP\" } } }, \"PrivateSubnet1ACIDR\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"Private subnet 1A CIDR in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1ACIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1ACIDR\" } } }, \"PrivateSubnet1AID\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"Private subnet 1A ID in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1A\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1AID\" } } }, \"PrivateSubnet1BCIDR\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Description\" : \"Private subnet 1B CIDR in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1BCIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1BCIDR\" } } }, \"PrivateSubnet1BID\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Description\" : \"Private subnet 1B ID in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1B\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1BID\" } } }, \"PrivateSubnet2ACIDR\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"Private subnet 2A CIDR in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2ACIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2ACIDR\" } } }, \"PrivateSubnet2AID\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"Private subnet 2A ID in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2A\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2AID\" } } }, \"PrivateSubnet2BCIDR\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Description\" : \"Private subnet 2B CIDR in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2BCIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2BCIDR\" } } }, \"PrivateSubnet2BID\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Description\" : \"Private subnet 2B ID in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2B\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2BID\" } } }, \"PrivateSubnet3ACIDR\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Description\" : \"Private subnet 3A CIDR in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3ACIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3ACIDR\" } } }, \"PrivateSubnet3AID\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Description\" : \"Private subnet 3A ID in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3A\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3AID\" } } }, \"PrivateSubnet3BCIDR\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Description\" : \"Private subnet 3B CIDR in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3BCIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3BCIDR\" } } }, \"PrivateSubnet3BID\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Description\" : \"Private subnet 3B ID in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3B\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3BID\" } } }, \"PrivateSubnet4ACIDR\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Description\" : \"Private subnet 4A CIDR in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4ACIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4ACIDR\" } } }, \"PrivateSubnet4AID\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Description\" : \"Private subnet 4A ID in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4A\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4AID\" } } }, \"PrivateSubnet4BCIDR\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Description\" : \"Private subnet 4B CIDR in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4BCIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4BCIDR\" } } }, \"PrivateSubnet4BID\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Description\" : \"Private subnet 4B ID in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4B\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4BID\" } } }, \"PublicSubnet1CIDR\" : { \"Description\" : \"Public subnet 1 CIDR in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PublicSubnet1CIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet1CIDR\" } } }, \"PublicSubnet1ID\" : { \"Description\" : \"Public subnet 1 ID in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PublicSubnet1\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet1ID\" } } }, \"PublicSubnet2CIDR\" : { \"Description\" : \"Public subnet 2 CIDR in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PublicSubnet2CIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet2CIDR\" } } }, \"PublicSubnet2ID\" : { \"Description\" : \"Public subnet 2 ID in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PublicSubnet2\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet2ID\" } } }, \"PublicSubnet3CIDR\" : { \"Condition\" : \"3AZCondition\" , \"Description\" : \"Public subnet 3 CIDR in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PublicSubnet3CIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet3CIDR\" } } }, \"PublicSubnet3ID\" : { \"Condition\" : \"3AZCondition\" , \"Description\" : \"Public subnet 3 ID in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PublicSubnet3\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet3ID\" } } }, \"PublicSubnet4CIDR\" : { \"Condition\" : \"4AZCondition\" , \"Description\" : \"Public subnet 4 CIDR in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PublicSubnet4CIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet4CIDR\" } } }, \"PublicSubnet4ID\" : { \"Condition\" : \"4AZCondition\" , \"Description\" : \"Public subnet 4 ID in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PublicSubnet4\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet4ID\" } } }, \"S3VPCEndpoint\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"S3 VPC Endpoint\" , \"Value\" : { \"Ref\" : \"S3VPCEndpoint\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-S3VPCEndpoint\" } } }, \"PrivateSubnet1ARouteTable\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1ARouteTable\" }, \"Description\" : \"Private subnet 1A route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1ARouteTable\" } } }, \"PrivateSubnet1BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1BRouteTable\" }, \"Description\" : \"Private subnet 1B route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1BRouteTable\" } } }, \"PrivateSubnet2ARouteTable\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2ARouteTable\" }, \"Description\" : \"Private subnet 2A route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2ARouteTable\" } } }, \"PrivateSubnet2BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2BRouteTable\" }, \"Description\" : \"Private subnet 2B route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2BRouteTable\" } } }, \"PrivateSubnet3ARouteTable\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3ARouteTable\" }, \"Description\" : \"Private subnet 3A route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3ARouteTable\" } } }, \"PrivateSubnet3BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3BRouteTable\" }, \"Description\" : \"Private subnet 3B route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3BRouteTable\" } } }, \"PrivateSubnet4ARouteTable\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4ARouteTable\" }, \"Description\" : \"Private subnet 4A route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4ARouteTable\" } } }, \"PrivateSubnet4BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4BRouteTable\" }, \"Description\" : \"Private subnet 4B route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4BRouteTable\" } } }, \"PublicSubnetRouteTable\" : { \"Value\" : { \"Ref\" : \"PublicSubnetRouteTable\" }, \"Description\" : \"Public subnet route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnetRouteTable\" } } }, \"VPCCIDR\" : { \"Value\" : { \"Ref\" : \"VPCCIDR\" }, \"Description\" : \"VPC CIDR\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-VPCCIDR\" } } }, \"VPCID\" : { \"Value\" : { \"Ref\" : \"VPC\" }, \"Description\" : \"VPC ID\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-VPCID\" } } } } } ManagedADStack \u00b6 AWSTemplateFormatVersion : '2010-09-09' Description : >- This template creates a managed Microsoft AD Directory Service into private subnets in separate Availability Zones inside a VPC. The default Domain Administrator user is 'admin'. For adding members to the domain, ensure that they are launched into the domain member security group created by this template and then configure them to use the AD instances fixed private IP addresses as the DNS server. **WARNING** This template creates Amazon EC2 Windows instance and related resources. You will be billed for the AWS resources used if you create a stack from this template. QS(0021) Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : Network Configuration Parameters : - VPCCIDR - VPCID - PrivateSubnet1CIDR - PrivateSubnet1ID - PrivateSubnet2CIDR - PrivateSubnet2ID - PublicSubnet1CIDR - PublicSubnet2CIDR - Label : default : Microsoft Active Directory Configuration Parameters : - DomainDNSName - DomainNetBIOSName - DomainAdminPassword - ADEdition - Label : default : AWS Systems Manager AMI configuration Parameters : - WS2019FULLBASE - Label : default : AWS Quick Start Configuration Parameters : - QSS3BucketName - QSS3KeyPrefix ParameterLabels : DomainAdminPassword : default : Domain Admin Password DomainDNSName : default : Domain DNS Name DomainNetBIOSName : default : Domain NetBIOS Name ADEdition : default : AWS Microsoft AD edition PrivateSubnet1CIDR : default : Private Subnet 1 CIDR PrivateSubnet1ID : default : Private Subnet 1 ID PrivateSubnet2CIDR : default : Private Subnet 2 CIDR PrivateSubnet2ID : default : Private Subnet 2 ID PublicSubnet1CIDR : default : Public Subnet 1 CIDR PublicSubnet2CIDR : default : Public Subnet 2 CIDR QSS3BucketName : default : Quick Start S3 Bucket Name QSS3KeyPrefix : default : Quick Start S3 Key Prefix VPCCIDR : default : VPC CIDR VPCID : default : VPC ID WS2019FULLBASE : default : Windows Server 2019 full base AMI Parameters : DomainAdminPassword : Description : Password for the domain admin user. Must be at least 8 characters containing letters, numbers and symbols Type : String MinLength : '8' MaxLength : '32' AllowedPattern : (?=^.{6,255}$)((?=.*\\d)(?=.*[A-Z])(?=.*[a-z])|(?=.*\\d)(?=.*[^A-Za-z0-9])(?=.*[a-z])|(?=.*[^A-Za-z0-9])(?=.*[A-Z])(?=.*[a-z])|(?=.*\\d)(?=.*[A-Z])(?=.*[^A-Za-z0-9]))^.* NoEcho : 'true' DomainDNSName : Description : Fully qualified domain name (FQDN) of the forest root domain e.g. example.com Type : String Default : example.com MinLength : '2' MaxLength : '255' AllowedPattern : '[a-zA-Z0-9\\-]+\\..+' DomainNetBIOSName : Description : NetBIOS name of the domain (upto 15 characters) for users of earlier versions of Windows e.g. EXAMPLE Type : String Default : example MinLength : '1' MaxLength : '15' AllowedPattern : '[a-zA-Z0-9\\-]+' ADEdition : AllowedValues : - Standard - Enterprise Default : Enterprise Description : The AWS Microsoft AD edition. Valid values include Standard and Enterprise. Type : String PrivateSubnet1CIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.0.0/19 Description : CIDR block for private subnet 1 located in Availability Zone 1. Type : String PrivateSubnet1ID : Description : ID of the private subnet 1 in Availability Zone 1 (e.g., subnet-a0246dcd) Type : AWS::EC2::Subnet::Id PrivateSubnet2CIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.32.0/19 Description : CIDR block for private subnet 2 located in Availability Zone 2. Type : String PrivateSubnet2ID : Description : ID of the private subnet 2 in Availability Zone 2 (e.g., subnet-a0246dcd) Type : AWS::EC2::Subnet::Id PublicSubnet1CIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.128.0/20 Description : CIDR Block for the public DMZ subnet 1 located in Availability Zone 1 Type : String PublicSubnet2CIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.144.0/20 Description : CIDR Block for the public DMZ subnet 2 located in Availability Zone 2 Type : String QSS3BucketName : AllowedPattern : ^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$ ConstraintDescription : Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-). Default : aws-quickstart Description : S3 bucket name for the Quick Start assets. Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-). Type : String QSS3KeyPrefix : AllowedPattern : ^[0-9a-zA-Z-/]*$ ConstraintDescription : Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/). Default : quickstart-microsoft-activedirectory/ Description : S3 key prefix for the Quick Start assets. Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/). Type : String VPCCIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.0.0/16 Description : CIDR Block for the VPC Type : String VPCID : Description : ID of the VPC (e.g., vpc-0343606e) Type : AWS::EC2::VPC::Id WS2019FULLBASE : Type : 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>' Default : '/aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base' Rules : SubnetsInVPC : Assertions : - Assert : !EachMemberIn - !ValueOfAll - AWS::EC2::Subnet::Id - VpcId - !RefAll 'AWS::EC2::VPC::Id' AssertDescription : All subnets must in the VPC Conditions : GovCloudCondition : !Equals - !Ref 'AWS::Region' - us-gov-west-1 Resources : DHCPOptions : Type : AWS::EC2::DHCPOptions DependsOn : - MicrosoftAD - SSMWaitCondition Properties : DomainName : !Ref 'DomainDNSName' DomainNameServers : !GetAtt 'MicrosoftAD.DnsIpAddresses' Tags : - Key : Domain Value : !Ref 'DomainDNSName' ADAdminSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'ADAdminSecret-${AWS::StackName}' Description : Admin User Seccrets for Manged AD Quick Start SecretString : !Sub '{\"username\":\"Admin\",\"password\":\"${DomainAdminPassword}\"}' MicrosoftAD : Type : AWS::DirectoryService::MicrosoftAD Properties : Name : !Ref 'DomainDNSName' Edition : !Ref 'ADEdition' ShortName : !Ref 'DomainNetBIOSName' Password : !Ref 'DomainAdminPassword' VpcSettings : SubnetIds : - !Ref 'PrivateSubnet1ID' - !Ref 'PrivateSubnet2ID' VpcId : !Ref 'VPCID' DomainMemberSG : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Domain Members VpcId : !Ref 'VPCID' SecurityGroupIngress : - IpProtocol : tcp FromPort : 5985 ToPort : 5985 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : tcp FromPort : 53 ToPort : 53 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : udp FromPort : 53 ToPort : 53 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : tcp FromPort : 49152 ToPort : 65535 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : udp FromPort : 49152 ToPort : 65535 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : tcp FromPort : 5985 ToPort : 5985 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : tcp FromPort : 53 ToPort : 53 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : udp FromPort : 53 ToPort : 53 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : tcp FromPort : 49152 ToPort : 65535 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : udp FromPort : 49152 ToPort : 65535 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : tcp FromPort : 3389 ToPort : 3389 CidrIp : !Ref 'PublicSubnet1CIDR' - IpProtocol : tcp FromPort : 3389 ToPort : 3389 CidrIp : !Ref 'PublicSubnet2CIDR' - IpProtocol : tcp FromPort : 636 ToPort : 636 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 3269 ToPort : 3269 CidrIp : !Ref VPCCIDR SSMAutomationRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject Resource : !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*' Effect : Allow PolicyName : aws-quick-start-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - cloudformation:SignalResource Resource : !Sub 'arn:${AWS::Partition}:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${AWS::StackName}/*' PolicyName : aws-quick-start-cfn-signal-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - iam:CreateRole - iam:PutRolePolicy - iam:getRolePolicy - iam:DetachRolePolicy - iam:AttachRolePolicy - iam:DeleteRolePolicy - iam:CreateInstanceProfile - iam:DeleteRole - iam:RemoveRoleFromInstanceProfile - iam:AddRoleToInstanceProfile - iam:DeleteInstanceProfile - iam:PassRole Resource : '*' PolicyName : aws-quick-start-create-role-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - secretsmanager:GetSecretValue - secretsmanager:DescribeSecret Resource : - !Ref 'ADAdminSecrets' - Effect : Allow Action : - ssm:StartAutomationExecution Resource : '*' PolicyName : AD-SSM-Secrets Path : / AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : - ssm.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMFullAccess' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AWSCloudFormationFullAccess' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonEC2FullAccess' LambdaSSMRole : DependsOn : MicrosoftAD Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - iam:PassRole Resource : !GetAtt SSMAutomationRole.Arn PolicyName : QS-SSM-PassRole Path : / AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : - lambda.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AmazonSSMAutomationRole' DNSForwarderSetup : Type : AWS::SSM::Document Properties : DocumentType : Automation Content : schemaVersion : \"0.3\" description : Setup DNS Forwarder for AWS Managed AD to VPC DNS assumeRole : \"{{AutomationAssumeRole}}\" parameters : StackName : description : \"Stack Name Input for cfn resource signal\" type : \"String\" DomainMemberSG : description : Security group ID that can communicate with domain controllers type : \"String\" VPCCIDR : description : VPC CIDR type : \"String\" QSS3BucketName : description : \"S3 bucket name for the Quick Start assets. Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).\" type : \"String\" QSS3KeyPrefix : description : \"S3 key prefix for the Quick Start assets. Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/).\" type : \"String\" PrivateSubnet1ID : description : \"Private Subnet 1 ID\" type : \"String\" AutomationAssumeRole : description : \"(Optional) The ARN of the role that allows Automation to perform the actions on your behalf.\" type : \"String\" DirectoryID : description : \"Alias of the directory ID\" type : \"String\" AWSRegion : description : \"Region\" type : \"String\" URLSuffix : default : \"amazonaws.com\" description : \"AWS URL suffix\" type : \"String\" mainSteps : - name : CreateStack action : aws:createStack onFailure : \"step:CFNSignalEnd\" inputs : StackName : \"SetDNSForwarder\" Capabilities : [ \"CAPABILITY_IAM\" ] TemplateBody : | Description: \"Deploy Instance to create DNS forwarder on AWS Managed AD\" Parameters: WINFULLBASE: Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>' Default: '/aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base' QSS3BucketName: Type: \"String\" Default: \"{{QSS3BucketName}}\" Description: \"Name of Target S3 Bucket\" QSS3KeyPrefix: Type: \"String\" Default: \"{{QSS3KeyPrefix}}\" Description: \"Name of Target S3 Prefix\" SecurityGroup: Description: Security Group to be able to talk Domain Controllers Default: \"{{DomainMemberSG}}\" Type: \"String\" Subnet: Description: \"Subnet to deploy the EC2 instnace\" Default: \"{{PrivateSubnet1ID}}\" Type: \"String\" Resources: ForwarderRole: Type : AWS::IAM::Role Properties: Policies: - PolicyDocument: Version: '2012-10-17' Statement: - Action: - s3:GetObject - s3:ListBucket Resource: - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*' - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}' Effect: Allow PolicyName: s3-instance-bucket-policy Path: / ManagedPolicyArns: - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AWSDirectoryServiceFullAccess' AssumeRolePolicyDocument: Version: \"2012-10-17\" Statement: - Effect: \"Allow\" Principal: Service: - \"ec2.amazonaws.com\" - \"ssm.amazonaws.com\" Action: \"sts:AssumeRole\" IamInstanceProfile: Type: \"AWS::IAM::InstanceProfile\" Properties: Roles: - !Ref ForwarderRole EC2Instance: Type: \"AWS::EC2::Instance\" Properties: ImageId: !Ref WINFULLBASE InstanceType: \"t3.medium\" IamInstanceProfile: !Ref IamInstanceProfile SecurityGroupIds: - !Ref 'SecurityGroup' SubnetId: !Ref Subnet Tags: - Key: \"Name\" Value: \"TempDNSForwarder\" - name : \"getInstanceId\" action : aws:executeAwsApi onFailure : \"step:CFNSignalEnd\" inputs : Service : ec2 Api : DescribeInstances Filters : - Name : \"tag:Name\" Values : [ \"TempDNSForwarder\" ] - Name : \"instance-state-name\" Values : [ \"running\" ] outputs : - Name : InstanceId Selector : \"$.Reservations[0].Instances[0].InstanceId\" Type : String - name : \"CreateDNSForward\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{getInstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : \"S3\" sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.{{URLSuffix}}/{{QSS3KeyPrefix}}scripts/AddDNSForward.ps1\"}' commandLine : \"./AddDNSForward.ps1 -DirectoryID {{DirectoryID}} -VPCCIDR {{VPCCIDR}} -AWSRegion {{AWSRegion}}\" # Determines if CFN Needs to be Signaled or if Work flow should just end - name : CFNSignalEnd action : aws:branch inputs : Choices : - NextStep : signalsuccess Not : Variable : \"{{StackName}}\" StringEquals : \"\" - NextStep : sleepend Variable : \"{{StackName}}\" StringEquals : \"\" # If all steps complete successfully signals CFN of Success - name : \"signalsuccess\" action : \"aws:executeAwsApi\" nextStep : \"deleteStack\" inputs : Service : cloudformation Api : SignalResource LogicalResourceId : \"SSMWaitCondition\" StackName : \"{{StackName}}\" Status : SUCCESS UniqueId : \"{{getInstanceId.InstanceId}}\" # If CFN Signl Not Needed this sleep ends work flow - name : \"sleepend\" action : \"aws:sleep\" nextStep : \"deleteStack\" inputs : Duration : PT1S # If any steps fails signals CFN of Failure - name : \"signalfailure\" action : \"aws:executeAwsApi\" nextStep : \"deleteStack\" inputs : Service : cloudformation Api : SignalResource LogicalResourceId : \"SSMWaitCondition\" StackName : \"{{StackName}}\" Status : FAILURE UniqueId : \"{{getInstanceId.InstanceId}}\" - name : deleteStack action : aws:deleteStack isEnd : true # onFailure: Continue inputs : StackName : \"SetDNSForwarder\" LambdaSSMExecute : DependsOn : LambdaSSMRole Type : AWS::Lambda::Function Properties : Description : Executes SSM Automation Documents Handler : index.handler Runtime : python3.7 Role : !GetAtt LambdaSSMRole.Arn Timeout : 900 Code : ZipFile : | def handler(event, context): import cfnresponse import boto3, os, json from botocore.vendored import requests ssm_cl = boto3.client('ssm') ecr_cl = boto3.client('ecr') req_type = event['RequestType'] print(event) SUCCESS = \"SUCCESS\" FAILED = \"FAILED\" def start_ssmautomation(event): doc_name = event['ResourceProperties']['DocumentName'] stack_name = event['ResourceProperties']['StackName'] ssm_role = event['ResourceProperties']['AutomationAssumeRole'] qs_bucket = event['ResourceProperties']['QSS3BucketName'] qs_bucket_prefix = event['ResourceProperties']['QSS3KeyPrefix'] security_group = event['ResourceProperties']['DomainMemberSG'] subnet_id = event['ResourceProperties']['PrivateSubnet1ID'] vpc_cidr = event['ResourceProperties']['VPCCIDR'] directory_id = event['ResourceProperties']['DirectoryID'] aws_region = event['ResourceProperties']['AWSRegion'] url_suffix = event['ResourceProperties']['URLSuffix'] start_automation = ssm_cl.start_automation_execution( DocumentName= doc_name, Parameters={ 'StackName': [ stack_name ], 'AutomationAssumeRole': [ ssm_role ], 'QSS3BucketName': [ qs_bucket ], 'QSS3KeyPrefix': [ qs_bucket_prefix ], 'DomainMemberSG': [ security_group ], 'PrivateSubnet1ID': [ subnet_id ], 'VPCCIDR': [ vpc_cidr ], 'DirectoryID': [ directory_id ], 'AWSRegion': [ aws_region ], 'URLSuffix': [ url_suffix ] }, ) cfnresponse.send(event, context, SUCCESS, start_automation, start_automation['AutomationExecutionId']) def delete_image(event): cfnresponse.send(event, context, SUCCESS, event, \"Deleted\") actions = { 'Create': start_ssmautomation, 'Delete': delete_image, 'Update': start_ssmautomation } try: actions.get(req_type)(event) except Exception as exc: error_msg = {'Error': '{}'.format(exc)} print(error_msg) cfnresponse.send(event, context, FAILED, error_msg) ExecuteSSMAutomation : DependsOn : LambdaSSMExecute Type : Custom::ExecuteSSMAutomation Properties : ServiceToken : !GetAtt LambdaSSMExecute.Arn DocumentName : !Ref DNSForwarderSetup DomainMemberSG : !Ref DomainMemberSG StackName : !Ref AWS::StackName QSS3BucketName : !Ref QSS3BucketName QSS3KeyPrefix : !Ref QSS3KeyPrefix AutomationAssumeRole : !GetAtt SSMAutomationRole.Arn PrivateSubnet1ID : !Ref PrivateSubnet1ID VPCCIDR : !Ref VPCCIDR DirectoryID : !GetAtt 'MicrosoftAD.Alias' AWSRegion : !Ref 'AWS::Region' URLSuffix : !Ref 'AWS::URLSuffix' SSMWaitHandle : Type : AWS::CloudFormation::WaitConditionHandle SSMWaitCondition : Type : AWS::CloudFormation::WaitCondition CreationPolicy : ResourceSignal : Timeout : PT60M Count : 1 DependsOn : - ExecuteSSMAutomation - SSMWaitHandle Properties : Handle : Ref : \"SSMWaitHandle\" Timeout : \"3600\" Count : 1 AWSQuickstartADRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:ListBucket Resource : - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*' - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}' Effect : Allow PolicyName : aws-quick-start-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - cloudformation:SignalResource Resource : !Sub 'arn:${AWS::Partition}:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${AWS::StackName}/*' - Effect : Allow Action : - ec2:DescribeInstances - ec2:DescribeInstanceStatus - ssm:* Resource : '*' PolicyName : AD-SSM-AutomationExecution Path : / AssumeRolePolicyDocument : Statement : - Action : - sts:AssumeRole Principal : Service : - ec2.amazonaws.com - ssm.amazonaws.com Effect : Allow Version : '2012-10-17' Outputs : ADServer1PrivateIP : Value : !Select - '0' - !GetAtt 'MicrosoftAD.DnsIpAddresses' Description : AD Server 1 Private IP Address (this may vary based on Directory Service order of IP addresses) ADServer2PrivateIP : Value : !Select - '1' - !GetAtt 'MicrosoftAD.DnsIpAddresses' Description : AD Server 2 Private IP Address (this may vary based on Directory Service order of IP addresses) DirectoryID : Value : !Ref 'MicrosoftAD' Description : Directory Services ID DomainAdmin : Value : !Join - '' - - !Ref 'DomainNetBIOSName' - \\admin Description : Domain administrator account DomainMemberSGID : Value : !Ref 'DomainMemberSG' Description : Domain Member Security Group ID ADSecretsArn : Value : !Ref 'ADAdminSecrets' Description : Managed AD Admin Secrets ADStack \u00b6 AWSTemplateFormatVersion : '2010-09-09' Description : This template creates 1 Windows 2019 Active Directory Domain Controllers into private subnets in separate Availability Zones inside a VPC. The default Domain Administrator password will be the one retrieved from the instance. For adding members to the domain, ensure that they are launched into the domain member security group created by this template and then configure them to use the AD instances fixed private IP addresses as the DNS server. **WARNING** This template creates Amazon EC2 Windows instance and related resources. You will be billed for the AWS resources used if you create a stack from this template. QS(0001) Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : Network Configuration Parameters : - VPCCIDR - VPCID - PrivateSubnet1ID - Label : default : Amazon EC2 Configuration Parameters : - ADServer1InstanceType - ADServer1NetBIOSName - ADServer1PrivateIP - ADServer2InstanceType - ADServer2NetBIOSName - ADServer2PrivateIP - WS2016FULLBASE - Label : default : Microsoft Active Directory Configuration Parameters : - DomainAdminPassword - DomainAdminUser - DomainDNSName - DomainNetBIOSName - Label : default : AWS Quick Start Configuration Parameters : - QSS3BucketName - QSS3KeyPrefix ParameterLabels : ADServer1InstanceType : default : Domain Controller 1 Instance Type ADServer1NetBIOSName : default : Domain Controller 1 NetBIOS Name ADServer1PrivateIP : default : Domain Controller 1 Private IP Address DomainAdminPassword : default : Domain Admin Password DomainAdminUser : default : Domain Admin User Name DomainDNSName : default : Domain DNS Name DomainNetBIOSName : default : Domain NetBIOS Name WS2016FULLBASE : default : SSM Parameter Value to grab the lastest AMI ID PrivateSubnet1ID : default : Private Subnet 1 ID QSS3BucketName : default : Quick Start S3 Bucket Name QSS3KeyPrefix : default : Quick Start S3 Key Prefix VPCCIDR : default : VPC CIDR VPCID : default : VPC ID Parameters : ADServer1InstanceType : AllowedValues : - t2.large - m4.large - m4.xlarge - m4.2xlarge - m4.4xlarge - m5.large - m5.xlarge - m5.2xlarge - m5.4xlarge Default : m5.xlarge Description : Amazon EC2 instance type for the first Active Directory instance Type : String ADServer1NetBIOSName : AllowedPattern : '[a-zA-Z0-9\\-]+' Default : DC1 Description : NetBIOS name of the first Active Directory server (up to 15 characters) MaxLength : '15' MinLength : '1' Type : String ADServer1PrivateIP : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$ Default : 10.0.0.10 Description : Fixed private IP for the first Active Directory server located in Availability Zone 1 Type : String DomainAdminPassword : AllowedPattern : (?=^.{6,255}$)((?=.*\\d)(?=.*[A-Z])(?=.*[a-z])|(?=.*\\d)(?=.*[^A-Za-z0-9])(?=.*[a-z])|(?=.*[^A-Za-z0-9])(?=.*[A-Z])(?=.*[a-z])|(?=.*\\d)(?=.*[A-Z])(?=.*[^A-Za-z0-9]))^.* Description : Password for the domain admin user. Must be at least 8 characters containing letters, numbers and symbols MaxLength : '32' MinLength : '8' NoEcho : 'true' Type : String DomainAdminUser : AllowedPattern : '[a-zA-Z0-9]*' Default : Admin Description : User name for the account that will be added as Domain Administrator. This is separate from the default \"Administrator\" account MaxLength : '25' MinLength : '5' Type : String DomainDNSName : AllowedPattern : '[a-zA-Z0-9\\-]+\\..+' Default : example.com Description : Fully qualified domain name (FQDN) of the forest root domain e.g. example.com MaxLength : '255' MinLength : '2' Type : String DomainNetBIOSName : AllowedPattern : '[a-zA-Z0-9\\-]+' Default : example Description : NetBIOS name of the domain (up to 15 characters) for users of earlier versions of Windows e.g. EXAMPLE MaxLength : '15' MinLength : '1' Type : String PrivateSubnet1ID : Description : ID of the private subnet 1 in Availability Zone 1 (e.g., subnet-a0246dcd) Type : AWS::EC2::Subnet::Id QSS3BucketName : AllowedPattern : ^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$ ConstraintDescription : Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-). Default : alpublic Description : S3 bucket name for the Quick Start assets. Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-). Type : String QSS3KeyPrefix : AllowedPattern : ^[0-9a-zA-Z-/]*$ ConstraintDescription : Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/). Default : quickstart-microsoft-activedirectory/ Description : S3 key prefix for the Quick Start assets. Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/). Type : String VPCCIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.0.0/16 Description : CIDR Block for the VPC Type : String VPCID : Description : ID of the VPC (e.g., vpc-0343606e) Type : AWS::EC2::VPC::Id WS2016FULLBASE : Type : 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>' Default : '/aws/service/ami-windows-latest/Windows_Server-2016-English-Full-Base' Rules : SubnetsInVPC : Assertions : - Assert : !EachMemberIn - !ValueOfAll - AWS::EC2::Subnet::Id - VpcId - !RefAll 'AWS::EC2::VPC::Id' AssertDescription : All subnets must in the VPC Conditions : GovCloudCondition : !Equals - !Ref 'AWS::Region' - us-gov-west-1 Resources : DSCBucket : Type : AWS::S3::Bucket Properties : LifecycleConfiguration : Rules : - Id : DeleteAfter30Days ExpirationInDays : 30 Status : Enabled Prefix : 'logs/' DHCPOptions : Type : AWS::EC2::DHCPOptions DependsOn : - DomainController1 Properties : DomainName : !Ref 'DomainDNSName' DomainNameServers : - !Ref 'ADServer1PrivateIP' Tags : - Key : Domain Value : !Ref 'DomainDNSName' VPCDHCPOptionsAssociation : Type : AWS::EC2::VPCDHCPOptionsAssociation Properties : VpcId : !Ref 'VPCID' DhcpOptionsId : !Ref 'DHCPOptions' AWSQuickstartActiveDirectoryDS : Type : AWS::SSM::Document Properties : DocumentType : Automation Content : schemaVersion : \"0.3\" description : \"Deploy AD with SSM Automation\" # Role that is utilized to perform the steps within the Automation Document. In this case to be able to Signal CFN and Describe Instances. assumeRole : \"{{AutomationAssumeRole}}\" # Gathering parameters needed to configure DCs in the Quick Start parameters : ADServer1NetBIOSName : default : \"DC1\" description : \"NetBIOS name of the first Active Directory server (up to 15 characters)\" type : \"String\" ADServer1PrivateIP : default : \"10.0.0.10\" description : \"Fixed private IP for the first Active Directory server located in Availability Zone 1\" type : \"String\" VPCCIDR : default : '10.0.0.0/16' description : \"CIDR block for private subnet 1 located in Availability Zone 1.\" type : \"String\" ADAdminSecParamName : description : \"AWS Secrets Parameter Name that has Password and User namer for the domain administrator.\" type : \"String\" ADAltUserSecParamName : description : \"AWS Secrets Parameter Name for the account that will be added as Domain Administrator. This is separate from the default Administrator account\" type : \"String\" RestoreModeSecParamName : description : \"AWS Secrets Parameter Name for the Restore Mode Password\" type : \"String\" DomainDNSName : default : \"example.com\" description : \"Fully qualified domain name (FQDN) of the forest root domain e.g. example.com\" type : \"String\" DomainNetBIOSName : default : \"example\" description : \"NetBIOS name of the domain (up to 15 characters) for users of earlier versions of Windows e.g. EXAMPLE\" type : \"String\" QSS3BucketName : default : \"aws-quickstart\" description : \"S3 bucket name for the Quick Start assets. Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).\" type : \"String\" QSS3KeyPrefix : default : \"quickstart-microsoft-activedirectory/\" description : \"S3 key prefix for the Quick Start assets. Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/).\" type : \"String\" URLSuffix : default : \"amazonaws.com\" description : \"AWS URL suffix\" type : \"String\" StackName : default : \"\" description : \"Stack Name Input for cfn resource signal\" type : \"String\" AutomationAssumeRole : default : \"\" description : \"(Optional) The ARN of the role that allows Automation to perform the actions on your behalf.\" type : \"String\" mainSteps : # This step grabs the Instance IDs for both nodes that will be configured as DCs in the Quick Start and Instance IDs for the for next steps. - name : \"dc1InstanceId\" action : aws:executeAwsApi onFailure : \"step:signalfailure\" inputs : Service : ec2 Api : DescribeInstances Filters : - Name : \"tag:Name\" Values : [ \"{{ADServer1NetBIOSName}}\" ] - Name : \"tag:aws:cloudformation:stack-name\" Values : [ \"{{StackName}}\" ] - Name : \"instance-state-name\" Values : [ \"running\" ] outputs : - Name : InstanceId Selector : \"$.Reservations[0].Instances[0].InstanceId\" Type : \"String\" # Installs needed Powershell DSC Modules and components on both nodes. - name : \"dcInstallDscModules\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : \"S3\" sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.amazonaws.com/{{QSS3KeyPrefix}}scripts/install-ad-modules.ps1\"}' commandLine : \"./install-ad-modules.ps1\" # Configures Local Configuration Manager on each of the nodes. - name : \"dcsLCMConfig\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : \"S3\" sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.amazonaws.com/{{QSS3KeyPrefix}}scripts/LCM-Config.ps1\"}' commandLine : \"./LCM-Config.ps1\" # Generates MOF file on first DC Node to be processed by LCM. - name : \"createDC1Mof\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" nextStep : \"configDC1\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : \"S3\" sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.amazonaws.com/{{QSS3KeyPrefix}}scripts/ConfigDC1.ps1\"}' commandLine : \"./ConfigDC1.ps1 -ADServer1NetBIOSName {{ADServer1NetBIOSName}} -DomainNetBIOSName {{DomainNetBIOSName}} -DomainDNSName {{DomainDNSName}} -ADAdminSecParam {{ADAdminSecParamName}} -ADAltUserSecParam {{ADAltUserSecParamName}} -RestoreModeSecParam {{RestoreModeSecParamName}} -SiteName {{global:REGION}} -VPCCIDR {{VPCCIDR}}\" # Kicks off DSC Configuration and loops\\reboots until Node matches Configuration defined in MOF file. - name : \"configDC1\" action : aws:runCommand onFailure : \"step:signalfailure\" inputs : DocumentName : AWS-RunPowerShellScript InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : commands : - | function DscStatusCheck () { $LCMState = (Get-DscLocalConfigurationManager).LCMState if ($LCMState -eq 'PendingConfiguration' -Or $LCMState -eq 'PendingReboot') { 'returning 3010, should continue after reboot' exit 3010 } else { 'Completed' } } Start-DscConfiguration 'C:\\AWSQuickstart\\ConfigDC1' -Wait -Verbose -Force DscStatusCheck # Ensure that AD servers point to themselves for DNS - name : \"DnsConfig\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : S3 sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.amazonaws.com/{{QSS3KeyPrefix}}scripts/Dns-Config.ps1\"}' commandLine : \"./Dns-Config.ps1 -ADServer1NetBIOSName {{ADServer1NetBIOSName}} -ADServer1PrivateIP {{ADServer1PrivateIP}} -DomainDNSName {{DomainDNSName}} -ADAdminSecParam {{ADAdminSecParamName}}\" # Determines if CFN Needs to be Signaled or if Work flow should just end - name : CFNSignalEnd action : aws:branch inputs : Choices : - NextStep : signalsuccess Not : Variable : \"{{StackName}}\" StringEquals : \"\" - NextStep : sleepend Variable : \"{{StackName}}\" StringEquals : \"\" # If all steps complete successfully signals CFN of Success - name : \"signalsuccess\" action : \"aws:executeAwsApi\" isEnd : True inputs : Service : cloudformation Api : SignalResource LogicalResourceId : \"DomainController1\" StackName : \"{{StackName}}\" Status : SUCCESS UniqueId : \"{{dc1InstanceId.InstanceId}}\" # If CFN Signl Not Needed this sleep ends work flow - name : \"sleepend\" action : \"aws:sleep\" isEnd : True inputs : Duration : PT1S # If any steps fails signals CFN of Failure - name : \"signalfailure\" action : \"aws:executeAwsApi\" inputs : Service : cloudformation Api : SignalResource LogicalResourceId : \"DomainController2\" StackName : \"{{StackName}}\" Status : FAILURE UniqueId : \"{{dc1InstanceId.InstanceId}}\" AWSQuickstartADDSRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:ListBucket Resource : - !Sub arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}* - !Sub arn:${AWS::Partition}:s3:::${QSS3BucketName} Effect : Allow PolicyName : aws-quick-start-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - cloudformation:SignalResource Resource : !Sub 'arn:aws:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${AWS::StackName}/*' - Effect : Allow Action : - ec2:DescribeInstances - ec2:DescribeInstanceStatus - ssm:* Resource : '*' PolicyName : AD-SSM-Automation Path : / AssumeRolePolicyDocument : Statement : - Action : - sts:AssumeRole Principal : Service : - ec2.amazonaws.com - ssm.amazonaws.com Effect : Allow Version : '2012-10-17' ADSsmPassRolePolicy : Type : AWS::IAM::Policy Properties : PolicyName : AD-SSM-PassRole PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - iam:PassRole Resource : !Sub 'arn:aws:iam::${AWS::AccountId}:role/${AWSQuickstartADDSRole}' Roles : - !Ref 'AWSQuickstartADDSRole' ADServerRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:ListBucket Resource : - !Sub arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}* - !Sub arn:${AWS::Partition}:s3:::${QSS3BucketName} Effect : Allow PolicyName : aws-quick-start-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject Resource : - !Sub 'arn:aws:s3:::aws-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::aws-windows-downloads-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-packages-${AWS::Region}/*' - !Sub 'arn:aws:s3:::${AWS::Region}-birdwatcher-prod/*' - !Sub 'arn:aws:s3:::patch-baseline-snapshot-${AWS::Region}/*' Effect : Allow PolicyName : ssm-custom-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - secretsmanager:GetSecretValue - secretsmanager:DescribeSecret Resource : - !Ref 'ADAdminSecrets' - !Ref 'RestoreModeSecrets' - !Ref 'ADAltUserSecrets' - Effect : Allow Action : - ssm:StartAutomationExecution Resource : '*' PolicyName : AD-SSM-Secrets - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - iam:PassRole Resource : !Sub 'arn:aws:iam::${AWS::AccountId}:role/${AWSQuickstartADDSRole}' PolicyName : AD-SSM-PassRole Path : / ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy' AssumeRolePolicyDocument : Statement : - Action : - sts:AssumeRole Principal : Service : - ec2.amazonaws.com Effect : Allow Version : '2012-10-17' ADServerProfile : Type : AWS::IAM::InstanceProfile Properties : Roles : - !Ref 'ADServerRole' Path : / ADAdminSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'ADAdministratorSecret-${AWS::StackName}' Description : Administrator Password for AD Quick Start GenerateSecretString : SecretStringTemplate : '{\"username\": \"Administrator\"}' GenerateStringKey : \"password\" PasswordLength : 30 ExcludeCharacters : '\"@/\\' RestoreModeSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'RestoreModeSecrets-${AWS::StackName}' Description : Restore Mode Password for AD Quick Start GenerateSecretString : SecretStringTemplate : '{\"username\": \"Administrator\"}' GenerateStringKey : \"password\" PasswordLength : 30 ExcludeCharacters : '\"@/\\' ADAltUserSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'ADAltUserSecrets-${AWS::StackName}' Description : Alternate AD Admin User from AD Quick Start SecretString : !Sub '{\"username\":\"${DomainAdminUser}\",\"password\":\"${DomainAdminPassword}\"}' DomainJoinSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'DomainJoinSecrets-${AWS::StackName}' Description : Alternate AD Admin User from AD Quick Start SecretString : !Sub '{\"username\":\"${DomainNetBIOSName}\\\\${DomainAdminUser}\",\"password\":\"${DomainAdminPassword}\"}' DomainNameParam : Type : \"AWS::SSM::Parameter\" Properties : Name : \"/demo/AD/DomainName\" Type : \"String\" Value : !Ref 'DomainDNSName' Description : \"SSM Parameter for Domain Name.\" DomainController1 : Type : AWS::EC2::Instance CreationPolicy : ResourceSignal : Timeout : PT60M Count : 1 Properties : ImageId : !Ref 'WS2016FULLBASE' IamInstanceProfile : !Ref 'ADServerProfile' InstanceType : !Ref 'ADServer1InstanceType' SubnetId : !Ref 'PrivateSubnet1ID' Tags : - Key : Name Value : !Ref 'ADServer1NetBIOSName' BlockDeviceMappings : - DeviceName : /dev/sda1 Ebs : VolumeSize : '100' VolumeType : gp2 SecurityGroupIds : - !Ref 'DomainControllersSG' PrivateIpAddress : !Ref 'ADServer1PrivateIP' UserData : !Base64 Fn::Join : - '' - - \"<powershell>\\n\" - 'Start-SSMAutomationExecution -DocumentName ' - !Sub '\"${AWSQuickstartActiveDirectoryDS}\"' - ' -Parameter @{' - '\"ADServer1NetBIOSName\"=' - !Sub '\"${ADServer1NetBIOSName}\"' - ';\"ADServer1PrivateIP\"=' - !Sub '\"${ADServer1PrivateIP}\"' - ';\"DomainDNSName\"=' - !Sub '\"${DomainDNSName}\"' - ';\"DomainNetBIOSName\"=' - !Sub '\"${DomainNetBIOSName}\"' - ';\"VPCCIDR\"=' - !Sub '\"${VPCCIDR}\"' - ';\"QSS3BucketName\"=' - !Sub '\"${QSS3BucketName}\"' - ';\"QSS3KeyPrefix\"=' - !Sub '\"${QSS3KeyPrefix}\"' - ';\"ADAdminSecParamName\"=' - !Sub '\"${ADAdminSecrets}\"' - ';\"ADAltUserSecParamName\"=' - !Sub '\"${ADAltUserSecrets}\"' - ';\"RestoreModeSecParamName\"=' - !Sub '\"${RestoreModeSecrets}\"' - ';\"StackName\"=' - !Sub '\"${AWS::StackName}\"' - ';\"URLSuffix\"=' - !Sub '\"${AWS::URLSuffix}\"' - ';\"AutomationAssumeRole\"=' - !Sub '\"arn:${AWS::Partition}:iam::${AWS::AccountId}:role/${AWSQuickstartADDSRole}\"' - '}' - \"\\n\" - \"</powershell>\\n\" DomainControllersSG : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Domain Controllers Security Group VpcId : Ref : VPCID SecurityGroupIngress : - IpProtocol : tcp FromPort : 5985 ToPort : 5985 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 80 ToPort : 80 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 53 ToPort : 53 CidrIp : !Ref VPCCIDR - IpProtocol : udp FromPort : 53 ToPort : 53 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 3389 ToPort : 3389 CidrIp : !Ref VPCCIDR - IpProtocol : udp FromPort : 123 ToPort : 123 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 135 ToPort : 135 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 9389 ToPort : 9389 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 138 ToPort : 138 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 445 ToPort : 445 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 445 ToPort : 445 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 464 ToPort : 464 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 464 ToPort : 464 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 49152 ToPort : 65535 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 49152 ToPort : 65535 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 389 ToPort : 389 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 389 ToPort : 389 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 636 ToPort : 636 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 3268 ToPort : 3268 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 3269 ToPort : 3269 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 9389 ToPort : 9389 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 88 ToPort : 88 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 88 ToPort : 88 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 5355 ToPort : 5355 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 137 ToPort : 137 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 139 ToPort : 139 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 5722 ToPort : 5722 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : icmp FromPort : -1 ToPort : -1 SourceSecurityGroupId : !Ref DomainMembersSG DomainMembersSG : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Domain Members VpcId : !Ref VPCID SecurityGroupIngress : - IpProtocol : tcp FromPort : 5985 ToPort : 5985 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 3389 ToPort : 3389 CidrIp : !Ref VPCCIDR DCSecurityGroupIngress : Type : AWS::EC2::SecurityGroupIngress Properties : Description : Security Group Rule between Domain Controllers GroupId : !Ref DomainControllersSG IpProtocol : -1 FromPort : -1 ToPort : -1 SourceSecurityGroupId : !Ref DomainControllersSG LambdaSSMRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - s3:PutObject Resource : - !Sub \"${DSCBucket.Arn}\" - !Sub \"${DSCBucket.Arn}/*\" PolicyName : write-mof-s3 Path : / AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : - lambda.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole' WriteMOFFunction : Type : AWS::Lambda::Function Properties : Code : ZipFile : > var AWS = require('aws-sdk'), s3 = new AWS.S3(); const response = require(\"cfn-response\"); exports.handler = async (event, context) => { console.log(JSON.stringify(event)); if (event.RequestType === 'Delete') { await postResponse(event, context, response.SUCCESS, {}) return; } function postResponse(event, context, status, data){ return new Promise((resolve, reject) => { setTimeout(() => response.send(event, context, status, data), 5000) }); } await s3.putObject({ Body: event.ResourceProperties.Body, Bucket: event.ResourceProperties.Bucket, Key: event.ResourceProperties.Key }).promise(); await postResponse(event, context, response.SUCCESS, {}); }; Handler : index.handler Role : !GetAtt LambdaSSMRole.Arn Runtime : nodejs10.x Timeout : 10 WriteDomainJoinMOF : Type : Custom::WriteMOFFile Properties : ServiceToken : !GetAtt WriteMOFFunction.Arn Bucket : !Ref DSCBucket Key : !Sub \"DomainJoin-${AWS::StackName}.mof\" Body : !Sub | /* @TargetNode='localhost' */ instance of MSFT_Credential as $MSFT_Credential1ref { Password = \"managementgovernancesample\"; UserName = \"${DomainJoinSecrets}\"; }; instance of DSC_Computer as $DSC_Computer1ref { ResourceID = \"[Computer]JoinDomain\"; Credential = $MSFT_Credential1ref; DomainName = \"{tag:DomainToJoin}\"; Name = \"{tag:Name}\"; ModuleName = \"ComputerManagementDsc\"; ModuleVersion = \"8.0.0\"; ConfigurationName = \"DomainJoin\"; }; instance of OMI_ConfigurationDocument { Version=\"2.0.0\"; MinimumCompatibleVersion = \"1.0.0\"; CompatibleVersionAdditionalProperties= {\"Omi_BaseResource:ConfigurationName\"}; Name=\"DomainJoin\"; }; DomainJoinAssociation : Type : AWS::SSM::Association Properties : # We are using the AWS-ApplyDSCMofs Document Name : AWS-ApplyDSCMofs Targets : - Key : \"tag:DomainToJoin\" Values : - !Ref \"DomainDNSName\" OutputLocation : S3Location : OutputS3BucketName : !Ref DSCBucket OutputS3KeyPrefix : 'logs/' # Here we have a Cron Expression that will run this everyday at 23:15 every day ScheduleExpression : \"cron(15 23 * * ? *)\" Parameters : # Here the Mof file resides in a S3 Bucket, notice the way that S3 is being referenced here. MofsToApply : - !Sub \"s3:${DSCBucket}:DomainJoin-${AWS::StackName}.mof\" ServicePath : - default MofOperationMode : - Apply ComplianceType : - Custom:DomainJoinSample ModuleSourceBucketName : - \"NONE\" AllowPSGalleryModuleSource : - \"True\" RebootBehavior : - \"AfterMof\" UseComputerNameForReporting : - \"False\" EnableVerboseLogging : - \"False\" EnableDebugLogging : - \"False\" Outputs : DomainAdmin : Value : !Join - '' - - !Ref 'DomainNetBIOSName' - \\ - !Ref 'DomainAdminUser' Description : Domain administrator account DomainMemberSGID : Value : !Ref 'DomainMembersSG' Description : Domain Member Security Group ID SecretsArn : Value : !Ref 'DomainJoinSecrets' DSCBucket : Value : !Ref 'DSCBucket' Lab CLoud Endure \u00b6 Guide","title":"Windows"},{"location":"platforms/aws/migrate/windows/#migrate-windows-workloads","text":"","title":"Migrate Windows Workloads"},{"location":"platforms/aws/migrate/windows/#applications","text":"CloudEndure AWS Server Migration Server -> Create AMIS","title":"Applications"},{"location":"platforms/aws/migrate/windows/#lab-trust-relationship-between-aws-managed-ad-and-on-premise-ad-with-fsx","text":"Lab Guide AWS Support Contact: Hans Moser - hansmose@amazon.ch Hans Moser - Info About D: Drive Regarding D: Drive: In our \u2018source Fileserver\u2019 we have the files on C:, hence the first export also include the directory paths to C: But on FSX, our target, the data drive is D: So to make the import work, we\u2019ve to change the paths, so that we target the correct folder paths on the destination (our FSX).","title":"Lab - Trust Relationship between AWS Managed AD and On-Premise AD with FSX"},{"location":"platforms/aws/migrate/windows/#cloudformation","text":"","title":"Cloudformation"},{"location":"platforms/aws/migrate/windows/#parameters","text":"Key Value ADServer1InstanceType m5.large ADServer1NetBIOSName DC1 ADServer1PrivateIP 10.0.0.10 ASGInstanceType m5.large ASGSize 2 AvailabilityZones us-west-2a,us-west-2b DemoInstances No DeployManagedAD Yes DomainAdminPassword **** DomainAdminUser Admin DomainDNSName example.com DomainNetBIOSName example ManagedADEdition Standard ManagedDomainDNSName managedexample.com ManagedDomainNetBIOSName managedexample NumberOfAZs 2 PrivateSubnet1CIDR 10.0.0.0/19 PrivateSubnet2CIDR 10.0.32.0/19 PrivateSubnet3CIDR - PublicSubnet1CIDR 10.0.128.0/20 PublicSubnet2CIDR 10.0.144.0/20 PublicSubnet3CIDR - QSS3BucketName alpublic QSS3KeyPrefix quickstart-microsoft-activedirectory/ SrvInstanceType t3.large VPCCIDR 10.0.0.0/16","title":"Parameters"},{"location":"platforms/aws/migrate/windows/#vpcstack","text":"{ \"AWSTemplateFormatVersion\" : \"2010-09-09\" , \"Description\" : \"This template creates a Multi-AZ, multi-subnet VPC infrastructure with managed NAT gateways in the public subnet for each Availability Zone. You can also create additional private subnets with dedicated custom network access control lists (ACLs). If you deploy the Quick Start in a region that doesn't support NAT gateways, NAT instances are deployed instead. **WARNING** This template creates AWS resources. You will be billed for the AWS resources used if you create a stack from this template. QS(0027)\" , \"Metadata\" : { \"AWS::CloudFormation::Interface\" : { \"ParameterGroups\" : [ { \"Label\" : { \"default\" : \"Availability Zone Configuration\" }, \"Parameters\" : [ \"AvailabilityZones\" , \"NumberOfAZs\" ] }, { \"Label\" : { \"default\" : \"Network Configuration\" }, \"Parameters\" : [ \"VPCCIDR\" , \"PublicSubnet1CIDR\" , \"PublicSubnet2CIDR\" , \"PublicSubnet3CIDR\" , \"PublicSubnet4CIDR\" , \"PublicSubnetTag1\" , \"PublicSubnetTag2\" , \"PublicSubnetTag3\" , \"CreatePrivateSubnets\" , \"PrivateSubnet1ACIDR\" , \"PrivateSubnet2ACIDR\" , \"PrivateSubnet3ACIDR\" , \"PrivateSubnet4ACIDR\" , \"PrivateSubnetATag1\" , \"PrivateSubnetATag2\" , \"PrivateSubnetATag3\" , \"CreateAdditionalPrivateSubnets\" , \"PrivateSubnet1BCIDR\" , \"PrivateSubnet2BCIDR\" , \"PrivateSubnet3BCIDR\" , \"PrivateSubnet4BCIDR\" , \"PrivateSubnetBTag1\" , \"PrivateSubnetBTag2\" , \"PrivateSubnetBTag3\" , \"VPCTenancy\" ] }, { \"Label\" : { \"default\" : \"Deprecated: NAT Instance Configuration\" }, \"Parameters\" : [ \"KeyPairName\" , \"NATInstanceType\" ] } ], \"ParameterLabels\" : { \"AvailabilityZones\" : { \"default\" : \"Availability Zones\" }, \"CreateAdditionalPrivateSubnets\" : { \"default\" : \"Create additional private subnets with dedicated network ACLs\" }, \"CreatePrivateSubnets\" : { \"default\" : \"Create private subnets\" }, \"KeyPairName\" : { \"default\" : \"Deprecated: Key pair name\" }, \"NATInstanceType\" : { \"default\" : \"Deprecated: NAT instance type\" }, \"NumberOfAZs\" : { \"default\" : \"Number of Availability Zones\" }, \"PrivateSubnet1ACIDR\" : { \"default\" : \"Private subnet 1A CIDR\" }, \"PrivateSubnet1BCIDR\" : { \"default\" : \"Private subnet 1B with dedicated network ACL CIDR\" }, \"PrivateSubnet2ACIDR\" : { \"default\" : \"Private subnet 2A CIDR\" }, \"PrivateSubnet2BCIDR\" : { \"default\" : \"Private subnet 2B with dedicated network ACL CIDR\" }, \"PrivateSubnet3ACIDR\" : { \"default\" : \"Private subnet 3A CIDR\" }, \"PrivateSubnet3BCIDR\" : { \"default\" : \"Private subnet 3B with dedicated network ACL CIDR\" }, \"PrivateSubnet4ACIDR\" : { \"default\" : \"Private subnet 4A CIDR\" }, \"PrivateSubnet4BCIDR\" : { \"default\" : \"Private subnet 4B with dedicated network ACL CIDR\" }, \"PrivateSubnetATag1\" : { \"default\" : \"Tag for Private A Subnets\" }, \"PrivateSubnetATag2\" : { \"default\" : \"Tag for Private A Subnets\" }, \"PrivateSubnetATag3\" : { \"default\" : \"Tag for Private A Subnets\" }, \"PrivateSubnetBTag1\" : { \"default\" : \"Tag for Private B Subnets\" }, \"PrivateSubnetBTag2\" : { \"default\" : \"Tag for Private B Subnets\" }, \"PrivateSubnetBTag3\" : { \"default\" : \"Tag for Private B Subnets\" }, \"PublicSubnet1CIDR\" : { \"default\" : \"Public subnet 1 CIDR\" }, \"PublicSubnet2CIDR\" : { \"default\" : \"Public subnet 2 CIDR\" }, \"PublicSubnet3CIDR\" : { \"default\" : \"Public subnet 3 CIDR\" }, \"PublicSubnet4CIDR\" : { \"default\" : \"Public subnet 4 CIDR\" }, \"PublicSubnetTag1\" : { \"default\" : \"Tag for Public Subnets\" }, \"PublicSubnetTag2\" : { \"default\" : \"Tag for Public Subnets\" }, \"PublicSubnetTag3\" : { \"default\" : \"Tag for Public Subnets\" }, \"VPCCIDR\" : { \"default\" : \"VPC CIDR\" }, \"VPCTenancy\" : { \"default\" : \"VPC Tenancy\" } } } }, \"Parameters\" : { \"AvailabilityZones\" : { \"Description\" : \"List of Availability Zones to use for the subnets in the VPC. Note: The logical order is preserved.\" , \"Type\" : \"List<AWS::EC2::AvailabilityZone::Name>\" }, \"CreateAdditionalPrivateSubnets\" : { \"AllowedValues\" : [ \"true\" , \"false\" ], \"Default\" : \"false\" , \"Description\" : \"Set to true to create a network ACL protected subnet in each Availability Zone. If false, the CIDR parameters for those subnets will be ignored. If true, it also requires that the 'Create private subnets' parameter is also true to have any effect.\" , \"Type\" : \"String\" }, \"CreatePrivateSubnets\" : { \"AllowedValues\" : [ \"true\" , \"false\" ], \"Default\" : \"true\" , \"Description\" : \"Set to false to create only public subnets. If false, the CIDR parameters for ALL private subnets will be ignored.\" , \"Type\" : \"String\" }, \"KeyPairName\" : { \"Description\" : \"Deprecated. NAT gateways are now supported in all regions.\" , \"Type\" : \"String\" , \"Default\" : \"deprecated\" }, \"NATInstanceType\" : { \"Default\" : \"deprecated\" , \"Description\" : \"Deprecated. NAT gateways are now supported in all regions.\" , \"Type\" : \"String\" }, \"NumberOfAZs\" : { \"AllowedValues\" : [ \"2\" , \"3\" , \"4\" ], \"Default\" : \"2\" , \"Description\" : \"Number of Availability Zones to use in the VPC. This must match your selections in the list of Availability Zones parameter.\" , \"Type\" : \"String\" }, \"PrivateSubnet1ACIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.0.0/19\" , \"Description\" : \"CIDR block for private subnet 1A located in Availability Zone 1\" , \"Type\" : \"String\" }, \"PrivateSubnet1BCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.192.0/21\" , \"Description\" : \"CIDR block for private subnet 1B with dedicated network ACL located in Availability Zone 1\" , \"Type\" : \"String\" }, \"PrivateSubnet2ACIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.32.0/19\" , \"Description\" : \"CIDR block for private subnet 2A located in Availability Zone 2\" , \"Type\" : \"String\" }, \"PrivateSubnet2BCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.200.0/21\" , \"Description\" : \"CIDR block for private subnet 2B with dedicated network ACL located in Availability Zone 2\" , \"Type\" : \"String\" }, \"PrivateSubnet3ACIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.64.0/19\" , \"Description\" : \"CIDR block for private subnet 3A located in Availability Zone 3\" , \"Type\" : \"String\" }, \"PrivateSubnet3BCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.208.0/21\" , \"Description\" : \"CIDR block for private subnet 3B with dedicated network ACL located in Availability Zone 3\" , \"Type\" : \"String\" }, \"PrivateSubnet4ACIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.96.0/19\" , \"Description\" : \"CIDR block for private subnet 4A located in Availability Zone 4\" , \"Type\" : \"String\" }, \"PrivateSubnet4BCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.216.0/21\" , \"Description\" : \"CIDR block for private subnet 4B with dedicated network ACL located in Availability Zone 4\" , \"Type\" : \"String\" }, \"PrivateSubnetATag1\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"Network=Private\" , \"Description\" : \"tag to add to private subnets A, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetATag2\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to private subnets A, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetATag3\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to private subnets A, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetBTag1\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"Network=Private\" , \"Description\" : \"tag to add to private subnets B, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetBTag2\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to private subnets B, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PrivateSubnetBTag3\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to private subnets B, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PublicSubnet1CIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.128.0/20\" , \"Description\" : \"CIDR block for the public DMZ subnet 1 located in Availability Zone 1\" , \"Type\" : \"String\" }, \"PublicSubnet2CIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.144.0/20\" , \"Description\" : \"CIDR block for the public DMZ subnet 2 located in Availability Zone 2\" , \"Type\" : \"String\" }, \"PublicSubnet3CIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.160.0/20\" , \"Description\" : \"CIDR block for the public DMZ subnet 3 located in Availability Zone 3\" , \"Type\" : \"String\" }, \"PublicSubnet4CIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.176.0/20\" , \"Description\" : \"CIDR block for the public DMZ subnet 4 located in Availability Zone 4\" , \"Type\" : \"String\" }, \"PublicSubnetTag1\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"Network=Public\" , \"Description\" : \"tag to add to public subnets, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PublicSubnetTag2\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to public subnets, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"PublicSubnetTag3\" : { \"AllowedPattern\" : \"^([a-zA-Z0-9+\\\\-._:/@]+=[a-zA-Z0-9+\\\\-.,_:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]*)?$\" , \"ConstraintDescription\" : \"tags must be in format \\\"Key=Value\\\" keys can only contain [a-zA-Z0-9+\\\\-._:/@], values can contain [a-zA-Z0-9+\\\\-._:/@ *\\\\\\\\\\\"'\\\\[\\\\]\\\\{\\\\}]\" , \"Default\" : \"\" , \"Description\" : \"tag to add to public subnets, in format Key=Value (Optional)\" , \"Type\" : \"String\" }, \"VPCCIDR\" : { \"AllowedPattern\" : \"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\\\/(1[6-9]|2[0-8]))$\" , \"ConstraintDescription\" : \"CIDR block parameter must be in the form x.x.x.x/16-28\" , \"Default\" : \"10.0.0.0/16\" , \"Description\" : \"CIDR block for the VPC\" , \"Type\" : \"String\" }, \"VPCTenancy\" : { \"AllowedValues\" : [ \"default\" , \"dedicated\" ], \"Default\" : \"default\" , \"Description\" : \"The allowed tenancy of instances launched into the VPC\" , \"Type\" : \"String\" } }, \"Conditions\" : { \"3AZCondition\" : { \"Fn::Or\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"NumberOfAZs\" }, \"3\" ] }, { \"Condition\" : \"4AZCondition\" } ] }, \"4AZCondition\" : { \"Fn::Equals\" : [ { \"Ref\" : \"NumberOfAZs\" }, \"4\" ] }, \"AdditionalPrivateSubnetsCondition\" : { \"Fn::And\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"CreatePrivateSubnets\" }, \"true\" ] }, { \"Fn::Equals\" : [ { \"Ref\" : \"CreateAdditionalPrivateSubnets\" }, \"true\" ] } ] }, \"AdditionalPrivateSubnets&3AZCondition\" : { \"Fn::And\" : [ { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" }, { \"Condition\" : \"3AZCondition\" } ] }, \"AdditionalPrivateSubnets&4AZCondition\" : { \"Fn::And\" : [ { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" }, { \"Condition\" : \"4AZCondition\" } ] }, \"GovCloudCondition\" : { \"Fn::Equals\" : [ { \"Ref\" : \"AWS::Region\" }, \"us-gov-west-1\" ] }, \"NVirginiaRegionCondition\" : { \"Fn::Equals\" : [ { \"Ref\" : \"AWS::Region\" }, \"us-east-1\" ] }, \"PrivateSubnetsCondition\" : { \"Fn::Equals\" : [ { \"Ref\" : \"CreatePrivateSubnets\" }, \"true\" ] }, \"PrivateSubnets&3AZCondition\" : { \"Fn::And\" : [ { \"Condition\" : \"PrivateSubnetsCondition\" }, { \"Condition\" : \"3AZCondition\" } ] }, \"PrivateSubnets&4AZCondition\" : { \"Fn::And\" : [ { \"Condition\" : \"PrivateSubnetsCondition\" }, { \"Condition\" : \"4AZCondition\" } ] }, \"PrivateSubnetATag1Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetATag1\" }, \"\" ] } ] }, \"PrivateSubnetATag2Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetATag2\" }, \"\" ] } ] }, \"PrivateSubnetATag3Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetATag3\" }, \"\" ] } ] }, \"PrivateSubnetBTag1Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetBTag1\" }, \"\" ] } ] }, \"PrivateSubnetBTag2Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetBTag2\" }, \"\" ] } ] }, \"PrivateSubnetBTag3Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PrivateSubnetBTag3\" }, \"\" ] } ] }, \"PublicSubnetTag1Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PublicSubnetTag1\" }, \"\" ] } ] }, \"PublicSubnetTag2Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PublicSubnetTag2\" }, \"\" ] } ] }, \"PublicSubnetTag3Condition\" : { \"Fn::Not\" : [ { \"Fn::Equals\" : [ { \"Ref\" : \"PublicSubnetTag3\" }, \"\" ] } ] } }, \"Resources\" : { \"DHCPOptions\" : { \"Type\" : \"AWS::EC2::DHCPOptions\" , \"Properties\" : { \"DomainName\" : { \"Fn::If\" : [ \"NVirginiaRegionCondition\" , \"ec2.internal\" , { \"Fn::Sub\" : \"${AWS::Region}.compute.internal\" } ] }, \"DomainNameServers\" : [ \"AmazonProvidedDNS\" ] } }, \"VPC\" : { \"Type\" : \"AWS::EC2::VPC\" , \"Properties\" : { \"CidrBlock\" : { \"Ref\" : \"VPCCIDR\" }, \"InstanceTenancy\" : { \"Ref\" : \"VPCTenancy\" }, \"EnableDnsSupport\" : true , \"EnableDnsHostnames\" : true , \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : { \"Ref\" : \"AWS::StackName\" } } ] } }, \"VPCDHCPOptionsAssociation\" : { \"Type\" : \"AWS::EC2::VPCDHCPOptionsAssociation\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"DhcpOptionsId\" : { \"Ref\" : \"DHCPOptions\" } } }, \"InternetGateway\" : { \"Type\" : \"AWS::EC2::InternetGateway\" , \"Properties\" : { \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : { \"Ref\" : \"AWS::StackName\" } } ] } }, \"VPCGatewayAttachment\" : { \"Type\" : \"AWS::EC2::VPCGatewayAttachment\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"InternetGatewayId\" : { \"Ref\" : \"InternetGateway\" } } }, \"PrivateSubnet1A\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet1ACIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"0\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 1A\" }, { \"Fn::If\" : [ \"PrivateSubnetATag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet1B\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet1BCIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"0\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 1B\" }, { \"Fn::If\" : [ \"PrivateSubnetBTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet2A\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet2ACIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"1\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 2A\" }, { \"Fn::If\" : [ \"PrivateSubnetATag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet2B\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet2BCIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"1\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 2B\" }, { \"Fn::If\" : [ \"PrivateSubnetBTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet3A\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet3ACIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"2\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 3A\" }, { \"Fn::If\" : [ \"PrivateSubnetATag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet3B\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet3BCIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"2\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 3B\" }, { \"Fn::If\" : [ \"PrivateSubnetBTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet4A\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet4ACIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"3\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 4A\" }, { \"Fn::If\" : [ \"PrivateSubnetATag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetATag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetATag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PrivateSubnet4B\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PrivateSubnet4BCIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"3\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 4B\" }, { \"Fn::If\" : [ \"PrivateSubnetBTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnetBTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PrivateSubnetBTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ] } }, \"PublicSubnet1\" : { \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PublicSubnet1CIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"0\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public subnet 1\" }, { \"Fn::If\" : [ \"PublicSubnetTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"MapPublicIpOnLaunch\" : true } }, \"PublicSubnet2\" : { \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PublicSubnet2CIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"1\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public subnet 2\" }, { \"Fn::If\" : [ \"PublicSubnetTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"MapPublicIpOnLaunch\" : true } }, \"PublicSubnet3\" : { \"Condition\" : \"3AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PublicSubnet3CIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"2\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public subnet 3\" }, { \"Fn::If\" : [ \"PublicSubnetTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"MapPublicIpOnLaunch\" : true } }, \"PublicSubnet4\" : { \"Condition\" : \"4AZCondition\" , \"Type\" : \"AWS::EC2::Subnet\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"CidrBlock\" : { \"Ref\" : \"PublicSubnet4CIDR\" }, \"AvailabilityZone\" : { \"Fn::Select\" : [ \"3\" , { \"Ref\" : \"AvailabilityZones\" } ] }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public subnet 4\" }, { \"Fn::If\" : [ \"PublicSubnetTag1Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag1\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag2Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag2\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PublicSubnetTag3Condition\" , { \"Key\" : { \"Fn::Select\" : [ \"0\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] }, \"Value\" : { \"Fn::Select\" : [ \"1\" , { \"Fn::Split\" : [ \"=\" , { \"Ref\" : \"PublicSubnetTag3\" } ] } ] } }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"MapPublicIpOnLaunch\" : true } }, \"PrivateSubnet1ARouteTable\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 1A\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet1ARoute\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet1ARouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway1\" } } }, \"PrivateSubnet1ARouteTableAssociation\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet1A\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet1ARouteTable\" } } }, \"PrivateSubnet2ARouteTable\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 2A\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet2ARoute\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet2ARouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway2\" } } }, \"PrivateSubnet2ARouteTableAssociation\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet2A\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet2ARouteTable\" } } }, \"PrivateSubnet3ARouteTable\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 3A\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet3ARoute\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet3ARouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway3\" } } }, \"PrivateSubnet3ARouteTableAssociation\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet3A\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet3ARouteTable\" } } }, \"PrivateSubnet4ARouteTable\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 4A\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet4ARoute\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet4ARouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway4\" } } }, \"PrivateSubnet4ARouteTableAssociation\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet4A\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet4ARouteTable\" } } }, \"PrivateSubnet1BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 1B\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet1BRoute\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet1BRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway1\" } } }, \"PrivateSubnet1BRouteTableAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet1B\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet1BRouteTable\" } } }, \"PrivateSubnet1BNetworkAcl\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAcl\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"NACL Protected subnet 1\" }, { \"Key\" : \"Network\" , \"Value\" : \"NACL Protected\" } ] } }, \"PrivateSubnet1BNetworkAclEntryInbound\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : false , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet1BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet1BNetworkAclEntryOutbound\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : true , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet1BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet1BNetworkAclAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetNetworkAclAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet1B\" }, \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet1BNetworkAcl\" } } }, \"PrivateSubnet2BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 2B\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet2BRoute\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet2BRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway2\" } } }, \"PrivateSubnet2BRouteTableAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet2B\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet2BRouteTable\" } } }, \"PrivateSubnet2BNetworkAcl\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAcl\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"NACL Protected subnet 2\" }, { \"Key\" : \"Network\" , \"Value\" : \"NACL Protected\" } ] } }, \"PrivateSubnet2BNetworkAclEntryInbound\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : false , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet2BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet2BNetworkAclEntryOutbound\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : true , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet2BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet2BNetworkAclAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::SubnetNetworkAclAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet2B\" }, \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet2BNetworkAcl\" } } }, \"PrivateSubnet3BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 3B\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet3BRoute\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet3BRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway3\" } } }, \"PrivateSubnet3BRouteTableAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet3B\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet3BRouteTable\" } } }, \"PrivateSubnet3BNetworkAcl\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAcl\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"NACL Protected subnet 3\" }, { \"Key\" : \"Network\" , \"Value\" : \"NACL Protected\" } ] } }, \"PrivateSubnet3BNetworkAclEntryInbound\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : false , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet3BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet3BNetworkAclEntryOutbound\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : true , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet3BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet3BNetworkAclAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Type\" : \"AWS::EC2::SubnetNetworkAclAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet3B\" }, \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet3BNetworkAcl\" } } }, \"PrivateSubnet4BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Private subnet 4B\" }, { \"Key\" : \"Network\" , \"Value\" : \"Private\" } ] } }, \"PrivateSubnet4BRoute\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet4BRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"NatGatewayId\" : { \"Ref\" : \"NATGateway4\" } } }, \"PrivateSubnet4BRouteTableAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet4B\" }, \"RouteTableId\" : { \"Ref\" : \"PrivateSubnet4BRouteTable\" } } }, \"PrivateSubnet4BNetworkAcl\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAcl\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"NACL Protected subnet 4\" }, { \"Key\" : \"Network\" , \"Value\" : \"NACL Protected\" } ] } }, \"PrivateSubnet4BNetworkAclEntryInbound\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : false , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet4BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet4BNetworkAclEntryOutbound\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::NetworkAclEntry\" , \"Properties\" : { \"CidrBlock\" : \"0.0.0.0/0\" , \"Egress\" : true , \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet4BNetworkAcl\" }, \"Protocol\" : -1 , \"RuleAction\" : \"allow\" , \"RuleNumber\" : 100 } }, \"PrivateSubnet4BNetworkAclAssociation\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Type\" : \"AWS::EC2::SubnetNetworkAclAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PrivateSubnet4B\" }, \"NetworkAclId\" : { \"Ref\" : \"PrivateSubnet4BNetworkAcl\" } } }, \"PublicSubnetRouteTable\" : { \"Type\" : \"AWS::EC2::RouteTable\" , \"Properties\" : { \"VpcId\" : { \"Ref\" : \"VPC\" }, \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"Public Subnets\" }, { \"Key\" : \"Network\" , \"Value\" : \"Public\" } ] } }, \"PublicSubnetRoute\" : { \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::Route\" , \"Properties\" : { \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" }, \"DestinationCidrBlock\" : \"0.0.0.0/0\" , \"GatewayId\" : { \"Ref\" : \"InternetGateway\" } } }, \"PublicSubnet1RouteTableAssociation\" : { \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" }, \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" } } }, \"PublicSubnet2RouteTableAssociation\" : { \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PublicSubnet2\" }, \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" } } }, \"PublicSubnet3RouteTableAssociation\" : { \"Condition\" : \"3AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PublicSubnet3\" }, \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" } } }, \"PublicSubnet4RouteTableAssociation\" : { \"Condition\" : \"4AZCondition\" , \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\" , \"Properties\" : { \"SubnetId\" : { \"Ref\" : \"PublicSubnet4\" }, \"RouteTableId\" : { \"Ref\" : \"PublicSubnetRouteTable\" } } }, \"NAT1EIP\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"Domain\" : \"vpc\" } }, \"NAT2EIP\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"Domain\" : \"vpc\" } }, \"NAT3EIP\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"Domain\" : \"vpc\" } }, \"NAT4EIP\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"Domain\" : \"vpc\" } }, \"NATGateway1\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::NatGateway\" , \"Properties\" : { \"AllocationId\" : { \"Fn::GetAtt\" : [ \"NAT1EIP\" , \"AllocationId\" ] }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" } } }, \"NATGateway2\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::NatGateway\" , \"Properties\" : { \"AllocationId\" : { \"Fn::GetAtt\" : [ \"NAT2EIP\" , \"AllocationId\" ] }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet2\" } } }, \"NATGateway3\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::NatGateway\" , \"Properties\" : { \"AllocationId\" : { \"Fn::GetAtt\" : [ \"NAT3EIP\" , \"AllocationId\" ] }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet3\" } } }, \"NATGateway4\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"DependsOn\" : \"VPCGatewayAttachment\" , \"Type\" : \"AWS::EC2::NatGateway\" , \"Properties\" : { \"AllocationId\" : { \"Fn::GetAtt\" : [ \"NAT4EIP\" , \"AllocationId\" ] }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet4\" } } }, \"S3VPCEndpoint\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Type\" : \"AWS::EC2::VPCEndpoint\" , \"Properties\" : { \"PolicyDocument\" : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"*\" , \"Effect\" : \"Allow\" , \"Resource\" : \"*\" , \"Principal\" : \"*\" } ] }, \"RouteTableIds\" : [ { \"Ref\" : \"PrivateSubnet1ARouteTable\" }, { \"Ref\" : \"PrivateSubnet2ARouteTable\" }, { \"Fn::If\" : [ \"PrivateSubnets&3AZCondition\" , { \"Ref\" : \"PrivateSubnet3ARouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"PrivateSubnets&4AZCondition\" , { \"Ref\" : \"PrivateSubnet4ARouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"AdditionalPrivateSubnetsCondition\" , { \"Ref\" : \"PrivateSubnet1BRouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"AdditionalPrivateSubnetsCondition\" , { \"Ref\" : \"PrivateSubnet2BRouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"AdditionalPrivateSubnets&3AZCondition\" , { \"Ref\" : \"PrivateSubnet3BRouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] }, { \"Fn::If\" : [ \"AdditionalPrivateSubnets&4AZCondition\" , { \"Ref\" : \"PrivateSubnet4BRouteTable\" }, { \"Ref\" : \"AWS::NoValue\" } ] } ], \"ServiceName\" : { \"Fn::Sub\" : \"com.amazonaws.${AWS::Region}.s3\" }, \"VpcId\" : { \"Ref\" : \"VPC\" } } } }, \"Outputs\" : { \"NAT1EIP\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"NAT 1 IP address\" , \"Value\" : { \"Ref\" : \"NAT1EIP\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-NAT1EIP\" } } }, \"NAT2EIP\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"NAT 2 IP address\" , \"Value\" : { \"Ref\" : \"NAT2EIP\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-NAT2EIP\" } } }, \"NAT3EIP\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Description\" : \"NAT 3 IP address\" , \"Value\" : { \"Ref\" : \"NAT3EIP\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-NAT3EIP\" } } }, \"NAT4EIP\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Description\" : \"NAT 4 IP address\" , \"Value\" : { \"Ref\" : \"NAT4EIP\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-NAT4EIP\" } } }, \"PrivateSubnet1ACIDR\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"Private subnet 1A CIDR in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1ACIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1ACIDR\" } } }, \"PrivateSubnet1AID\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"Private subnet 1A ID in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1A\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1AID\" } } }, \"PrivateSubnet1BCIDR\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Description\" : \"Private subnet 1B CIDR in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1BCIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1BCIDR\" } } }, \"PrivateSubnet1BID\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Description\" : \"Private subnet 1B ID in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1B\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1BID\" } } }, \"PrivateSubnet2ACIDR\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"Private subnet 2A CIDR in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2ACIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2ACIDR\" } } }, \"PrivateSubnet2AID\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"Private subnet 2A ID in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2A\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2AID\" } } }, \"PrivateSubnet2BCIDR\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Description\" : \"Private subnet 2B CIDR in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2BCIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2BCIDR\" } } }, \"PrivateSubnet2BID\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Description\" : \"Private subnet 2B ID in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2B\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2BID\" } } }, \"PrivateSubnet3ACIDR\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Description\" : \"Private subnet 3A CIDR in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3ACIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3ACIDR\" } } }, \"PrivateSubnet3AID\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Description\" : \"Private subnet 3A ID in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3A\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3AID\" } } }, \"PrivateSubnet3BCIDR\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Description\" : \"Private subnet 3B CIDR in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3BCIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3BCIDR\" } } }, \"PrivateSubnet3BID\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Description\" : \"Private subnet 3B ID in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3B\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3BID\" } } }, \"PrivateSubnet4ACIDR\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Description\" : \"Private subnet 4A CIDR in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4ACIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4ACIDR\" } } }, \"PrivateSubnet4AID\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Description\" : \"Private subnet 4A ID in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4A\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4AID\" } } }, \"PrivateSubnet4BCIDR\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Description\" : \"Private subnet 4B CIDR in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4BCIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4BCIDR\" } } }, \"PrivateSubnet4BID\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Description\" : \"Private subnet 4B ID in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4B\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4BID\" } } }, \"PublicSubnet1CIDR\" : { \"Description\" : \"Public subnet 1 CIDR in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PublicSubnet1CIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet1CIDR\" } } }, \"PublicSubnet1ID\" : { \"Description\" : \"Public subnet 1 ID in Availability Zone 1\" , \"Value\" : { \"Ref\" : \"PublicSubnet1\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet1ID\" } } }, \"PublicSubnet2CIDR\" : { \"Description\" : \"Public subnet 2 CIDR in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PublicSubnet2CIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet2CIDR\" } } }, \"PublicSubnet2ID\" : { \"Description\" : \"Public subnet 2 ID in Availability Zone 2\" , \"Value\" : { \"Ref\" : \"PublicSubnet2\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet2ID\" } } }, \"PublicSubnet3CIDR\" : { \"Condition\" : \"3AZCondition\" , \"Description\" : \"Public subnet 3 CIDR in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PublicSubnet3CIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet3CIDR\" } } }, \"PublicSubnet3ID\" : { \"Condition\" : \"3AZCondition\" , \"Description\" : \"Public subnet 3 ID in Availability Zone 3\" , \"Value\" : { \"Ref\" : \"PublicSubnet3\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet3ID\" } } }, \"PublicSubnet4CIDR\" : { \"Condition\" : \"4AZCondition\" , \"Description\" : \"Public subnet 4 CIDR in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PublicSubnet4CIDR\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet4CIDR\" } } }, \"PublicSubnet4ID\" : { \"Condition\" : \"4AZCondition\" , \"Description\" : \"Public subnet 4 ID in Availability Zone 4\" , \"Value\" : { \"Ref\" : \"PublicSubnet4\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnet4ID\" } } }, \"S3VPCEndpoint\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Description\" : \"S3 VPC Endpoint\" , \"Value\" : { \"Ref\" : \"S3VPCEndpoint\" }, \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-S3VPCEndpoint\" } } }, \"PrivateSubnet1ARouteTable\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1ARouteTable\" }, \"Description\" : \"Private subnet 1A route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1ARouteTable\" } } }, \"PrivateSubnet1BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet1BRouteTable\" }, \"Description\" : \"Private subnet 1B route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet1BRouteTable\" } } }, \"PrivateSubnet2ARouteTable\" : { \"Condition\" : \"PrivateSubnetsCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2ARouteTable\" }, \"Description\" : \"Private subnet 2A route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2ARouteTable\" } } }, \"PrivateSubnet2BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnetsCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet2BRouteTable\" }, \"Description\" : \"Private subnet 2B route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet2BRouteTable\" } } }, \"PrivateSubnet3ARouteTable\" : { \"Condition\" : \"PrivateSubnets&3AZCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3ARouteTable\" }, \"Description\" : \"Private subnet 3A route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3ARouteTable\" } } }, \"PrivateSubnet3BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnets&3AZCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet3BRouteTable\" }, \"Description\" : \"Private subnet 3B route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet3BRouteTable\" } } }, \"PrivateSubnet4ARouteTable\" : { \"Condition\" : \"PrivateSubnets&4AZCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4ARouteTable\" }, \"Description\" : \"Private subnet 4A route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4ARouteTable\" } } }, \"PrivateSubnet4BRouteTable\" : { \"Condition\" : \"AdditionalPrivateSubnets&4AZCondition\" , \"Value\" : { \"Ref\" : \"PrivateSubnet4BRouteTable\" }, \"Description\" : \"Private subnet 4B route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PrivateSubnet4BRouteTable\" } } }, \"PublicSubnetRouteTable\" : { \"Value\" : { \"Ref\" : \"PublicSubnetRouteTable\" }, \"Description\" : \"Public subnet route table\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-PublicSubnetRouteTable\" } } }, \"VPCCIDR\" : { \"Value\" : { \"Ref\" : \"VPCCIDR\" }, \"Description\" : \"VPC CIDR\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-VPCCIDR\" } } }, \"VPCID\" : { \"Value\" : { \"Ref\" : \"VPC\" }, \"Description\" : \"VPC ID\" , \"Export\" : { \"Name\" : { \"Fn::Sub\" : \"${AWS::StackName}-VPCID\" } } } } }","title":"VPCStack"},{"location":"platforms/aws/migrate/windows/#managedadstack","text":"AWSTemplateFormatVersion : '2010-09-09' Description : >- This template creates a managed Microsoft AD Directory Service into private subnets in separate Availability Zones inside a VPC. The default Domain Administrator user is 'admin'. For adding members to the domain, ensure that they are launched into the domain member security group created by this template and then configure them to use the AD instances fixed private IP addresses as the DNS server. **WARNING** This template creates Amazon EC2 Windows instance and related resources. You will be billed for the AWS resources used if you create a stack from this template. QS(0021) Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : Network Configuration Parameters : - VPCCIDR - VPCID - PrivateSubnet1CIDR - PrivateSubnet1ID - PrivateSubnet2CIDR - PrivateSubnet2ID - PublicSubnet1CIDR - PublicSubnet2CIDR - Label : default : Microsoft Active Directory Configuration Parameters : - DomainDNSName - DomainNetBIOSName - DomainAdminPassword - ADEdition - Label : default : AWS Systems Manager AMI configuration Parameters : - WS2019FULLBASE - Label : default : AWS Quick Start Configuration Parameters : - QSS3BucketName - QSS3KeyPrefix ParameterLabels : DomainAdminPassword : default : Domain Admin Password DomainDNSName : default : Domain DNS Name DomainNetBIOSName : default : Domain NetBIOS Name ADEdition : default : AWS Microsoft AD edition PrivateSubnet1CIDR : default : Private Subnet 1 CIDR PrivateSubnet1ID : default : Private Subnet 1 ID PrivateSubnet2CIDR : default : Private Subnet 2 CIDR PrivateSubnet2ID : default : Private Subnet 2 ID PublicSubnet1CIDR : default : Public Subnet 1 CIDR PublicSubnet2CIDR : default : Public Subnet 2 CIDR QSS3BucketName : default : Quick Start S3 Bucket Name QSS3KeyPrefix : default : Quick Start S3 Key Prefix VPCCIDR : default : VPC CIDR VPCID : default : VPC ID WS2019FULLBASE : default : Windows Server 2019 full base AMI Parameters : DomainAdminPassword : Description : Password for the domain admin user. Must be at least 8 characters containing letters, numbers and symbols Type : String MinLength : '8' MaxLength : '32' AllowedPattern : (?=^.{6,255}$)((?=.*\\d)(?=.*[A-Z])(?=.*[a-z])|(?=.*\\d)(?=.*[^A-Za-z0-9])(?=.*[a-z])|(?=.*[^A-Za-z0-9])(?=.*[A-Z])(?=.*[a-z])|(?=.*\\d)(?=.*[A-Z])(?=.*[^A-Za-z0-9]))^.* NoEcho : 'true' DomainDNSName : Description : Fully qualified domain name (FQDN) of the forest root domain e.g. example.com Type : String Default : example.com MinLength : '2' MaxLength : '255' AllowedPattern : '[a-zA-Z0-9\\-]+\\..+' DomainNetBIOSName : Description : NetBIOS name of the domain (upto 15 characters) for users of earlier versions of Windows e.g. EXAMPLE Type : String Default : example MinLength : '1' MaxLength : '15' AllowedPattern : '[a-zA-Z0-9\\-]+' ADEdition : AllowedValues : - Standard - Enterprise Default : Enterprise Description : The AWS Microsoft AD edition. Valid values include Standard and Enterprise. Type : String PrivateSubnet1CIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.0.0/19 Description : CIDR block for private subnet 1 located in Availability Zone 1. Type : String PrivateSubnet1ID : Description : ID of the private subnet 1 in Availability Zone 1 (e.g., subnet-a0246dcd) Type : AWS::EC2::Subnet::Id PrivateSubnet2CIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.32.0/19 Description : CIDR block for private subnet 2 located in Availability Zone 2. Type : String PrivateSubnet2ID : Description : ID of the private subnet 2 in Availability Zone 2 (e.g., subnet-a0246dcd) Type : AWS::EC2::Subnet::Id PublicSubnet1CIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.128.0/20 Description : CIDR Block for the public DMZ subnet 1 located in Availability Zone 1 Type : String PublicSubnet2CIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.144.0/20 Description : CIDR Block for the public DMZ subnet 2 located in Availability Zone 2 Type : String QSS3BucketName : AllowedPattern : ^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$ ConstraintDescription : Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-). Default : aws-quickstart Description : S3 bucket name for the Quick Start assets. Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-). Type : String QSS3KeyPrefix : AllowedPattern : ^[0-9a-zA-Z-/]*$ ConstraintDescription : Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/). Default : quickstart-microsoft-activedirectory/ Description : S3 key prefix for the Quick Start assets. Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/). Type : String VPCCIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.0.0/16 Description : CIDR Block for the VPC Type : String VPCID : Description : ID of the VPC (e.g., vpc-0343606e) Type : AWS::EC2::VPC::Id WS2019FULLBASE : Type : 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>' Default : '/aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base' Rules : SubnetsInVPC : Assertions : - Assert : !EachMemberIn - !ValueOfAll - AWS::EC2::Subnet::Id - VpcId - !RefAll 'AWS::EC2::VPC::Id' AssertDescription : All subnets must in the VPC Conditions : GovCloudCondition : !Equals - !Ref 'AWS::Region' - us-gov-west-1 Resources : DHCPOptions : Type : AWS::EC2::DHCPOptions DependsOn : - MicrosoftAD - SSMWaitCondition Properties : DomainName : !Ref 'DomainDNSName' DomainNameServers : !GetAtt 'MicrosoftAD.DnsIpAddresses' Tags : - Key : Domain Value : !Ref 'DomainDNSName' ADAdminSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'ADAdminSecret-${AWS::StackName}' Description : Admin User Seccrets for Manged AD Quick Start SecretString : !Sub '{\"username\":\"Admin\",\"password\":\"${DomainAdminPassword}\"}' MicrosoftAD : Type : AWS::DirectoryService::MicrosoftAD Properties : Name : !Ref 'DomainDNSName' Edition : !Ref 'ADEdition' ShortName : !Ref 'DomainNetBIOSName' Password : !Ref 'DomainAdminPassword' VpcSettings : SubnetIds : - !Ref 'PrivateSubnet1ID' - !Ref 'PrivateSubnet2ID' VpcId : !Ref 'VPCID' DomainMemberSG : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Domain Members VpcId : !Ref 'VPCID' SecurityGroupIngress : - IpProtocol : tcp FromPort : 5985 ToPort : 5985 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : tcp FromPort : 53 ToPort : 53 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : udp FromPort : 53 ToPort : 53 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : tcp FromPort : 49152 ToPort : 65535 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : udp FromPort : 49152 ToPort : 65535 CidrIp : !Ref 'PrivateSubnet1CIDR' - IpProtocol : tcp FromPort : 5985 ToPort : 5985 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : tcp FromPort : 53 ToPort : 53 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : udp FromPort : 53 ToPort : 53 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : tcp FromPort : 49152 ToPort : 65535 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : udp FromPort : 49152 ToPort : 65535 CidrIp : !Ref 'PrivateSubnet2CIDR' - IpProtocol : tcp FromPort : 3389 ToPort : 3389 CidrIp : !Ref 'PublicSubnet1CIDR' - IpProtocol : tcp FromPort : 3389 ToPort : 3389 CidrIp : !Ref 'PublicSubnet2CIDR' - IpProtocol : tcp FromPort : 636 ToPort : 636 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 3269 ToPort : 3269 CidrIp : !Ref VPCCIDR SSMAutomationRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject Resource : !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*' Effect : Allow PolicyName : aws-quick-start-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - cloudformation:SignalResource Resource : !Sub 'arn:${AWS::Partition}:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${AWS::StackName}/*' PolicyName : aws-quick-start-cfn-signal-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - iam:CreateRole - iam:PutRolePolicy - iam:getRolePolicy - iam:DetachRolePolicy - iam:AttachRolePolicy - iam:DeleteRolePolicy - iam:CreateInstanceProfile - iam:DeleteRole - iam:RemoveRoleFromInstanceProfile - iam:AddRoleToInstanceProfile - iam:DeleteInstanceProfile - iam:PassRole Resource : '*' PolicyName : aws-quick-start-create-role-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - secretsmanager:GetSecretValue - secretsmanager:DescribeSecret Resource : - !Ref 'ADAdminSecrets' - Effect : Allow Action : - ssm:StartAutomationExecution Resource : '*' PolicyName : AD-SSM-Secrets Path : / AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : - ssm.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMFullAccess' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AWSCloudFormationFullAccess' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonEC2FullAccess' LambdaSSMRole : DependsOn : MicrosoftAD Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - iam:PassRole Resource : !GetAtt SSMAutomationRole.Arn PolicyName : QS-SSM-PassRole Path : / AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : - lambda.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AmazonSSMAutomationRole' DNSForwarderSetup : Type : AWS::SSM::Document Properties : DocumentType : Automation Content : schemaVersion : \"0.3\" description : Setup DNS Forwarder for AWS Managed AD to VPC DNS assumeRole : \"{{AutomationAssumeRole}}\" parameters : StackName : description : \"Stack Name Input for cfn resource signal\" type : \"String\" DomainMemberSG : description : Security group ID that can communicate with domain controllers type : \"String\" VPCCIDR : description : VPC CIDR type : \"String\" QSS3BucketName : description : \"S3 bucket name for the Quick Start assets. Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).\" type : \"String\" QSS3KeyPrefix : description : \"S3 key prefix for the Quick Start assets. Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/).\" type : \"String\" PrivateSubnet1ID : description : \"Private Subnet 1 ID\" type : \"String\" AutomationAssumeRole : description : \"(Optional) The ARN of the role that allows Automation to perform the actions on your behalf.\" type : \"String\" DirectoryID : description : \"Alias of the directory ID\" type : \"String\" AWSRegion : description : \"Region\" type : \"String\" URLSuffix : default : \"amazonaws.com\" description : \"AWS URL suffix\" type : \"String\" mainSteps : - name : CreateStack action : aws:createStack onFailure : \"step:CFNSignalEnd\" inputs : StackName : \"SetDNSForwarder\" Capabilities : [ \"CAPABILITY_IAM\" ] TemplateBody : | Description: \"Deploy Instance to create DNS forwarder on AWS Managed AD\" Parameters: WINFULLBASE: Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>' Default: '/aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base' QSS3BucketName: Type: \"String\" Default: \"{{QSS3BucketName}}\" Description: \"Name of Target S3 Bucket\" QSS3KeyPrefix: Type: \"String\" Default: \"{{QSS3KeyPrefix}}\" Description: \"Name of Target S3 Prefix\" SecurityGroup: Description: Security Group to be able to talk Domain Controllers Default: \"{{DomainMemberSG}}\" Type: \"String\" Subnet: Description: \"Subnet to deploy the EC2 instnace\" Default: \"{{PrivateSubnet1ID}}\" Type: \"String\" Resources: ForwarderRole: Type : AWS::IAM::Role Properties: Policies: - PolicyDocument: Version: '2012-10-17' Statement: - Action: - s3:GetObject - s3:ListBucket Resource: - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*' - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}' Effect: Allow PolicyName: s3-instance-bucket-policy Path: / ManagedPolicyArns: - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AWSDirectoryServiceFullAccess' AssumeRolePolicyDocument: Version: \"2012-10-17\" Statement: - Effect: \"Allow\" Principal: Service: - \"ec2.amazonaws.com\" - \"ssm.amazonaws.com\" Action: \"sts:AssumeRole\" IamInstanceProfile: Type: \"AWS::IAM::InstanceProfile\" Properties: Roles: - !Ref ForwarderRole EC2Instance: Type: \"AWS::EC2::Instance\" Properties: ImageId: !Ref WINFULLBASE InstanceType: \"t3.medium\" IamInstanceProfile: !Ref IamInstanceProfile SecurityGroupIds: - !Ref 'SecurityGroup' SubnetId: !Ref Subnet Tags: - Key: \"Name\" Value: \"TempDNSForwarder\" - name : \"getInstanceId\" action : aws:executeAwsApi onFailure : \"step:CFNSignalEnd\" inputs : Service : ec2 Api : DescribeInstances Filters : - Name : \"tag:Name\" Values : [ \"TempDNSForwarder\" ] - Name : \"instance-state-name\" Values : [ \"running\" ] outputs : - Name : InstanceId Selector : \"$.Reservations[0].Instances[0].InstanceId\" Type : String - name : \"CreateDNSForward\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{getInstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : \"S3\" sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.{{URLSuffix}}/{{QSS3KeyPrefix}}scripts/AddDNSForward.ps1\"}' commandLine : \"./AddDNSForward.ps1 -DirectoryID {{DirectoryID}} -VPCCIDR {{VPCCIDR}} -AWSRegion {{AWSRegion}}\" # Determines if CFN Needs to be Signaled or if Work flow should just end - name : CFNSignalEnd action : aws:branch inputs : Choices : - NextStep : signalsuccess Not : Variable : \"{{StackName}}\" StringEquals : \"\" - NextStep : sleepend Variable : \"{{StackName}}\" StringEquals : \"\" # If all steps complete successfully signals CFN of Success - name : \"signalsuccess\" action : \"aws:executeAwsApi\" nextStep : \"deleteStack\" inputs : Service : cloudformation Api : SignalResource LogicalResourceId : \"SSMWaitCondition\" StackName : \"{{StackName}}\" Status : SUCCESS UniqueId : \"{{getInstanceId.InstanceId}}\" # If CFN Signl Not Needed this sleep ends work flow - name : \"sleepend\" action : \"aws:sleep\" nextStep : \"deleteStack\" inputs : Duration : PT1S # If any steps fails signals CFN of Failure - name : \"signalfailure\" action : \"aws:executeAwsApi\" nextStep : \"deleteStack\" inputs : Service : cloudformation Api : SignalResource LogicalResourceId : \"SSMWaitCondition\" StackName : \"{{StackName}}\" Status : FAILURE UniqueId : \"{{getInstanceId.InstanceId}}\" - name : deleteStack action : aws:deleteStack isEnd : true # onFailure: Continue inputs : StackName : \"SetDNSForwarder\" LambdaSSMExecute : DependsOn : LambdaSSMRole Type : AWS::Lambda::Function Properties : Description : Executes SSM Automation Documents Handler : index.handler Runtime : python3.7 Role : !GetAtt LambdaSSMRole.Arn Timeout : 900 Code : ZipFile : | def handler(event, context): import cfnresponse import boto3, os, json from botocore.vendored import requests ssm_cl = boto3.client('ssm') ecr_cl = boto3.client('ecr') req_type = event['RequestType'] print(event) SUCCESS = \"SUCCESS\" FAILED = \"FAILED\" def start_ssmautomation(event): doc_name = event['ResourceProperties']['DocumentName'] stack_name = event['ResourceProperties']['StackName'] ssm_role = event['ResourceProperties']['AutomationAssumeRole'] qs_bucket = event['ResourceProperties']['QSS3BucketName'] qs_bucket_prefix = event['ResourceProperties']['QSS3KeyPrefix'] security_group = event['ResourceProperties']['DomainMemberSG'] subnet_id = event['ResourceProperties']['PrivateSubnet1ID'] vpc_cidr = event['ResourceProperties']['VPCCIDR'] directory_id = event['ResourceProperties']['DirectoryID'] aws_region = event['ResourceProperties']['AWSRegion'] url_suffix = event['ResourceProperties']['URLSuffix'] start_automation = ssm_cl.start_automation_execution( DocumentName= doc_name, Parameters={ 'StackName': [ stack_name ], 'AutomationAssumeRole': [ ssm_role ], 'QSS3BucketName': [ qs_bucket ], 'QSS3KeyPrefix': [ qs_bucket_prefix ], 'DomainMemberSG': [ security_group ], 'PrivateSubnet1ID': [ subnet_id ], 'VPCCIDR': [ vpc_cidr ], 'DirectoryID': [ directory_id ], 'AWSRegion': [ aws_region ], 'URLSuffix': [ url_suffix ] }, ) cfnresponse.send(event, context, SUCCESS, start_automation, start_automation['AutomationExecutionId']) def delete_image(event): cfnresponse.send(event, context, SUCCESS, event, \"Deleted\") actions = { 'Create': start_ssmautomation, 'Delete': delete_image, 'Update': start_ssmautomation } try: actions.get(req_type)(event) except Exception as exc: error_msg = {'Error': '{}'.format(exc)} print(error_msg) cfnresponse.send(event, context, FAILED, error_msg) ExecuteSSMAutomation : DependsOn : LambdaSSMExecute Type : Custom::ExecuteSSMAutomation Properties : ServiceToken : !GetAtt LambdaSSMExecute.Arn DocumentName : !Ref DNSForwarderSetup DomainMemberSG : !Ref DomainMemberSG StackName : !Ref AWS::StackName QSS3BucketName : !Ref QSS3BucketName QSS3KeyPrefix : !Ref QSS3KeyPrefix AutomationAssumeRole : !GetAtt SSMAutomationRole.Arn PrivateSubnet1ID : !Ref PrivateSubnet1ID VPCCIDR : !Ref VPCCIDR DirectoryID : !GetAtt 'MicrosoftAD.Alias' AWSRegion : !Ref 'AWS::Region' URLSuffix : !Ref 'AWS::URLSuffix' SSMWaitHandle : Type : AWS::CloudFormation::WaitConditionHandle SSMWaitCondition : Type : AWS::CloudFormation::WaitCondition CreationPolicy : ResourceSignal : Timeout : PT60M Count : 1 DependsOn : - ExecuteSSMAutomation - SSMWaitHandle Properties : Handle : Ref : \"SSMWaitHandle\" Timeout : \"3600\" Count : 1 AWSQuickstartADRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:ListBucket Resource : - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*' - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}' Effect : Allow PolicyName : aws-quick-start-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - cloudformation:SignalResource Resource : !Sub 'arn:${AWS::Partition}:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${AWS::StackName}/*' - Effect : Allow Action : - ec2:DescribeInstances - ec2:DescribeInstanceStatus - ssm:* Resource : '*' PolicyName : AD-SSM-AutomationExecution Path : / AssumeRolePolicyDocument : Statement : - Action : - sts:AssumeRole Principal : Service : - ec2.amazonaws.com - ssm.amazonaws.com Effect : Allow Version : '2012-10-17' Outputs : ADServer1PrivateIP : Value : !Select - '0' - !GetAtt 'MicrosoftAD.DnsIpAddresses' Description : AD Server 1 Private IP Address (this may vary based on Directory Service order of IP addresses) ADServer2PrivateIP : Value : !Select - '1' - !GetAtt 'MicrosoftAD.DnsIpAddresses' Description : AD Server 2 Private IP Address (this may vary based on Directory Service order of IP addresses) DirectoryID : Value : !Ref 'MicrosoftAD' Description : Directory Services ID DomainAdmin : Value : !Join - '' - - !Ref 'DomainNetBIOSName' - \\admin Description : Domain administrator account DomainMemberSGID : Value : !Ref 'DomainMemberSG' Description : Domain Member Security Group ID ADSecretsArn : Value : !Ref 'ADAdminSecrets' Description : Managed AD Admin Secrets","title":"ManagedADStack"},{"location":"platforms/aws/migrate/windows/#adstack","text":"AWSTemplateFormatVersion : '2010-09-09' Description : This template creates 1 Windows 2019 Active Directory Domain Controllers into private subnets in separate Availability Zones inside a VPC. The default Domain Administrator password will be the one retrieved from the instance. For adding members to the domain, ensure that they are launched into the domain member security group created by this template and then configure them to use the AD instances fixed private IP addresses as the DNS server. **WARNING** This template creates Amazon EC2 Windows instance and related resources. You will be billed for the AWS resources used if you create a stack from this template. QS(0001) Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : Network Configuration Parameters : - VPCCIDR - VPCID - PrivateSubnet1ID - Label : default : Amazon EC2 Configuration Parameters : - ADServer1InstanceType - ADServer1NetBIOSName - ADServer1PrivateIP - ADServer2InstanceType - ADServer2NetBIOSName - ADServer2PrivateIP - WS2016FULLBASE - Label : default : Microsoft Active Directory Configuration Parameters : - DomainAdminPassword - DomainAdminUser - DomainDNSName - DomainNetBIOSName - Label : default : AWS Quick Start Configuration Parameters : - QSS3BucketName - QSS3KeyPrefix ParameterLabels : ADServer1InstanceType : default : Domain Controller 1 Instance Type ADServer1NetBIOSName : default : Domain Controller 1 NetBIOS Name ADServer1PrivateIP : default : Domain Controller 1 Private IP Address DomainAdminPassword : default : Domain Admin Password DomainAdminUser : default : Domain Admin User Name DomainDNSName : default : Domain DNS Name DomainNetBIOSName : default : Domain NetBIOS Name WS2016FULLBASE : default : SSM Parameter Value to grab the lastest AMI ID PrivateSubnet1ID : default : Private Subnet 1 ID QSS3BucketName : default : Quick Start S3 Bucket Name QSS3KeyPrefix : default : Quick Start S3 Key Prefix VPCCIDR : default : VPC CIDR VPCID : default : VPC ID Parameters : ADServer1InstanceType : AllowedValues : - t2.large - m4.large - m4.xlarge - m4.2xlarge - m4.4xlarge - m5.large - m5.xlarge - m5.2xlarge - m5.4xlarge Default : m5.xlarge Description : Amazon EC2 instance type for the first Active Directory instance Type : String ADServer1NetBIOSName : AllowedPattern : '[a-zA-Z0-9\\-]+' Default : DC1 Description : NetBIOS name of the first Active Directory server (up to 15 characters) MaxLength : '15' MinLength : '1' Type : String ADServer1PrivateIP : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$ Default : 10.0.0.10 Description : Fixed private IP for the first Active Directory server located in Availability Zone 1 Type : String DomainAdminPassword : AllowedPattern : (?=^.{6,255}$)((?=.*\\d)(?=.*[A-Z])(?=.*[a-z])|(?=.*\\d)(?=.*[^A-Za-z0-9])(?=.*[a-z])|(?=.*[^A-Za-z0-9])(?=.*[A-Z])(?=.*[a-z])|(?=.*\\d)(?=.*[A-Z])(?=.*[^A-Za-z0-9]))^.* Description : Password for the domain admin user. Must be at least 8 characters containing letters, numbers and symbols MaxLength : '32' MinLength : '8' NoEcho : 'true' Type : String DomainAdminUser : AllowedPattern : '[a-zA-Z0-9]*' Default : Admin Description : User name for the account that will be added as Domain Administrator. This is separate from the default \"Administrator\" account MaxLength : '25' MinLength : '5' Type : String DomainDNSName : AllowedPattern : '[a-zA-Z0-9\\-]+\\..+' Default : example.com Description : Fully qualified domain name (FQDN) of the forest root domain e.g. example.com MaxLength : '255' MinLength : '2' Type : String DomainNetBIOSName : AllowedPattern : '[a-zA-Z0-9\\-]+' Default : example Description : NetBIOS name of the domain (up to 15 characters) for users of earlier versions of Windows e.g. EXAMPLE MaxLength : '15' MinLength : '1' Type : String PrivateSubnet1ID : Description : ID of the private subnet 1 in Availability Zone 1 (e.g., subnet-a0246dcd) Type : AWS::EC2::Subnet::Id QSS3BucketName : AllowedPattern : ^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$ ConstraintDescription : Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-). Default : alpublic Description : S3 bucket name for the Quick Start assets. Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-). Type : String QSS3KeyPrefix : AllowedPattern : ^[0-9a-zA-Z-/]*$ ConstraintDescription : Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/). Default : quickstart-microsoft-activedirectory/ Description : S3 key prefix for the Quick Start assets. Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/). Type : String VPCCIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Default : 10.0.0.0/16 Description : CIDR Block for the VPC Type : String VPCID : Description : ID of the VPC (e.g., vpc-0343606e) Type : AWS::EC2::VPC::Id WS2016FULLBASE : Type : 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>' Default : '/aws/service/ami-windows-latest/Windows_Server-2016-English-Full-Base' Rules : SubnetsInVPC : Assertions : - Assert : !EachMemberIn - !ValueOfAll - AWS::EC2::Subnet::Id - VpcId - !RefAll 'AWS::EC2::VPC::Id' AssertDescription : All subnets must in the VPC Conditions : GovCloudCondition : !Equals - !Ref 'AWS::Region' - us-gov-west-1 Resources : DSCBucket : Type : AWS::S3::Bucket Properties : LifecycleConfiguration : Rules : - Id : DeleteAfter30Days ExpirationInDays : 30 Status : Enabled Prefix : 'logs/' DHCPOptions : Type : AWS::EC2::DHCPOptions DependsOn : - DomainController1 Properties : DomainName : !Ref 'DomainDNSName' DomainNameServers : - !Ref 'ADServer1PrivateIP' Tags : - Key : Domain Value : !Ref 'DomainDNSName' VPCDHCPOptionsAssociation : Type : AWS::EC2::VPCDHCPOptionsAssociation Properties : VpcId : !Ref 'VPCID' DhcpOptionsId : !Ref 'DHCPOptions' AWSQuickstartActiveDirectoryDS : Type : AWS::SSM::Document Properties : DocumentType : Automation Content : schemaVersion : \"0.3\" description : \"Deploy AD with SSM Automation\" # Role that is utilized to perform the steps within the Automation Document. In this case to be able to Signal CFN and Describe Instances. assumeRole : \"{{AutomationAssumeRole}}\" # Gathering parameters needed to configure DCs in the Quick Start parameters : ADServer1NetBIOSName : default : \"DC1\" description : \"NetBIOS name of the first Active Directory server (up to 15 characters)\" type : \"String\" ADServer1PrivateIP : default : \"10.0.0.10\" description : \"Fixed private IP for the first Active Directory server located in Availability Zone 1\" type : \"String\" VPCCIDR : default : '10.0.0.0/16' description : \"CIDR block for private subnet 1 located in Availability Zone 1.\" type : \"String\" ADAdminSecParamName : description : \"AWS Secrets Parameter Name that has Password and User namer for the domain administrator.\" type : \"String\" ADAltUserSecParamName : description : \"AWS Secrets Parameter Name for the account that will be added as Domain Administrator. This is separate from the default Administrator account\" type : \"String\" RestoreModeSecParamName : description : \"AWS Secrets Parameter Name for the Restore Mode Password\" type : \"String\" DomainDNSName : default : \"example.com\" description : \"Fully qualified domain name (FQDN) of the forest root domain e.g. example.com\" type : \"String\" DomainNetBIOSName : default : \"example\" description : \"NetBIOS name of the domain (up to 15 characters) for users of earlier versions of Windows e.g. EXAMPLE\" type : \"String\" QSS3BucketName : default : \"aws-quickstart\" description : \"S3 bucket name for the Quick Start assets. Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-).\" type : \"String\" QSS3KeyPrefix : default : \"quickstart-microsoft-activedirectory/\" description : \"S3 key prefix for the Quick Start assets. Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/).\" type : \"String\" URLSuffix : default : \"amazonaws.com\" description : \"AWS URL suffix\" type : \"String\" StackName : default : \"\" description : \"Stack Name Input for cfn resource signal\" type : \"String\" AutomationAssumeRole : default : \"\" description : \"(Optional) The ARN of the role that allows Automation to perform the actions on your behalf.\" type : \"String\" mainSteps : # This step grabs the Instance IDs for both nodes that will be configured as DCs in the Quick Start and Instance IDs for the for next steps. - name : \"dc1InstanceId\" action : aws:executeAwsApi onFailure : \"step:signalfailure\" inputs : Service : ec2 Api : DescribeInstances Filters : - Name : \"tag:Name\" Values : [ \"{{ADServer1NetBIOSName}}\" ] - Name : \"tag:aws:cloudformation:stack-name\" Values : [ \"{{StackName}}\" ] - Name : \"instance-state-name\" Values : [ \"running\" ] outputs : - Name : InstanceId Selector : \"$.Reservations[0].Instances[0].InstanceId\" Type : \"String\" # Installs needed Powershell DSC Modules and components on both nodes. - name : \"dcInstallDscModules\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : \"S3\" sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.amazonaws.com/{{QSS3KeyPrefix}}scripts/install-ad-modules.ps1\"}' commandLine : \"./install-ad-modules.ps1\" # Configures Local Configuration Manager on each of the nodes. - name : \"dcsLCMConfig\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : \"S3\" sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.amazonaws.com/{{QSS3KeyPrefix}}scripts/LCM-Config.ps1\"}' commandLine : \"./LCM-Config.ps1\" # Generates MOF file on first DC Node to be processed by LCM. - name : \"createDC1Mof\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" nextStep : \"configDC1\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : \"S3\" sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.amazonaws.com/{{QSS3KeyPrefix}}scripts/ConfigDC1.ps1\"}' commandLine : \"./ConfigDC1.ps1 -ADServer1NetBIOSName {{ADServer1NetBIOSName}} -DomainNetBIOSName {{DomainNetBIOSName}} -DomainDNSName {{DomainDNSName}} -ADAdminSecParam {{ADAdminSecParamName}} -ADAltUserSecParam {{ADAltUserSecParamName}} -RestoreModeSecParam {{RestoreModeSecParamName}} -SiteName {{global:REGION}} -VPCCIDR {{VPCCIDR}}\" # Kicks off DSC Configuration and loops\\reboots until Node matches Configuration defined in MOF file. - name : \"configDC1\" action : aws:runCommand onFailure : \"step:signalfailure\" inputs : DocumentName : AWS-RunPowerShellScript InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : commands : - | function DscStatusCheck () { $LCMState = (Get-DscLocalConfigurationManager).LCMState if ($LCMState -eq 'PendingConfiguration' -Or $LCMState -eq 'PendingReboot') { 'returning 3010, should continue after reboot' exit 3010 } else { 'Completed' } } Start-DscConfiguration 'C:\\AWSQuickstart\\ConfigDC1' -Wait -Verbose -Force DscStatusCheck # Ensure that AD servers point to themselves for DNS - name : \"DnsConfig\" action : \"aws:runCommand\" onFailure : \"step:signalfailure\" inputs : DocumentName : \"AWS-RunRemoteScript\" InstanceIds : - \"{{dc1InstanceId.InstanceId}}\" CloudWatchOutputConfig : CloudWatchOutputEnabled : \"true\" CloudWatchLogGroupName : !Sub '/aws/Quick_Start/${AWS::StackName}' Parameters : sourceType : S3 sourceInfo : !Sub '{\"path\": \"https://${QSS3BucketName}.s3.amazonaws.com/{{QSS3KeyPrefix}}scripts/Dns-Config.ps1\"}' commandLine : \"./Dns-Config.ps1 -ADServer1NetBIOSName {{ADServer1NetBIOSName}} -ADServer1PrivateIP {{ADServer1PrivateIP}} -DomainDNSName {{DomainDNSName}} -ADAdminSecParam {{ADAdminSecParamName}}\" # Determines if CFN Needs to be Signaled or if Work flow should just end - name : CFNSignalEnd action : aws:branch inputs : Choices : - NextStep : signalsuccess Not : Variable : \"{{StackName}}\" StringEquals : \"\" - NextStep : sleepend Variable : \"{{StackName}}\" StringEquals : \"\" # If all steps complete successfully signals CFN of Success - name : \"signalsuccess\" action : \"aws:executeAwsApi\" isEnd : True inputs : Service : cloudformation Api : SignalResource LogicalResourceId : \"DomainController1\" StackName : \"{{StackName}}\" Status : SUCCESS UniqueId : \"{{dc1InstanceId.InstanceId}}\" # If CFN Signl Not Needed this sleep ends work flow - name : \"sleepend\" action : \"aws:sleep\" isEnd : True inputs : Duration : PT1S # If any steps fails signals CFN of Failure - name : \"signalfailure\" action : \"aws:executeAwsApi\" inputs : Service : cloudformation Api : SignalResource LogicalResourceId : \"DomainController2\" StackName : \"{{StackName}}\" Status : FAILURE UniqueId : \"{{dc1InstanceId.InstanceId}}\" AWSQuickstartADDSRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:ListBucket Resource : - !Sub arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}* - !Sub arn:${AWS::Partition}:s3:::${QSS3BucketName} Effect : Allow PolicyName : aws-quick-start-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - cloudformation:SignalResource Resource : !Sub 'arn:aws:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${AWS::StackName}/*' - Effect : Allow Action : - ec2:DescribeInstances - ec2:DescribeInstanceStatus - ssm:* Resource : '*' PolicyName : AD-SSM-Automation Path : / AssumeRolePolicyDocument : Statement : - Action : - sts:AssumeRole Principal : Service : - ec2.amazonaws.com - ssm.amazonaws.com Effect : Allow Version : '2012-10-17' ADSsmPassRolePolicy : Type : AWS::IAM::Policy Properties : PolicyName : AD-SSM-PassRole PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - iam:PassRole Resource : !Sub 'arn:aws:iam::${AWS::AccountId}:role/${AWSQuickstartADDSRole}' Roles : - !Ref 'AWSQuickstartADDSRole' ADServerRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:ListBucket Resource : - !Sub arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}* - !Sub arn:${AWS::Partition}:s3:::${QSS3BucketName} Effect : Allow PolicyName : aws-quick-start-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject Resource : - !Sub 'arn:aws:s3:::aws-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::aws-windows-downloads-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-packages-${AWS::Region}/*' - !Sub 'arn:aws:s3:::${AWS::Region}-birdwatcher-prod/*' - !Sub 'arn:aws:s3:::patch-baseline-snapshot-${AWS::Region}/*' Effect : Allow PolicyName : ssm-custom-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - secretsmanager:GetSecretValue - secretsmanager:DescribeSecret Resource : - !Ref 'ADAdminSecrets' - !Ref 'RestoreModeSecrets' - !Ref 'ADAltUserSecrets' - Effect : Allow Action : - ssm:StartAutomationExecution Resource : '*' PolicyName : AD-SSM-Secrets - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - iam:PassRole Resource : !Sub 'arn:aws:iam::${AWS::AccountId}:role/${AWSQuickstartADDSRole}' PolicyName : AD-SSM-PassRole Path : / ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy' AssumeRolePolicyDocument : Statement : - Action : - sts:AssumeRole Principal : Service : - ec2.amazonaws.com Effect : Allow Version : '2012-10-17' ADServerProfile : Type : AWS::IAM::InstanceProfile Properties : Roles : - !Ref 'ADServerRole' Path : / ADAdminSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'ADAdministratorSecret-${AWS::StackName}' Description : Administrator Password for AD Quick Start GenerateSecretString : SecretStringTemplate : '{\"username\": \"Administrator\"}' GenerateStringKey : \"password\" PasswordLength : 30 ExcludeCharacters : '\"@/\\' RestoreModeSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'RestoreModeSecrets-${AWS::StackName}' Description : Restore Mode Password for AD Quick Start GenerateSecretString : SecretStringTemplate : '{\"username\": \"Administrator\"}' GenerateStringKey : \"password\" PasswordLength : 30 ExcludeCharacters : '\"@/\\' ADAltUserSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'ADAltUserSecrets-${AWS::StackName}' Description : Alternate AD Admin User from AD Quick Start SecretString : !Sub '{\"username\":\"${DomainAdminUser}\",\"password\":\"${DomainAdminPassword}\"}' DomainJoinSecrets : Type : AWS::SecretsManager::Secret Properties : Name : !Sub 'DomainJoinSecrets-${AWS::StackName}' Description : Alternate AD Admin User from AD Quick Start SecretString : !Sub '{\"username\":\"${DomainNetBIOSName}\\\\${DomainAdminUser}\",\"password\":\"${DomainAdminPassword}\"}' DomainNameParam : Type : \"AWS::SSM::Parameter\" Properties : Name : \"/demo/AD/DomainName\" Type : \"String\" Value : !Ref 'DomainDNSName' Description : \"SSM Parameter for Domain Name.\" DomainController1 : Type : AWS::EC2::Instance CreationPolicy : ResourceSignal : Timeout : PT60M Count : 1 Properties : ImageId : !Ref 'WS2016FULLBASE' IamInstanceProfile : !Ref 'ADServerProfile' InstanceType : !Ref 'ADServer1InstanceType' SubnetId : !Ref 'PrivateSubnet1ID' Tags : - Key : Name Value : !Ref 'ADServer1NetBIOSName' BlockDeviceMappings : - DeviceName : /dev/sda1 Ebs : VolumeSize : '100' VolumeType : gp2 SecurityGroupIds : - !Ref 'DomainControllersSG' PrivateIpAddress : !Ref 'ADServer1PrivateIP' UserData : !Base64 Fn::Join : - '' - - \"<powershell>\\n\" - 'Start-SSMAutomationExecution -DocumentName ' - !Sub '\"${AWSQuickstartActiveDirectoryDS}\"' - ' -Parameter @{' - '\"ADServer1NetBIOSName\"=' - !Sub '\"${ADServer1NetBIOSName}\"' - ';\"ADServer1PrivateIP\"=' - !Sub '\"${ADServer1PrivateIP}\"' - ';\"DomainDNSName\"=' - !Sub '\"${DomainDNSName}\"' - ';\"DomainNetBIOSName\"=' - !Sub '\"${DomainNetBIOSName}\"' - ';\"VPCCIDR\"=' - !Sub '\"${VPCCIDR}\"' - ';\"QSS3BucketName\"=' - !Sub '\"${QSS3BucketName}\"' - ';\"QSS3KeyPrefix\"=' - !Sub '\"${QSS3KeyPrefix}\"' - ';\"ADAdminSecParamName\"=' - !Sub '\"${ADAdminSecrets}\"' - ';\"ADAltUserSecParamName\"=' - !Sub '\"${ADAltUserSecrets}\"' - ';\"RestoreModeSecParamName\"=' - !Sub '\"${RestoreModeSecrets}\"' - ';\"StackName\"=' - !Sub '\"${AWS::StackName}\"' - ';\"URLSuffix\"=' - !Sub '\"${AWS::URLSuffix}\"' - ';\"AutomationAssumeRole\"=' - !Sub '\"arn:${AWS::Partition}:iam::${AWS::AccountId}:role/${AWSQuickstartADDSRole}\"' - '}' - \"\\n\" - \"</powershell>\\n\" DomainControllersSG : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Domain Controllers Security Group VpcId : Ref : VPCID SecurityGroupIngress : - IpProtocol : tcp FromPort : 5985 ToPort : 5985 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 80 ToPort : 80 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 53 ToPort : 53 CidrIp : !Ref VPCCIDR - IpProtocol : udp FromPort : 53 ToPort : 53 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 3389 ToPort : 3389 CidrIp : !Ref VPCCIDR - IpProtocol : udp FromPort : 123 ToPort : 123 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 135 ToPort : 135 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 9389 ToPort : 9389 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 138 ToPort : 138 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 445 ToPort : 445 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 445 ToPort : 445 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 464 ToPort : 464 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 464 ToPort : 464 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 49152 ToPort : 65535 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 49152 ToPort : 65535 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 389 ToPort : 389 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 389 ToPort : 389 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 636 ToPort : 636 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 3268 ToPort : 3268 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 3269 ToPort : 3269 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 9389 ToPort : 9389 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 88 ToPort : 88 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 88 ToPort : 88 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 5355 ToPort : 5355 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : udp FromPort : 137 ToPort : 137 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 139 ToPort : 139 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : tcp FromPort : 5722 ToPort : 5722 SourceSecurityGroupId : !Ref DomainMembersSG - IpProtocol : icmp FromPort : -1 ToPort : -1 SourceSecurityGroupId : !Ref DomainMembersSG DomainMembersSG : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Domain Members VpcId : !Ref VPCID SecurityGroupIngress : - IpProtocol : tcp FromPort : 5985 ToPort : 5985 CidrIp : !Ref VPCCIDR - IpProtocol : tcp FromPort : 3389 ToPort : 3389 CidrIp : !Ref VPCCIDR DCSecurityGroupIngress : Type : AWS::EC2::SecurityGroupIngress Properties : Description : Security Group Rule between Domain Controllers GroupId : !Ref DomainControllersSG IpProtocol : -1 FromPort : -1 ToPort : -1 SourceSecurityGroupId : !Ref DomainControllersSG LambdaSSMRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Action : - s3:PutObject Resource : - !Sub \"${DSCBucket.Arn}\" - !Sub \"${DSCBucket.Arn}/*\" PolicyName : write-mof-s3 Path : / AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : - lambda.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole' WriteMOFFunction : Type : AWS::Lambda::Function Properties : Code : ZipFile : > var AWS = require('aws-sdk'), s3 = new AWS.S3(); const response = require(\"cfn-response\"); exports.handler = async (event, context) => { console.log(JSON.stringify(event)); if (event.RequestType === 'Delete') { await postResponse(event, context, response.SUCCESS, {}) return; } function postResponse(event, context, status, data){ return new Promise((resolve, reject) => { setTimeout(() => response.send(event, context, status, data), 5000) }); } await s3.putObject({ Body: event.ResourceProperties.Body, Bucket: event.ResourceProperties.Bucket, Key: event.ResourceProperties.Key }).promise(); await postResponse(event, context, response.SUCCESS, {}); }; Handler : index.handler Role : !GetAtt LambdaSSMRole.Arn Runtime : nodejs10.x Timeout : 10 WriteDomainJoinMOF : Type : Custom::WriteMOFFile Properties : ServiceToken : !GetAtt WriteMOFFunction.Arn Bucket : !Ref DSCBucket Key : !Sub \"DomainJoin-${AWS::StackName}.mof\" Body : !Sub | /* @TargetNode='localhost' */ instance of MSFT_Credential as $MSFT_Credential1ref { Password = \"managementgovernancesample\"; UserName = \"${DomainJoinSecrets}\"; }; instance of DSC_Computer as $DSC_Computer1ref { ResourceID = \"[Computer]JoinDomain\"; Credential = $MSFT_Credential1ref; DomainName = \"{tag:DomainToJoin}\"; Name = \"{tag:Name}\"; ModuleName = \"ComputerManagementDsc\"; ModuleVersion = \"8.0.0\"; ConfigurationName = \"DomainJoin\"; }; instance of OMI_ConfigurationDocument { Version=\"2.0.0\"; MinimumCompatibleVersion = \"1.0.0\"; CompatibleVersionAdditionalProperties= {\"Omi_BaseResource:ConfigurationName\"}; Name=\"DomainJoin\"; }; DomainJoinAssociation : Type : AWS::SSM::Association Properties : # We are using the AWS-ApplyDSCMofs Document Name : AWS-ApplyDSCMofs Targets : - Key : \"tag:DomainToJoin\" Values : - !Ref \"DomainDNSName\" OutputLocation : S3Location : OutputS3BucketName : !Ref DSCBucket OutputS3KeyPrefix : 'logs/' # Here we have a Cron Expression that will run this everyday at 23:15 every day ScheduleExpression : \"cron(15 23 * * ? *)\" Parameters : # Here the Mof file resides in a S3 Bucket, notice the way that S3 is being referenced here. MofsToApply : - !Sub \"s3:${DSCBucket}:DomainJoin-${AWS::StackName}.mof\" ServicePath : - default MofOperationMode : - Apply ComplianceType : - Custom:DomainJoinSample ModuleSourceBucketName : - \"NONE\" AllowPSGalleryModuleSource : - \"True\" RebootBehavior : - \"AfterMof\" UseComputerNameForReporting : - \"False\" EnableVerboseLogging : - \"False\" EnableDebugLogging : - \"False\" Outputs : DomainAdmin : Value : !Join - '' - - !Ref 'DomainNetBIOSName' - \\ - !Ref 'DomainAdminUser' Description : Domain administrator account DomainMemberSGID : Value : !Ref 'DomainMembersSG' Description : Domain Member Security Group ID SecretsArn : Value : !Ref 'DomainJoinSecrets' DSCBucket : Value : !Ref 'DSCBucket'","title":"ADStack"},{"location":"platforms/aws/migrate/windows/#lab-cloud-endure","text":"Guide","title":"Lab CLoud Endure"},{"location":"platforms/aws/security/acm/","text":"Self-Certificate \u00b6 Create Self-Certificate","title":"ACM"},{"location":"platforms/aws/security/acm/#self-certificate","text":"Create Self-Certificate","title":"Self-Certificate"},{"location":"platforms/aws/serverless/amplify/","text":"AWS Amplify \u00b6 Getting Started \u00b6 # Install Cli npm i -g amplify-cli # Init Project amplify init # Status amplify status # Delete amplify delete # Add Feature amplify add auth amplify add analytics amplify add api amplify configure api amplify update api # Environment amplify env list amplify env add amplify env remove <env> amplify env checkout <env> # Add Library to project npm install aws-amplify # Add framework-specific library npm install aws-amplify-react React Libs \u00b6 yarn add aws-amplify aws-amplify-react # Or npm install aws-amplify aws-amplify-react","title":"Amplify"},{"location":"platforms/aws/serverless/amplify/#aws-amplify","text":"","title":"AWS Amplify"},{"location":"platforms/aws/serverless/amplify/#getting-started","text":"# Install Cli npm i -g amplify-cli # Init Project amplify init # Status amplify status # Delete amplify delete # Add Feature amplify add auth amplify add analytics amplify add api amplify configure api amplify update api # Environment amplify env list amplify env add amplify env remove <env> amplify env checkout <env> # Add Library to project npm install aws-amplify # Add framework-specific library npm install aws-amplify-react","title":"Getting Started"},{"location":"platforms/aws/serverless/amplify/#react-libs","text":"yarn add aws-amplify aws-amplify-react # Or npm install aws-amplify aws-amplify-react","title":"React Libs"},{"location":"platforms/aws/serverless/apigateway/","text":"Api Gateway \u00b6 Integration \u00b6 Step Functions \u00b6 Request \u00b6 { \"input\" : \"$util.escapeJavaScript($input.json('$'))\" , \"stateMachineArn\" : \"${RegisterDeviceStateMachine}\" } Response \u00b6 { \"stateMachineExecution\" : \"$input.json('$.executionArn').split(':')[7].replace('\" ' , \"\" )\" } Models \u00b6 { \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"title\" : \"Request Validator\" , \"type\" : \"object\" , \"properties\" : { \"DeviceID\" : { \"type\" : \"string\" }, \"Name\" : { \"type\" : \"string\" } }, \"required\" : [ \"DeviceID\" , \"Name\" ], \"additionalProperties\" : false }","title":"ApiGateway"},{"location":"platforms/aws/serverless/apigateway/#api-gateway","text":"","title":"Api Gateway"},{"location":"platforms/aws/serverless/apigateway/#integration","text":"","title":"Integration"},{"location":"platforms/aws/serverless/apigateway/#step-functions","text":"","title":"Step Functions"},{"location":"platforms/aws/serverless/apigateway/#request","text":"{ \"input\" : \"$util.escapeJavaScript($input.json('$'))\" , \"stateMachineArn\" : \"${RegisterDeviceStateMachine}\" }","title":"Request"},{"location":"platforms/aws/serverless/apigateway/#response","text":"{ \"stateMachineExecution\" : \"$input.json('$.executionArn').split(':')[7].replace('\" ' , \"\" )\" }","title":"Response"},{"location":"platforms/aws/serverless/apigateway/#models","text":"{ \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"title\" : \"Request Validator\" , \"type\" : \"object\" , \"properties\" : { \"DeviceID\" : { \"type\" : \"string\" }, \"Name\" : { \"type\" : \"string\" } }, \"required\" : [ \"DeviceID\" , \"Name\" ], \"additionalProperties\" : false }","title":"Models"},{"location":"platforms/aws/serverless/cognito/","text":"Change User Status FORCE_CHANGE_PASSWORD \u00b6 Link - StackOverflow Use AWS CLI to change the users password. Get a session token for the desired user: aws cognito-idp admin-initiate-auth --user-pool-id %USER POOL ID% --client-id %APP CLIENT ID% --auth-flow ADMIN_NO_SRP_AUTH --auth-parameters USERNAME = %USERS USERNAME%,PASSWORD = %USERS CURRENT PASSWORD% This will respond with the challenge \"NEW_PASSWORD_REQUIRED\", other challenge parameters and the users session key. Then, run the second command to issue the challenge response: aws cognito-idp admin-respond-to-auth-challenge --user-pool-id %USER POOL ID% --client-id %CLIENT ID% --challenge-name NEW_PASSWORD_REQUIRED --challenge-responses NEW_PASSWORD = %DESIRED PASSWORD%,USERNAME = %USERS USERNAME% --session %SESSION KEY FROM PREVIOUS COMMAND with \"\" %","title":"Cognito"},{"location":"platforms/aws/serverless/cognito/#change-user-status-force_change_password","text":"Link - StackOverflow Use AWS CLI to change the users password. Get a session token for the desired user: aws cognito-idp admin-initiate-auth --user-pool-id %USER POOL ID% --client-id %APP CLIENT ID% --auth-flow ADMIN_NO_SRP_AUTH --auth-parameters USERNAME = %USERS USERNAME%,PASSWORD = %USERS CURRENT PASSWORD% This will respond with the challenge \"NEW_PASSWORD_REQUIRED\", other challenge parameters and the users session key. Then, run the second command to issue the challenge response: aws cognito-idp admin-respond-to-auth-challenge --user-pool-id %USER POOL ID% --client-id %CLIENT ID% --challenge-name NEW_PASSWORD_REQUIRED --challenge-responses NEW_PASSWORD = %DESIRED PASSWORD%,USERNAME = %USERS USERNAME% --session %SESSION KEY FROM PREVIOUS COMMAND with \"\" %","title":"Change User Status FORCE_CHANGE_PASSWORD"},{"location":"platforms/aws/serverless/lambda/","text":"Build \u00b6 Runtime Python 3.6 How to create an AWS Lambda python 3.6 deployment package using Docker sudo docker run -it dacut/amazon-linux-python-3.6 mkdir <DOCKER_PROJEC_DIR> cd <DOCKER_PROJECT_DIR> pip3 install <PACKAGE_NAME> -t ./ zip -r <PROJECT_NAME>.zip * exit sudo docker ps -a | grep \"dacut\" | awk '{print $1}' sudo docker cp <CONTAINER_ID>:<DOCKER_PROJECT_DIR>/<PROJECT_NAME>.zip <LOCAL_PROJECT_PATH> unzip <LOCAL_PROJECT_PATH>/<PROJECT_NAME>.zip rm <LOCAL_PROJECT_PATH>/<PROJECT_NAME>.zip # Add Lambda File to zip zip -ur <PROJECT_NAME>.zip <PATH_TO_LAMBDA_FUNCTION_FILE>","title":"Lambda"},{"location":"platforms/aws/serverless/lambda/#build","text":"Runtime Python 3.6 How to create an AWS Lambda python 3.6 deployment package using Docker sudo docker run -it dacut/amazon-linux-python-3.6 mkdir <DOCKER_PROJEC_DIR> cd <DOCKER_PROJECT_DIR> pip3 install <PACKAGE_NAME> -t ./ zip -r <PROJECT_NAME>.zip * exit sudo docker ps -a | grep \"dacut\" | awk '{print $1}' sudo docker cp <CONTAINER_ID>:<DOCKER_PROJECT_DIR>/<PROJECT_NAME>.zip <LOCAL_PROJECT_PATH> unzip <LOCAL_PROJECT_PATH>/<PROJECT_NAME>.zip rm <LOCAL_PROJECT_PATH>/<PROJECT_NAME>.zip # Add Lambda File to zip zip -ur <PROJECT_NAME>.zip <PATH_TO_LAMBDA_FUNCTION_FILE>","title":"Build"},{"location":"platforms/aws/serverless/stepfunctions/","text":"Task \u00b6 Invoke Lambda \u00b6 \"GetLinks\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::lambda:invoke\" , \"Parameters\" : { \"FunctionName\" : \"arn:aws:lambda:eu-central-1:951313074793:function:test-api-branch_io-links:$LATEST\" , \"Payload\" : { \"Input.$\" : \"$\" } }, \"OutputPath\" : \"$.Payload\" , \"Next\" : \"SendLinkToSQSQueue\" } DynamoDB \u00b6 Put Item \u00b6 \"StoreDeviceInDynamoDB\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::dynamodb:putItem\" , \"Parameters\" : { \"TableName\" : \"Devices\" , \"Item\" : { \"DeviceID\" : { \"N.$\" : \"$.DeviceID\" }, \"Name\" : { \"S.$\" : \"$.Name\" } } }, \"Retry\" : [ { \"ErrorEquals\" : [ \"States.Timeout\" ], \"IntervalSeconds\" : 3 , \"MaxAttempts\" : 2 , \"BackoffRate\" : 1.5 } ] } Map \u00b6 SQS: Callback Pattern \"SendLinkToSQSQueue\" : { \"Type\" : \"Map\" , \"MaxConcurrency\" : 20 , \"ItemsPath\" : \"$\" , \"Iterator\" : { \"StartAt\" : \"Send Link to SQS\" , \"States\" : { \"Send Link to SQS\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::sqs:sendMessage.waitForTaskToken\" , \"Parameters\" : { \"QueueUrl\" : \"https://sqs.eu-central-1.amazonaws.com/951313074793/test--branch_io-queue\" , \"MessageBody\" : { \"Url.$\" : \"$.MessageBody\" , \"TaskToken.$\" : \"$$.Task.Token\" }, \"MessageAttributes.$\" : \"$.MessageAttributes\" }, \"End\" : true } } }, \"Next\" : \"World\" }","title":"StepFunctions"},{"location":"platforms/aws/serverless/stepfunctions/#task","text":"","title":"Task"},{"location":"platforms/aws/serverless/stepfunctions/#invoke-lambda","text":"\"GetLinks\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::lambda:invoke\" , \"Parameters\" : { \"FunctionName\" : \"arn:aws:lambda:eu-central-1:951313074793:function:test-api-branch_io-links:$LATEST\" , \"Payload\" : { \"Input.$\" : \"$\" } }, \"OutputPath\" : \"$.Payload\" , \"Next\" : \"SendLinkToSQSQueue\" }","title":"Invoke Lambda"},{"location":"platforms/aws/serverless/stepfunctions/#dynamodb","text":"","title":"DynamoDB"},{"location":"platforms/aws/serverless/stepfunctions/#put-item","text":"\"StoreDeviceInDynamoDB\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::dynamodb:putItem\" , \"Parameters\" : { \"TableName\" : \"Devices\" , \"Item\" : { \"DeviceID\" : { \"N.$\" : \"$.DeviceID\" }, \"Name\" : { \"S.$\" : \"$.Name\" } } }, \"Retry\" : [ { \"ErrorEquals\" : [ \"States.Timeout\" ], \"IntervalSeconds\" : 3 , \"MaxAttempts\" : 2 , \"BackoffRate\" : 1.5 } ] }","title":"Put Item"},{"location":"platforms/aws/serverless/stepfunctions/#map","text":"SQS: Callback Pattern \"SendLinkToSQSQueue\" : { \"Type\" : \"Map\" , \"MaxConcurrency\" : 20 , \"ItemsPath\" : \"$\" , \"Iterator\" : { \"StartAt\" : \"Send Link to SQS\" , \"States\" : { \"Send Link to SQS\" : { \"Type\" : \"Task\" , \"Resource\" : \"arn:aws:states:::sqs:sendMessage.waitForTaskToken\" , \"Parameters\" : { \"QueueUrl\" : \"https://sqs.eu-central-1.amazonaws.com/951313074793/test--branch_io-queue\" , \"MessageBody\" : { \"Url.$\" : \"$.MessageBody\" , \"TaskToken.$\" : \"$$.Task.Token\" }, \"MessageAttributes.$\" : \"$.MessageAttributes\" }, \"End\" : true } } }, \"Next\" : \"World\" }","title":"Map"},{"location":"platforms/aws/storage/s3/","text":"MultiPart Upload \u00b6 Documentation # Split the File export BUCKET = fsantos-vmware-ova-templates export FILE = aws.ova split -b <bytes> <file> # Upload aws s3api create-multipart-upload --bucket ${ BUCKET } --key ${ FILE } export UPLOAD_ID = # Repeat to all parts - Changing the body and increase part number aws s3api upload-part --bucket ${ BUCKET } --key ${ FILE } --part-number 1 --body <file-part1> --upload-id ${ UPLOAD_ID } # List all uploaded parts and get ETags aws s3api list-parts --bucket ${ BUCKET } --key ${ FILE } --upload-id ${ UPLOAD_ID } # Create fileparts.json with all Etags # Complete Multipart upload aws s3api complete-multipart-upload --multipart-upload file://fileparts.json --bucket ${ BUCKET } --key ${ FILE } --upload-id ${ UPLOAD_ID } filepats.json example { \"Parts\" : [{ \"ETag\" : \"\\\"example8be9a0268ebfb8b115d4c1fd3\\\"\" , \"PartNumber\" : 1 }, .... { \"ETag\" : \"\\\"example246e31ab807da6f62802c1ae8\\\"\" , \"PartNumber\" : 4 }] }","title":"S3"},{"location":"platforms/aws/storage/s3/#multipart-upload","text":"Documentation # Split the File export BUCKET = fsantos-vmware-ova-templates export FILE = aws.ova split -b <bytes> <file> # Upload aws s3api create-multipart-upload --bucket ${ BUCKET } --key ${ FILE } export UPLOAD_ID = # Repeat to all parts - Changing the body and increase part number aws s3api upload-part --bucket ${ BUCKET } --key ${ FILE } --part-number 1 --body <file-part1> --upload-id ${ UPLOAD_ID } # List all uploaded parts and get ETags aws s3api list-parts --bucket ${ BUCKET } --key ${ FILE } --upload-id ${ UPLOAD_ID } # Create fileparts.json with all Etags # Complete Multipart upload aws s3api complete-multipart-upload --multipart-upload file://fileparts.json --bucket ${ BUCKET } --key ${ FILE } --upload-id ${ UPLOAD_ID } filepats.json example { \"Parts\" : [{ \"ETag\" : \"\\\"example8be9a0268ebfb8b115d4c1fd3\\\"\" , \"PartNumber\" : 1 }, .... { \"ETag\" : \"\\\"example246e31ab807da6f62802c1ae8\\\"\" , \"PartNumber\" : 4 }] }","title":"MultiPart Upload"},{"location":"platforms/kubernetes/admin/admin/","text":"Pre-Requisites \u00b6 Redhat \u00b6 Disable SELinux sudo setenforce 0 sudo sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux Enable the br_netfilter module for cluster communication sudo modprobe br_netfilter sudo echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables Disable swap to prevent memory allocation issues swapoff -a sudo vi /etc/fstab -> Comment out the swap line Docker \u00b6 Redhat \u00b6 Install the Docker prerequisites sudo yum install -y yum-utils device-mapper-persistent-data lvm2 Add the Docker repo and install Docker sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install -y docker-ce Configure the Docker Cgroup Driver to systemd, enable and start Docker sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker --now Ubuntu \u00b6 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce = 18 .06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce Amazon AMI Linux 2 \u00b6 sudo amazon-linux-extras install -y docker sudo cat > /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo systemctl daemon-reload sudo systemctl enable docker sudo systemctl start docker sudo usermod -a -G docker ec2-user # Exit e login na sessao kubernetes \u00b6 Redhat \u00b6 cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF sudo yum install -y kubelet kubeadm kubectl sudo systemctl enable kubelet kubeadm init --pod-network-cidr = 10 .244.0.0/16 # Exit sudo user mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config Ubuntu \u00b6 curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .12.7-00 kubeadm = 1 .12.7-00 kubectl = 1 .12.7-00 sudo apt-mark hold kubelet kubeadm kubectl echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config Upgrade \u00b6 Ubuntu \u00b6 kubectl version --short # Release the hold on versions of kubeadm and kubelet sudo apt-mark unhold kubeadm kubelet sudo apt install -y kubeadm = 1 .14.1-00 sudo apt-mark hold kubeadm kubeadm version sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.14.1 sudo apt-mark unhold kubectl sudo apt install -y kubectl = 1 .14.1-00 sudo apt-mark hold kubectl sudo apt-mark unhold kubelet sudo apt install -y kubelet = 1 .14.1-00 sudo apt-mark hold kubelet Maintenance And Troubleshooting \u00b6 Evict Pods \u00b6 # Evict the pods on a node kubectl drain [ node_name ] --ignore-daemonsets # Watch as the node changes status: kubectl get nodes -w # Rollback - Schedule pods to the node after maintenance is complete kubectl uncordon [ node_name ] Delete Node \u00b6 # Remove a node from the cluster: kubectl delete node [ node_name ] sudo kubeadm token generate # List the tokens: sudo kubeadm token list # Print the kubeadm join command to join a node to the cluster sudo kubeadm token create [ token_name ] --ttl 2h --print-join-command Taint \u00b6 kubectl taint node <node_name> node-type = prod:NoSchedule Pod with Toleration \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : prod spec : replicas : 1 selector : matchLabels : app : prod template : metadata : labels : app : prod spec : containers : - args : - sleep - \"3600\" image : busybox name : main tolerations : - key : node-type operator : Equal value : prod effect : NoSchedule Commands \u00b6 kubectl get componentstatus kubectl api-resources -o wide Autocomplete \u00b6 Redhat \u00b6 # Enable Epel Repo sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel # Instal bash completion yum install bash-completion bash-completion-extras vi /.bashrc # Add Following lines # alias k=kubectl # source <(kubectl completion bash | sed s/kubectl/k/g) Kubectl \u00b6 Install \u00b6 wget https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/ kubectl version --client Set Remote \u00b6 kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://localhost:6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way Insecure \u00b6 File: $HOME/.kube/config - cluster : insecure-skip-tls-verify : true Api Resources \u00b6 kubectl api-resources -o name Kube Config \u00b6 Generate Kube Configs \u00b6 See How Generate TLS Certificates # Create an environment variable to store the address of the Kubernetes API, and set it to the private IP of your load balancer cloud server: KUBERNETES_ADDRESS = <load balancer private ip> # Generate a kubelet kubeconfig for each worker node: for instance in <worker 1 hostname> <worker 2 hostname> ; do kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_ADDRESS } :6443 \\ --kubeconfig = ${ instance } .kubeconfig kubectl config set-credentials system:node: ${ instance } \\ --client-certificate = ${ instance } .pem \\ --client-key = ${ instance } -key.pem \\ --embed-certs = true \\ --kubeconfig = ${ instance } .kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:node: ${ instance } \\ --kubeconfig = ${ instance } .kubeconfig kubectl config use-context default --kubeconfig = ${ instance } .kubeconfig done # Generate a kube-proxy kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_ADDRESS } :6443 \\ --kubeconfig = kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate = kube-proxy.pem \\ --client-key = kube-proxy-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-proxy.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-proxy \\ --kubeconfig = kube-proxy.kubeconfig kubectl config use-context default --kubeconfig = kube-proxy.kubeconfig } # Generate a kube-controller-manager kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig } # Generate a kube-scheduler kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate = kube-scheduler.pem \\ --client-key = kube-scheduler-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-scheduler \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig = kube-scheduler.kubeconfig } # Generate an admin kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem \\ --embed-certs = true \\ --kubeconfig = admin.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = admin \\ --kubeconfig = admin.kubeconfig kubectl config use-context default --kubeconfig = admin.kubeconfig } Move kubeconfig files to the worker nodes: \u00b6 scp <worker 1 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 1 public IP>:~/ scp <worker 2 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 2 public IP>:~/ Move kubeconfig files to the master nodes: \u00b6 scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 1 public IP>:~/ scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 2 public IP>:~/ Data Encryption Config \u00b6 # Generate the Kubernetes Data encrpytion config file containing the encrpytion key: export ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) cat > encryption-config.yaml << EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF # Copy the file to both master servers: scp encryption-config.yaml user@<master 1 public ip>:~/ scp encryption-config.yaml user@<master 2 public ip>:~/ LoadBalancer \u00b6 Setting UP # Here are the commands you can use to set up the nginx load balancer. Run these on the server that you have designated as your load balancer server: sudo apt-get install -y nginx sudo systemctl enable nginx sudo mkdir -p /etc/nginx/tcpconf.d sudo vi /etc/nginx/nginx.conf # Add the following to the end of nginx.conf: include /etc/nginx/tcpconf.d/* ; # Set up some environment variables for the lead balancer config file: export CONTROLLER0_IP = <controller 0 private ip> export CONTROLLER1_IP = <controller 1 private ip> # Create the load balancer nginx config file: cat << EOF | sudo tee /etc/nginx/tcpconf.d/kubernetes.conf stream { upstream kubernetes { server $CONTROLLER0_IP:6443; server $CONTROLLER1_IP:6443; } server { listen 6443; listen 443; proxy_pass kubernetes; } } EOF # Reload the nginx configuration: sudo nginx -s reload # You can verify that the load balancer is working like so: curl -k https://localhost:6443/version","title":"Administrator"},{"location":"platforms/kubernetes/admin/admin/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"platforms/kubernetes/admin/admin/#redhat","text":"Disable SELinux sudo setenforce 0 sudo sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux Enable the br_netfilter module for cluster communication sudo modprobe br_netfilter sudo echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables Disable swap to prevent memory allocation issues swapoff -a sudo vi /etc/fstab -> Comment out the swap line","title":"Redhat"},{"location":"platforms/kubernetes/admin/admin/#docker","text":"","title":"Docker"},{"location":"platforms/kubernetes/admin/admin/#redhat_1","text":"Install the Docker prerequisites sudo yum install -y yum-utils device-mapper-persistent-data lvm2 Add the Docker repo and install Docker sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install -y docker-ce Configure the Docker Cgroup Driver to systemd, enable and start Docker sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker --now","title":"Redhat"},{"location":"platforms/kubernetes/admin/admin/#ubuntu","text":"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce = 18 .06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce","title":"Ubuntu"},{"location":"platforms/kubernetes/admin/admin/#amazon-ami-linux-2","text":"sudo amazon-linux-extras install -y docker sudo cat > /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo systemctl daemon-reload sudo systemctl enable docker sudo systemctl start docker sudo usermod -a -G docker ec2-user # Exit e login na sessao","title":"Amazon AMI Linux 2"},{"location":"platforms/kubernetes/admin/admin/#kubernetes","text":"","title":"kubernetes"},{"location":"platforms/kubernetes/admin/admin/#redhat_2","text":"cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF sudo yum install -y kubelet kubeadm kubectl sudo systemctl enable kubelet kubeadm init --pod-network-cidr = 10 .244.0.0/16 # Exit sudo user mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config","title":"Redhat"},{"location":"platforms/kubernetes/admin/admin/#ubuntu_1","text":"curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .12.7-00 kubeadm = 1 .12.7-00 kubectl = 1 .12.7-00 sudo apt-mark hold kubelet kubeadm kubectl echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config","title":"Ubuntu"},{"location":"platforms/kubernetes/admin/admin/#upgrade","text":"","title":"Upgrade"},{"location":"platforms/kubernetes/admin/admin/#ubuntu_2","text":"kubectl version --short # Release the hold on versions of kubeadm and kubelet sudo apt-mark unhold kubeadm kubelet sudo apt install -y kubeadm = 1 .14.1-00 sudo apt-mark hold kubeadm kubeadm version sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.14.1 sudo apt-mark unhold kubectl sudo apt install -y kubectl = 1 .14.1-00 sudo apt-mark hold kubectl sudo apt-mark unhold kubelet sudo apt install -y kubelet = 1 .14.1-00 sudo apt-mark hold kubelet","title":"Ubuntu"},{"location":"platforms/kubernetes/admin/admin/#maintenance-and-troubleshooting","text":"","title":"Maintenance And Troubleshooting"},{"location":"platforms/kubernetes/admin/admin/#evict-pods","text":"# Evict the pods on a node kubectl drain [ node_name ] --ignore-daemonsets # Watch as the node changes status: kubectl get nodes -w # Rollback - Schedule pods to the node after maintenance is complete kubectl uncordon [ node_name ]","title":"Evict Pods"},{"location":"platforms/kubernetes/admin/admin/#delete-node","text":"# Remove a node from the cluster: kubectl delete node [ node_name ] sudo kubeadm token generate # List the tokens: sudo kubeadm token list # Print the kubeadm join command to join a node to the cluster sudo kubeadm token create [ token_name ] --ttl 2h --print-join-command","title":"Delete Node"},{"location":"platforms/kubernetes/admin/admin/#taint","text":"kubectl taint node <node_name> node-type = prod:NoSchedule","title":"Taint"},{"location":"platforms/kubernetes/admin/admin/#pod-with-toleration","text":"apiVersion : apps/v1 kind : Deployment metadata : name : prod spec : replicas : 1 selector : matchLabels : app : prod template : metadata : labels : app : prod spec : containers : - args : - sleep - \"3600\" image : busybox name : main tolerations : - key : node-type operator : Equal value : prod effect : NoSchedule","title":"Pod with Toleration"},{"location":"platforms/kubernetes/admin/admin/#commands","text":"kubectl get componentstatus kubectl api-resources -o wide","title":"Commands"},{"location":"platforms/kubernetes/admin/admin/#autocomplete","text":"","title":"Autocomplete"},{"location":"platforms/kubernetes/admin/admin/#redhat_3","text":"# Enable Epel Repo sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel # Instal bash completion yum install bash-completion bash-completion-extras vi /.bashrc # Add Following lines # alias k=kubectl # source <(kubectl completion bash | sed s/kubectl/k/g)","title":"Redhat"},{"location":"platforms/kubernetes/admin/admin/#kubectl","text":"","title":"Kubectl"},{"location":"platforms/kubernetes/admin/admin/#install","text":"wget https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/ kubectl version --client","title":"Install"},{"location":"platforms/kubernetes/admin/admin/#set-remote","text":"kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://localhost:6443 kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster = kubernetes-the-hard-way \\ --user = admin kubectl config use-context kubernetes-the-hard-way","title":"Set Remote"},{"location":"platforms/kubernetes/admin/admin/#insecure","text":"File: $HOME/.kube/config - cluster : insecure-skip-tls-verify : true","title":"Insecure"},{"location":"platforms/kubernetes/admin/admin/#api-resources","text":"kubectl api-resources -o name","title":"Api Resources"},{"location":"platforms/kubernetes/admin/admin/#kube-config","text":"","title":"Kube Config"},{"location":"platforms/kubernetes/admin/admin/#generate-kube-configs","text":"See How Generate TLS Certificates # Create an environment variable to store the address of the Kubernetes API, and set it to the private IP of your load balancer cloud server: KUBERNETES_ADDRESS = <load balancer private ip> # Generate a kubelet kubeconfig for each worker node: for instance in <worker 1 hostname> <worker 2 hostname> ; do kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_ADDRESS } :6443 \\ --kubeconfig = ${ instance } .kubeconfig kubectl config set-credentials system:node: ${ instance } \\ --client-certificate = ${ instance } .pem \\ --client-key = ${ instance } -key.pem \\ --embed-certs = true \\ --kubeconfig = ${ instance } .kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:node: ${ instance } \\ --kubeconfig = ${ instance } .kubeconfig kubectl config use-context default --kubeconfig = ${ instance } .kubeconfig done # Generate a kube-proxy kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https:// ${ KUBERNETES_ADDRESS } :6443 \\ --kubeconfig = kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate = kube-proxy.pem \\ --client-key = kube-proxy-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-proxy.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-proxy \\ --kubeconfig = kube-proxy.kubeconfig kubectl config use-context default --kubeconfig = kube-proxy.kubeconfig } # Generate a kube-controller-manager kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate = kube-controller-manager.pem \\ --client-key = kube-controller-manager-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-controller-manager \\ --kubeconfig = kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig = kube-controller-manager.kubeconfig } # Generate a kube-scheduler kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate = kube-scheduler.pem \\ --client-key = kube-scheduler-key.pem \\ --embed-certs = true \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = system:kube-scheduler \\ --kubeconfig = kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig = kube-scheduler.kubeconfig } # Generate an admin kubeconfig { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority = ca.pem \\ --embed-certs = true \\ --server = https://127.0.0.1:6443 \\ --kubeconfig = admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate = admin.pem \\ --client-key = admin-key.pem \\ --embed-certs = true \\ --kubeconfig = admin.kubeconfig kubectl config set-context default \\ --cluster = kubernetes-the-hard-way \\ --user = admin \\ --kubeconfig = admin.kubeconfig kubectl config use-context default --kubeconfig = admin.kubeconfig }","title":"Generate Kube Configs"},{"location":"platforms/kubernetes/admin/admin/#move-kubeconfig-files-to-the-worker-nodes","text":"scp <worker 1 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 1 public IP>:~/ scp <worker 2 hostname>.kubeconfig kube-proxy.kubeconfig user@<worker 2 public IP>:~/","title":"Move kubeconfig files to the worker nodes:"},{"location":"platforms/kubernetes/admin/admin/#move-kubeconfig-files-to-the-master-nodes","text":"scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 1 public IP>:~/ scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig user@<master 2 public IP>:~/","title":"Move kubeconfig files to the master nodes:"},{"location":"platforms/kubernetes/admin/admin/#data-encryption-config","text":"# Generate the Kubernetes Data encrpytion config file containing the encrpytion key: export ENCRYPTION_KEY = $( head -c 32 /dev/urandom | base64 ) cat > encryption-config.yaml << EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF # Copy the file to both master servers: scp encryption-config.yaml user@<master 1 public ip>:~/ scp encryption-config.yaml user@<master 2 public ip>:~/","title":"Data Encryption Config"},{"location":"platforms/kubernetes/admin/admin/#loadbalancer","text":"Setting UP # Here are the commands you can use to set up the nginx load balancer. Run these on the server that you have designated as your load balancer server: sudo apt-get install -y nginx sudo systemctl enable nginx sudo mkdir -p /etc/nginx/tcpconf.d sudo vi /etc/nginx/nginx.conf # Add the following to the end of nginx.conf: include /etc/nginx/tcpconf.d/* ; # Set up some environment variables for the lead balancer config file: export CONTROLLER0_IP = <controller 0 private ip> export CONTROLLER1_IP = <controller 1 private ip> # Create the load balancer nginx config file: cat << EOF | sudo tee /etc/nginx/tcpconf.d/kubernetes.conf stream { upstream kubernetes { server $CONTROLLER0_IP:6443; server $CONTROLLER1_IP:6443; } server { listen 6443; listen 443; proxy_pass kubernetes; } } EOF # Reload the nginx configuration: sudo nginx -s reload # You can verify that the load balancer is working like so: curl -k https://localhost:6443/version","title":"LoadBalancer"},{"location":"platforms/kubernetes/admin/etcd/","text":"Create Cluster \u00b6 # Here are the commands used in the demo (note that these have to be run on both controller servers, with a few differences between them): wget -q --show-progress --https-only --timestamping \\ \"https://github.com/coreos/etcd/releases/download/v3.3.5/etcd-v3.3.5-linux-amd64.tar.gz\" tar -xvf etcd-v3.3.5-linux-amd64.tar.gz sudo mv etcd-v3.3.5-linux-amd64/etcd* /usr/local/bin/ sudo mkdir -p /etc/etcd /var/lib/etcd sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ # Set up the following environment variables. Be sure you replace all of the <placeholder values> with their corresponding real values: export ETCD_NAME = <cloud server hostname> export INTERNAL_IP = $( curl http://169.254.169.254/latest/meta-data/local-ipv4 ) export INITIAL_CLUSTER = <controller 1 hostname> = https://<controller 1 private ip>:2380,<controller 2 hostname> = https://<controller 2 private ip>:2380 # Create the systemd unit file for etcd using this command. Note that this command uses the environment variables that were set earlier: cat << EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster ${INITIAL_CLUSTER} \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the etcd service: sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd # You can verify that the etcd service started up successfully like so: sudo systemctl status etcd # Use this command to verify that etcd is working correctly. The output should list your two etcd nodes: sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem ETCD Command Line \u00b6 Install \u00b6 wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz tar xvf etcd-v3.3.12-linux-amd64.tar.gz sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin Snapshot \u00b6 sudo ETCDCTL_API = 3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View that the snapshot was successful ETCDCTL_API = 3 etcdctl --write-out = table snapshot status snapshot.db Docker \u00b6 Find Command Parameters \u00b6 ps -ef | grep etcd Get All Keys \u00b6 docker exec -it 3606376c1aba /bin/sh -c \"export ETCDCTL_API=3 && etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\" Backup and Restore \u00b6 # Get the etcd binaries: wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz # Unzip the compressed binaries: tar xvf etcd-v3.3.12-linux-amd64.tar.gz # Move the files into /usr/local/bin: sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin # Take a snapshot of the etcd datastore using etcdctl: sudo ETCDCTL_API = 3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View the help page for etcdctl: ETCDCTL_API = 3 etcdctl --help # Browse to the folder that contains the certificate files: cd /etc/kubernetes/pki/etcd/ # View that the snapshot was successful: ETCDCTL_API = 3 etcdctl --write-out = table snapshot status snapshot.db # Zip up the contents of the etcd directory: sudo tar -zcvf etcd.tar.gz /etc/kubernetes/pki/etcd # Copy the etcd directory to another server: scp etcd.tar.gz cloud_user@18.219.235.42:~/","title":"Etcd"},{"location":"platforms/kubernetes/admin/etcd/#create-cluster","text":"# Here are the commands used in the demo (note that these have to be run on both controller servers, with a few differences between them): wget -q --show-progress --https-only --timestamping \\ \"https://github.com/coreos/etcd/releases/download/v3.3.5/etcd-v3.3.5-linux-amd64.tar.gz\" tar -xvf etcd-v3.3.5-linux-amd64.tar.gz sudo mv etcd-v3.3.5-linux-amd64/etcd* /usr/local/bin/ sudo mkdir -p /etc/etcd /var/lib/etcd sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ # Set up the following environment variables. Be sure you replace all of the <placeholder values> with their corresponding real values: export ETCD_NAME = <cloud server hostname> export INTERNAL_IP = $( curl http://169.254.169.254/latest/meta-data/local-ipv4 ) export INITIAL_CLUSTER = <controller 1 hostname> = https://<controller 1 private ip>:2380,<controller 2 hostname> = https://<controller 2 private ip>:2380 # Create the systemd unit file for etcd using this command. Note that this command uses the environment variables that were set earlier: cat << EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster ${INITIAL_CLUSTER} \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the etcd service: sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd # You can verify that the etcd service started up successfully like so: sudo systemctl status etcd # Use this command to verify that etcd is working correctly. The output should list your two etcd nodes: sudo ETCDCTL_API = 3 etcdctl member list \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem","title":"Create Cluster"},{"location":"platforms/kubernetes/admin/etcd/#etcd-command-line","text":"","title":"ETCD Command Line"},{"location":"platforms/kubernetes/admin/etcd/#install","text":"wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz tar xvf etcd-v3.3.12-linux-amd64.tar.gz sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin","title":"Install"},{"location":"platforms/kubernetes/admin/etcd/#snapshot","text":"sudo ETCDCTL_API = 3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View that the snapshot was successful ETCDCTL_API = 3 etcdctl --write-out = table snapshot status snapshot.db","title":"Snapshot"},{"location":"platforms/kubernetes/admin/etcd/#docker","text":"","title":"Docker"},{"location":"platforms/kubernetes/admin/etcd/#find-command-parameters","text":"ps -ef | grep etcd","title":"Find Command Parameters"},{"location":"platforms/kubernetes/admin/etcd/#get-all-keys","text":"docker exec -it 3606376c1aba /bin/sh -c \"export ETCDCTL_API=3 && etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\"","title":"Get All Keys"},{"location":"platforms/kubernetes/admin/etcd/#backup-and-restore","text":"# Get the etcd binaries: wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz # Unzip the compressed binaries: tar xvf etcd-v3.3.12-linux-amd64.tar.gz # Move the files into /usr/local/bin: sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin # Take a snapshot of the etcd datastore using etcdctl: sudo ETCDCTL_API = 3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View the help page for etcdctl: ETCDCTL_API = 3 etcdctl --help # Browse to the folder that contains the certificate files: cd /etc/kubernetes/pki/etcd/ # View that the snapshot was successful: ETCDCTL_API = 3 etcdctl --write-out = table snapshot status snapshot.db # Zip up the contents of the etcd directory: sudo tar -zcvf etcd.tar.gz /etc/kubernetes/pki/etcd # Copy the etcd directory to another server: scp etcd.tar.gz cloud_user@18.219.235.42:~/","title":"Backup and Restore"},{"location":"platforms/kubernetes/admin/hardway/","text":"Master \u00b6 Install # You can install the control plane binaries on each master node like this: sudo mkdir -p /etc/kubernetes/config wget -q --timestamping \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-apiserver\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-controller-manager\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-scheduler\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl\" chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/ API-Server \u00b6 Setting Up # You can configure the Kubernetes API server like so: sudo mkdir -p /var/lib/kubernetes/ sudo cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/ # Set some environment variables that will be used to create the systemd unit file. Make sure you replace the placeholders with their actual values: export INTERNAL_IP = $( curl http://169.254.169.254/latest/meta-data/local-ipv4 ) export CONTROLLER0_IP = <private ip of controller 0 > export CONTROLLER1_IP = <private ip of controller 1 > # Generate the kube-apiserver unit file for systemd: cat << EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://$CONTROLLER0_IP:2379,https://$CONTROLLER1_IP:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 \\\\ --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-apiserver sudo systemctl start kube-apiserver sudo systemctl status kube-apiserver Controller Manager \u00b6 Setting Up # You can configure the Kubernetes Controller Manager like so: sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/ # Generate the kube-controller-manager systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-controller-manager sudo systemctl start kube-controller-manager sudo systemctl status kube-controller-manager Scheduler \u00b6 Setting Up # Copy kube-scheduler.kubeconfig into the proper location: sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/ # Generate the kube-scheduler yaml config file. cat << EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: componentconfig/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: true EOF # Create the kube-scheduler systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-scheduler sudo systemctl start kube-scheduler sudo systemctl status kube-scheduler Default Scheduler \u00b6 Does the node have adequate hardware resources? Is the node running out of resources? Does the pod request a specific node? Does the node have a matching label? If the pod requests a port, is it available? If the pod requests a volume, can it be mounted? Does the pod tolerate the taints of the node? Does the pod specify node or pod affinity? Custom Scheduler \u00b6 ClusterRole \u00b6 apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : csinodes-admin rules : - apiGroups : [ \"storage.k8s.io\" ] resources : [ \"csinodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] ClusterRoleBinding \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-csinodes-global subjects : - kind : ServiceAccount name : my-scheduler namespace : kube-system roleRef : kind : ClusterRole name : csinodes-admin apiGroup : rbac.authorization.k8s.io Role \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : system:serviceaccount:kube-system:my-scheduler namespace : kube-system rules : - apiGroups : - storage.k8s.io resources : - csinodes verbs : - get - list - watch RoleBinding \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : read-csinodes namespace : kube-system subjects : - kind : User name : kubernetes-admin apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : system:serviceaccount:kube-system:my-scheduler apiGroup : rbac.authorization.k8s.io Custom-scheduler \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : my-scheduler namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : my-scheduler-as-kube-scheduler subjects : - kind : ServiceAccount name : my-scheduler namespace : kube-system roleRef : kind : ClusterRole name : system:kube-scheduler apiGroup : rbac.authorization.k8s.io --- apiVersion : apps/v1 kind : Deployment metadata : labels : component : scheduler tier : control-plane name : my-scheduler namespace : kube-system spec : selector : matchLabels : component : scheduler tier : control-plane replicas : 1 template : metadata : labels : component : scheduler tier : control-plane version : second spec : serviceAccountName : my-scheduler containers : - command : - /usr/local/bin/kube-scheduler - --address=0.0.0.0 - --leader-elect=false - --scheduler-name=my-scheduler image : chadmcrowell/custom-scheduler livenessProbe : httpGet : path : /healthz port : 10251 initialDelaySeconds : 15 name : kube-second-scheduler readinessProbe : httpGet : path : /healthz port : 10251 resources : requests : cpu : '0.1' securityContext : privileged : false volumeMounts : [] hostNetwork : false hostPID : false volumes : [] Deploy \u00b6 kubectl create -f clusterrole.yaml kubectl create -f clusterrolebinding.yaml kubectl create -f role.yaml kubectl create -f rolebinding.yaml kubectl edit clusterrole system:kube-scheduler # And add the following - apiGroups : - \"\" resourceNames : - kube-scheduler - my-scheduler resources : - endpoints verbs : - delete - get - patch - update - apiGroups : - storage.k8s.io resources : - storageclasses verbs : - watch - list - get kubectl create -f my-scheduler.yaml kubectl get pods -n kube-system Get config \u00b6 kubectl get endpoints kube-scheduler -n kube-system -o yaml Troubleshooting \u00b6 kubectl describe pods [ scheduler_pod_name ] -n kube-system kubectl logs [ kube_scheduler_pod_name ] -n kube-system cat /var/log/kube-scheduler.log RBAC \u00b6 Setting UP # Create a role with the necessary permissions: cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF # Bind the role to the kubernetes user cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF Nodes \u00b6 Install \u00b6 # You can install the worker binaries like so. Run these commands on both worker nodes: sudo apt-get -y install socat conntrack ipset wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.0/containerd-1.1.0.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.0.linux-amd64.tar.gz -C / ContainerD \u00b6 # You can configure the containerd service like so. Run these commands on both worker nodes: sudo mkdir -p /etc/containerd/ # Create the containerd config.toml: cat << EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\" [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runsc\" runtime_root = \"/run/containerd/runsc\" EOF # Create the containerd unit file: cat << EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/bin/containerd Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable containerd sudo systemctl start containerd sudo systemctl status containerd Kubelet \u00b6 # Set a HOSTNAME environment variable that will be used to generate your config files. Make sure you set the HOSTNAME appropriately for each worker node: export HOSTNAME = $( hostname ) sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ # Create the kubelet config file: cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" authorization: mode: Webhook clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${HOSTNAME}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${HOSTNAME}-key.pem\" EOF # Create the kubelet unit file: cat << EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 \\\\ --hostname-override=${HOSTNAME} \\\\ --allow-privileged=true Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kubelet sudo systemctl start kubelet sudo systemctl status kubelet Kube-Proxy \u00b6 # You can configure the kube-proxy service like so. Run these commands on both worker nodes: sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig # Create the kube-proxy config file: cat << EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"10.200.0.0/16\" EOF # Create the kube-proxy unit file: cat << EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kube-proxy sudo systemctl start kube-proxy sudo systemctl status kube-proxy Commands \u00b6 Components Status \u00b6 # Use kubectl to check componentstatuses: kubectl get componentstatuses --kubeconfig admin.kubeconfig # You should get output that looks like this: NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy { \"health\" : \"true\" } etcd-1 Healthy { \"health\" : \"true\" } Enable HTTP Health Checks \u00b6 Using Nginx Load Balancer \u00b6 On Master Nodes # You can set up a basic nginx proxy for the healthz endpoint by first installing nginx\" sudo apt-get install -y nginx # Create an nginx configuration for the health check proxy: cat > kubernetes.default.svc.cluster.local << EOF server { listen 80; server_name kubernetes.default.svc.cluster.local; location /healthz { proxy_pass https://127.0.0.1:6443/healthz; proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem; } } EOF # Set up the proxy configuration so that it is loaded by nginx: sudo mv kubernetes.default.svc.cluster.local /etc/nginx/sites-available/kubernetes.default.svc.cluster.local sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/ sudo systemctl restart nginx sudo systemctl enable nginx # You can verify that everything is working like so: curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz","title":"Hardway"},{"location":"platforms/kubernetes/admin/hardway/#master","text":"Install # You can install the control plane binaries on each master node like this: sudo mkdir -p /etc/kubernetes/config wget -q --timestamping \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-apiserver\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-controller-manager\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-scheduler\" \\ \"https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl\" chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/","title":"Master"},{"location":"platforms/kubernetes/admin/hardway/#api-server","text":"Setting Up # You can configure the Kubernetes API server like so: sudo mkdir -p /var/lib/kubernetes/ sudo cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/ # Set some environment variables that will be used to create the systemd unit file. Make sure you replace the placeholders with their actual values: export INTERNAL_IP = $( curl http://169.254.169.254/latest/meta-data/local-ipv4 ) export CONTROLLER0_IP = <private ip of controller 0 > export CONTROLLER1_IP = <private ip of controller 1 > # Generate the kube-apiserver unit file for systemd: cat << EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://$CONTROLLER0_IP:2379,https://$CONTROLLER1_IP:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 \\\\ --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-apiserver sudo systemctl start kube-apiserver sudo systemctl status kube-apiserver","title":"API-Server"},{"location":"platforms/kubernetes/admin/hardway/#controller-manager","text":"Setting Up # You can configure the Kubernetes Controller Manager like so: sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/ # Generate the kube-controller-manager systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-controller-manager sudo systemctl start kube-controller-manager sudo systemctl status kube-controller-manager","title":"Controller Manager"},{"location":"platforms/kubernetes/admin/hardway/#scheduler","text":"Setting Up # Copy kube-scheduler.kubeconfig into the proper location: sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/ # Generate the kube-scheduler yaml config file. cat << EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: componentconfig/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: true EOF # Create the kube-scheduler systemd unit file: cat << EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF # Start and enable the service: sudo systemctl daemon-reload sudo systemctl enable kube-scheduler sudo systemctl start kube-scheduler sudo systemctl status kube-scheduler","title":"Scheduler"},{"location":"platforms/kubernetes/admin/hardway/#default-scheduler","text":"Does the node have adequate hardware resources? Is the node running out of resources? Does the pod request a specific node? Does the node have a matching label? If the pod requests a port, is it available? If the pod requests a volume, can it be mounted? Does the pod tolerate the taints of the node? Does the pod specify node or pod affinity?","title":"Default Scheduler"},{"location":"platforms/kubernetes/admin/hardway/#custom-scheduler","text":"","title":"Custom Scheduler"},{"location":"platforms/kubernetes/admin/hardway/#clusterrole","text":"apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : csinodes-admin rules : - apiGroups : [ \"storage.k8s.io\" ] resources : [ \"csinodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ]","title":"ClusterRole"},{"location":"platforms/kubernetes/admin/hardway/#clusterrolebinding","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-csinodes-global subjects : - kind : ServiceAccount name : my-scheduler namespace : kube-system roleRef : kind : ClusterRole name : csinodes-admin apiGroup : rbac.authorization.k8s.io","title":"ClusterRoleBinding"},{"location":"platforms/kubernetes/admin/hardway/#role","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : system:serviceaccount:kube-system:my-scheduler namespace : kube-system rules : - apiGroups : - storage.k8s.io resources : - csinodes verbs : - get - list - watch","title":"Role"},{"location":"platforms/kubernetes/admin/hardway/#rolebinding","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : read-csinodes namespace : kube-system subjects : - kind : User name : kubernetes-admin apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : system:serviceaccount:kube-system:my-scheduler apiGroup : rbac.authorization.k8s.io","title":"RoleBinding"},{"location":"platforms/kubernetes/admin/hardway/#custom-scheduler_1","text":"apiVersion : v1 kind : ServiceAccount metadata : name : my-scheduler namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : my-scheduler-as-kube-scheduler subjects : - kind : ServiceAccount name : my-scheduler namespace : kube-system roleRef : kind : ClusterRole name : system:kube-scheduler apiGroup : rbac.authorization.k8s.io --- apiVersion : apps/v1 kind : Deployment metadata : labels : component : scheduler tier : control-plane name : my-scheduler namespace : kube-system spec : selector : matchLabels : component : scheduler tier : control-plane replicas : 1 template : metadata : labels : component : scheduler tier : control-plane version : second spec : serviceAccountName : my-scheduler containers : - command : - /usr/local/bin/kube-scheduler - --address=0.0.0.0 - --leader-elect=false - --scheduler-name=my-scheduler image : chadmcrowell/custom-scheduler livenessProbe : httpGet : path : /healthz port : 10251 initialDelaySeconds : 15 name : kube-second-scheduler readinessProbe : httpGet : path : /healthz port : 10251 resources : requests : cpu : '0.1' securityContext : privileged : false volumeMounts : [] hostNetwork : false hostPID : false volumes : []","title":"Custom-scheduler"},{"location":"platforms/kubernetes/admin/hardway/#deploy","text":"kubectl create -f clusterrole.yaml kubectl create -f clusterrolebinding.yaml kubectl create -f role.yaml kubectl create -f rolebinding.yaml kubectl edit clusterrole system:kube-scheduler # And add the following - apiGroups : - \"\" resourceNames : - kube-scheduler - my-scheduler resources : - endpoints verbs : - delete - get - patch - update - apiGroups : - storage.k8s.io resources : - storageclasses verbs : - watch - list - get kubectl create -f my-scheduler.yaml kubectl get pods -n kube-system","title":"Deploy"},{"location":"platforms/kubernetes/admin/hardway/#get-config","text":"kubectl get endpoints kube-scheduler -n kube-system -o yaml","title":"Get config"},{"location":"platforms/kubernetes/admin/hardway/#troubleshooting","text":"kubectl describe pods [ scheduler_pod_name ] -n kube-system kubectl logs [ kube_scheduler_pod_name ] -n kube-system cat /var/log/kube-scheduler.log","title":"Troubleshooting"},{"location":"platforms/kubernetes/admin/hardway/#rbac","text":"Setting UP # Create a role with the necessary permissions: cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF # Bind the role to the kubernetes user cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF","title":"RBAC"},{"location":"platforms/kubernetes/admin/hardway/#nodes","text":"","title":"Nodes"},{"location":"platforms/kubernetes/admin/hardway/#install","text":"# You can install the worker binaries like so. Run these commands on both worker nodes: sudo apt-get -y install socat conntrack ipset wget -q --show-progress --https-only --timestamping \\ https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-the-hard-way/runsc \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \\ https://github.com/containerd/containerd/releases/download/v1.1.0/containerd-1.1.0.linux-amd64.tar.gz \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \\ https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet sudo mkdir -p \\ /etc/cni/net.d \\ /opt/cni/bin \\ /var/lib/kubelet \\ /var/lib/kube-proxy \\ /var/lib/kubernetes \\ /var/run/kubernetes chmod +x kubectl kube-proxy kubelet runc.amd64 runsc sudo mv runc.amd64 runc sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/ sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/ sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/ sudo tar -xvf containerd-1.1.0.linux-amd64.tar.gz -C /","title":"Install"},{"location":"platforms/kubernetes/admin/hardway/#containerd","text":"# You can configure the containerd service like so. Run these commands on both worker nodes: sudo mkdir -p /etc/containerd/ # Create the containerd config.toml: cat << EOF | sudo tee /etc/containerd/config.toml [plugins] [plugins.cri.containerd] snapshotter = \"overlayfs\" [plugins.cri.containerd.default_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runc\" runtime_root = \"\" [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \"io.containerd.runtime.v1.linux\" runtime_engine = \"/usr/local/bin/runsc\" runtime_root = \"/run/containerd/runsc\" EOF # Create the containerd unit file: cat << EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/bin/containerd Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable containerd sudo systemctl start containerd sudo systemctl status containerd","title":"ContainerD"},{"location":"platforms/kubernetes/admin/hardway/#kubelet","text":"# Set a HOSTNAME environment variable that will be used to generate your config files. Make sure you set the HOSTNAME appropriately for each worker node: export HOSTNAME = $( hostname ) sudo mv ${ HOSTNAME } -key.pem ${ HOSTNAME } .pem /var/lib/kubelet/ sudo mv ${ HOSTNAME } .kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ # Create the kubelet config file: cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" authorization: mode: Webhook clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${HOSTNAME}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${HOSTNAME}-key.pem\" EOF # Create the kubelet unit file: cat << EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 \\\\ --hostname-override=${HOSTNAME} \\\\ --allow-privileged=true Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kubelet sudo systemctl start kubelet sudo systemctl status kubelet","title":"Kubelet"},{"location":"platforms/kubernetes/admin/hardway/#kube-proxy","text":"# You can configure the kube-proxy service like so. Run these commands on both worker nodes: sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig # Create the kube-proxy config file: cat << EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"10.200.0.0/16\" EOF # Create the kube-proxy unit file: cat << EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable kube-proxy sudo systemctl start kube-proxy sudo systemctl status kube-proxy","title":"Kube-Proxy"},{"location":"platforms/kubernetes/admin/hardway/#commands","text":"","title":"Commands"},{"location":"platforms/kubernetes/admin/hardway/#components-status","text":"# Use kubectl to check componentstatuses: kubectl get componentstatuses --kubeconfig admin.kubeconfig # You should get output that looks like this: NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy { \"health\" : \"true\" } etcd-1 Healthy { \"health\" : \"true\" }","title":"Components Status"},{"location":"platforms/kubernetes/admin/hardway/#enable-http-health-checks","text":"","title":"Enable HTTP Health Checks"},{"location":"platforms/kubernetes/admin/hardway/#using-nginx-load-balancer","text":"On Master Nodes # You can set up a basic nginx proxy for the healthz endpoint by first installing nginx\" sudo apt-get install -y nginx # Create an nginx configuration for the health check proxy: cat > kubernetes.default.svc.cluster.local << EOF server { listen 80; server_name kubernetes.default.svc.cluster.local; location /healthz { proxy_pass https://127.0.0.1:6443/healthz; proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem; } } EOF # Set up the proxy configuration so that it is loaded by nginx: sudo mv kubernetes.default.svc.cluster.local /etc/nginx/sites-available/kubernetes.default.svc.cluster.local sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/ sudo systemctl restart nginx sudo systemctl enable nginx # You can verify that everything is working like so: curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz","title":"Using Nginx Load Balancer"},{"location":"platforms/kubernetes/admin/kops/","text":"Kops \u00b6 Install \u00b6 # Install Kops wget https://github.com/kubernetes/kops/releases/download/1.13.0/kops-linux-amd64 chmod +x kops-linux-amd64 sudo mv kops-linux-amd64 /usr/local/bin/kops export REGION = eu-west-1 # Create S3 aws s3 mb s3://fsantos-k8s-state-store --region ${ REGION } aws s3api put-bucket-versioning --bucket fsantos-k8s-state-store --versioning-configuration Status = Enabled aws s3api put-bucket-encryption --bucket fsantos-k8s-state-store --server-side-encryption-configuration '{\"Rules\":[{\"ApplyServerSideEncryptionByDefault\":{\"SSEAlgorithm\":\"AES256\"}}]}' export KOPS_STATE_STORE = s3://fsantos-k8s-state-store export NAME = tiagomsantos.com kops create secret --name ${ NAME } sshpublickey admin -i ~/.ssh/id_rsa.pub kops create cluster \\ --state ${ KOPS_STATE_STORE } \\ --zones \"eu-west-1a,eu-west-1b,eu-west-1c\" \\ --api-loadbalancer-type public \\ --master-count 3 \\ --master-size = t2.micro \\ --master-volume-size 8 \\ --node-count 3 \\ --node-size = t2.micro \\ --node-volume-size 8 \\ --name ${ NAME } \\ --cloud aws \\ --networking calico \\ --topology private \\ --ssh-public-key ~/.ssh/id_rsa.pub \\ --api-ssl-certificate arn:aws:acm:eu-west-1:395563851492:certificate/9190b1fe-8a63-4432-9f6f-01a631c4f1b2 \\ --bastion \\ --yes Suspend Autoscaling Process \u00b6 for groupName in $( aws autoscaling describe-auto-scaling-groups --query 'AutoScalingGroups[*].AutoScalingGroupName' --output text ) ; do if [[ $groupName = ~ ${ NAME } ]] then echo \"Processing autoscaling $groupName ...\" aws autoscaling suspend-processes --auto-scaling-group-name $groupName --scaling-processes HealthCheck ReplaceUnhealthy AlarmNotification AZRebalance Launch Terminate ScheduledActions AddToLoadBalancer RemoveFromLoadBalancerLowPriority fi done","title":"Kops"},{"location":"platforms/kubernetes/admin/kops/#kops","text":"","title":"Kops"},{"location":"platforms/kubernetes/admin/kops/#install","text":"# Install Kops wget https://github.com/kubernetes/kops/releases/download/1.13.0/kops-linux-amd64 chmod +x kops-linux-amd64 sudo mv kops-linux-amd64 /usr/local/bin/kops export REGION = eu-west-1 # Create S3 aws s3 mb s3://fsantos-k8s-state-store --region ${ REGION } aws s3api put-bucket-versioning --bucket fsantos-k8s-state-store --versioning-configuration Status = Enabled aws s3api put-bucket-encryption --bucket fsantos-k8s-state-store --server-side-encryption-configuration '{\"Rules\":[{\"ApplyServerSideEncryptionByDefault\":{\"SSEAlgorithm\":\"AES256\"}}]}' export KOPS_STATE_STORE = s3://fsantos-k8s-state-store export NAME = tiagomsantos.com kops create secret --name ${ NAME } sshpublickey admin -i ~/.ssh/id_rsa.pub kops create cluster \\ --state ${ KOPS_STATE_STORE } \\ --zones \"eu-west-1a,eu-west-1b,eu-west-1c\" \\ --api-loadbalancer-type public \\ --master-count 3 \\ --master-size = t2.micro \\ --master-volume-size 8 \\ --node-count 3 \\ --node-size = t2.micro \\ --node-volume-size 8 \\ --name ${ NAME } \\ --cloud aws \\ --networking calico \\ --topology private \\ --ssh-public-key ~/.ssh/id_rsa.pub \\ --api-ssl-certificate arn:aws:acm:eu-west-1:395563851492:certificate/9190b1fe-8a63-4432-9f6f-01a631c4f1b2 \\ --bastion \\ --yes","title":"Install"},{"location":"platforms/kubernetes/admin/kops/#suspend-autoscaling-process","text":"for groupName in $( aws autoscaling describe-auto-scaling-groups --query 'AutoScalingGroups[*].AutoScalingGroupName' --output text ) ; do if [[ $groupName = ~ ${ NAME } ]] then echo \"Processing autoscaling $groupName ...\" aws autoscaling suspend-processes --auto-scaling-group-name $groupName --scaling-processes HealthCheck ReplaceUnhealthy AlarmNotification AZRebalance Launch Terminate ScheduledActions AddToLoadBalancer RemoveFromLoadBalancerLowPriority fi done","title":"Suspend Autoscaling Process"},{"location":"platforms/kubernetes/admin/kubecontrollermanager/","text":"Manifest File \u00b6 Edit and re-deploy is automatically /etc/kubernetes/manifests/kube-controller-manager.yaml Metrics-Server \u00b6 Change Scale Down Time Change --horizontal-pod-autoscaler-downscale-stabilization Default 5 minutes Link","title":"KubeControllerManager"},{"location":"platforms/kubernetes/admin/kubecontrollermanager/#manifest-file","text":"Edit and re-deploy is automatically /etc/kubernetes/manifests/kube-controller-manager.yaml","title":"Manifest File"},{"location":"platforms/kubernetes/admin/kubecontrollermanager/#metrics-server","text":"Change Scale Down Time Change --horizontal-pod-autoscaler-downscale-stabilization Default 5 minutes Link","title":"Metrics-Server"},{"location":"platforms/kubernetes/admin/monitoring/","text":"Monitoring \u00b6 Cluster Components \u00b6 Metrics Server \u00b6 git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/ # Get a response from the metrics server API: kubectl get --raw /apis/metrics.k8s.io/ kubectl top node kubectl top pods kubectl top pods --all-namespaces kubectl top pods -n kube-system kubectl top pod -l run = pod-with-defaults kubectl top pod pod-with-defaults # Get the CPU and memory of the containers inside the pod kubectl top pods group-context --containers Applications \u00b6 Liveness Probe \u00b6 apiVersion : v1 kind : Pod metadata : name : liveness spec : containers : - image : linuxacademycontent/kubeserve name : kubeserve livenessProbe : httpGet : path : / port : 80 apiVersion : v1 kind : Pod metadata : name : my-liveness-pod spec : containers : - name : myapp-container image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] livenessProbe : exec : command : - echo - testing initialDelaySeconds : 5 periodSeconds : 5 Readiness Probe \u00b6 apiVersion : v1 kind : Service metadata : name : nginx spec : type : LoadBalancer ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : v1 kind : Pod metadata : name : nginx labels : app : nginx spec : containers : - name : nginx image : nginx readinessProbe : httpGet : path : / port : 80 initialDelaySeconds : 5 periodSeconds : 5 --- apiVersion : v1 kind : Pod metadata : name : nginxpd labels : app : nginx spec : containers : - name : nginx image : nginx:191 readinessProbe : httpGet : path : / port : 80 initialDelaySeconds : 5 periodSeconds : 5 Logs \u00b6 Cluster \u00b6 Dirs \u00b6 The directory where the continainer logs reside ls /var/log/containers The directory where kubelet stores its logs ls /var/log SideCar Container \u00b6 The YAML for a sidecar container that will tail the logs for each type apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : busybox args : - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts : - name : varlog mountPath : /var/log - name : count-log-1 image : busybox args : [ /bin/sh , -c , 'tail -n+1 -f /var/log/1.log' ] volumeMounts : - name : varlog mountPath : /var/log - name : count-log-2 image : busybox args : [ /bin/sh , -c , 'tail -n+1 -f /var/log/2.log' ] volumeMounts : - name : varlog mountPath : /var/log volumes : - name : varlog emptyDir : {} kubectl logs counter count-log-1 kubectl logs counter count-log-2 Application \u00b6 # Get the logs from a pod: kubectl logs nginx # Get the logs from a specific container on a pod: kubectl logs counter -c count-log-1 # Get the logs from all containers on the pod: kubectl logs counter --all-containers = true # Get the logs from containers with a certain label: kubectl logs -lapp = nginx # Get the logs from a previously terminated container within a pod: kubectl logs -p -c nginx nginx # Stream the logs from a container in a pod: kubectl logs -f -c count-log-1 counter # Tail the logs to only view a certain number of lines: kubectl logs --tail = 20 nginx # View the logs from a previous time duration: kubectl logs --since = 1h nginx # View the logs from a container within a pod within a deployment: kubectl logs deployment/nginx -c nginx # Redirect the output of the logs to a file: kubectl logs counter -c count-log-1 > count.log Troubleshooting \u00b6 Applications \u00b6 Use Termination Reason \u00b6 apiVersion : v1 kind : Pod metadata : name : pod2 spec : containers : - image : busybox name : main command : - sh - -c - 'echo \"I '' ve had enough\" > /var/termination-reason ; exit 1' terminationMessagePath : /var/termination-reason Healthz \u00b6 Not all pods have healthz configured apiVersion : v1 kind : Pod metadata : name : liveness spec : containers : - image : linuxacademycontent/candy-service:2 name : kubeserve livenessProbe : httpGet : path : /healthz port : 8081 Steps \u00b6 kubectl describe po pod2 kubectl logs pod-with-defaults kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml Cluster \u00b6 # Check the events in the kube-system namespace for errors kubectl get events -n kube-system kubectl logs [ kube_scheduler_pod_name ] -n kube-system # Check the status of the Docker service: sudo systemctl status docker sudo systemctl enable docker && systemctl start docker # Check the status of the kubelet service: sudo systemctl status kubelet sudo systemctl enable kubelet && systemctl start kubelet # Turn off swap on your machine sudo su - swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # Check if you have a firewall running: sudo systemctl status firewalld sudo systemctl disable firewalld && systemctl stop firewalld Worker Node \u00b6 kubectl get nodes kubectl describe nodes chadcrowell2c.mylabserver.com # Create New Worker Server # Generate a new token after spinning up a new server: sudo kubeadm token generate # Create the kubeadm join command for your new worker node: # sudo kubeadm token create [token_name] --ttl 2h --print-join-command # View the journalctl logs: sudo journalctl -u kubelet # View the syslogs: sudo more syslog | tail -120 | grep kubelet Networking \u00b6 DNS \u00b6 # Run an interactive busybox pod: kubectl run -it --rm --restart = Never busybox --image = busybox:1.28 sh # From the pod, check if DNS is resolving hostnames: nslookup hostnames # From the pod, cat out the /etc/resolv.conf file: cat /etc/resolv.conf # From the pod, look up the DNS name of the Kubernetes service: nslookup kubernetes.default nslookup kube-dns.kube-system.svc.cluster.loca # Look up a service in your Kubernetes cluster nslookup [ pod-ip-address ] .default.pod.cluster.local # Logs Core Dns kubectl logs [ coredns-pod-name ] Kube-Proxy \u00b6 # View the endpoints for your service: kubectl get ep # Communicate with the pod directly (without the service): wget -qO- 10 .244.1.6:9376 # Check if kube-proxy is running on the nodes: ps auxw | grep kube-proxy # Check if kube-proxy is writing iptables: kubectl get services -o wide iptables-save | grep hostnames sudo iptables-save | grep KUBE | grep <service-name> # View the list of kube-system pods: kubectl get pods -n kube-system # Connect to your kube-proxy pod in the kube-system namespace: kubectl exec -it kube-proxy-cqptg -n kube-system -- sh Change CNI Plugin \u00b6 # Delete the flannel CNI plugin: kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml # Apply the Weave Net CNI plugin: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \" Smoke Testing \u00b6 Data Encryption \u00b6 # Create a test secret: kubectl create secret generic kubernetes-the-hard-way --from-literal = \"mykey=mydata\" # Log in to one of your master servers, and get the raw data for the test secret from etcd: sudo ETCDCTL_API = 3 etcdctl get \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem \\ /registry/secrets/default/kubernetes-the-hard-way | hexdump -C # Your output should look something like this: 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 | /registry/secret | 00000010 73 2f 64 65 66 61 75 6c 74 2f 6b 75 62 65 72 6e | s/default/kubern | 00000020 65 74 65 73 2d 74 68 65 2d 68 61 72 64 2d 77 61 | etes-the-hard-wa | 00000030 79 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 | y.k8s:enc:aescbc | 00000040 3a 76 31 3a 6b 65 79 31 3a fc 21 ee dc e5 84 8a | :v1:key1:.!..... | 00000050 53 8e fd a9 72 a8 75 25 65 30 55 0e 72 43 1f 20 | S...r.u%e0U.rC. | 00000060 9f 07 15 4f 69 8a 79 a4 70 62 e9 ab f9 14 93 2e | ...Oi.y.pb...... | 00000070 e5 59 3f ab a7 b2 d8 d6 05 84 84 aa c3 6f 8d 5c | .Y?..........o. \\| 00000080 09 7a 2f 82 81 b5 d5 ec ba c7 23 34 46 d9 43 02 | .z/.......#4F.C. | 00000090 88 93 57 26 66 da 4e 8e 5c 24 44 6e 3e ec 9c 8e | ..W & f.N. \\$ Dn>... | 000000a0 83 ff 40 9a fb 94 07 3c 08 52 0e 77 50 81 c9 d0 | ..@....<.R.wP... | 000000b0 b7 30 68 ba b1 b3 26 eb b1 9f 3f f1 d7 76 86 09 | .0h... & ...?..v.. | 000000c0 d8 14 02 12 09 30 b0 60 b2 ad dc bb cf f5 77 e0 | .....0. ` ......w. | 000000d0 4f 0b 1f 74 79 c1 e7 20 1d 32 b2 68 01 19 93 fc | O..ty.. .2.h.... | 000000e0 f5 c8 8b 0b 16 7b 4f c2 6a 0a | ..... { O.j. | 000000ea # Look for k8s:enc:aescbc:v1:key1 on the right of the output to verify that the data is stored in an encrypted format! Deployments \u00b6 # Create a a simple nginx deployment: kubectl run nginx --image = nginx # Verify that the deployment created a pod and that the pod is running: kubectl get pods -l run = nginx # Verify that the output looks something like this: NAME READY STATUS RESTARTS AGE nginx-65899c769f-9xnqm 1 /1 Running 0 30s Port Forwarding \u00b6 # First, get the pod name of the nginx pod and store it an an environment variable: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # Forward port 8081 to the nginx pod: kubectl port-forward $POD_NAME 8081 :80 # Open up a new terminal on the same machine running the kubectl port-forward command and verify that the port forward works. curl --head http://127.0.0.1:8081 # You should get an http 200 OK response from the nginx pod. Logs \u00b6 # First, let's set an environment variable to the name of the nginx pod: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # Get the logs from the nginx pod: kubectl logs $POD_NAME # This command should return the nginx pod's logs. It will look something like this (but there could be more lines): 127 .0.0.1 - - [ 10 /Sep/2018:19:29:01 +0000 ] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\" Exec \u00b6 # First, let's set an environment variable to the name of the nginx pod: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # To test kubectl exec, execute a simple nginx -v command inside the nginx pod: kubectl exec -ti $POD_NAME -- nginx -v # This command should return the nginx version output, which should look like this: nginx version: nginx/1.15.3 Services \u00b6 # First, create a service to expose the nginx deployment: kubectl expose deployment nginx --port 80 --type NodePort # Get the node port assigned to the newly-created service and assign it to an environment variable: kubectl get svc # The output should look something like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .32.0.1 <none> 443 /TCP 20d nginx NodePort 10 .32.0.81 <none> 80 :32642/TCP 2m # Look for the service called nginx in that output. Under PORT(S), look for the second port, listed after 80:. In the example above, it is 32642. That is the node port, so make note of that value since you will need it in a moment. # Next, log in to one of your worker servers and make a request to the service using the node port. Be sure to replace the placeholder with the actual node port: curl -I localhost:<node port> # You should get an http 200 OK response. Untrusted Workloads \u00b6 # First, create an untrusted pod: cat << EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: untrusted annotations: io.kubernetes.cri.untrusted-workload: \"true\" spec: containers: - name: webserver image: gcr.io/hightowerlabs/helloworld:2.0.0 EOF # Make sure that the untrusted pod is running: kubectl get pods untrusted -o wide # Take note of which worker node the untrusted pod is running on, then log into that worker node. # On the worker node, list all of the containers running under gVisor: sudo runsc --root /run/containerd/runsc/k8s.io list # Get the pod ID of the untrusted pod and store it in an environment variable: POD_ID = $( sudo crictl -r unix:///var/run/containerd/containerd.sock \\ pods --name untrusted -q ) # Get the container ID of the container running in the untrusted pod and store it in an environment variable: CONTAINER_ID = $( sudo crictl -r unix:///var/run/containerd/containerd.sock \\ ps -p ${ POD_ID } -q ) # Get information about the process running in the container: sudo runsc --root /run/containerd/runsc/k8s.io ps ${ CONTAINER_ID } # Since we were able to get the process info using runsc, we know that the untrusted container is running securely as expected.","title":"Monitoring"},{"location":"platforms/kubernetes/admin/monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"platforms/kubernetes/admin/monitoring/#cluster-components","text":"","title":"Cluster Components"},{"location":"platforms/kubernetes/admin/monitoring/#metrics-server","text":"git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/ # Get a response from the metrics server API: kubectl get --raw /apis/metrics.k8s.io/ kubectl top node kubectl top pods kubectl top pods --all-namespaces kubectl top pods -n kube-system kubectl top pod -l run = pod-with-defaults kubectl top pod pod-with-defaults # Get the CPU and memory of the containers inside the pod kubectl top pods group-context --containers","title":"Metrics Server"},{"location":"platforms/kubernetes/admin/monitoring/#applications","text":"","title":"Applications"},{"location":"platforms/kubernetes/admin/monitoring/#liveness-probe","text":"apiVersion : v1 kind : Pod metadata : name : liveness spec : containers : - image : linuxacademycontent/kubeserve name : kubeserve livenessProbe : httpGet : path : / port : 80 apiVersion : v1 kind : Pod metadata : name : my-liveness-pod spec : containers : - name : myapp-container image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] livenessProbe : exec : command : - echo - testing initialDelaySeconds : 5 periodSeconds : 5","title":"Liveness Probe"},{"location":"platforms/kubernetes/admin/monitoring/#readiness-probe","text":"apiVersion : v1 kind : Service metadata : name : nginx spec : type : LoadBalancer ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : v1 kind : Pod metadata : name : nginx labels : app : nginx spec : containers : - name : nginx image : nginx readinessProbe : httpGet : path : / port : 80 initialDelaySeconds : 5 periodSeconds : 5 --- apiVersion : v1 kind : Pod metadata : name : nginxpd labels : app : nginx spec : containers : - name : nginx image : nginx:191 readinessProbe : httpGet : path : / port : 80 initialDelaySeconds : 5 periodSeconds : 5","title":"Readiness Probe"},{"location":"platforms/kubernetes/admin/monitoring/#logs","text":"","title":"Logs"},{"location":"platforms/kubernetes/admin/monitoring/#cluster","text":"","title":"Cluster"},{"location":"platforms/kubernetes/admin/monitoring/#dirs","text":"The directory where the continainer logs reside ls /var/log/containers The directory where kubelet stores its logs ls /var/log","title":"Dirs"},{"location":"platforms/kubernetes/admin/monitoring/#sidecar-container","text":"The YAML for a sidecar container that will tail the logs for each type apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : busybox args : - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts : - name : varlog mountPath : /var/log - name : count-log-1 image : busybox args : [ /bin/sh , -c , 'tail -n+1 -f /var/log/1.log' ] volumeMounts : - name : varlog mountPath : /var/log - name : count-log-2 image : busybox args : [ /bin/sh , -c , 'tail -n+1 -f /var/log/2.log' ] volumeMounts : - name : varlog mountPath : /var/log volumes : - name : varlog emptyDir : {} kubectl logs counter count-log-1 kubectl logs counter count-log-2","title":"SideCar Container"},{"location":"platforms/kubernetes/admin/monitoring/#application","text":"# Get the logs from a pod: kubectl logs nginx # Get the logs from a specific container on a pod: kubectl logs counter -c count-log-1 # Get the logs from all containers on the pod: kubectl logs counter --all-containers = true # Get the logs from containers with a certain label: kubectl logs -lapp = nginx # Get the logs from a previously terminated container within a pod: kubectl logs -p -c nginx nginx # Stream the logs from a container in a pod: kubectl logs -f -c count-log-1 counter # Tail the logs to only view a certain number of lines: kubectl logs --tail = 20 nginx # View the logs from a previous time duration: kubectl logs --since = 1h nginx # View the logs from a container within a pod within a deployment: kubectl logs deployment/nginx -c nginx # Redirect the output of the logs to a file: kubectl logs counter -c count-log-1 > count.log","title":"Application"},{"location":"platforms/kubernetes/admin/monitoring/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"platforms/kubernetes/admin/monitoring/#applications_1","text":"","title":"Applications"},{"location":"platforms/kubernetes/admin/monitoring/#use-termination-reason","text":"apiVersion : v1 kind : Pod metadata : name : pod2 spec : containers : - image : busybox name : main command : - sh - -c - 'echo \"I '' ve had enough\" > /var/termination-reason ; exit 1' terminationMessagePath : /var/termination-reason","title":"Use Termination Reason"},{"location":"platforms/kubernetes/admin/monitoring/#healthz","text":"Not all pods have healthz configured apiVersion : v1 kind : Pod metadata : name : liveness spec : containers : - image : linuxacademycontent/candy-service:2 name : kubeserve livenessProbe : httpGet : path : /healthz port : 8081","title":"Healthz"},{"location":"platforms/kubernetes/admin/monitoring/#steps","text":"kubectl describe po pod2 kubectl logs pod-with-defaults kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml","title":"Steps"},{"location":"platforms/kubernetes/admin/monitoring/#cluster_1","text":"# Check the events in the kube-system namespace for errors kubectl get events -n kube-system kubectl logs [ kube_scheduler_pod_name ] -n kube-system # Check the status of the Docker service: sudo systemctl status docker sudo systemctl enable docker && systemctl start docker # Check the status of the kubelet service: sudo systemctl status kubelet sudo systemctl enable kubelet && systemctl start kubelet # Turn off swap on your machine sudo su - swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab # Check if you have a firewall running: sudo systemctl status firewalld sudo systemctl disable firewalld && systemctl stop firewalld","title":"Cluster"},{"location":"platforms/kubernetes/admin/monitoring/#worker-node","text":"kubectl get nodes kubectl describe nodes chadcrowell2c.mylabserver.com # Create New Worker Server # Generate a new token after spinning up a new server: sudo kubeadm token generate # Create the kubeadm join command for your new worker node: # sudo kubeadm token create [token_name] --ttl 2h --print-join-command # View the journalctl logs: sudo journalctl -u kubelet # View the syslogs: sudo more syslog | tail -120 | grep kubelet","title":"Worker Node"},{"location":"platforms/kubernetes/admin/monitoring/#networking","text":"","title":"Networking"},{"location":"platforms/kubernetes/admin/monitoring/#dns","text":"# Run an interactive busybox pod: kubectl run -it --rm --restart = Never busybox --image = busybox:1.28 sh # From the pod, check if DNS is resolving hostnames: nslookup hostnames # From the pod, cat out the /etc/resolv.conf file: cat /etc/resolv.conf # From the pod, look up the DNS name of the Kubernetes service: nslookup kubernetes.default nslookup kube-dns.kube-system.svc.cluster.loca # Look up a service in your Kubernetes cluster nslookup [ pod-ip-address ] .default.pod.cluster.local # Logs Core Dns kubectl logs [ coredns-pod-name ]","title":"DNS"},{"location":"platforms/kubernetes/admin/monitoring/#kube-proxy","text":"# View the endpoints for your service: kubectl get ep # Communicate with the pod directly (without the service): wget -qO- 10 .244.1.6:9376 # Check if kube-proxy is running on the nodes: ps auxw | grep kube-proxy # Check if kube-proxy is writing iptables: kubectl get services -o wide iptables-save | grep hostnames sudo iptables-save | grep KUBE | grep <service-name> # View the list of kube-system pods: kubectl get pods -n kube-system # Connect to your kube-proxy pod in the kube-system namespace: kubectl exec -it kube-proxy-cqptg -n kube-system -- sh","title":"Kube-Proxy"},{"location":"platforms/kubernetes/admin/monitoring/#change-cni-plugin","text":"# Delete the flannel CNI plugin: kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml # Apply the Weave Net CNI plugin: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \"","title":"Change CNI Plugin"},{"location":"platforms/kubernetes/admin/monitoring/#smoke-testing","text":"","title":"Smoke Testing"},{"location":"platforms/kubernetes/admin/monitoring/#data-encryption","text":"# Create a test secret: kubectl create secret generic kubernetes-the-hard-way --from-literal = \"mykey=mydata\" # Log in to one of your master servers, and get the raw data for the test secret from etcd: sudo ETCDCTL_API = 3 etcdctl get \\ --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/etcd/ca.pem \\ --cert = /etc/etcd/kubernetes.pem \\ --key = /etc/etcd/kubernetes-key.pem \\ /registry/secrets/default/kubernetes-the-hard-way | hexdump -C # Your output should look something like this: 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 | /registry/secret | 00000010 73 2f 64 65 66 61 75 6c 74 2f 6b 75 62 65 72 6e | s/default/kubern | 00000020 65 74 65 73 2d 74 68 65 2d 68 61 72 64 2d 77 61 | etes-the-hard-wa | 00000030 79 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 | y.k8s:enc:aescbc | 00000040 3a 76 31 3a 6b 65 79 31 3a fc 21 ee dc e5 84 8a | :v1:key1:.!..... | 00000050 53 8e fd a9 72 a8 75 25 65 30 55 0e 72 43 1f 20 | S...r.u%e0U.rC. | 00000060 9f 07 15 4f 69 8a 79 a4 70 62 e9 ab f9 14 93 2e | ...Oi.y.pb...... | 00000070 e5 59 3f ab a7 b2 d8 d6 05 84 84 aa c3 6f 8d 5c | .Y?..........o. \\| 00000080 09 7a 2f 82 81 b5 d5 ec ba c7 23 34 46 d9 43 02 | .z/.......#4F.C. | 00000090 88 93 57 26 66 da 4e 8e 5c 24 44 6e 3e ec 9c 8e | ..W & f.N. \\$ Dn>... | 000000a0 83 ff 40 9a fb 94 07 3c 08 52 0e 77 50 81 c9 d0 | ..@....<.R.wP... | 000000b0 b7 30 68 ba b1 b3 26 eb b1 9f 3f f1 d7 76 86 09 | .0h... & ...?..v.. | 000000c0 d8 14 02 12 09 30 b0 60 b2 ad dc bb cf f5 77 e0 | .....0. ` ......w. | 000000d0 4f 0b 1f 74 79 c1 e7 20 1d 32 b2 68 01 19 93 fc | O..ty.. .2.h.... | 000000e0 f5 c8 8b 0b 16 7b 4f c2 6a 0a | ..... { O.j. | 000000ea # Look for k8s:enc:aescbc:v1:key1 on the right of the output to verify that the data is stored in an encrypted format!","title":"Data Encryption"},{"location":"platforms/kubernetes/admin/monitoring/#deployments","text":"# Create a a simple nginx deployment: kubectl run nginx --image = nginx # Verify that the deployment created a pod and that the pod is running: kubectl get pods -l run = nginx # Verify that the output looks something like this: NAME READY STATUS RESTARTS AGE nginx-65899c769f-9xnqm 1 /1 Running 0 30s","title":"Deployments"},{"location":"platforms/kubernetes/admin/monitoring/#port-forwarding","text":"# First, get the pod name of the nginx pod and store it an an environment variable: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # Forward port 8081 to the nginx pod: kubectl port-forward $POD_NAME 8081 :80 # Open up a new terminal on the same machine running the kubectl port-forward command and verify that the port forward works. curl --head http://127.0.0.1:8081 # You should get an http 200 OK response from the nginx pod.","title":"Port Forwarding"},{"location":"platforms/kubernetes/admin/monitoring/#logs_1","text":"# First, let's set an environment variable to the name of the nginx pod: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # Get the logs from the nginx pod: kubectl logs $POD_NAME # This command should return the nginx pod's logs. It will look something like this (but there could be more lines): 127 .0.0.1 - - [ 10 /Sep/2018:19:29:01 +0000 ] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\"","title":"Logs"},{"location":"platforms/kubernetes/admin/monitoring/#exec","text":"# First, let's set an environment variable to the name of the nginx pod: POD_NAME = $( kubectl get pods -l run = nginx -o jsonpath = \"{.items[0].metadata.name}\" ) # To test kubectl exec, execute a simple nginx -v command inside the nginx pod: kubectl exec -ti $POD_NAME -- nginx -v # This command should return the nginx version output, which should look like this: nginx version: nginx/1.15.3","title":"Exec"},{"location":"platforms/kubernetes/admin/monitoring/#services","text":"# First, create a service to expose the nginx deployment: kubectl expose deployment nginx --port 80 --type NodePort # Get the node port assigned to the newly-created service and assign it to an environment variable: kubectl get svc # The output should look something like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .32.0.1 <none> 443 /TCP 20d nginx NodePort 10 .32.0.81 <none> 80 :32642/TCP 2m # Look for the service called nginx in that output. Under PORT(S), look for the second port, listed after 80:. In the example above, it is 32642. That is the node port, so make note of that value since you will need it in a moment. # Next, log in to one of your worker servers and make a request to the service using the node port. Be sure to replace the placeholder with the actual node port: curl -I localhost:<node port> # You should get an http 200 OK response.","title":"Services"},{"location":"platforms/kubernetes/admin/monitoring/#untrusted-workloads","text":"# First, create an untrusted pod: cat << EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: untrusted annotations: io.kubernetes.cri.untrusted-workload: \"true\" spec: containers: - name: webserver image: gcr.io/hightowerlabs/helloworld:2.0.0 EOF # Make sure that the untrusted pod is running: kubectl get pods untrusted -o wide # Take note of which worker node the untrusted pod is running on, then log into that worker node. # On the worker node, list all of the containers running under gVisor: sudo runsc --root /run/containerd/runsc/k8s.io list # Get the pod ID of the untrusted pod and store it in an environment variable: POD_ID = $( sudo crictl -r unix:///var/run/containerd/containerd.sock \\ pods --name untrusted -q ) # Get the container ID of the container running in the untrusted pod and store it in an environment variable: CONTAINER_ID = $( sudo crictl -r unix:///var/run/containerd/containerd.sock \\ ps -p ${ POD_ID } -q ) # Get information about the process running in the container: sudo runsc --root /run/containerd/runsc/k8s.io ps ${ CONTAINER_ID } # Since we were able to get the process info using runsc, we know that the untrusted container is running securely as expected.","title":"Untrusted Workloads"},{"location":"platforms/kubernetes/admin/networking/","text":"Flannel \u00b6 Redhat \u00b6 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Ubuntu \u00b6 On all nodes echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Master kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml Weave Net \u00b6 Setting up # First, log in to both worker nodes and enable IP forwarding: sudo sysctl net.ipv4.conf.all.forwarding = 1 echo \"net.ipv4.conf.all.forwarding=1\" | sudo tee -a /etc/sysctl.conf # The remaining commands can be done using kubectl. To connect with kubectl, you can either log in to one of the control nodes and run kubectl there or open an SSH tunnel for port 6443 to the load balancer server and use kubectl locally. # You can open the SSH tunnel by running this in a separate terminal. Leave the session open while you are working to keep the tunnel active: ssh -L 6443 :localhost:6443 user@<your Load balancer cloud server public IP> kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) &env.IPALLOC_RANGE=10.200.0.0/16\" # Now Weave Net is installed, but we need to test our network to make sure everything is working. # First, make sure the Weave Net pods are up and running: kubectl get pods -n kube-system #cThis should return two Weave Net pods, and look something like this: NAME READY STATUS RESTARTS AGE weave-net-m69xq 2 /2 Running 0 11s weave-net-vmb2n 2 /2 Running 0 11s # Next, we want to test that pods can connect to each other and that they can connect to services. We will set up two Nginx pods and a service for those two pods. Then, we will create a busybox pod and use it to test connectivity to both Nginx pods and the service. # First, create an Nginx deployment with 2 replicas: cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: run: nginx replicas: 2 template: metadata: labels: run: nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EOF # Next, create a service for that deployment so that we can test connectivity to services as well: # kubectl expose deployment/nginx # Now let's start up another pod. We will use this pod to test our networking. We will test whether we can connect to the other pods and services from this pod. kubectl run busybox --image = radial/busyboxplus:curl --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) # Now let's get the IP addresses of our two Nginx pods: kubectl get ep nginx # There should be two IP addresses listed under ENDPOINTS, for example: NAME ENDPOINTS AGE nginx 10 .200.0.2:80,10.200.128.1:80 50m # Now let's make sure the busybox pod can connect to the Nginx pods on both of those IP addresses. kubectl exec $POD_NAME -- curl <first nginx pod IP address> kubectl exec $POD_NAME -- curl <second nginx pod IP address> # Both commands should return some HTML with the title \"Welcome to Nginx!\" This means that we can successfully connect to other pods. # Now let's verify that we can connect to services. kubectl get svc # This should display the IP address for our Nginx service. For example, in this case, the IP is 10.32.0.54: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .32.0.1 <none> 443 /TCP 1h nginx ClusterIP 10 .32.0.54 <none> 80 /TCP 53m # Let's see if we can access the service from the busybox pod! kubectl exec $POD_NAME -- curl <nginx service IP address> Network Policies \u00b6 Plugin Canal \u00b6 wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml Deny All \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : deny-all spec : podSelector : {} policyTypes : - Ingress Pod Selector \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : db-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - podSelector : matchLabels : app : web ports : - port : 5432 Namespace Policy \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ns-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - namespaceSelector : matchLabels : tenant : web ports : - port : 5432 Block IP \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ipblock-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - ipBlock : cidr : 192.168.1.0/24 Egress Policy \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : egress-netpol spec : podSelector : matchLabels : app : web egress : - to : - podSelector : matchLabels : app : db ports : - port : 5432 DNS \u00b6 Kube-Dns \u00b6 kubectl create -f https://storage.googleapis.com/kubernetes-the-hard-way/kube-dns.yaml # Verify that the kube-dns pod starts up correctly: kubectl get pods -l k8s-app = kube-dns -n kube-system # You should get output showing the kube-dns pod. It should look something like this: NAME READY STATUS RESTARTS AGE kube-dns-598d7bf7d4-spbmj 3 /3 Running 0 36s # Make sure that 3/3 containers are ready, and that the pod has a status of Running. It may take a moment for the pod to be fully up and running, so if READY is not 3/3 at first, check again after a few moments. # Now let's test our kube-dns installation by doing a DNS lookup from within a pod. First, we need to start up a pod that we can use for testing: kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) # Next, run an nslookup from inside the busybox container: kubectl exec -ti $POD_NAME -- nslookup kubernetes # You should get output that looks something like this: Server: 10 .32.0.10 Address 1 : 10 .32.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .32.0.1 kubernetes.default.svc.cluster.local Custom DNS \u00b6 apiVersion : v1 kind : Pod metadata : namespace : default name : dns-example spec : containers : - name : test image : nginx dnsPolicy : \"None\" dnsConfig : nameservers : - 8.8.8.8 searches : - ns1.svc.cluster.local - my.dns.search.suffix options : - name : ndots value : \"2\" - name : edns0","title":"Networking"},{"location":"platforms/kubernetes/admin/networking/#flannel","text":"","title":"Flannel"},{"location":"platforms/kubernetes/admin/networking/#redhat","text":"kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml","title":"Redhat"},{"location":"platforms/kubernetes/admin/networking/#ubuntu","text":"On all nodes echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Master kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml","title":"Ubuntu"},{"location":"platforms/kubernetes/admin/networking/#weave-net","text":"Setting up # First, log in to both worker nodes and enable IP forwarding: sudo sysctl net.ipv4.conf.all.forwarding = 1 echo \"net.ipv4.conf.all.forwarding=1\" | sudo tee -a /etc/sysctl.conf # The remaining commands can be done using kubectl. To connect with kubectl, you can either log in to one of the control nodes and run kubectl there or open an SSH tunnel for port 6443 to the load balancer server and use kubectl locally. # You can open the SSH tunnel by running this in a separate terminal. Leave the session open while you are working to keep the tunnel active: ssh -L 6443 :localhost:6443 user@<your Load balancer cloud server public IP> kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) &env.IPALLOC_RANGE=10.200.0.0/16\" # Now Weave Net is installed, but we need to test our network to make sure everything is working. # First, make sure the Weave Net pods are up and running: kubectl get pods -n kube-system #cThis should return two Weave Net pods, and look something like this: NAME READY STATUS RESTARTS AGE weave-net-m69xq 2 /2 Running 0 11s weave-net-vmb2n 2 /2 Running 0 11s # Next, we want to test that pods can connect to each other and that they can connect to services. We will set up two Nginx pods and a service for those two pods. Then, we will create a busybox pod and use it to test connectivity to both Nginx pods and the service. # First, create an Nginx deployment with 2 replicas: cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: run: nginx replicas: 2 template: metadata: labels: run: nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EOF # Next, create a service for that deployment so that we can test connectivity to services as well: # kubectl expose deployment/nginx # Now let's start up another pod. We will use this pod to test our networking. We will test whether we can connect to the other pods and services from this pod. kubectl run busybox --image = radial/busyboxplus:curl --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) # Now let's get the IP addresses of our two Nginx pods: kubectl get ep nginx # There should be two IP addresses listed under ENDPOINTS, for example: NAME ENDPOINTS AGE nginx 10 .200.0.2:80,10.200.128.1:80 50m # Now let's make sure the busybox pod can connect to the Nginx pods on both of those IP addresses. kubectl exec $POD_NAME -- curl <first nginx pod IP address> kubectl exec $POD_NAME -- curl <second nginx pod IP address> # Both commands should return some HTML with the title \"Welcome to Nginx!\" This means that we can successfully connect to other pods. # Now let's verify that we can connect to services. kubectl get svc # This should display the IP address for our Nginx service. For example, in this case, the IP is 10.32.0.54: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .32.0.1 <none> 443 /TCP 1h nginx ClusterIP 10 .32.0.54 <none> 80 /TCP 53m # Let's see if we can access the service from the busybox pod! kubectl exec $POD_NAME -- curl <nginx service IP address>","title":"Weave Net"},{"location":"platforms/kubernetes/admin/networking/#network-policies","text":"","title":"Network Policies"},{"location":"platforms/kubernetes/admin/networking/#plugin-canal","text":"wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml","title":"Plugin Canal"},{"location":"platforms/kubernetes/admin/networking/#deny-all","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : deny-all spec : podSelector : {} policyTypes : - Ingress","title":"Deny All"},{"location":"platforms/kubernetes/admin/networking/#pod-selector","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : db-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - podSelector : matchLabels : app : web ports : - port : 5432","title":"Pod Selector"},{"location":"platforms/kubernetes/admin/networking/#namespace-policy","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ns-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - namespaceSelector : matchLabels : tenant : web ports : - port : 5432","title":"Namespace Policy"},{"location":"platforms/kubernetes/admin/networking/#block-ip","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : ipblock-netpolicy spec : podSelector : matchLabels : app : db ingress : - from : - ipBlock : cidr : 192.168.1.0/24","title":"Block IP"},{"location":"platforms/kubernetes/admin/networking/#egress-policy","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : egress-netpol spec : podSelector : matchLabels : app : web egress : - to : - podSelector : matchLabels : app : db ports : - port : 5432","title":"Egress Policy"},{"location":"platforms/kubernetes/admin/networking/#dns","text":"","title":"DNS"},{"location":"platforms/kubernetes/admin/networking/#kube-dns","text":"kubectl create -f https://storage.googleapis.com/kubernetes-the-hard-way/kube-dns.yaml # Verify that the kube-dns pod starts up correctly: kubectl get pods -l k8s-app = kube-dns -n kube-system # You should get output showing the kube-dns pod. It should look something like this: NAME READY STATUS RESTARTS AGE kube-dns-598d7bf7d4-spbmj 3 /3 Running 0 36s # Make sure that 3/3 containers are ready, and that the pod has a status of Running. It may take a moment for the pod to be fully up and running, so if READY is not 3/3 at first, check again after a few moments. # Now let's test our kube-dns installation by doing a DNS lookup from within a pod. First, we need to start up a pod that we can use for testing: kubectl run busybox --image = busybox:1.28 --command -- sleep 3600 POD_NAME = $( kubectl get pods -l run = busybox -o jsonpath = \"{.items[0].metadata.name}\" ) # Next, run an nslookup from inside the busybox container: kubectl exec -ti $POD_NAME -- nslookup kubernetes # You should get output that looks something like this: Server: 10 .32.0.10 Address 1 : 10 .32.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1 : 10 .32.0.1 kubernetes.default.svc.cluster.local","title":"Kube-Dns"},{"location":"platforms/kubernetes/admin/networking/#custom-dns","text":"apiVersion : v1 kind : Pod metadata : namespace : default name : dns-example spec : containers : - name : test image : nginx dnsPolicy : \"None\" dnsConfig : nameservers : - 8.8.8.8 searches : - ns1.svc.cluster.local - my.dns.search.suffix options : - name : ndots value : \"2\" - name : edns0","title":"Custom DNS"},{"location":"platforms/kubernetes/admin/security/","text":"Service Accounts \u00b6 Get \u00b6 kubectl get serviceaccounts Create \u00b6 kubectl get serviceaccounts kubectl get serviceaccounts jenkins -o yaml Pod Example \u00b6 apiVersion : v1 kind : Pod metadata : name : busybox namespace : default spec : serviceAccountName : jenkins containers : - image : busybox:1.28.4 command : - sleep - \"3600\" imagePullPolicy : IfNotPresent name : busybox restartPolicy : Always View the token file from within a pod \u00b6 kubectl get pods -n my-ns kubectl exec -it <name-of-pod> -n my-ns sh cat /var/run/secrets/kubernetes.io/serviceaccount/token Users \u00b6 Create \u00b6 kubectl config view kubectl config set-credentials chad --username = chad --password = password # Create a role binding for anonymous users (not recommended in production): kubectl create clusterrolebinding cluster-system-anonymous --clusterrole = cluster-admin --user = system:anonymous # Need Copy /etc/kubernetes/pki/ca.crt to remote machine # Remote Machine (Install Kubectl) kubectl config set-cluster kubernetes --server = https://172.31.41.61:6443 --certificate-authority = ca.crt --embed-certs = true kubectl config set-credentials chad --username = chad --password = password kubectl config set-context kubernetes --cluster = kubernetes --user = chad --namespace = default kubectl config use-context kubernetes Generate client Cert \u00b6 Generate Private Key openssl genrsa -out mia.key 2048 Generate CSR openssl req -new -key mia.key -out mia.csr -subj \"/CN=mia/O=acg\" Generate Client Certificate openssl x509 -req \\ -in mia.csr \\ -CA cluster.crt \\ -CAkey cluster.key \\ -CAcreateserial \\ -out mia.crt \\ -days 365 Kubectl kubectl config set-credentials mia --client-certificate = mia.crt --client-key = mia.key kubectl config set-context mia --cluster = tiagomsantos.com --namespace = development --user = mia kubectl config use-context mia Roles \u00b6 Role \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : web name : service-reader rules : - apiGroups : [ \"\" ] verbs : [ \"get\" , \"list\" ] resources : [ \"services\" ] RoleBinding \u00b6 kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web Cluster Roles \u00b6 Cluster Role \u00b6 kubectl create clusterrole pv-reader --verb = get,list --resource = persistentvolumes Cluster Role Binding \u00b6 kubectl create clusterrolebinding pv-test --clusterrole = pv-reader --serviceaccount = web:default Test \u00b6 apiVersion : v1 kind : Pod metadata : name : curlpod namespace : web spec : containers : - image : tutum/curl command : [ \"sleep\" , \"9999999\" ] name : main - image : linuxacademycontent/kubectl-proxy name : proxy restartPolicy : Always kubectl apply -f curl-pod.yaml kubectl get pods -n web kubectl exec -it curlpod -n web -- sh curl localhost:8001/api/v1/persistentvolumes TLS Certficates \u00b6 Install cfssl \u00b6 # Download the binaries for the cfssl tool: wget -q --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 # Make the binary files executable: chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson Create Certificate Authority to Kubernetes \u00b6 cd ~/ mkdir kthw cd kthw/ { cat > ca-config.json << EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json << EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca } Generating Client Certificates \u00b6 will generate the following client certificates: admin, kubelet (one for each worker node), kube-controller-manager, kube-proxy, and kube-scheduler Admin Client Certificate { cat > admin-csr.json << EOF { \"CN\": \"admin\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ admin-csr.json | cfssljson -bare admin } Kubelet Client certificates export WORKER0_HOST = <Public hostname of your first worker node cloud server> export WORKER0_IP = <Private IP of your first worker node cloud server> export WORKER1_HOST = <Public hostname of your second worker node cloud server> export WORKER1_IP = <Private IP of your second worker node cloud server> { cat > ${ WORKER0_HOST } -csr.json << EOF { \"CN\": \"system:node:${WORKER0_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ WORKER0_IP } , ${ WORKER0_HOST } \\ -profile = kubernetes \\ ${ WORKER0_HOST } -csr.json | cfssljson -bare ${ WORKER0_HOST } cat > ${ WORKER1_HOST } -csr.json << EOF { \"CN\": \"system:node:${WORKER1_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ WORKER1_IP } , ${ WORKER1_HOST } \\ -profile = kubernetes \\ ${ WORKER1_HOST } -csr.json | cfssljson -bare ${ WORKER1_HOST } } Controller Manager Client certificate { cat > kube-controller-manager-csr.json << EOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager } Kube-proxy Client certificate { cat > kube-proxy-csr.json << EOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy } Kube Scheduler Client Certificate { cat > kube-scheduler-csr.json << EOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler } Generating the Kubernetes API Server Certificate \u00b6 Note: 10.32.0.1 - Common use this IP. Can be used by the pods in some scenarios cd ~/kthw export CERT_HOSTNAME = 10 .32.0.1,<controller node 1 Private IP>,<controller node 1 hostname>,<controller node 2 Private IP>,<controller node 2 hostname>,<API load balancer Private IP>,<API load balancer hostname>,127.0.0.1,localhost,kubernetes.default { cat > kubernetes-csr.json << EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ CERT_HOSTNAME } \\ -profile = kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes } Generating the Service Account Key Pair \u00b6 cd ~/kthw { cat > service-account-csr.json << EOF { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ service-account-csr.json | cfssljson -bare service-account } Distributing the Certificate Files \u00b6 Move certificate files to the worker nodes: \u00b6 scp ca.pem <worker 1 hostname>-key.pem <worker 1 hostname>.pem user@<worker 1 public IP>:~/ scp ca.pem <worker 2 hostname>-key.pem <worker 2 hostname>.pem user@<worker 2 public IP>:~/ Move certificate files to the Master nodes: \u00b6 scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 1 public IP>:~/ scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 2 public IP>:~/ Create TLS For Applications (Not Cluster, only same pods) \u00b6 # Find the CA certificate on a pod in your cluster: kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount cfssl version # Create a CSR file - Need instal cfssl tool cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"my-svc.my-namespace.svc.cluster.local\", \"my-pod.my-namespace.pod.cluster.local\", \"172.168.0.24\", \"10.0.34.2\" ], \"CN\": \"my-pod.my-namespace.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF # Create a CertificateSigningRequest API object: cat <<EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: pod-csr.web spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF # View the CSRs in the cluster: kubectl get csr # View additional details about the CSR: kubectl describe csr pod-csr.web # Approve the CSR: kubectl certificate approve pod-csr.web # View the certificate within your CSR: kubectl get csr pod-csr.web -o yaml # Extract and decode your certificate to use in a file: kubectl get csr pod-csr.web -o jsonpath = '{.status.certificate}' \\ | base64 --decode > server.crt Container Registry \u00b6 Create # Create a new docker-registry secret: kubectl create secret docker-registry acr --docker-server = https://podofminerva.azurecr.io --docker-username = podofminerva --docker-password = 'otj701c9OucKZOCx5qrRblofcNRf3W+e' --docker-email = user@example.com # Modify the default service account to use your new docker-registry secret: kubectl patch sa default -p '{\"imagePullSecrets\": [{\"name\": \"acr\"}]}' apiVersion : v1 kind : Pod metadata : name : acr-pod labels : app : busybox spec : containers : - name : busybox image : podofminerva.azurecr.io/busybox:latest command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] imagePullPolicy : Always Security Contexts \u00b6 The YAML for a container that runs as a user apiVersion : v1 kind : Pod metadata : name : alpine-user-context spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 405 The YAML for a pod that runs the container as non-root apiVersion : v1 kind : Pod metadata : name : alpine-nonroot spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsNonRoot : true The YAML for a privileged container pod apiVersion : v1 kind : Pod metadata : name : privileged-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : privileged : true The YAML for a container that will allow you to change the time apiVersion : v1 kind : Pod metadata : name : kernelchange-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : capabilities : add : - SYS_TIME The YAML for a container that removes capabilities apiVersion : v1 kind : Pod metadata : name : remove-capabilities spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : capabilities : drop : - CHOWN The YAML for a pod container that can\u2019t write to the local filesystem apiVersion : v1 kind : Pod metadata : name : readonly-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : readOnlyRootFilesystem : true volumeMounts : - name : my-volume mountPath : /volume readOnly : false volumes : - name : my-volume emptyDir : The YAML for a pod that has different group permissions for different pods apiVersion : v1 kind : Pod metadata : name : group-context spec : securityContext : fsGroup : 555 supplementalGroups : [ 666 , 777 ] containers : - name : first image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 1111 volumeMounts : - name : shared-volume mountPath : /volume readOnly : false - name : second image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 2222 volumeMounts : - name : shared-volume mountPath : /volume readOnly : false volumes : - name : shared-volume emptyDir : Persistent Key Value Store \u00b6 # Generate a key for your https server: openssl genrsa -out https.key 2048 # Generate a certificate for the https server: openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN = www.example.com # Create an empty file to create the secret: touch file # Create a secret from your key, cert, and file: kubectl create secret generic example-https --from-file = https.key --from-file = https.cert --from-file = file Create the configMap that will mount to your pod apiVersion : v1 kind : ConfigMap metadata : name : config data : my-nginx-config.conf : | server { listen 80; listen 443 ssl; server_name www.example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval : | 25 The YAML for a pod using the new secret apiVersion : v1 kind : Pod metadata : name : example-https spec : containers : - image : linuxacademycontent/fortune name : html-web env : - name : INTERVAL valueFrom : configMapKeyRef : name : config key : sleep-interval volumeMounts : - name : html mountPath : /var/htdocs - image : nginx:alpine name : web-server volumeMounts : - name : html mountPath : /usr/share/nginx/html readOnly : true - name : config mountPath : /etc/nginx/conf.d readOnly : true - name : certs mountPath : /etc/nginx/certs/ readOnly : true ports : - containerPort : 80 - containerPort : 443 volumes : - name : html emptyDir : {} - name : config configMap : name : config items : - key : my-nginx-config.conf path : https.conf - name : certs secret : secretName : example-https # Use port forwarding on the pod to server traffic from 443: kubectl port-forward example-https 8443 :443 & # Curl the web server to get a response: curl https://localhost:8443 -k","title":"Security"},{"location":"platforms/kubernetes/admin/security/#service-accounts","text":"","title":"Service Accounts"},{"location":"platforms/kubernetes/admin/security/#get","text":"kubectl get serviceaccounts","title":"Get"},{"location":"platforms/kubernetes/admin/security/#create","text":"kubectl get serviceaccounts kubectl get serviceaccounts jenkins -o yaml","title":"Create"},{"location":"platforms/kubernetes/admin/security/#pod-example","text":"apiVersion : v1 kind : Pod metadata : name : busybox namespace : default spec : serviceAccountName : jenkins containers : - image : busybox:1.28.4 command : - sleep - \"3600\" imagePullPolicy : IfNotPresent name : busybox restartPolicy : Always","title":"Pod Example"},{"location":"platforms/kubernetes/admin/security/#view-the-token-file-from-within-a-pod","text":"kubectl get pods -n my-ns kubectl exec -it <name-of-pod> -n my-ns sh cat /var/run/secrets/kubernetes.io/serviceaccount/token","title":"View the token file from within a pod"},{"location":"platforms/kubernetes/admin/security/#users","text":"","title":"Users"},{"location":"platforms/kubernetes/admin/security/#create_1","text":"kubectl config view kubectl config set-credentials chad --username = chad --password = password # Create a role binding for anonymous users (not recommended in production): kubectl create clusterrolebinding cluster-system-anonymous --clusterrole = cluster-admin --user = system:anonymous # Need Copy /etc/kubernetes/pki/ca.crt to remote machine # Remote Machine (Install Kubectl) kubectl config set-cluster kubernetes --server = https://172.31.41.61:6443 --certificate-authority = ca.crt --embed-certs = true kubectl config set-credentials chad --username = chad --password = password kubectl config set-context kubernetes --cluster = kubernetes --user = chad --namespace = default kubectl config use-context kubernetes","title":"Create"},{"location":"platforms/kubernetes/admin/security/#generate-client-cert","text":"Generate Private Key openssl genrsa -out mia.key 2048 Generate CSR openssl req -new -key mia.key -out mia.csr -subj \"/CN=mia/O=acg\" Generate Client Certificate openssl x509 -req \\ -in mia.csr \\ -CA cluster.crt \\ -CAkey cluster.key \\ -CAcreateserial \\ -out mia.crt \\ -days 365 Kubectl kubectl config set-credentials mia --client-certificate = mia.crt --client-key = mia.key kubectl config set-context mia --cluster = tiagomsantos.com --namespace = development --user = mia kubectl config use-context mia","title":"Generate client Cert"},{"location":"platforms/kubernetes/admin/security/#roles","text":"","title":"Roles"},{"location":"platforms/kubernetes/admin/security/#role","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : web name : service-reader rules : - apiGroups : [ \"\" ] verbs : [ \"get\" , \"list\" ] resources : [ \"services\" ]","title":"Role"},{"location":"platforms/kubernetes/admin/security/#rolebinding","text":"kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web","title":"RoleBinding"},{"location":"platforms/kubernetes/admin/security/#cluster-roles","text":"","title":"Cluster Roles"},{"location":"platforms/kubernetes/admin/security/#cluster-role","text":"kubectl create clusterrole pv-reader --verb = get,list --resource = persistentvolumes","title":"Cluster Role"},{"location":"platforms/kubernetes/admin/security/#cluster-role-binding","text":"kubectl create clusterrolebinding pv-test --clusterrole = pv-reader --serviceaccount = web:default","title":"Cluster Role Binding"},{"location":"platforms/kubernetes/admin/security/#test","text":"apiVersion : v1 kind : Pod metadata : name : curlpod namespace : web spec : containers : - image : tutum/curl command : [ \"sleep\" , \"9999999\" ] name : main - image : linuxacademycontent/kubectl-proxy name : proxy restartPolicy : Always kubectl apply -f curl-pod.yaml kubectl get pods -n web kubectl exec -it curlpod -n web -- sh curl localhost:8001/api/v1/persistentvolumes","title":"Test"},{"location":"platforms/kubernetes/admin/security/#tls-certficates","text":"","title":"TLS Certficates"},{"location":"platforms/kubernetes/admin/security/#install-cfssl","text":"# Download the binaries for the cfssl tool: wget -q --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 # Make the binary files executable: chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson","title":"Install cfssl"},{"location":"platforms/kubernetes/admin/security/#create-certificate-authority-to-kubernetes","text":"cd ~/ mkdir kthw cd kthw/ { cat > ca-config.json << EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json << EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca }","title":"Create Certificate Authority to Kubernetes"},{"location":"platforms/kubernetes/admin/security/#generating-client-certificates","text":"will generate the following client certificates: admin, kubelet (one for each worker node), kube-controller-manager, kube-proxy, and kube-scheduler Admin Client Certificate { cat > admin-csr.json << EOF { \"CN\": \"admin\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ admin-csr.json | cfssljson -bare admin } Kubelet Client certificates export WORKER0_HOST = <Public hostname of your first worker node cloud server> export WORKER0_IP = <Private IP of your first worker node cloud server> export WORKER1_HOST = <Public hostname of your second worker node cloud server> export WORKER1_IP = <Private IP of your second worker node cloud server> { cat > ${ WORKER0_HOST } -csr.json << EOF { \"CN\": \"system:node:${WORKER0_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ WORKER0_IP } , ${ WORKER0_HOST } \\ -profile = kubernetes \\ ${ WORKER0_HOST } -csr.json | cfssljson -bare ${ WORKER0_HOST } cat > ${ WORKER1_HOST } -csr.json << EOF { \"CN\": \"system:node:${WORKER1_HOST}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ WORKER1_IP } , ${ WORKER1_HOST } \\ -profile = kubernetes \\ ${ WORKER1_HOST } -csr.json | cfssljson -bare ${ WORKER1_HOST } } Controller Manager Client certificate { cat > kube-controller-manager-csr.json << EOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager } Kube-proxy Client certificate { cat > kube-proxy-csr.json << EOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy } Kube Scheduler Client Certificate { cat > kube-scheduler-csr.json << EOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler }","title":"Generating Client Certificates"},{"location":"platforms/kubernetes/admin/security/#generating-the-kubernetes-api-server-certificate","text":"Note: 10.32.0.1 - Common use this IP. Can be used by the pods in some scenarios cd ~/kthw export CERT_HOSTNAME = 10 .32.0.1,<controller node 1 Private IP>,<controller node 1 hostname>,<controller node 2 Private IP>,<controller node 2 hostname>,<API load balancer Private IP>,<API load balancer hostname>,127.0.0.1,localhost,kubernetes.default { cat > kubernetes-csr.json << EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -hostname = ${ CERT_HOSTNAME } \\ -profile = kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes }","title":"Generating the Kubernetes API Server Certificate"},{"location":"platforms/kubernetes/admin/security/#generating-the-service-account-key-pair","text":"cd ~/kthw { cat > service-account-csr.json << EOF { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Oregon\" } ] } EOF cfssl gencert \\ -ca = ca.pem \\ -ca-key = ca-key.pem \\ -config = ca-config.json \\ -profile = kubernetes \\ service-account-csr.json | cfssljson -bare service-account }","title":"Generating the Service Account Key Pair"},{"location":"platforms/kubernetes/admin/security/#distributing-the-certificate-files","text":"","title":"Distributing the Certificate Files"},{"location":"platforms/kubernetes/admin/security/#move-certificate-files-to-the-worker-nodes","text":"scp ca.pem <worker 1 hostname>-key.pem <worker 1 hostname>.pem user@<worker 1 public IP>:~/ scp ca.pem <worker 2 hostname>-key.pem <worker 2 hostname>.pem user@<worker 2 public IP>:~/","title":"Move certificate files to the worker nodes:"},{"location":"platforms/kubernetes/admin/security/#move-certificate-files-to-the-master-nodes","text":"scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 1 public IP>:~/ scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem user@<master 2 public IP>:~/","title":"Move certificate files to the Master nodes:"},{"location":"platforms/kubernetes/admin/security/#create-tls-for-applications-not-cluster-only-same-pods","text":"# Find the CA certificate on a pod in your cluster: kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount cfssl version # Create a CSR file - Need instal cfssl tool cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"my-svc.my-namespace.svc.cluster.local\", \"my-pod.my-namespace.pod.cluster.local\", \"172.168.0.24\", \"10.0.34.2\" ], \"CN\": \"my-pod.my-namespace.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF # Create a CertificateSigningRequest API object: cat <<EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: pod-csr.web spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF # View the CSRs in the cluster: kubectl get csr # View additional details about the CSR: kubectl describe csr pod-csr.web # Approve the CSR: kubectl certificate approve pod-csr.web # View the certificate within your CSR: kubectl get csr pod-csr.web -o yaml # Extract and decode your certificate to use in a file: kubectl get csr pod-csr.web -o jsonpath = '{.status.certificate}' \\ | base64 --decode > server.crt","title":"Create TLS For Applications (Not Cluster, only same pods)"},{"location":"platforms/kubernetes/admin/security/#container-registry","text":"Create # Create a new docker-registry secret: kubectl create secret docker-registry acr --docker-server = https://podofminerva.azurecr.io --docker-username = podofminerva --docker-password = 'otj701c9OucKZOCx5qrRblofcNRf3W+e' --docker-email = user@example.com # Modify the default service account to use your new docker-registry secret: kubectl patch sa default -p '{\"imagePullSecrets\": [{\"name\": \"acr\"}]}' apiVersion : v1 kind : Pod metadata : name : acr-pod labels : app : busybox spec : containers : - name : busybox image : podofminerva.azurecr.io/busybox:latest command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] imagePullPolicy : Always","title":"Container Registry"},{"location":"platforms/kubernetes/admin/security/#security-contexts","text":"The YAML for a container that runs as a user apiVersion : v1 kind : Pod metadata : name : alpine-user-context spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 405 The YAML for a pod that runs the container as non-root apiVersion : v1 kind : Pod metadata : name : alpine-nonroot spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsNonRoot : true The YAML for a privileged container pod apiVersion : v1 kind : Pod metadata : name : privileged-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : privileged : true The YAML for a container that will allow you to change the time apiVersion : v1 kind : Pod metadata : name : kernelchange-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : capabilities : add : - SYS_TIME The YAML for a container that removes capabilities apiVersion : v1 kind : Pod metadata : name : remove-capabilities spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : capabilities : drop : - CHOWN The YAML for a pod container that can\u2019t write to the local filesystem apiVersion : v1 kind : Pod metadata : name : readonly-pod spec : containers : - name : main image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : readOnlyRootFilesystem : true volumeMounts : - name : my-volume mountPath : /volume readOnly : false volumes : - name : my-volume emptyDir : The YAML for a pod that has different group permissions for different pods apiVersion : v1 kind : Pod metadata : name : group-context spec : securityContext : fsGroup : 555 supplementalGroups : [ 666 , 777 ] containers : - name : first image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 1111 volumeMounts : - name : shared-volume mountPath : /volume readOnly : false - name : second image : alpine command : [ \"/bin/sleep\" , \"999999\" ] securityContext : runAsUser : 2222 volumeMounts : - name : shared-volume mountPath : /volume readOnly : false volumes : - name : shared-volume emptyDir :","title":"Security Contexts"},{"location":"platforms/kubernetes/admin/security/#persistent-key-value-store","text":"# Generate a key for your https server: openssl genrsa -out https.key 2048 # Generate a certificate for the https server: openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN = www.example.com # Create an empty file to create the secret: touch file # Create a secret from your key, cert, and file: kubectl create secret generic example-https --from-file = https.key --from-file = https.cert --from-file = file Create the configMap that will mount to your pod apiVersion : v1 kind : ConfigMap metadata : name : config data : my-nginx-config.conf : | server { listen 80; listen 443 ssl; server_name www.example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval : | 25 The YAML for a pod using the new secret apiVersion : v1 kind : Pod metadata : name : example-https spec : containers : - image : linuxacademycontent/fortune name : html-web env : - name : INTERVAL valueFrom : configMapKeyRef : name : config key : sleep-interval volumeMounts : - name : html mountPath : /var/htdocs - image : nginx:alpine name : web-server volumeMounts : - name : html mountPath : /usr/share/nginx/html readOnly : true - name : config mountPath : /etc/nginx/conf.d readOnly : true - name : certs mountPath : /etc/nginx/certs/ readOnly : true ports : - containerPort : 80 - containerPort : 443 volumes : - name : html emptyDir : {} - name : config configMap : name : config items : - key : my-nginx-config.conf path : https.conf - name : certs secret : secretName : example-https # Use port forwarding on the pod to server traffic from 443: kubectl port-forward example-https 8443 :443 & # Curl the web server to get a response: curl https://localhost:8443 -k","title":"Persistent Key Value Store"},{"location":"platforms/kubernetes/developer/daemonsets/","text":"apiVersion : apps/v1beta2 kind : DaemonSet metadata : name : ssd-monitor spec : selector : matchLabels : app : ssd-monitor template : metadata : labels : app : ssd-monitor spec : nodeSelector : disk : ssd containers : - name : main image : linuxacademycontent/ssd-monitor","title":"DaemonSets"},{"location":"platforms/kubernetes/developer/deployments/","text":"Example \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : example-deployment labels : app : nginx spec : replicas : 2 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : darealmc/nginx-k8s:v1 ports : - containerPort : 80 Node affinity \u00b6 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : pref spec : replicas : 5 template : metadata : labels : app : pref spec : affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 80 preference : matchExpressions : - key : availability-zone operator : In values : - zone1 - weight : 20 preference : matchExpressions : - key : share-type operator : In values : - dedicated containers : - args : - sleep - \"99999\" image : busybox name : main Update Image \u00b6 kubectl set image deployment.v1.apps/example-deployment nginx = darealmc/nginx-k8s:v2 MicroServices Example \u00b6 cd ~/ git clone https://github.com/linuxacademy/robot-shop.git kubectl create namespace robot-shop kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors/ kubectl get pods -n robot-shop -w # Access in http://$kube_server_public_ip:30080 Application LifeCycle Manager \u00b6 Update \u00b6 kubectl apply -f kubeserve-deployment.yaml kubectl replace -f kubeserve-deployment.yaml Rolling Update \u00b6 kubectl set image deployments/kubeserve app = linuxacademycontent/kubeserve:v2 --v 6 Rollback \u00b6 # Use --record flag to create the deployment - kubectl create -f kubeserve-deployment.yaml --record kubectl rollout undo deployments kubeserve kubectl rollout history deployment kubeserve kubectl rollout undo deployment kubeserve --to-revision = 2 Pause / Resume \u00b6 kubectl rollout undo deployment kubeserve --to-revision = 2 kubectl rollout resume deployment kubeserve Readiness Probe \u00b6 apiVersion: apps/v1 kind: Deployment metadata: name: kubeserve spec: replicas: 3 selector: matchLabels: app: kubeserve minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubeserve labels: app: kubeserve spec: containers: - image: linuxacademycontent/kubeserve:v3 name: app readinessProbe: periodSeconds: 1 httpGet: path: / port: 80","title":"Deployments"},{"location":"platforms/kubernetes/developer/deployments/#example","text":"apiVersion : apps/v1 kind : Deployment metadata : name : example-deployment labels : app : nginx spec : replicas : 2 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : darealmc/nginx-k8s:v1 ports : - containerPort : 80","title":"Example"},{"location":"platforms/kubernetes/developer/deployments/#node-affinity","text":"apiVersion : extensions/v1beta1 kind : Deployment metadata : name : pref spec : replicas : 5 template : metadata : labels : app : pref spec : affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 80 preference : matchExpressions : - key : availability-zone operator : In values : - zone1 - weight : 20 preference : matchExpressions : - key : share-type operator : In values : - dedicated containers : - args : - sleep - \"99999\" image : busybox name : main","title":"Node affinity"},{"location":"platforms/kubernetes/developer/deployments/#update-image","text":"kubectl set image deployment.v1.apps/example-deployment nginx = darealmc/nginx-k8s:v2","title":"Update Image"},{"location":"platforms/kubernetes/developer/deployments/#microservices-example","text":"cd ~/ git clone https://github.com/linuxacademy/robot-shop.git kubectl create namespace robot-shop kubectl -n robot-shop create -f ~/robot-shop/K8s/descriptors/ kubectl get pods -n robot-shop -w # Access in http://$kube_server_public_ip:30080","title":"MicroServices Example"},{"location":"platforms/kubernetes/developer/deployments/#application-lifecycle-manager","text":"","title":"Application LifeCycle Manager"},{"location":"platforms/kubernetes/developer/deployments/#update","text":"kubectl apply -f kubeserve-deployment.yaml kubectl replace -f kubeserve-deployment.yaml","title":"Update"},{"location":"platforms/kubernetes/developer/deployments/#rolling-update","text":"kubectl set image deployments/kubeserve app = linuxacademycontent/kubeserve:v2 --v 6","title":"Rolling Update"},{"location":"platforms/kubernetes/developer/deployments/#rollback","text":"# Use --record flag to create the deployment - kubectl create -f kubeserve-deployment.yaml --record kubectl rollout undo deployments kubeserve kubectl rollout history deployment kubeserve kubectl rollout undo deployment kubeserve --to-revision = 2","title":"Rollback"},{"location":"platforms/kubernetes/developer/deployments/#pause-resume","text":"kubectl rollout undo deployment kubeserve --to-revision = 2 kubectl rollout resume deployment kubeserve","title":"Pause / Resume"},{"location":"platforms/kubernetes/developer/deployments/#readiness-probe","text":"apiVersion: apps/v1 kind: Deployment metadata: name: kubeserve spec: replicas: 3 selector: matchLabels: app: kubeserve minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubeserve labels: app: kubeserve spec: containers: - image: linuxacademycontent/kubeserve:v3 name: app readinessProbe: periodSeconds: 1 httpGet: path: / port: 80","title":"Readiness Probe"},{"location":"platforms/kubernetes/developer/ingress/","text":"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: service-ingress spec: rules: - host: kubeserve.example.com http: paths: - backend: serviceName: kubeserve2 servicePort: 80 - host: app.example.com http: paths: - backend: serviceName: nginx servicePort: 80 - http: paths: - backend: serviceName: httpd servicePort: 80","title":"Ingress"},{"location":"platforms/kubernetes/developer/jobs/","text":"Job \u00b6 apiVersion : batch/v1 kind : Job metadata : name : pi spec : template : spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never backoffLimit : 4 CronJob \u00b6 apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure","title":"Jobs"},{"location":"platforms/kubernetes/developer/jobs/#job","text":"apiVersion : batch/v1 kind : Job metadata : name : pi spec : template : spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never backoffLimit : 4","title":"Job"},{"location":"platforms/kubernetes/developer/jobs/#cronjob","text":"apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure","title":"CronJob"},{"location":"platforms/kubernetes/developer/pods/","text":"Pods \u00b6 Get \u00b6 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl get pods --namespace = podexample -o wide kubectl get pods -o custom-columns = POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system Example \u00b6 apiVersion : v1 kind : Pod metadata : name : examplepod namespace : pod-example spec : schedulerName : default-scheduler # To change scheduler - my-scheduler volumes : - name : html emptyDir : {} containers : - name : webcontainer image : nginx volumeMounts : - name : html mountPath : /usr/share/nginx/html - name : filecontainer image : debian volumeMounts : - name : html mountPath : /html command : [ \"/bin/sh\" , \"-c\" ] args : - while true; do date >> /html/index.html; sleep 1; done Resources Requests and Limits \u00b6 apiVersion : v1 kind : Pod metadata : name : resource-pod2 spec : nodeSelector : kubernetes.io/hostname : \"chadcrowell3c.mylabserver.com\" containers : - image : busybox command : [ \"dd\" , \"if=/dev/zero\" , \"of=/dev/null\" ] name : pod2 resources : requests : cpu : 1000m memory : 20Mi apiVersion : v1 kind : Pod metadata : name : limited-pod spec : containers : - image : busybox command : [ \"dd\" , \"if=/dev/zero\" , \"of=/dev/null\" ] name : main resources : limits : cpu : 1 memory : 20Mi Get Containers Name inside a pod \u00b6 kubectl get pods examplepod -n podexample -o jsonpath = '{.spec.containers[*].name}*' Use port forwarding to access a pod directly \u00b6 kubectl port-forward $pod_name 8081 :80","title":"Pods"},{"location":"platforms/kubernetes/developer/pods/#pods","text":"","title":"Pods"},{"location":"platforms/kubernetes/developer/pods/#get","text":"kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl get pods --namespace = podexample -o wide kubectl get pods -o custom-columns = POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system","title":"Get"},{"location":"platforms/kubernetes/developer/pods/#example","text":"apiVersion : v1 kind : Pod metadata : name : examplepod namespace : pod-example spec : schedulerName : default-scheduler # To change scheduler - my-scheduler volumes : - name : html emptyDir : {} containers : - name : webcontainer image : nginx volumeMounts : - name : html mountPath : /usr/share/nginx/html - name : filecontainer image : debian volumeMounts : - name : html mountPath : /html command : [ \"/bin/sh\" , \"-c\" ] args : - while true; do date >> /html/index.html; sleep 1; done","title":"Example"},{"location":"platforms/kubernetes/developer/pods/#resources-requests-and-limits","text":"apiVersion : v1 kind : Pod metadata : name : resource-pod2 spec : nodeSelector : kubernetes.io/hostname : \"chadcrowell3c.mylabserver.com\" containers : - image : busybox command : [ \"dd\" , \"if=/dev/zero\" , \"of=/dev/null\" ] name : pod2 resources : requests : cpu : 1000m memory : 20Mi apiVersion : v1 kind : Pod metadata : name : limited-pod spec : containers : - image : busybox command : [ \"dd\" , \"if=/dev/zero\" , \"of=/dev/null\" ] name : main resources : limits : cpu : 1 memory : 20Mi","title":"Resources Requests and Limits"},{"location":"platforms/kubernetes/developer/pods/#get-containers-name-inside-a-pod","text":"kubectl get pods examplepod -n podexample -o jsonpath = '{.spec.containers[*].name}*'","title":"Get Containers Name inside a pod"},{"location":"platforms/kubernetes/developer/pods/#use-port-forwarding-to-access-a-pod-directly","text":"kubectl port-forward $pod_name 8081 :80","title":"Use port forwarding to access a pod directly"},{"location":"platforms/kubernetes/developer/replicasets/","text":"apiVersion : apps/v1 kind : ReplicaSet metadata : name : frontend labels : app : nginx tier : frontend spec : replicas : 2 selector : matchLabels : tier : frontend matchExpressions : - { key : tier , operator : In , values : [ frontend ]} template : metadata : labels : app : nginx tier : frontend spec : containers : - name : nginx image : darealmc/nginx-k8s:v1 ports : - containerPort : 80","title":"Replicasets"},{"location":"platforms/kubernetes/developer/secrets_configmaps/","text":"Config Maps \u00b6 Create \u00b6 kubectl create configmap appconfig --from-literal = key1 = value1 --from-literal = key2 = value2 Deploy Pods \u00b6 Env Vars apiVersion : v1 kind : Pod metadata : name : configmap-pod spec : containers : - name : app-container image : busybox:1.28 command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] env : - name : MY_VAR valueFrom : configMapKeyRef : name : appconfig key : key1 Volume apiVersion : v1 kind : Pod metadata : name : configmap-volume-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] volumeMounts : - name : configmapvolume mountPath : /etc/config volumes : - name : configmapvolume configMap : name : appconfig Get \u00b6 kubectl get configmaps --all-namespaces kubectl get configmaps -n kube-system/kube-flannel-cfg kubectl get configmaps/kube-flannel-cfg -n kube-system kubectl describe configmaps/kube-flannel-cfg -n kube-system kubectl get configmap appconfig -o yaml Secrets \u00b6 Create \u00b6 apiVersion : v1 kind : Secret metadata : name : appsecret stringData : cert : value key : value Deploy Pods \u00b6 Env Vars apiVersion : v1 kind : Pod metadata : name : secret-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] env : - name : MY_CERT valueFrom : secretKeyRef : name : appsecret key : cert Volume apiVersion : v1 kind : Pod metadata : name : secret-volume-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] volumeMounts : - name : secretvolume mountPath : /etc/certs volumes : - name : secretvolume secret : secretName : appsecret","title":"Secrets&ConfigMaps"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#config-maps","text":"","title":"Config Maps"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#create","text":"kubectl create configmap appconfig --from-literal = key1 = value1 --from-literal = key2 = value2","title":"Create"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#deploy-pods","text":"Env Vars apiVersion : v1 kind : Pod metadata : name : configmap-pod spec : containers : - name : app-container image : busybox:1.28 command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] env : - name : MY_VAR valueFrom : configMapKeyRef : name : appconfig key : key1 Volume apiVersion : v1 kind : Pod metadata : name : configmap-volume-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] volumeMounts : - name : configmapvolume mountPath : /etc/config volumes : - name : configmapvolume configMap : name : appconfig","title":"Deploy Pods"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#get","text":"kubectl get configmaps --all-namespaces kubectl get configmaps -n kube-system/kube-flannel-cfg kubectl get configmaps/kube-flannel-cfg -n kube-system kubectl describe configmaps/kube-flannel-cfg -n kube-system kubectl get configmap appconfig -o yaml","title":"Get"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#secrets","text":"","title":"Secrets"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#create_1","text":"apiVersion : v1 kind : Secret metadata : name : appsecret stringData : cert : value key : value","title":"Create"},{"location":"platforms/kubernetes/developer/secrets_configmaps/#deploy-pods_1","text":"Env Vars apiVersion : v1 kind : Pod metadata : name : secret-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] env : - name : MY_CERT valueFrom : secretKeyRef : name : appsecret key : cert Volume apiVersion : v1 kind : Pod metadata : name : secret-volume-pod spec : containers : - name : app-container image : busybox command : [ 'sh' , '-c' , \"echo $(MY_VAR) && sleep 3600\" ] volumeMounts : - name : secretvolume mountPath : /etc/certs volumes : - name : secretvolume secret : secretName : appsecret","title":"Deploy Pods"},{"location":"platforms/kubernetes/developer/services/","text":"Cluster IP \u00b6 kind : Service apiVersion : v1 metadata : name : my-awesome-service spec : type : ClusterIP selector : app : nginx ports : - protocol : TCP port : 32768 targetPort : 80 NodePort \u00b6 kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 32768 # Service Port targetPort: 80 # Pod Port nodePort: 30080 # Node Port LoadBalancer \u00b6 apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx # Study Things # Set the annotation to route load balancer traffic local to the node: kubectl annotate service kubeserve2 externalTrafficPolicy = Local # https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip Headless \u00b6 Test Excert from Kubenertes in Action by Marco Luksa Each connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is? For a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it. apiVersion : v1 kind : Service metadata : name : kube-headless spec : clusterIP : None ports : - port : 80 targetPort : 8080 selector : app : kubserve2 Troubleshooting \u00b6 # Look at the iptables rules for your services: sudo iptables-save | grep KUBE | grep nginx","title":"Services"},{"location":"platforms/kubernetes/developer/services/#cluster-ip","text":"kind : Service apiVersion : v1 metadata : name : my-awesome-service spec : type : ClusterIP selector : app : nginx ports : - protocol : TCP port : 32768 targetPort : 80","title":"Cluster IP"},{"location":"platforms/kubernetes/developer/services/#nodeport","text":"kind: Service apiVersion: v1 metadata: name: my-awesome-service spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 32768 # Service Port targetPort: 80 # Pod Port nodePort: 30080 # Node Port","title":"NodePort"},{"location":"platforms/kubernetes/developer/services/#loadbalancer","text":"apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx # Study Things # Set the annotation to route load balancer traffic local to the node: kubectl annotate service kubeserve2 externalTrafficPolicy = Local # https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip","title":"LoadBalancer"},{"location":"platforms/kubernetes/developer/services/#headless","text":"Test Excert from Kubenertes in Action by Marco Luksa Each connection to the service is forwarded to one randomly selected backing pod. But what if the client needs to connect to all of those pods? What if the backing pods themselves need to each connect to all the other backing pods. Connecting through the service clearly isn\u2019t the way to do this. What is? For a client to connect to all pods, it needs to figure out the the IP of each individual pod. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn\u2019t ideal Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP \u2014 the service\u2019s cluster IP. But if you tell Kubernetes you don\u2019t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the service at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won\u2019t assign it a cluster IP through which clients could connect to the pods backing it. apiVersion : v1 kind : Service metadata : name : kube-headless spec : clusterIP : None ports : - port : 80 targetPort : 8080 selector : app : kubserve2","title":"Headless"},{"location":"platforms/kubernetes/developer/services/#troubleshooting","text":"# Look at the iptables rules for your services: sudo iptables-save | grep KUBE | grep nginx","title":"Troubleshooting"},{"location":"platforms/kubernetes/developer/statefulsets/","text":"apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : serviceName : \"nginx\" replicas : 2 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi","title":"StatefulSets"},{"location":"platforms/kubernetes/developer/volumes/","text":"PV \u00b6 HostPath \u00b6 apiVersion : v1 kind : PersistentVolume metadata : name : pv-hostpath spec : storageClassName : local-storage capacity : storage : 1Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" PV Claim \u00b6 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mongodb-pvc spec : resources : requests : storage : 1Gi accessModes : - ReadWriteOnce storageClassName : \"local-storage\" Deploy Pod \u00b6 apiVersion : v1 kind : Pod metadata : name : mongodb spec : containers : - image : mongo name : mongodb volumeMounts : - name : mongodb-data mountPath : /data/db ports : - containerPort : 27017 protocol : TCP volumes : - name : mongodb-data persistentVolumeClaim : claimName : mongodb-pvc Storage Class \u00b6 apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : fast provisioner : kubernetes.io/gce-pd parameters : type : pd-ssd Empty Dir \u00b6 apiVersion : v1 kind : Pod metadata : name : emptydir-pod spec : containers : - image : busybox name : busybox command : [ \"/bin/sh\" , \"-c\" , \"while true; do sleep 3600; done\" ] volumeMounts : - mountPath : /tmp/storage name : vol volumes : - name : vol emptyDir : {}","title":"Volumes"},{"location":"platforms/kubernetes/developer/volumes/#pv","text":"","title":"PV"},{"location":"platforms/kubernetes/developer/volumes/#hostpath","text":"apiVersion : v1 kind : PersistentVolume metadata : name : pv-hostpath spec : storageClassName : local-storage capacity : storage : 1Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\"","title":"HostPath"},{"location":"platforms/kubernetes/developer/volumes/#pv-claim","text":"apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mongodb-pvc spec : resources : requests : storage : 1Gi accessModes : - ReadWriteOnce storageClassName : \"local-storage\"","title":"PV Claim"},{"location":"platforms/kubernetes/developer/volumes/#deploy-pod","text":"apiVersion : v1 kind : Pod metadata : name : mongodb spec : containers : - image : mongo name : mongodb volumeMounts : - name : mongodb-data mountPath : /data/db ports : - containerPort : 27017 protocol : TCP volumes : - name : mongodb-data persistentVolumeClaim : claimName : mongodb-pvc","title":"Deploy Pod"},{"location":"platforms/kubernetes/developer/volumes/#storage-class","text":"apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : fast provisioner : kubernetes.io/gce-pd parameters : type : pd-ssd","title":"Storage Class"},{"location":"platforms/kubernetes/developer/volumes/#empty-dir","text":"apiVersion : v1 kind : Pod metadata : name : emptydir-pod spec : containers : - image : busybox name : busybox command : [ \"/bin/sh\" , \"-c\" , \"while true; do sleep 3600; done\" ] volumeMounts : - mountPath : /tmp/storage name : vol volumes : - name : vol emptyDir : {}","title":"Empty Dir"},{"location":"platforms/unix/archlinux/","text":"Packages \u00b6 Packages Update Package sudo pacman -U <link - see above> Update corrupted or invalid database pacman-key --delete 91FFE0700E80619CEB73235CA88E23E377514E00 pacman-key --populate archlinux Resolve ICU Package Problem Link Download and Install old \"icu\" package ; wget https://archive.archlinux.org/packages/i/icu/icu-62.1-1-x86_64.pkg.tar.xz sudo pacman -U icu-62.1-1-x86_64.pkg.tar.xz Copy all \"icu\" files to a backup directory ; sudo mkdir /usr/lib/backup sudo cp -r /usr/lib/libicu* /usr/lib/backup/ Install new version of \"icu\" again ; sudo pacman -U /var/cache/pacman/pkg/icu-62.1-1-x86_64.pkg.tar.xz Copy this three files back to /var/lib/ direcotry ; sudo cp /var/lib/libicui18n.so.61 /usr/lib/ sudo cp /var/lib/libicuuc.so.61 /usr/lib/ sudo cp /var/lib/libicudata.so.61 /usr/lib/ You can now cleanup backup files ; sudo rm -rf /var/lib/backup","title":"Archlinux"},{"location":"platforms/unix/archlinux/#packages","text":"Packages Update Package sudo pacman -U <link - see above> Update corrupted or invalid database pacman-key --delete 91FFE0700E80619CEB73235CA88E23E377514E00 pacman-key --populate archlinux Resolve ICU Package Problem Link Download and Install old \"icu\" package ; wget https://archive.archlinux.org/packages/i/icu/icu-62.1-1-x86_64.pkg.tar.xz sudo pacman -U icu-62.1-1-x86_64.pkg.tar.xz Copy all \"icu\" files to a backup directory ; sudo mkdir /usr/lib/backup sudo cp -r /usr/lib/libicu* /usr/lib/backup/ Install new version of \"icu\" again ; sudo pacman -U /var/cache/pacman/pkg/icu-62.1-1-x86_64.pkg.tar.xz Copy this three files back to /var/lib/ direcotry ; sudo cp /var/lib/libicui18n.so.61 /usr/lib/ sudo cp /var/lib/libicuuc.so.61 /usr/lib/ sudo cp /var/lib/libicudata.so.61 /usr/lib/ You can now cleanup backup files ; sudo rm -rf /var/lib/backup","title":"Packages"},{"location":"platforms/unix/commands/","text":"Commands \u00b6 Folders \u00b6 Create Multiple Folders mkdir -p { networking,compute,storage } Create Multiple Files in Multiple Folders touch { networking,compute,storage } / { main.tf,variables.tf,outputs.tf } Services \u00b6 Journalctl \u00b6 journalctl -u kubelet.service journalctl # All journalctl -b # Last Boot journalctl -b 1 # Before Last Boot journalctl --since \"2 days ago\" vi /etc/system.d/journald.conf # Config Files Inodes \u00b6 Get File Inode ls -i /etc/passwd Find File by Inode find / -inum 55116 /etc/passwd Get Inodes Available df -i Disk Space \u00b6 du -h du -s # To sum the total du -s / du -h --max-depth = 1 # Show the disk usage by folder, and not the sub dirs because of Depth = 1 Tar \u00b6 # List files without extract tar -tvf etcd-v3.3.13-linux-amd64.tar.gz # Extract only one file tar -xvf etcd-v3.3.13-linux-amd64.tar.gz etcd-v3.3.13-linux-amd64/etcdctl Filesystem \u00b6 fsck (filesystem check) \u00b6 To check errors from filesystem Don't do in mounted filesystem sudo fsck /dev/sdb1 sudo umount /dev/sdb1 dumpe2fs \u00b6 Get info about filesystem sudo dumpe2fs -h /dev/sdb1 tune2fs \u00b6 To get info and tune filesystem tune2fs -l /dev/xvda1 Set Volume Name tune2fs -L Photos /dev/xvda1 Set mount counts will be filesystem checked tune2fs -c 10 /dev/xvda1 fuser \u00b6 Show which process is using the directory # fuser /mount/Photos /mount/Photos: 3157c # ps aux | grep 3157 -- Will see the process, probably /bin/bash # ps auxf -> Show which command executed and the login session called Kernel Modules \u00b6 # To view who use the modules lsmod # To remove sudo rmmod video # To enable sudo modprobe video Libs \u00b6 # Get Shared libs location cat /etc/ld.so.conf # To load new lib ldconfig # Change Library Path temporary export LD_LIBRARY_PATH = /home/nick/lib # Print shared libs from app ldd /bin/ls Hardware \u00b6 List pci Devices lspci lspci -v lspci -vvv Syslog \u00b6 Config Files cd /etc/rsyslog.d/ Rotate Files Conf vi /etc/logrotate.conf","title":"Commands"},{"location":"platforms/unix/commands/#commands","text":"","title":"Commands"},{"location":"platforms/unix/commands/#folders","text":"Create Multiple Folders mkdir -p { networking,compute,storage } Create Multiple Files in Multiple Folders touch { networking,compute,storage } / { main.tf,variables.tf,outputs.tf }","title":"Folders"},{"location":"platforms/unix/commands/#services","text":"","title":"Services"},{"location":"platforms/unix/commands/#journalctl","text":"journalctl -u kubelet.service journalctl # All journalctl -b # Last Boot journalctl -b 1 # Before Last Boot journalctl --since \"2 days ago\" vi /etc/system.d/journald.conf # Config Files","title":"Journalctl"},{"location":"platforms/unix/commands/#inodes","text":"Get File Inode ls -i /etc/passwd Find File by Inode find / -inum 55116 /etc/passwd Get Inodes Available df -i","title":"Inodes"},{"location":"platforms/unix/commands/#disk-space","text":"du -h du -s # To sum the total du -s / du -h --max-depth = 1 # Show the disk usage by folder, and not the sub dirs because of Depth = 1","title":"Disk Space"},{"location":"platforms/unix/commands/#tar","text":"# List files without extract tar -tvf etcd-v3.3.13-linux-amd64.tar.gz # Extract only one file tar -xvf etcd-v3.3.13-linux-amd64.tar.gz etcd-v3.3.13-linux-amd64/etcdctl","title":"Tar"},{"location":"platforms/unix/commands/#filesystem","text":"","title":"Filesystem"},{"location":"platforms/unix/commands/#fsck-filesystem-check","text":"To check errors from filesystem Don't do in mounted filesystem sudo fsck /dev/sdb1 sudo umount /dev/sdb1","title":"fsck (filesystem check)"},{"location":"platforms/unix/commands/#dumpe2fs","text":"Get info about filesystem sudo dumpe2fs -h /dev/sdb1","title":"dumpe2fs"},{"location":"platforms/unix/commands/#tune2fs","text":"To get info and tune filesystem tune2fs -l /dev/xvda1 Set Volume Name tune2fs -L Photos /dev/xvda1 Set mount counts will be filesystem checked tune2fs -c 10 /dev/xvda1","title":"tune2fs"},{"location":"platforms/unix/commands/#fuser","text":"Show which process is using the directory # fuser /mount/Photos /mount/Photos: 3157c # ps aux | grep 3157 -- Will see the process, probably /bin/bash # ps auxf -> Show which command executed and the login session called","title":"fuser"},{"location":"platforms/unix/commands/#kernel-modules","text":"# To view who use the modules lsmod # To remove sudo rmmod video # To enable sudo modprobe video","title":"Kernel Modules"},{"location":"platforms/unix/commands/#libs","text":"# Get Shared libs location cat /etc/ld.so.conf # To load new lib ldconfig # Change Library Path temporary export LD_LIBRARY_PATH = /home/nick/lib # Print shared libs from app ldd /bin/ls","title":"Libs"},{"location":"platforms/unix/commands/#hardware","text":"List pci Devices lspci lspci -v lspci -vvv","title":"Hardware"},{"location":"platforms/unix/commands/#syslog","text":"Config Files cd /etc/rsyslog.d/ Rotate Files Conf vi /etc/logrotate.conf","title":"Syslog"},{"location":"platforms/unix/debian/","text":"Debian \u00b6 Know Errors \u00b6 Unable to lock the administration directory \u00b6 Desc: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? Solution sudo rm /var/lib/apt/lists/lock Dpkg \u00b6 # To list all installed packages dpkg -l | less # To install package sudo dpkg -i dlocate_1.02+nmu3_all.deb # To Remove sudo dpkg --purge dlocate Apt \u00b6 # Sources file cat /etc/apt/sources.list sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade # Better # To Get dependences apt-cache depends apache2 | less","title":"Debian"},{"location":"platforms/unix/debian/#debian","text":"","title":"Debian"},{"location":"platforms/unix/debian/#know-errors","text":"","title":"Know Errors"},{"location":"platforms/unix/debian/#unable-to-lock-the-administration-directory","text":"Desc: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? Solution sudo rm /var/lib/apt/lists/lock","title":"Unable to lock the administration directory"},{"location":"platforms/unix/debian/#dpkg","text":"# To list all installed packages dpkg -l | less # To install package sudo dpkg -i dlocate_1.02+nmu3_all.deb # To Remove sudo dpkg --purge dlocate","title":"Dpkg"},{"location":"platforms/unix/debian/#apt","text":"# Sources file cat /etc/apt/sources.list sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade # Better # To Get dependences apt-cache depends apache2 | less","title":"Apt"},{"location":"platforms/unix/disks/","text":"Disks \u00b6 Mount Disk \u00b6 List Devices fdisk -l Create Partition # fdisk /dev/sdb Command ( m for help ) : n = > Create Partition Partition type: p primary ( 0 primary, 0 extended, 4 free ) e extended Command ( m for help ) : p = > Get partition Command ( m for help ) : w = > Write Format and Create Label # mkfs.xfs /dev/sdb1 -L elastic Mount # vi /etc/fstab LABEL = elastic /elastic xfs defaults 0 0 ## Add this line # mount -a Create ext4 Filesystem \u00b6 Can use parted command # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 88 .5M 1 loop /snap/core/7270 loop1 7 :1 0 18M 1 loop /snap/amazon-ssm-agent/1455 xvda 202 :0 0 8G 0 disk \u2514\u2500xvda1 202 :1 0 8G 0 part / xvdf 202 :80 0 8G 0 disk # fdisk /dev/xvdf Command ( m for help ) : n = > Create Partition Partition type: p primary ( 0 primary, 0 extended, 4 free ) e extended Command ( m for help ) : p = > Print partition Command ( m for help ) : w = > Write # mkfs -L projectA -t ext4 /dev/xvdf1 # Format Partition and create label # mkdir /mnt/ProjectA # mount /dev/xvdf1 /mnt/ProjectA/ # vi /etc/fstab LABEL = projectA /mnt/ProjectA/ ext4 defaults 0 0 # Add This Line umount /mnt/ProjectA/ Create Swap Partition \u00b6 Can use parted command # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 1 Hex code ( type L to list all codes ) : 82 Command ( m for help ) : w # mkswap /dev/xvdf1 # swapon /dev/xvdf1 # to shutoff -> swapoff /dev/xvdf2 # free -m # vi /etc/fstab UUID = 99fd52d5-e821-45a5-9366-30666046406f swap swap default 0 0 Create LVM Filesystem \u00b6 # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 2 Hex code ( type L to list all codes ) : 8e Changed type of partition 'Linux' to 'Linux LVM' . Command ( m for help ) : w The partition table has been altered. Syncing disks. # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 8e Linux LVM /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 5 Hex code ( type L to list all codes ) : 8e Changed type of partition 'Linux' to 'Linux LVM' . Command ( m for help ) : w The partition table has been altered. Syncing disks. # pvcreate /dev/xvdf2 Physical volume \"/dev/xvdf2\" successfully created. # pvcreate /dev/xvdf5 Physical volume \"/dev/xvdf5\" successfully created. # vgcreate VG1 /dev/xvdf2 /dev/xvdf5 Volume group \"VG1\" successfully created root@ip-172-31-81-196:~# vgdisplay /dev/VG1 --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 3 .99 GiB PE Size 4 .00 MiB Total PE 1022 Alloc PE / Size 0 / 0 Free PE / Size 1022 / 3 .99 GiB VG UUID ELopyA-2vai-XuiQ-vm7M-RF9q-k0ve-yUeXM6 # lvcreate VG1 -L +3.9G -n LV1 Rounding up size to full physical extent 3 .90 GiB Logical volume \"LV1\" created. root@ip-172-31-81-196:~# lvdisplay --- Logical volume --- LV Path /dev/VG1/LV1 LV Name LV1 VG Name VG1 LV UUID Q0mXgw-q9E7-ncuH-Vv2t-NTqx-epn9-3EK3DH LV Write Access read/write LV Creation host, time ip-172-31-81-196, 2019 -08-02 21 :45:49 +0000 LV Status available # open 0 LV Size 3 .90 GiB Current LE 999 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253 :0 # mkfs.ext4 /dev/VG1/LV1 mke2fs 1 .44.1 ( 24 -Mar-2018 ) Creating filesystem with 1022976 4k blocks and 256000 inodes Filesystem UUID: 3cc6d97e-a315-4051-a572-16e06bf9c9f9 Superblock backups stored on blocks: 32768 , 98304 , 163840 , 229376 , 294912 , 819200 , 884736 Allocating group tables: done Writing inode tables: done Creating journal ( 16384 blocks ) : done Writing superblocks and filesystem accounting information: done root@ip-172-31-81-196:~# mkdir /mnt/LV1-mount root@ip-172-31-81-196:~# mount /dev/VG1/LV1 /mnt/LV1-mount root@ip-172-31-81-196:~# vi /etc/fstab /dev/VG1/LV1 /mnt/LV1-mount ext4 defaults 0 0 # Add This Line # Can edit first the /etc/fsatab file and after do mount -a (mount everything in /etc/fstab file) instead mount command Mount Shared Disk \u00b6 Install NFS yum install -y nfs-utils systemctl enable nfs systemctl start nfs Master \u00b6 Configure Export # vi /etc/exports /snapshots * ( rw ) = > Add Line # exportfs /snapshots <world> Enable Service firewall-cmd --add-service nfs --permanent firewall-cmd --permanent --add-service = rpc-bind firewall-cmd --permanent --add-service = mountd firewall-cmd --permanent --add-port = 2049 /tcp firewall-cmd --permanent --add-port = 2049 /udp firewall-cmd --reload Slaves \u00b6 # vi /etc/fstab 10 .240.100.18:/snapshots /elastic/snapshots nfs _netdev,rw 0 0 # mount -a # mount | grep elastic Disk Quotas \u00b6 ```bash vi /etc/fstab \u00b6 LABEL=projectA /mnt/ProjectA/ ext4 defaults,usrquota 0 0 mount -a \u00b6 apt-get install quota \u00b6 cd /mnt/ProjectA \u00b6 quotacheck -avugc # Create filesystem to support quotas \u00b6 quotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. quotacheck: Scanning /dev/xvdf7 [/mnt/ProjectA] done quotacheck: Cannot stat old user quota file /mnt/ProjectA/aquota.user: No such file or directory. Usage will not be subtracted. quotacheck: Old group file name could not been determined. Usage will not be subtracted. quotacheck: Checked 3 directories and 0 files quotacheck: Old file not found. edquota -u fsantos # To edit quotas to user \u00b6 quotaon /mnt/Photos # Enable quotas \u00b6 quota -v # To see quotas usage \u00b6 sudo repquota /mnt/Photos # To see a resume from users \u00b6","title":"Disks"},{"location":"platforms/unix/disks/#disks","text":"","title":"Disks"},{"location":"platforms/unix/disks/#mount-disk","text":"List Devices fdisk -l Create Partition # fdisk /dev/sdb Command ( m for help ) : n = > Create Partition Partition type: p primary ( 0 primary, 0 extended, 4 free ) e extended Command ( m for help ) : p = > Get partition Command ( m for help ) : w = > Write Format and Create Label # mkfs.xfs /dev/sdb1 -L elastic Mount # vi /etc/fstab LABEL = elastic /elastic xfs defaults 0 0 ## Add this line # mount -a","title":"Mount Disk"},{"location":"platforms/unix/disks/#create-ext4-filesystem","text":"Can use parted command # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 :0 0 88 .5M 1 loop /snap/core/7270 loop1 7 :1 0 18M 1 loop /snap/amazon-ssm-agent/1455 xvda 202 :0 0 8G 0 disk \u2514\u2500xvda1 202 :1 0 8G 0 part / xvdf 202 :80 0 8G 0 disk # fdisk /dev/xvdf Command ( m for help ) : n = > Create Partition Partition type: p primary ( 0 primary, 0 extended, 4 free ) e extended Command ( m for help ) : p = > Print partition Command ( m for help ) : w = > Write # mkfs -L projectA -t ext4 /dev/xvdf1 # Format Partition and create label # mkdir /mnt/ProjectA # mount /dev/xvdf1 /mnt/ProjectA/ # vi /etc/fstab LABEL = projectA /mnt/ProjectA/ ext4 defaults 0 0 # Add This Line umount /mnt/ProjectA/","title":"Create ext4 Filesystem"},{"location":"platforms/unix/disks/#create-swap-partition","text":"Can use parted command # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 1 Hex code ( type L to list all codes ) : 82 Command ( m for help ) : w # mkswap /dev/xvdf1 # swapon /dev/xvdf1 # to shutoff -> swapoff /dev/xvdf2 # free -m # vi /etc/fstab UUID = 99fd52d5-e821-45a5-9366-30666046406f swap swap default 0 0","title":"Create Swap Partition"},{"location":"platforms/unix/disks/#create-lvm-filesystem","text":"# fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 83 Linux /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 2 Hex code ( type L to list all codes ) : 8e Changed type of partition 'Linux' to 'Linux LVM' . Command ( m for help ) : w The partition table has been altered. Syncing disks. # fdisk /dev/xvdf Welcome to fdisk ( util-linux 2 .31.1 ) . Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command ( m for help ) : p Disk /dev/xvdf: 8 GiB, 8589934592 bytes, 16777216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size ( logical/physical ) : 512 bytes / 512 bytes I/O size ( minimum/optimal ) : 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xaef5270b Device Boot Start End Sectors Size Id Type /dev/xvdf1 2048 4196351 4194304 2G 82 Linux swap / Solaris /dev/xvdf2 4196352 10487807 6291456 3G 8e Linux LVM /dev/xvdf3 10487808 16777215 6289408 3G 5 Extended /dev/xvdf5 10489856 12587007 2097152 1G 83 Linux /dev/xvdf6 12589056 14686207 2097152 1G 83 Linux /dev/xvdf7 14688256 16777215 2088960 1020M 83 Linux Command ( m for help ) : t Partition number ( 1 -3,5-7, default 7 ) : 5 Hex code ( type L to list all codes ) : 8e Changed type of partition 'Linux' to 'Linux LVM' . Command ( m for help ) : w The partition table has been altered. Syncing disks. # pvcreate /dev/xvdf2 Physical volume \"/dev/xvdf2\" successfully created. # pvcreate /dev/xvdf5 Physical volume \"/dev/xvdf5\" successfully created. # vgcreate VG1 /dev/xvdf2 /dev/xvdf5 Volume group \"VG1\" successfully created root@ip-172-31-81-196:~# vgdisplay /dev/VG1 --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 3 .99 GiB PE Size 4 .00 MiB Total PE 1022 Alloc PE / Size 0 / 0 Free PE / Size 1022 / 3 .99 GiB VG UUID ELopyA-2vai-XuiQ-vm7M-RF9q-k0ve-yUeXM6 # lvcreate VG1 -L +3.9G -n LV1 Rounding up size to full physical extent 3 .90 GiB Logical volume \"LV1\" created. root@ip-172-31-81-196:~# lvdisplay --- Logical volume --- LV Path /dev/VG1/LV1 LV Name LV1 VG Name VG1 LV UUID Q0mXgw-q9E7-ncuH-Vv2t-NTqx-epn9-3EK3DH LV Write Access read/write LV Creation host, time ip-172-31-81-196, 2019 -08-02 21 :45:49 +0000 LV Status available # open 0 LV Size 3 .90 GiB Current LE 999 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253 :0 # mkfs.ext4 /dev/VG1/LV1 mke2fs 1 .44.1 ( 24 -Mar-2018 ) Creating filesystem with 1022976 4k blocks and 256000 inodes Filesystem UUID: 3cc6d97e-a315-4051-a572-16e06bf9c9f9 Superblock backups stored on blocks: 32768 , 98304 , 163840 , 229376 , 294912 , 819200 , 884736 Allocating group tables: done Writing inode tables: done Creating journal ( 16384 blocks ) : done Writing superblocks and filesystem accounting information: done root@ip-172-31-81-196:~# mkdir /mnt/LV1-mount root@ip-172-31-81-196:~# mount /dev/VG1/LV1 /mnt/LV1-mount root@ip-172-31-81-196:~# vi /etc/fstab /dev/VG1/LV1 /mnt/LV1-mount ext4 defaults 0 0 # Add This Line # Can edit first the /etc/fsatab file and after do mount -a (mount everything in /etc/fstab file) instead mount command","title":"Create LVM Filesystem"},{"location":"platforms/unix/disks/#mount-shared-disk","text":"Install NFS yum install -y nfs-utils systemctl enable nfs systemctl start nfs","title":"Mount Shared Disk"},{"location":"platforms/unix/disks/#master","text":"Configure Export # vi /etc/exports /snapshots * ( rw ) = > Add Line # exportfs /snapshots <world> Enable Service firewall-cmd --add-service nfs --permanent firewall-cmd --permanent --add-service = rpc-bind firewall-cmd --permanent --add-service = mountd firewall-cmd --permanent --add-port = 2049 /tcp firewall-cmd --permanent --add-port = 2049 /udp firewall-cmd --reload","title":"Master"},{"location":"platforms/unix/disks/#slaves","text":"# vi /etc/fstab 10 .240.100.18:/snapshots /elastic/snapshots nfs _netdev,rw 0 0 # mount -a # mount | grep elastic","title":"Slaves"},{"location":"platforms/unix/disks/#disk-quotas","text":"```bash","title":"Disk Quotas"},{"location":"platforms/unix/disks/#vi-etcfstab","text":"LABEL=projectA /mnt/ProjectA/ ext4 defaults,usrquota 0 0","title":"vi /etc/fstab"},{"location":"platforms/unix/disks/#mount-a","text":"","title":"mount -a"},{"location":"platforms/unix/disks/#apt-get-install-quota","text":"","title":"apt-get install quota"},{"location":"platforms/unix/disks/#cd-mntprojecta","text":"","title":"cd /mnt/ProjectA"},{"location":"platforms/unix/disks/#quotacheck-avugc-create-filesystem-to-support-quotas","text":"quotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. quotacheck: Scanning /dev/xvdf7 [/mnt/ProjectA] done quotacheck: Cannot stat old user quota file /mnt/ProjectA/aquota.user: No such file or directory. Usage will not be subtracted. quotacheck: Old group file name could not been determined. Usage will not be subtracted. quotacheck: Checked 3 directories and 0 files quotacheck: Old file not found.","title":"quotacheck -avugc # Create filesystem to support quotas"},{"location":"platforms/unix/disks/#edquota-u-fsantos-to-edit-quotas-to-user","text":"","title":"edquota -u fsantos # To edit quotas to user"},{"location":"platforms/unix/disks/#quotaon-mntphotos-enable-quotas","text":"","title":"quotaon /mnt/Photos # Enable quotas"},{"location":"platforms/unix/disks/#quota-v-to-see-quotas-usage","text":"","title":"quota -v # To see quotas usage"},{"location":"platforms/unix/disks/#sudo-repquota-mntphotos-to-see-a-resume-from-users","text":"","title":"sudo repquota /mnt/Photos # To see a resume from users"},{"location":"platforms/unix/networking/","text":"Networking \u00b6 Configuration \u00b6 ifconfig \u00b6 ip addr ip addr show ifconfig ifconfig -a ifdown enp0s3 ifup enp0s3 ifconfig enp0s3 192 .168.0.150/24 # Not Permanent Add Secondary IP \u00b6 Link ifconfig eth0:1 172 .31.81.196 netmask 255 .255.240.0 up # Not Permanent Remove Secondary Ip \u00b6 ip addr del 172 .31.81.196/20 dev eth0:1 # Not Permanent Network-Manager \u00b6 sudo apt install network-manager sudo service network-manager restart nmcli nmcli connection # Connection information # Add Static Route sudo nmcli connection add con-name STATIC ipv4.addresses 192 .168.58.1/24 ifname eth0 type ethernet nmcli connection show STATIC sudo nmcli connection modify STATIC +ipv4.routes \"172.16.0.0/16 192.168.58.254\" ipv4.dns 172 .16.58.254 Routes \u00b6 route route -n route add default gw 192 .168.0.254 Dns \u00b6 cat /etc/hosts cat /etc/nsswitch.conf # show how priority to will resolve dns cat /etc/resolv.conf cat /etc/dhcp/dhclient.conf sudo service networking restart File \u00b6 cat /etc/network/interfaces IP Tables \u00b6 Nat sudo iptables -t nat -L | grep 100 .66.20.131 sudo iptables -t nat -L | grep nginx-service Troubleshooting \u00b6 Netstat \u00b6 $ sudo netstat -nl -p tcp | grep 8123 $ sudo netstat -nl -p tcp | head Get open ports \u00b6 nmap localhost netstat -at SS \u00b6 ss -an | grep -i listen sudo ss -ntlp # To get process Who listen on Port \u00b6 sudo lsof -i :8000 Test Remote Connection to port \u00b6 nc -v 10 .240.100.18 2049 netcat -l 12345 # To listen a Port Get Ports than services are listen \u00b6 cat /etc/services MTR \u00b6 mtr acloud.com DNS \u00b6 host google.com dig google.com TCP Dump \u00b6 sudo tcpdump -v -l -i any 'host 100.65.17.226' tcpdump -n port 6443 tcpdump port 443 and '(tcp-syn|tcp-ack)!=0' VPN \u00b6 Permanent VPN Connection \u00b6 Article","title":"Networking"},{"location":"platforms/unix/networking/#networking","text":"","title":"Networking"},{"location":"platforms/unix/networking/#configuration","text":"","title":"Configuration"},{"location":"platforms/unix/networking/#ifconfig","text":"ip addr ip addr show ifconfig ifconfig -a ifdown enp0s3 ifup enp0s3 ifconfig enp0s3 192 .168.0.150/24 # Not Permanent","title":"ifconfig"},{"location":"platforms/unix/networking/#add-secondary-ip","text":"Link ifconfig eth0:1 172 .31.81.196 netmask 255 .255.240.0 up # Not Permanent","title":"Add Secondary IP"},{"location":"platforms/unix/networking/#remove-secondary-ip","text":"ip addr del 172 .31.81.196/20 dev eth0:1 # Not Permanent","title":"Remove Secondary Ip"},{"location":"platforms/unix/networking/#network-manager","text":"sudo apt install network-manager sudo service network-manager restart nmcli nmcli connection # Connection information # Add Static Route sudo nmcli connection add con-name STATIC ipv4.addresses 192 .168.58.1/24 ifname eth0 type ethernet nmcli connection show STATIC sudo nmcli connection modify STATIC +ipv4.routes \"172.16.0.0/16 192.168.58.254\" ipv4.dns 172 .16.58.254","title":"Network-Manager"},{"location":"platforms/unix/networking/#routes","text":"route route -n route add default gw 192 .168.0.254","title":"Routes"},{"location":"platforms/unix/networking/#dns","text":"cat /etc/hosts cat /etc/nsswitch.conf # show how priority to will resolve dns cat /etc/resolv.conf cat /etc/dhcp/dhclient.conf sudo service networking restart","title":"Dns"},{"location":"platforms/unix/networking/#file","text":"cat /etc/network/interfaces","title":"File"},{"location":"platforms/unix/networking/#ip-tables","text":"Nat sudo iptables -t nat -L | grep 100 .66.20.131 sudo iptables -t nat -L | grep nginx-service","title":"IP Tables"},{"location":"platforms/unix/networking/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"platforms/unix/networking/#netstat","text":"$ sudo netstat -nl -p tcp | grep 8123 $ sudo netstat -nl -p tcp | head","title":"Netstat"},{"location":"platforms/unix/networking/#get-open-ports","text":"nmap localhost netstat -at","title":"Get open ports"},{"location":"platforms/unix/networking/#ss","text":"ss -an | grep -i listen sudo ss -ntlp # To get process","title":"SS"},{"location":"platforms/unix/networking/#who-listen-on-port","text":"sudo lsof -i :8000","title":"Who listen on Port"},{"location":"platforms/unix/networking/#test-remote-connection-to-port","text":"nc -v 10 .240.100.18 2049 netcat -l 12345 # To listen a Port","title":"Test Remote Connection to port"},{"location":"platforms/unix/networking/#get-ports-than-services-are-listen","text":"cat /etc/services","title":"Get Ports than services are listen"},{"location":"platforms/unix/networking/#mtr","text":"mtr acloud.com","title":"MTR"},{"location":"platforms/unix/networking/#dns_1","text":"host google.com dig google.com","title":"DNS"},{"location":"platforms/unix/networking/#tcp-dump","text":"sudo tcpdump -v -l -i any 'host 100.65.17.226' tcpdump -n port 6443 tcpdump port 443 and '(tcp-syn|tcp-ack)!=0'","title":"TCP Dump"},{"location":"platforms/unix/networking/#vpn","text":"","title":"VPN"},{"location":"platforms/unix/networking/#permanent-vpn-connection","text":"Article","title":"Permanent VPN Connection"},{"location":"platforms/unix/redhat/","text":"Redhat \u00b6 Recover Root Password \u00b6 Na console send ctrl-alt-del press \"e\" = > TO Edit Na linha linux16 -- No final da linha adicionar rd.break press \"ctrl-x\" mount -o remount,rw /sysroot cd /sysroot chroot . touch .autorelabel passwd teste123ibm4 ctrl d ctrl d SSH \u00b6 Permit SSH Root Login (CentOs) vi /etc/ssh/sshd_config # Comment PasswordAuthentication no # Descomment PasswordAuthentication yes systemctl restart sshd Configure Network Interface (CentOs) \u00b6 vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR = 172 .16.1.180 GATEWAY = 172 .16.1.16 DNS1 = 172 .16.1.112 DNS2 = 172 .16.1.119 systemctl restart network Package Manager \u00b6 RPM \u00b6 sudo rpm -i wget-xxxxxx.rpm # To see which lib file belongs rpm -qf /etc/protocols # To see if some file is missing from lib sudo rpm --verify setup sudo rpm -Va # To verify entire machine YUM \u00b6 # Config file cat /etc/yum.conf # Repos dir cd /etc/yum.repos.d sudo yum update # To download but not install sudo yum install --downloadonly --downloaddir = /tmp wget sudo yum remove wget","title":"Redhat"},{"location":"platforms/unix/redhat/#redhat","text":"","title":"Redhat"},{"location":"platforms/unix/redhat/#recover-root-password","text":"Na console send ctrl-alt-del press \"e\" = > TO Edit Na linha linux16 -- No final da linha adicionar rd.break press \"ctrl-x\" mount -o remount,rw /sysroot cd /sysroot chroot . touch .autorelabel passwd teste123ibm4 ctrl d ctrl d","title":"Recover Root Password"},{"location":"platforms/unix/redhat/#ssh","text":"Permit SSH Root Login (CentOs) vi /etc/ssh/sshd_config # Comment PasswordAuthentication no # Descomment PasswordAuthentication yes systemctl restart sshd","title":"SSH"},{"location":"platforms/unix/redhat/#configure-network-interface-centos","text":"vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR = 172 .16.1.180 GATEWAY = 172 .16.1.16 DNS1 = 172 .16.1.112 DNS2 = 172 .16.1.119 systemctl restart network","title":"Configure Network Interface (CentOs)"},{"location":"platforms/unix/redhat/#package-manager","text":"","title":"Package Manager"},{"location":"platforms/unix/redhat/#rpm","text":"sudo rpm -i wget-xxxxxx.rpm # To see which lib file belongs rpm -qf /etc/protocols # To see if some file is missing from lib sudo rpm --verify setup sudo rpm -Va # To verify entire machine","title":"RPM"},{"location":"platforms/unix/redhat/#yum","text":"# Config file cat /etc/yum.conf # Repos dir cd /etc/yum.repos.d sudo yum update # To download but not install sudo yum install --downloadonly --downloaddir = /tmp wget sudo yum remove wget","title":"YUM"},{"location":"platforms/unix/security/","text":"Security \u00b6 FirewallD \u00b6 # Logs /var/log/secure # Get State sudo firewall-cmd --state # list available Zones sudo firewall-cmd --get-zones # List zones used by network interfaces sudo firewall-cmd --get-active-zones # Get Default Zone sudo firewall-cmd --get-default-zone # Get Zone configuration Settings sudo firewall-cmd --zone = public --list-all # Open Port sudo firewall-cmd --zone = public --add-port = 80 /tcp --permanent sudo firewall-cmd --reload # Close Port sudo firewall-cmd --zone = public --remove-port = 80 /tcp --permanent sudo firewall-cmd --reload # Firewall services -XMl service Files sudo cd /usr/lib/firewalld/services/ # Add Service sudo firewall-cmd --permanent --zone = public --add-service = myHttp sudo firewall-cmd --reload # Remove Services sudo firewall-cmd --permanent --zone = public --remove-service = myHttp sudo firewall-cmd --reload # Allow traffic from one IP to a port sudo firewall-cmd --permanent --zone-public --add-rich-rule = ' rule family =\"ipv4\" source address=\"<IP or Network>\" port protocol=\"<PROTOCOL>\" port=\">PORT_NUMBER>\" accept' # Allow traffic from a list of IPS (Whitelist) sudo firewall-cmd --permanent --zone = public --add-source = <IP or NETWORK> # Blacklist sudo firewall-cmd --permanent --new-ipset = blacklist --type = hash:ip # Create IpSet sudo firewall-cmd --get-ipsets sudo firewall-cmd --info-ipset = blacklist sudo firewall-cmd --ipset = blacklist --add-entry = 222 .186.15.114 # Add Entry IpSet sudo firewall-cmd --ipset = blacklist --get-entries sudo firewall-cmd --permanent --zone = drop --add-source = ipset:blacklist # Apply IpSet sudo firewall-cmd --permanent --ipset = <SET_NAME> --add-entries-from-file = <FILE_NAME> # Add Entries From File sudo firewall-cmd --permanent --ipset = <SET_NAME> --remove-entries-from-file = <FILE_NAME> # Remove Entries From file # Forward traffic sudo firewall-cmd --zone = <ZONE_NAME> --add-forward-port = port = <TO_PORT>:proto = tcp:toaddr = <IP> # LockDown - Restrict Apps change Firewalld sudo firewall-cmd --query-lockdown # Check if is in lockdown mode sudo vi /etc/firewalld/firewalld.conf # Apply Lock lockdown mode Lockdown = yes sudo systemctl restart firewalld # Panic - Restrict traffic (Yourself too) - Extreme, Need access via console to recover sudo firewall-cmd --query-panic sudo firewall-cmd --panic-on sudo firewall-cmd --panic-off Block Ping \u00b6 sudo vim /etc/sysctl.conf net.ipv4.icmp.echo_ignore_all = 1 # Apply changes sudo sysctl -p SE Linux \u00b6 States \u00b6 sudo getenforce # Get current state sudo sestatus sudo setenforce 0 # Run-Time state change to permissive sudo setenforce 1 # Run-Time state change to Enforcing Vi /etc/selinux/config # Permanently Disable SELINUUX = disabled # Reboot the system Contexts \u00b6 Operations Move - Moved files will retain their current context Copy - Copied files might not retain their current content Create - Created files will inherit the context for the location where they are created Context system_u:object_r:httpd_sys_content_t system_u = User Context system_u - This is for system users user_u - This is for your average user that logs into the Linux machine root - This will limit to root-user-only access object_r = role context object_r - Generally used for process and domains for files httpd_sys_content_t = Type Content This Allows us to have an easy way of fine tuning control ls -Z # To see files and the contexts ps -eZ # To see proccess and the contexts semanage fcontext -l # List SE Contexts # Change Context sudo semanage fcontext -a -t httpd_sys_content_t /var/www/html/index.html sudo restorecon -v /var/www/html/index.html # Remove Context sudo semanage fcontext -d /var/www/html/index.html sudo restorecon -v /var/www/html/index.html # Change Context Type -> Temporary chcon -t user_home_dir_t /etc/shadow # Checks if the default SElinux context is set matchpathconf -V <path to file> Booleans \u00b6 getsebool -a # List booleans semanage boolean -l | sort | less # Set boolean, temporary change setsebool <boolean> on/off # Set boolean, permanent change setsebool -P <boolean> on/off Ports \u00b6 The Rules will apply in the initial binding. if you change httpd port, you need restart # List Ports sudo semanage port -l | grep http # Add Ports or Protocol to label sudo semanage port -a -t http_port_t -p tcp 61613 sudo semanage port -m -t http_port_t -p tcp 61613 # If the port is already in use # Remove port or protocol sudo semanage port -d -t http_port_t -p tcp 61613 Domains \u00b6 # Get Domain ps -eZ | grep httpd # Configure a domain to run in permissive mode sudo semanage permissive -a httpd_t # Remove permissive mode on a domain sudo semanage permissive -d httpd_t # Disable permissive mode across all domains semodule -d permissivedomains Modules \u00b6 # List SELinux Modules sudo semodule -l # Generate policy module based on the log entries sudo grep test.html /var/log/audit/audit.log | audit2allow -M myModule # Load Module semodule -i <FILE NAME> # Disable module semodule -d <MODULE NAME> Logs \u00b6 #log File /var/log/audit/audit.log # Install Tools sudo yum install -y setroubleshoot setools # Analyze logs sudo grep httpd /var/log/audit/audit.log | audit2why sudo ausearch -m USER_LOGIN -sv no # Check for user activity sudo ausearch -ua <USERNAME> -ts yesterday -te now Host \u00b6 cat /etc/hosts.allow # Hosts allowed to access cat /etc/hosts.deny # Hosts not allowed to access Nologin \u00b6 touch /etc/nologin # Denies login to all users (except root). Need remove the file Troubleshooting \u00b6 grep http /var/log/audit/audit.log","title":"Security"},{"location":"platforms/unix/security/#security","text":"","title":"Security"},{"location":"platforms/unix/security/#firewalld","text":"# Logs /var/log/secure # Get State sudo firewall-cmd --state # list available Zones sudo firewall-cmd --get-zones # List zones used by network interfaces sudo firewall-cmd --get-active-zones # Get Default Zone sudo firewall-cmd --get-default-zone # Get Zone configuration Settings sudo firewall-cmd --zone = public --list-all # Open Port sudo firewall-cmd --zone = public --add-port = 80 /tcp --permanent sudo firewall-cmd --reload # Close Port sudo firewall-cmd --zone = public --remove-port = 80 /tcp --permanent sudo firewall-cmd --reload # Firewall services -XMl service Files sudo cd /usr/lib/firewalld/services/ # Add Service sudo firewall-cmd --permanent --zone = public --add-service = myHttp sudo firewall-cmd --reload # Remove Services sudo firewall-cmd --permanent --zone = public --remove-service = myHttp sudo firewall-cmd --reload # Allow traffic from one IP to a port sudo firewall-cmd --permanent --zone-public --add-rich-rule = ' rule family =\"ipv4\" source address=\"<IP or Network>\" port protocol=\"<PROTOCOL>\" port=\">PORT_NUMBER>\" accept' # Allow traffic from a list of IPS (Whitelist) sudo firewall-cmd --permanent --zone = public --add-source = <IP or NETWORK> # Blacklist sudo firewall-cmd --permanent --new-ipset = blacklist --type = hash:ip # Create IpSet sudo firewall-cmd --get-ipsets sudo firewall-cmd --info-ipset = blacklist sudo firewall-cmd --ipset = blacklist --add-entry = 222 .186.15.114 # Add Entry IpSet sudo firewall-cmd --ipset = blacklist --get-entries sudo firewall-cmd --permanent --zone = drop --add-source = ipset:blacklist # Apply IpSet sudo firewall-cmd --permanent --ipset = <SET_NAME> --add-entries-from-file = <FILE_NAME> # Add Entries From File sudo firewall-cmd --permanent --ipset = <SET_NAME> --remove-entries-from-file = <FILE_NAME> # Remove Entries From file # Forward traffic sudo firewall-cmd --zone = <ZONE_NAME> --add-forward-port = port = <TO_PORT>:proto = tcp:toaddr = <IP> # LockDown - Restrict Apps change Firewalld sudo firewall-cmd --query-lockdown # Check if is in lockdown mode sudo vi /etc/firewalld/firewalld.conf # Apply Lock lockdown mode Lockdown = yes sudo systemctl restart firewalld # Panic - Restrict traffic (Yourself too) - Extreme, Need access via console to recover sudo firewall-cmd --query-panic sudo firewall-cmd --panic-on sudo firewall-cmd --panic-off","title":"FirewallD"},{"location":"platforms/unix/security/#block-ping","text":"sudo vim /etc/sysctl.conf net.ipv4.icmp.echo_ignore_all = 1 # Apply changes sudo sysctl -p","title":"Block Ping"},{"location":"platforms/unix/security/#se-linux","text":"","title":"SE Linux"},{"location":"platforms/unix/security/#states","text":"sudo getenforce # Get current state sudo sestatus sudo setenforce 0 # Run-Time state change to permissive sudo setenforce 1 # Run-Time state change to Enforcing Vi /etc/selinux/config # Permanently Disable SELINUUX = disabled # Reboot the system","title":"States"},{"location":"platforms/unix/security/#contexts","text":"Operations Move - Moved files will retain their current context Copy - Copied files might not retain their current content Create - Created files will inherit the context for the location where they are created Context system_u:object_r:httpd_sys_content_t system_u = User Context system_u - This is for system users user_u - This is for your average user that logs into the Linux machine root - This will limit to root-user-only access object_r = role context object_r - Generally used for process and domains for files httpd_sys_content_t = Type Content This Allows us to have an easy way of fine tuning control ls -Z # To see files and the contexts ps -eZ # To see proccess and the contexts semanage fcontext -l # List SE Contexts # Change Context sudo semanage fcontext -a -t httpd_sys_content_t /var/www/html/index.html sudo restorecon -v /var/www/html/index.html # Remove Context sudo semanage fcontext -d /var/www/html/index.html sudo restorecon -v /var/www/html/index.html # Change Context Type -> Temporary chcon -t user_home_dir_t /etc/shadow # Checks if the default SElinux context is set matchpathconf -V <path to file>","title":"Contexts"},{"location":"platforms/unix/security/#booleans","text":"getsebool -a # List booleans semanage boolean -l | sort | less # Set boolean, temporary change setsebool <boolean> on/off # Set boolean, permanent change setsebool -P <boolean> on/off","title":"Booleans"},{"location":"platforms/unix/security/#ports","text":"The Rules will apply in the initial binding. if you change httpd port, you need restart # List Ports sudo semanage port -l | grep http # Add Ports or Protocol to label sudo semanage port -a -t http_port_t -p tcp 61613 sudo semanage port -m -t http_port_t -p tcp 61613 # If the port is already in use # Remove port or protocol sudo semanage port -d -t http_port_t -p tcp 61613","title":"Ports"},{"location":"platforms/unix/security/#domains","text":"# Get Domain ps -eZ | grep httpd # Configure a domain to run in permissive mode sudo semanage permissive -a httpd_t # Remove permissive mode on a domain sudo semanage permissive -d httpd_t # Disable permissive mode across all domains semodule -d permissivedomains","title":"Domains"},{"location":"platforms/unix/security/#modules","text":"# List SELinux Modules sudo semodule -l # Generate policy module based on the log entries sudo grep test.html /var/log/audit/audit.log | audit2allow -M myModule # Load Module semodule -i <FILE NAME> # Disable module semodule -d <MODULE NAME>","title":"Modules"},{"location":"platforms/unix/security/#logs","text":"#log File /var/log/audit/audit.log # Install Tools sudo yum install -y setroubleshoot setools # Analyze logs sudo grep httpd /var/log/audit/audit.log | audit2why sudo ausearch -m USER_LOGIN -sv no # Check for user activity sudo ausearch -ua <USERNAME> -ts yesterday -te now","title":"Logs"},{"location":"platforms/unix/security/#host","text":"cat /etc/hosts.allow # Hosts allowed to access cat /etc/hosts.deny # Hosts not allowed to access","title":"Host"},{"location":"platforms/unix/security/#nologin","text":"touch /etc/nologin # Denies login to all users (except root). Need remove the file","title":"Nologin"},{"location":"platforms/unix/security/#troubleshooting","text":"grep http /var/log/audit/audit.log","title":"Troubleshooting"},{"location":"platforms/unix/virsh/","text":"Snapshot \u00b6 $ ssh -l rgameiro 10 .242.12.23 $ virsh list --all Id Name State ---------------------------------------------------- 1 bm02vsc01 running 2 bm02alinternal01 running 3 architect running 4 bm02ifwinternal01 running 5 bm02vsd01 running 6 bm02sfinternal02 running 7 bm02acinternal01 running 8 bm02dirinternal01 running - bm02nes01 shut off $ virsh destroy bm02sfinternal02 $ virsh dumpxml bm02sfinternal02 # Find Disk File <domain type = 'kvm' > <name>bm02sfinternal02</name> <uuid>af7d246d-4578-4660-96fa-ac40c87c24eb</uuid> <memory unit = 'KiB' >2097152</memory> <currentMemory unit = 'KiB' >2097152</currentMemory> <vcpu placement = 'static' >4</vcpu> <os> < type arch = 'x86_64' machine = 'pc-i440fx-rhel7.3.0' >hvm</type> <boot dev = 'hd' /> </os> <features> <acpi/> <apic/> <vmport state = 'off' /> </features> <cpu mode = 'host-model' check = 'partial' > <model fallback = 'allow' /> </cpu> <clock offset = 'utc' > <timer name = 'rtc' tickpolicy = 'catchup' /> <timer name = 'pit' tickpolicy = 'delay' /> <timer name = 'hpet' present = 'no' /> </clock> <on_poweroff>destroy</on_poweroff> <on_reboot>restart</on_reboot> <on_crash>restart</on_crash> <pm> <suspend-to-mem enabled = 'no' /> <suspend-to-disk enabled = 'no' /> </pm> <devices> <emulator>/usr/libexec/qemu-kvm</emulator> <disk type = 'file' device = 'disk' > <driver name = 'qemu' type = 'qcow2' cache = 'none' io = 'native' /> < source file = '/var/lib/libvirt/images/bm02sfinternal02.qcow2' /> <target dev = 'vda' bus = 'virtio' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x06' function = '0x0' /> </disk> <disk type = 'file' device = 'cdrom' > <driver name = 'qemu' type = 'raw' /> <target dev = 'hda' bus = 'ide' /> <readonly/> <address type = 'drive' controller = '0' bus = '0' target = '0' unit = '0' /> </disk> <controller type = 'usb' index = '0' model = 'ich9-ehci1' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x7' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci1' > <master startport = '0' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x0' multifunction = 'on' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci2' > <master startport = '2' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x1' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci3' > <master startport = '4' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x2' /> </controller> <controller type = 'pci' index = '0' model = 'pci-root' /> <controller type = 'ide' index = '0' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x01' function = '0x1' /> </controller> <controller type = 'virtio-serial' index = '0' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x04' function = '0x0' /> </controller> <interface type = 'bridge' > <mac address = '52:54:00:84:7e:d0' /> < source bridge = 'br100' /> <model type = 'virtio' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x03' function = '0x0' /> </interface> <serial type = 'pty' > <target type = 'isa-serial' port = '0' > <model name = 'isa-serial' /> </target> </serial> <console type = 'pty' > <target type = 'serial' port = '0' /> </console> <channel type = 'unix' > <target type = 'virtio' name = 'org.qemu.guest_agent.0' /> <address type = 'virtio-serial' controller = '0' bus = '0' port = '1' /> </channel> <channel type = 'spicevmc' > <target type = 'virtio' name = 'com.redhat.spice.0' /> <address type = 'virtio-serial' controller = '0' bus = '0' port = '2' /> </channel> <input type = 'tablet' bus = 'usb' > <address type = 'usb' bus = '0' port = '1' /> </input> <input type = 'mouse' bus = 'ps2' /> <input type = 'keyboard' bus = 'ps2' /> <graphics type = 'spice' autoport = 'yes' > <listen type = 'address' /> </graphics> <video> <model type = 'qxl' ram = '65536' vram = '65536' vgamem = '16384' heads = '1' primary = 'yes' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x02' function = '0x0' /> </video> <redirdev bus = 'usb' type = 'spicevmc' > <address type = 'usb' bus = '0' port = '2' /> </redirdev> <redirdev bus = 'usb' type = 'spicevmc' > <address type = 'usb' bus = '0' port = '3' /> </redirdev> <memballoon model = 'virtio' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x07' function = '0x0' /> </memballoon> </devices> </domain> $ ls -la /var/lib/libvirt/images/bm02sfinternal02.qcow2 $ df -B 1g # Ensure we have space to copy disk $ cp /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ bg # Por o Processo em Brackground $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 rw------- 1 root root 17607884800 Oct 30 08 :21 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 # Terminou [ 1 ] + Done cp -i /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02* $ virsh start bm02sfinternal02","title":"Virsh"},{"location":"platforms/unix/virsh/#snapshot","text":"$ ssh -l rgameiro 10 .242.12.23 $ virsh list --all Id Name State ---------------------------------------------------- 1 bm02vsc01 running 2 bm02alinternal01 running 3 architect running 4 bm02ifwinternal01 running 5 bm02vsd01 running 6 bm02sfinternal02 running 7 bm02acinternal01 running 8 bm02dirinternal01 running - bm02nes01 shut off $ virsh destroy bm02sfinternal02 $ virsh dumpxml bm02sfinternal02 # Find Disk File <domain type = 'kvm' > <name>bm02sfinternal02</name> <uuid>af7d246d-4578-4660-96fa-ac40c87c24eb</uuid> <memory unit = 'KiB' >2097152</memory> <currentMemory unit = 'KiB' >2097152</currentMemory> <vcpu placement = 'static' >4</vcpu> <os> < type arch = 'x86_64' machine = 'pc-i440fx-rhel7.3.0' >hvm</type> <boot dev = 'hd' /> </os> <features> <acpi/> <apic/> <vmport state = 'off' /> </features> <cpu mode = 'host-model' check = 'partial' > <model fallback = 'allow' /> </cpu> <clock offset = 'utc' > <timer name = 'rtc' tickpolicy = 'catchup' /> <timer name = 'pit' tickpolicy = 'delay' /> <timer name = 'hpet' present = 'no' /> </clock> <on_poweroff>destroy</on_poweroff> <on_reboot>restart</on_reboot> <on_crash>restart</on_crash> <pm> <suspend-to-mem enabled = 'no' /> <suspend-to-disk enabled = 'no' /> </pm> <devices> <emulator>/usr/libexec/qemu-kvm</emulator> <disk type = 'file' device = 'disk' > <driver name = 'qemu' type = 'qcow2' cache = 'none' io = 'native' /> < source file = '/var/lib/libvirt/images/bm02sfinternal02.qcow2' /> <target dev = 'vda' bus = 'virtio' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x06' function = '0x0' /> </disk> <disk type = 'file' device = 'cdrom' > <driver name = 'qemu' type = 'raw' /> <target dev = 'hda' bus = 'ide' /> <readonly/> <address type = 'drive' controller = '0' bus = '0' target = '0' unit = '0' /> </disk> <controller type = 'usb' index = '0' model = 'ich9-ehci1' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x7' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci1' > <master startport = '0' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x0' multifunction = 'on' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci2' > <master startport = '2' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x1' /> </controller> <controller type = 'usb' index = '0' model = 'ich9-uhci3' > <master startport = '4' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x05' function = '0x2' /> </controller> <controller type = 'pci' index = '0' model = 'pci-root' /> <controller type = 'ide' index = '0' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x01' function = '0x1' /> </controller> <controller type = 'virtio-serial' index = '0' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x04' function = '0x0' /> </controller> <interface type = 'bridge' > <mac address = '52:54:00:84:7e:d0' /> < source bridge = 'br100' /> <model type = 'virtio' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x03' function = '0x0' /> </interface> <serial type = 'pty' > <target type = 'isa-serial' port = '0' > <model name = 'isa-serial' /> </target> </serial> <console type = 'pty' > <target type = 'serial' port = '0' /> </console> <channel type = 'unix' > <target type = 'virtio' name = 'org.qemu.guest_agent.0' /> <address type = 'virtio-serial' controller = '0' bus = '0' port = '1' /> </channel> <channel type = 'spicevmc' > <target type = 'virtio' name = 'com.redhat.spice.0' /> <address type = 'virtio-serial' controller = '0' bus = '0' port = '2' /> </channel> <input type = 'tablet' bus = 'usb' > <address type = 'usb' bus = '0' port = '1' /> </input> <input type = 'mouse' bus = 'ps2' /> <input type = 'keyboard' bus = 'ps2' /> <graphics type = 'spice' autoport = 'yes' > <listen type = 'address' /> </graphics> <video> <model type = 'qxl' ram = '65536' vram = '65536' vgamem = '16384' heads = '1' primary = 'yes' /> <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x02' function = '0x0' /> </video> <redirdev bus = 'usb' type = 'spicevmc' > <address type = 'usb' bus = '0' port = '2' /> </redirdev> <redirdev bus = 'usb' type = 'spicevmc' > <address type = 'usb' bus = '0' port = '3' /> </redirdev> <memballoon model = 'virtio' > <address type = 'pci' domain = '0x0000' bus = '0x00' slot = '0x07' function = '0x0' /> </memballoon> </devices> </domain> $ ls -la /var/lib/libvirt/images/bm02sfinternal02.qcow2 $ df -B 1g # Ensure we have space to copy disk $ cp /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ bg # Por o Processo em Brackground $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 rw------- 1 root root 17607884800 Oct 30 08 :21 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 # Terminou [ 1 ] + Done cp -i /var/lib/libvirt/images/bm02sfinternal02.qcow2 /var/lib/libvirt/images/bm02sfinternal02-pre-php-upgrade.qcow2 $ ls -la /var/lib/libvirt/images/bm02sfinternal02* $ virsh start bm02sfinternal02","title":"Snapshot"}]}